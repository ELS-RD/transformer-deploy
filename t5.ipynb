{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Callable, Dict, Set\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from onnx import ModelProto\n",
    "from tensorrt import ICudaEngine\n",
    "from tensorrt.tensorrt import Logger, Runtime\n",
    "from torch.nn import Linear\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PretrainedConfig, T5ForConditionalGeneration, TensorType\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Stack\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx\n",
    "from transformer_deploy.backends.trt_utils import (\n",
    "    TensorRTShape,\n",
    "    add_output_nodes,\n",
    "    build_engine,\n",
    "    get_adjency_dict,\n",
    "    get_fix_fp16_network_func,\n",
    "    get_list_fp32_nodes,\n",
    "    load_engine,\n",
    "    save_engine,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids: torch.Tensor = tokenizer(\"Studies show that\", return_tensors=TensorType.PYTORCH).input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "out_full: Seq2SeqLMOutput = model(input_ids=input_ids, decoder_input_ids=input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model.encoder,\n",
    "    output_path=\"test-enc.onnx\",\n",
    "    inputs_pytorch={\"input_ids\": input_ids},\n",
    "    var_output_seq=True,\n",
    "    quantization=False,\n",
    ")\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-enc.onnx\", onnx_optim_model_path=\"test-enc-opt.onnx\", architecture=\"bert\", use_cuda=True, fp16=True\n",
    ")\n",
    "\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "enc_onnx_out = inference_onnx_binding(\n",
    "    model_onnx=enc_onnx,\n",
    "    inputs={\"input_ids\": input_ids},\n",
    "    device=input_ids.device.type,\n",
    "    output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    ")[\"output\"]\n",
    "assert np.allclose(enc_onnx_out.detach().cpu().numpy(), out_enc.last_hidden_state.detach().cpu().numpy(), atol=1e-2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ExportT5(torch.nn.Module):\n",
    "    def __init__(self, decoder: T5Stack, lm_head: Linear):\n",
    "        super(ExportT5, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, past_key_values: Tuple = None):\n",
    "        out_dec = self.decoder.forward(\n",
    "            input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values\n",
    "        )\n",
    "        # Rescale output before projecting on vocab\n",
    "        out_dec[\"last_hidden_state\"] = out_dec[\"last_hidden_state\"] * (model.model_dim**-0.5)\n",
    "        # out_lm = self.lm_head(out_dec)\n",
    "        out_dec[\"last_hidden_state\"] = self.lm_head(out_dec[\"last_hidden_state\"])\n",
    "        return out_dec\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model_decoder = ExportT5(decoder=model.decoder, lm_head=model.lm_head).eval()\n",
    "out_model_export: torch.Tensor = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "assert np.allclose(\n",
    "    out_model_export[\"last_hidden_state\"].detach().cpu().numpy(), out_full.logits.detach().cpu().numpy(), atol=1e-5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:529: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "model_decoder.cuda()\n",
    "# decoder output one step before\n",
    "out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_ids\": input_ids[:, -1:].type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "    \"past_key_values\": out_dec_pytorch.past_key_values,\n",
    "}\n",
    "\n",
    "input_names = [\n",
    "    \"input_ids\",\n",
    "    \"encoder_hidden_states\",\n",
    "    \"past_key_values.0.decoder.key\",\n",
    "    \"past_key_values.0.decoder.value\",\n",
    "    \"past_key_values.0.encoder.key\",\n",
    "    \"past_key_values.0.encoder.value\",\n",
    "    \"past_key_values.1.decoder.key\",\n",
    "    \"past_key_values.1.decoder.value\",\n",
    "    \"past_key_values.1.encoder.key\",\n",
    "    \"past_key_values.1.encoder.value\",\n",
    "    \"past_key_values.2.decoder.key\",\n",
    "    \"past_key_values.2.decoder.value\",\n",
    "    \"past_key_values.2.encoder.key\",\n",
    "    \"past_key_values.2.encoder.value\",\n",
    "    \"past_key_values.3.decoder.key\",\n",
    "    \"past_key_values.3.decoder.value\",\n",
    "    \"past_key_values.3.encoder.key\",\n",
    "    \"past_key_values.3.encoder.value\",\n",
    "    \"past_key_values.4.decoder.key\",\n",
    "    \"past_key_values.4.decoder.value\",\n",
    "    \"past_key_values.4.encoder.key\",\n",
    "    \"past_key_values.4.encoder.value\",\n",
    "    \"past_key_values.5.decoder.key\",\n",
    "    \"past_key_values.5.decoder.value\",\n",
    "    \"past_key_values.5.encoder.key\",\n",
    "    \"past_key_values.5.encoder.value\",\n",
    "]\n",
    "\n",
    "output_names = [\n",
    "    \"logits\",\n",
    "    \"present.0.decoder.key\",\n",
    "    \"present.0.decoder.value\",\n",
    "    \"present.0.encoder.key\",\n",
    "    \"present.0.encoder.value\",\n",
    "    \"present.1.decoder.key\",\n",
    "    \"present.1.decoder.value\",\n",
    "    \"present.1.encoder.key\",\n",
    "    \"present.1.encoder.value\",\n",
    "    \"present.2.decoder.key\",\n",
    "    \"present.2.decoder.value\",\n",
    "    \"present.2.encoder.key\",\n",
    "    \"present.2.encoder.value\",\n",
    "    \"present.3.decoder.key\",\n",
    "    \"present.3.decoder.value\",\n",
    "    \"present.3.encoder.key\",\n",
    "    \"present.3.encoder.value\",\n",
    "    \"present.4.decoder.key\",\n",
    "    \"present.4.decoder.value\",\n",
    "    \"present.4.encoder.key\",\n",
    "    \"present.4.encoder.value\",\n",
    "    \"present.5.decoder.key\",\n",
    "    \"present.5.decoder.value\",\n",
    "    \"present.5.encoder.key\",\n",
    "    \"present.5.encoder.value\",\n",
    "]\n",
    "\n",
    "dynamic_axis = {\n",
    "    \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
    "    \"encoder_hidden_states\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"logits\": {0: \"batch\", 1: \"decoder_sequence\"},\n",
    "    \"present.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model_decoder,\n",
    "        (model_inputs,),\n",
    "        f=\"test-dec-cache.onnx\",\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axis,\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=13,\n",
    "    )\n",
    "\n",
    "model_inputs_no_cache = {\n",
    "    \"input_ids\": input_ids.type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model_decoder,\n",
    "        (model_inputs_no_cache,),\n",
    "        f=\"test-dec-no-cache.onnx\",\n",
    "        input_names=list(model_inputs_no_cache.keys()),\n",
    "        output_names=output_names,\n",
    "        dynamic_axes={k: v for k, v in dynamic_axis.items() if \"past_key_values\" not in k},\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=13,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: cache_node_1260 - size: 0.01\n",
      "name: cache_node_1261 - size: 0.01\n",
      "name: cache_node_1271 - size: 0.01\n",
      "name: cache_node_1272 - size: 0.01\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx import GraphProto, ModelProto, helper\n",
    "\n",
    "onnx_model_cache = onnx.load(\"test-dec-cache.onnx\")\n",
    "onnx_model_no_cache = onnx.load(\"test-dec-no-cache.onnx\")\n",
    "\n",
    "\n",
    "prefix = \"cache_node_\"\n",
    "mapping_initializer_cache_to_no_cache = dict()\n",
    "to_add = list()\n",
    "for node_cache in onnx_model_cache.graph.initializer:\n",
    "    found = False\n",
    "    for node_no_cache in onnx_model_no_cache.graph.initializer:\n",
    "        if node_cache.raw_data == node_no_cache.raw_data:\n",
    "            found = True\n",
    "            mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name\n",
    "            break\n",
    "    if not found:\n",
    "        node_cache.name = prefix + node_cache.name\n",
    "        to_add.append(node_cache)\n",
    "        mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name\n",
    "        print(f\"name: {node_cache.name} - size: {len(node_cache.raw_data)/1024:.2f}\")\n",
    "\n",
    "onnx_model_no_cache.graph.initializer.extend(to_add)\n",
    "# I/O model names should not be prefixed\n",
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "for node in onnx_model_cache.graph.node:\n",
    "    for index, input_name in enumerate(node.input):\n",
    "        if input_name in model_io_names:\n",
    "            continue\n",
    "        node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name)\n",
    "    for index, output_name in enumerate(node.output):\n",
    "        if output_name in model_io_names:\n",
    "            continue\n",
    "        node.output[index] = prefix + output_name\n",
    "    node.name = prefix + node.name\n",
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "prefix = \"init_\"\n",
    "cache = dict()\n",
    "for node in onnx_model_no_cache.graph.initializer:\n",
    "    if node.name in model_io_names:\n",
    "        new_name = prefix + node.name\n",
    "        cache[node.name] = new_name\n",
    "        node.name = new_name\n",
    "\n",
    "for node in onnx_model_no_cache.graph.node:\n",
    "    for index, n in enumerate(node.input):\n",
    "        node.input[index] = cache.get(n, n)\n",
    "\n",
    "# mandatory for subgraph in if/else node\n",
    "assert len(onnx_model_cache.graph.output) == len(onnx_model_no_cache.graph.output)\n",
    "\n",
    "graph_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_cache.graph.node),\n",
    "    name=\"graph-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "graph_no_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_no_cache.graph.node),\n",
    "    name=\"graph-no-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "enable_cache = onnx.helper.make_tensor_value_info(name=\"enable_cache\", elem_type=onnx.TensorProto.BOOL, shape=[1])\n",
    "\n",
    "if_node = onnx.helper.make_node(\n",
    "    op_type=\"If\",\n",
    "    inputs=[\"enable_cache\"],\n",
    "    outputs=[o.name for o in list(onnx_model_no_cache.graph.output)],\n",
    "    then_branch=graph_cache,\n",
    "    else_branch=graph_no_cache,\n",
    ")\n",
    "\n",
    "if_graph_def: GraphProto = helper.make_graph(\n",
    "    nodes=[if_node],\n",
    "    name=\"if-model\",\n",
    "    inputs=list(onnx_model_cache.graph.input) + [enable_cache],\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=list(onnx_model_no_cache.graph.initializer),\n",
    ")\n",
    "\n",
    "model_def: ModelProto = helper.make_model(if_graph_def, producer_name=\"onnx-example\")\n",
    "\n",
    "onnx.checker.check_model(model_def)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 21:31:32.155804671 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1207'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155836105 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1157'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155841281 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1073'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155845637 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1056'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155849310 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1006'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155853666 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_922'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155857875 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_319'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155862185 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_218'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155866044 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1224'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155873380 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_553'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155876860 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_855'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155881525 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_754'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155885868 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_368'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155891238 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_469'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155894792 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_905'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155899784 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_771'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155903301 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_603'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155907285 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_620'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155910727 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_704'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.155913915 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_452'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156086558 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156091631 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1307'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156095566 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1143'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156100707 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1044'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156106909 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156110361 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '402'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156113592 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '945'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156119361 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156123329 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '269'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156130397 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '419'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156134601 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156138559 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '764'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156141820 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '583'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156145755 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1324'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156149200 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '682'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156152554 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1126'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156157186 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '863'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156161066 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '501'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156164685 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '781'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.156168347 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '962'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181047213 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_164'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181060137 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_152'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181064980 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_150'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181216386 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181240287 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.181244918 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.207208282 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_165'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.207383015 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.257475412 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1228'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:31:32.257634118 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1328'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "model_decoder = model_decoder.cpu()\n",
    "input_ids = input_ids.cpu()\n",
    "\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "ort_cachable = create_model_for_provider(model_def.SerializeToString(), \"CPUExecutionProvider\")\n",
    "input_ort = dict()\n",
    "input_ort[\"input_ids\"] = input_ids\n",
    "input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "input_ort[\"enable_cache\"] = torch.tensor([False], device=\"cpu\", dtype=torch.bool)\n",
    "\n",
    "output_shape = {\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.vocab_size),)}\n",
    "\n",
    "result_no_cache = inference_onnx_binding(\n",
    "    model_onnx=ort_cachable,\n",
    "    inputs=input_ort,\n",
    "    device=input_ids.device.type,\n",
    "    output_shape=output_shape,\n",
    ")\n",
    "\n",
    "input_ort[\"enable_cache\"] = torch.tensor([True], device=\"cpu\", dtype=torch.bool)\n",
    "input_ort[\"input_ids\"] = input_ort[\"input_ids\"][:, -1:].type(torch.int32)\n",
    "output_shape = {\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.vocab_size),)}\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "    input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "    input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "    input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "\n",
    "\n",
    "result_cache = inference_onnx_binding(\n",
    "    model_onnx=ort_cachable,\n",
    "    inputs=input_ort,\n",
    "    device=input_ids.device.type,\n",
    "    output_shape=output_shape,\n",
    ")\n",
    "\n",
    "assert np.allclose(a=result_cache[\"logits\"][:, -1:, :], b=result_no_cache[\"logits\"][:, -1:, :], atol=1e-2)\n",
    "\n",
    "result_python = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "assert np.allclose(\n",
    "    a=result_no_cache[\"logits\"][:, -1:, :], b=result_python.last_hidden_state[:, -1:, :].detach().numpy(), atol=1e-2\n",
    ")\n",
    "\n",
    "# del result_python\n",
    "# del result_cache\n",
    "del result_no_cache\n",
    "del ort_cachable\n",
    "del input_ort\n",
    "del out_enc\n",
    "del out_dec_pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 22:09:05.204572444 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1272'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.204602931 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1261'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.204612159 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1271'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.204618382 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1260'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223103677 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223113737 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1307'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223117956 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1143'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223121982 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1044'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223128061 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223131264 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '402'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223134450 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '945'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223139784 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223143368 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '269'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223149782 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '419'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223153722 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223157448 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '764'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223160435 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '583'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223164362 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1324'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223167766 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '682'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223171022 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1126'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223175531 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '863'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223179180 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '501'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223182691 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '781'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223186188 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '962'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223351061 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223355739 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1307'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223359478 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1143'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223363276 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1044'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223369156 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223372285 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '402'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223375351 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '945'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223380738 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223384362 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '269'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223391351 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '419'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223395502 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223399223 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '764'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223402209 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '583'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223405938 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1324'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223409137 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '682'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223412391 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1126'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223416737 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '863'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223420325 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '501'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223423600 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '781'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.223427056 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '962'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248149656 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248165936 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248170515 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248325811 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248337445 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.248341392 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.263543179 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.263711698 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.315817930 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1328'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 22:09:05.315988791 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1328'. It is not used by any node and should be removed from the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder\n",
      "<pad> Studien studies show that study studies have shown that a number of studies study suggest that the study study is based on e Studies.\n",
      "<pad> Studien studies show that study studies have shown that a number of studies study suggest that the study study is based on e Studies.\n",
      "encoder\n",
      "encoder\n",
      "encoder\n",
      "16.8681697845459\n",
      "12.968188285827637\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "model_decoder = model_decoder.cuda()\n",
    "model = model.eval()\n",
    "model_decoder = model_decoder.eval()\n",
    "input_ids = input_ids.cuda()\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "# model_def.SerializeToString()\n",
    "# \"test-dec-no-cache.onnx\"\n",
    "dec_onnx = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def decoder_pytorch_inference(input_ids: torch.Tensor, last_hidden_state: torch.Tensor):\n",
    "    return model_decoder(input_ids=input_ids, encoder_hidden_states=last_hidden_state).last_hidden_state\n",
    "\n",
    "\n",
    "# TODO export past present values from model\n",
    "def decoder_onnx_inference(input_ids: torch.Tensor, last_hidden_state: torch.Tensor, enable_cache: torch.Tensor):\n",
    "    output_shape = {\"logits\": tuple(input_ids.shape) + (int(model.config.vocab_size),)}\n",
    "    result_dict = inference_onnx_binding(\n",
    "        model_onnx=dec_onnx,\n",
    "        inputs={\"input_ids\": input_ids, \"encoder_hidden_states\": last_hidden_state, \"enable_cache\": enable_cache},\n",
    "        device=input_ids.device.type,\n",
    "        output_shape=output_shape,\n",
    "    )\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=result_dict[\"logits\"],\n",
    "    )  # past_key_values=((torch.tensor(1, device=\"cuda\"),),)\n",
    "\n",
    "\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "dec_onnx_out = decoder_onnx_inference(\n",
    "    input_ids=input_ids, last_hidden_state=out_enc.last_hidden_state, enable_cache=torch.tensor([False], device=\"cuda\")\n",
    ").last_hidden_state\n",
    "assert np.allclose(a=dec_onnx_out.detach().cpu().numpy(), b=out_full.logits.detach().cpu().numpy(), atol=1e-1)\n",
    "\n",
    "\n",
    "def encoder_onnx_inference(input_ids: torch.Tensor, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    last_hidden_state = inference_onnx_binding(\n",
    "        model_onnx=enc_onnx,  # noqa: F821\n",
    "        inputs={\"input_ids\": input_ids},\n",
    "        output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    "        device=input_ids.device.type,\n",
    "    )[\"output\"]\n",
    "    print(\"encoder\")\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "\n",
    "\n",
    "def encoder_pytorch_inference(input_ids, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    return model.encoder(input_ids=input_ids)\n",
    "\n",
    "\n",
    "# https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py\n",
    "class ExtT5(torch.nn.Module, GenerationMixin):\n",
    "    def __init__(self, config: PretrainedConfig, device: torch.device, encoder_func: Callable, decoder_func: Callable):\n",
    "        super(ExtT5, self).__init__()\n",
    "        self.main_input_name = \"input_ids\"  # https://github.com/huggingface/transformers/pull/14803\n",
    "        self.config: PretrainedConfig = config\n",
    "        self.device: torch.device = device\n",
    "\n",
    "        self.encoder_func = encoder_func\n",
    "        self.decoder_func = decoder_func\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder_func\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder_func\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        # if past is not None:\n",
    "        #     print(\"past is present\")\n",
    "        #     input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            self.main_input_name: input_ids,\n",
    "            \"encoder_hidden_states\": kwargs[\"encoder_outputs\"][\"last_hidden_state\"],\n",
    "            \"enable_cache\": torch.tensor([False], device=\"cuda\", dtype=torch.bool),\n",
    "        }\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, **_):\n",
    "        dec_output = self.get_decoder()(\n",
    "            input_ids=input_ids, last_hidden_state=encoder_hidden_states, enable_cache=enable_cache\n",
    "        )\n",
    "        return Seq2SeqLMOutput(logits=dec_output.last_hidden_state)\n",
    "\n",
    "\n",
    "model_gen = (\n",
    "    ExtT5(\n",
    "        config=model.config,\n",
    "        device=model.device,\n",
    "        encoder_func=encoder_onnx_inference,  # encoder_pytorch_inference\n",
    "        decoder_func=decoder_onnx_inference,  # decoder_pytorch_inference\n",
    "    )\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model_gen.generate(inputs=input_ids, min_length=30, max_length=30, num_beams=7, no_repeat_ngram_size=2)[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(inputs=input_ids, min_length=30, max_length=30, num_beams=7, no_repeat_ngram_size=2)[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "start = time()\n",
    "for _ in range(3):\n",
    "    model_gen.generate(inputs=input_ids, max_length=500, num_beams=1, no_repeat_ngram_size=2, min_length=500)\n",
    "print(time() - start)\n",
    "\n",
    "model.config.use_cache = False\n",
    "with torch.inference_mode():\n",
    "    start = time()\n",
    "    for _ in range(3):\n",
    "        model.generate(inputs=input_ids, max_length=500, num_beams=1, no_repeat_ngram_size=2, min_length=500)\n",
    "    print(time() - start)\n",
    "\n",
    "model = model.cpu()\n",
    "model_decoder = model_decoder.cpu()\n",
    "del enc_onnx\n",
    "del dec_onnx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model_to_export = ExportT5(decoder=model.decoder, lm_head=model.lm_head).eval()\n",
    "# out_model_export: torch.Tensor = model_to_export(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "# assert np.allclose(out_model_export.detach().cpu().numpy(), out_full.logits.detach().cpu().numpy(), atol=1e-5)\n",
    "#\n",
    "# inputs_onnx = {\"input_ids\": input_ids, \"encoder_hidden_states\": out_enc.last_hidden_state}\n",
    "#\n",
    "# convert_to_onnx(\n",
    "#     model_pytorch=model_to_export,\n",
    "#     output_path=\"test-dec.onnx\",\n",
    "#     inputs_pytorch=inputs_onnx,\n",
    "#     var_output_seq=False,\n",
    "#     quantization=False,\n",
    "#     fix_output_dim_size=False,  # specific to decoder part\n",
    "# )\n",
    "# optimize_onnx(\n",
    "#     onnx_path=\"test-dec.onnx\",\n",
    "#     onnx_optim_model_path=\"test-dec-opt.onnx\",\n",
    "#     architecture=\"bert\",\n",
    "#     use_cuda=True,\n",
    "#     fp16=True,\n",
    "#     num_attention_heads=model.config.num_heads,\n",
    "#     hidden_size=model.config.d_model,\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "trt_model_name = \"trt-t5-dec.plan\"\n",
    "\n",
    "# create only of does not exist because it's slow to run...\n",
    "\n",
    "# 768 for base model, 512 for small, make it dependent from the Pytorch model configuration\n",
    "input_id_shape = TensorRTShape(min_shape=[5, 1], optimal_shape=[5, 500], max_shape=[5, 500], input_name=\"input_ids\")\n",
    "encoder_hidden_states_shape = TensorRTShape(\n",
    "    min_shape=[5, 1, 512], optimal_shape=[5, 500 // 2, 512], max_shape=[5, 500, 512], input_name=\"encoder_hidden_states\"\n",
    ")\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "model_onnx: ModelProto = onnx.load(\"test-dec.onnx\")\n",
    "model_onnx_all_nodes = add_output_nodes(model=model_onnx)\n",
    "onnx_graph: Dict[str, Set[str]] = get_adjency_dict(model=model_onnx)\n",
    "ort_model_all_nodes = create_model_for_provider(model_onnx_all_nodes.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "# use info from tokenizer size and max shape provided through the command line\n",
    "def get_random_input():\n",
    "    input = torch.randint(high=tokenizer.vocab_size, size=(5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "    hidden_state = model.encoder(input_ids=input).last_hidden_state.detach().cpu().numpy()\n",
    "    return {\"input_ids\": input.detach().cpu().numpy(), \"encoder_hidden_states\": hidden_state}\n",
    "\n",
    "\n",
    "keep_fp32 = get_list_fp32_nodes(\n",
    "    onnx_graph=onnx_graph, model=ort_model_all_nodes, get_input=get_random_input, nb_try=200\n",
    ")\n",
    "model = model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "engine: ICudaEngine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=\"test-dec.onnx\",\n",
    "    logger=trt_logger,\n",
    "    workspace_size=20000 * 1024**2,\n",
    "    fp16=True,\n",
    "    int8=False,\n",
    "    input_shapes=[input_id_shape, encoder_hidden_states_shape],\n",
    "    fp16_fix=get_fix_fp16_network_func(keep_fp32=keep_fp32),\n",
    ")\n",
    "save_engine(engine, trt_model_name)\n",
    "\n",
    "tensorrt_model = load_engine(runtime=runtime, engine_file_path=trt_model_name)\n",
    "a = tensorrt_model(\n",
    "    {\n",
    "        \"input_ids\": input_ids.type(torch.int32).repeat((5, 1)),\n",
    "        \"encoder_hidden_states\": out_enc.last_hidden_state.repeat((5, 1, 1)),\n",
    "    }\n",
    ")\n",
    "print(a[0])\n",
    "\n",
    "benchmark_input = torch.ones((5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "benchmark_enc_output = out_enc.last_hidden_state.repeat((5, 1, 1))\n",
    "for _ in range(10):\n",
    "    tensorrt_model(\n",
    "        {\n",
    "            \"input_ids\": benchmark_input,\n",
    "            \"encoder_hidden_states\": benchmark_enc_output,\n",
    "        }\n",
    "    )\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    tensorrt_model(\n",
    "        {\n",
    "            \"input_ids\": benchmark_input,\n",
    "            \"encoder_hidden_states\": benchmark_enc_output,\n",
    "        }\n",
    "    )\n",
    "print(time() - start)\n",
    "\n",
    "dec_onnx = create_model_for_provider(\"test-dec-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "dec_onnx_out = decoder_onnx_inference(input_ids=input_ids, last_hidden_state=out_enc.last_hidden_state)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "print(time() - start)\n",
    "\n",
    "model.cuda()\n",
    "for _ in range(10):\n",
    "    model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "print(time() - start)\n",
    "\n",
    "# TensorRT, ONNX Runtime, Pytorch\n",
    "\n",
    "# sequence 500\n",
    "# 0.8640644550323486\n",
    "# 0.6695075035095215\n",
    "# 1.1308434009552002\n",
    "\n",
    "# sequence 250\n",
    "# 0.9177014827728271\n",
    "# 0.6861860752105713\n",
    "# 1.1923034191131592"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "model.decoder(\n",
    "    input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state, past_key_values=None\n",
    ").last_hidden_state[:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "model.decoder(\n",
    "    input_ids=input_ids[:, -1:],\n",
    "    encoder_hidden_states=out_enc.last_hidden_state,\n",
    "    past_key_values=out_dec_pytorch.past_key_values,\n",
    ").last_hidden_state[:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# from transformers.onnx.features import FeaturesManager\n",
    "#\n",
    "# feature = \"seq2seq-lm-with-past\"\n",
    "# model = FeaturesManager.get_model_from_feature(feature, model_name)\n",
    "# model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "# onnx_config = model_onnx_config(model.config)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     model.config.return_dict = True\n",
    "#     model.eval()\n",
    "#\n",
    "#     # Check if we need to override certain configuration item\n",
    "#     if onnx_config.values_override is not None:\n",
    "#         for override_config_key, override_config_value in onnx_config.values_override.items():\n",
    "#             setattr(model.config, override_config_key, override_config_value)\n",
    "#\n",
    "#     # Ensure inputs match\n",
    "#     model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH)\n",
    "#     for k, v in model_inputs.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             model_inputs[k] = model_inputs[k].type(torch.int32)\n",
    "#     onnx_outputs = list(onnx_config.outputs.keys())\n",
    "#\n",
    "#     onnx_config.patch_ops()\n",
    "#\n",
    "#     # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "#     torch.onnx.export(\n",
    "#         model,\n",
    "#         (model_inputs,),\n",
    "#         f=\"test-dec-cache.onnx\",\n",
    "#         input_names=list(onnx_config.inputs.keys()),\n",
    "#         output_names=onnx_outputs,\n",
    "#         dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())},\n",
    "#         do_constant_folding=True,\n",
    "#         use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()),\n",
    "#         enable_onnx_checker=True,\n",
    "#         opset_version=13,\n",
    "#     )\n",
    "#\n",
    "#     onnx_config.restore_ops()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CPUExecutionProvider\")\n",
    "input_ort = dict()\n",
    "input_ort[\"input_ids\"] = input_ids[:, -1:].type(torch.int32).detach().cpu().numpy()\n",
    "input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "    input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "    input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "    input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "\n",
    "ort_cache.run([\"logits\"], input_ort)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ort_no_cache = create_model_for_provider(\"test-dec-no-cache.onnx\", \"CPUExecutionProvider\")\n",
    "input_no_cache = dict()\n",
    "input_no_cache[\"input_ids\"] = input_ids.type(torch.int32).detach().cpu().numpy()\n",
    "input_no_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "ort_no_cache.run([\"logits\"], input_no_cache)[0][:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids[:, -1:]\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach()\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.cuda()\n",
    "#\n",
    "#\n",
    "# print(inference_onnx_binding(model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (1, 1, 512)})[\n",
    "#     \"logits\"\n",
    "# ])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ort_cache = create_model_for_provider(\"test-dec-no-cache.onnx\", \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach()\n",
    "#\n",
    "# print(inference_onnx_binding(model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (1, 4, 512)})[\n",
    "#     \"logits\"\n",
    "# ][:,-1,:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "# out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CPUExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids.type(torch.int32).detach().cpu().numpy()\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "# input_cache[\"enable_cache\"] = np.array([False])\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "#\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0][:,-1,:][:, :10])\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0].shape)\n",
    "#\n",
    "#\n",
    "# input_cache[\"enable_cache\"] = np.array([True])\n",
    "# input_cache[\"input_ids\"] = input_cache[\"input_ids\"][:, -1:]\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0][:,-1,:][:, :10])\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 21:14:58.513001016 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1207'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513051249 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1157'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513062059 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1073'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513069042 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1056'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513076180 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1006'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513086405 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_922'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513094856 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_319'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513104081 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_218'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513112313 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1224'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513131120 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_553'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513138196 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_855'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513148874 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_754'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513158818 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_368'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513172271 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_469'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513179405 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_905'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513190729 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_771'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513197752 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_603'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513205962 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_620'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513213405 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_704'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513219891 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_452'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513594033 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513603729 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1307'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513615933 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1143'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513626058 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1044'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513643887 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513650714 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '402'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513657371 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '945'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513673400 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513682473 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '269'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513703417 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '419'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513713325 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513723235 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '764'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513729981 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '583'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513740037 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1324'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513747435 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '682'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513754648 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1126'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513767286 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '863'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513775780 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '501'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513783551 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '781'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.513791775 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '962'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583266365 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_164'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583311834 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_152'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583325241 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_150'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583708707 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583752190 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.583763003 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.643911045 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_165'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.644315790 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.768062658 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1228'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:58.768344435 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1328'. It is not used by any node and should be removed from the model.\n",
      "2022-04-24 21:14:59.075233605 [E:onnxruntime:Default, cuda_call.cc:118 CudaCall] CUDA failure 1: invalid argument ; GPU=0 ; hostname=geantvert.local ; expr=cudaMemcpyAsync(dst_data, src_data, bytes, cudaMemcpyHostToDevice, GetStream(kCudaStreamDefault)); \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in execution: CUDA error executing cudaMemcpyAsync(dst_data, src_data, bytes, cudaMemcpyHostToDevice, GetStream(kCudaStreamDefault))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m start \u001B[38;5;241m=\u001B[39m time()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m---> 26\u001B[0m     result_dict \u001B[38;5;241m=\u001B[39m \u001B[43minference_onnx_binding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_onnx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mort_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids_benchmark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogits\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43minput_ort\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43md_model\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(time() \u001B[38;5;241m-\u001B[39m start)\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(result_dict[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlogits\u001B[39m\u001B[38;5;124m\"\u001B[39m][:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:, :][\u001B[38;5;241m0\u001B[39m, :, :\u001B[38;5;241m10\u001B[39m])\n",
      "File \u001B[0;32m~/workspace/fast_transformer/src/transformer_deploy/backends/ort_utils.py:213\u001B[0m, in \u001B[0;36minference_onnx_binding\u001B[0;34m(model_onnx, inputs, device, output_shape, device_id)\u001B[0m\n\u001B[1;32m    204\u001B[0m     outputs[output_name] \u001B[38;5;241m=\u001B[39m tensor\n\u001B[1;32m    205\u001B[0m     binding\u001B[38;5;241m.\u001B[39mbind_output(\n\u001B[1;32m    206\u001B[0m         name\u001B[38;5;241m=\u001B[39moutput_name,\n\u001B[1;32m    207\u001B[0m         device_type\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    211\u001B[0m         buffer_ptr\u001B[38;5;241m=\u001B[39mtensor\u001B[38;5;241m.\u001B[39mdata_ptr(),\n\u001B[1;32m    212\u001B[0m     )\n\u001B[0;32m--> 213\u001B[0m \u001B[43mmodel_onnx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_with_iobinding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbinding\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:276\u001B[0m, in \u001B[0;36mSession.run_with_iobinding\u001B[0;34m(self, iobinding, run_options)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_with_iobinding\u001B[39m(\u001B[38;5;28mself\u001B[39m, iobinding, run_options\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    271\u001B[0m \u001B[38;5;124;03m     Compute the predictions.\u001B[39;00m\n\u001B[1;32m    272\u001B[0m \n\u001B[1;32m    273\u001B[0m \u001B[38;5;124;03m     :param iobinding: the iobinding object that has graph inputs/outputs bind.\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;124;03m     :param run_options: See :class:`onnxruntime.RunOptions`.\u001B[39;00m\n\u001B[1;32m    275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 276\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sess\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_with_iobinding\u001B[49m\u001B[43m(\u001B[49m\u001B[43miobinding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_iobinding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_options\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error in execution: CUDA error executing cudaMemcpyAsync(dst_data, src_data, bytes, cudaMemcpyHostToDevice, GetStream(kCudaStreamDefault))"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "model.cuda()\n",
    "input_ids_benchmark = torch.ones((4, 200), dtype=torch.int32, device=\"cuda\")\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids_benchmark)  #\n",
    "out_dec_pytorch = model.decoder(input_ids=input_ids_benchmark[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "input_ort = dict()\n",
    "input_ort[\"input_ids\"] = input_ids_benchmark.type(torch.int32)\n",
    "input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "input_ort[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "    input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "    input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "    input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "\n",
    "\n",
    "start = time()\n",
    "for _ in range(10):\n",
    "    result_dict = inference_onnx_binding(\n",
    "        model_onnx=ort_cache,\n",
    "        inputs=input_ort,\n",
    "        device=input_ids_benchmark.device.type,\n",
    "        output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.d_model),)},\n",
    "    )\n",
    "print(time() - start)\n",
    "print(result_dict[\"logits\"][:, -1:, :][0, :, :10])\n",
    "\n",
    "input_ort[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool)\n",
    "input_ort[\"input_ids\"] = input_ort[\"input_ids\"][:, -1:].type(torch.int32)\n",
    "start = time()\n",
    "for _ in range(10):\n",
    "    result_dict = inference_onnx_binding(\n",
    "        model_onnx=ort_cache,\n",
    "        inputs=input_ort,\n",
    "        device=input_ids.device.type,\n",
    "        output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.d_model),)},\n",
    "    )\n",
    "print(time() - start)\n",
    "print(result_dict[\"logits\"][:, -1:, :][0, :, :10])\n",
    "\n",
    "del input_ids_benchmark\n",
    "del ort_cache\n",
    "del input_ort\n",
    "del out_enc\n",
    "del out_dec_pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del input_ids_benchmark\n",
    "del out_enc\n",
    "del out_dec_pytorch\n",
    "del input_ort\n",
    "del ort_cache"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tout_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0))\n",
    "# out_dec_pytorch = model.decoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0), encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids.type(torch.int32)\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "# input_cache[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # v_enc.detach().cpu().numpy()\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     result_dict = inference_onnx_binding(\n",
    "#     model_onnx=ort_cache,\n",
    "#     inputs=input_cache,\n",
    "#     device=input_ids.device.type,\n",
    "#     output_shape={\"logits\" : tuple(input_ids.shape) + (int(model.config.vocab_size),)},\n",
    "#     )\n",
    "# print(time()-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0))\n",
    "# out_dec_pytorch = model.decoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0), encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids[:, :-1].type(torch.int32).detach().cpu().numpy()\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "# input_cache[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     ort_cache.run([\"logits\"], input_cache)\n",
    "# print(time()-start)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}