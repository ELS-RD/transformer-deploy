{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Efficient, scalable and enterprise-grade CPU / GPU inference server for Hugging Face transformer models At Lefebvre Dalloz we run in production semantic search engines in the legal domain, in non-marketing language it's a re-ranker, and we based ours on Transformer . In those setup, latency is key to provide good user experience, and relevancy inference is done online for hundreds of snippets per user query. We have tested many solutions, and below is what we found: Pytorch + FastAPI = \ud83d\udc22 Most tutorials on Transformer deployment in production are built over Pytorch and FastAPI. Both are great tools but not very performant in inference (actual measures below). Microsoft ONNX Runtime + Nvidia Triton inference server = \ufe0f\ud83c\udfc3\ud83d\udca8 Then, if you spend some time, you can build something over ONNX Runtime and Triton inference server. You will usually get from 2X to 4X faster inference compared to vanilla Pytorch. It's cool! Nvidia TensorRT + Nvidia Triton inference server = \u26a1\ufe0f\ud83c\udfc3\ud83d\udca8\ud83d\udca8 However, if you want the best in class performances on GPU , there is only a single possible combination: Nvidia TensorRT and Triton. You will usually get 5X faster inference compared to vanilla Pytorch. Sometimes it can rise up to 10X faster inference . Buuuuttt... TensorRT can ask some efforts to master, it requires tricks not easy to come up with, we implemented them for you! Detailed tool comparison table Features # Heavily optimize transformer models for inference ( CPU and GPU ) -> between 5X and 10X speedup deploy models on Nvidia Triton inference servers (enterprise grade), 6X faster than FastAPI add quantization support for both CPU and GPU simple to use: optimization done in a single command line! supported model: any model that can be exported to ONNX (-> most of them) supported tasks: classification, feature extraction (aka sentence-transformers dense embeddings) Want to understand how it works under the hood? read \ud83e\udd17 Hugging Face Transformer inference UNDER 1 millisecond latency \ud83d\udcd6 Want to check by yourself in 3 minutes? # To have a raw idea of what kind of acceleration you will get on your own model, you can try the docker only run below. For GPU run, you need to have installed on your machine Nvidia drivers and NVIDIA Container Toolkit . 3 tasks are covered below: Classification, feature extraction (text to dense embeddings) text generation (GPT-2 style). Moreover, we have added a GPU quantization notebook to open directly on Docker to play with. First, clone the repo as some commands below expect to find the demo folder: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy # docker image may take a few minutes docker pull ghcr.io/els-rd/transformer-deploy:0.4.0 Classification/reranking (encoder model) # Classification is a common task in NLP, and large language models have shown great results. This task is also used for search engines to provide Google like relevancy (cf. arxiv ) Optimize existing model # This will optimize models, generate Triton configuration and Triton folder layout in a single command: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=5.43ms, sd=0.70ms, min=4.88ms, max=7.81ms, median=5.09ms, 95p=7.01ms, 99p=7.53ms # [Pytorch (FP16)] mean=6.55ms, sd=1.00ms, min=5.75ms, max=10.38ms, median=6.01ms, 95p=8.57ms, 99p=9.21ms # [TensorRT (FP16)] mean=0.53ms, sd=0.03ms, min=0.49ms, max=0.61ms, median=0.52ms, 95p=0.57ms, 99p=0.58ms # [ONNX Runtime (FP32)] mean=1.57ms, sd=0.05ms, min=1.49ms, max=1.90ms, median=1.57ms, 95p=1.63ms, 99p=1.76ms # [ONNX Runtime (optimized)] mean=0.90ms, sd=0.03ms, min=0.88ms, max=1.23ms, median=0.89ms, 95p=0.95ms, 99p=0.97ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output It will output mean latency and other statistics. Usually Nvidia TensorRT is the fastest option and ONNX Runtime is usually a strong second option. On ONNX Runtime, optimized means that kernel fusion and mixed precision are enabled. Pytorch is never competitive on transformer inference, including mixed precision, whatever the model size. Run Nvidia Triton inference server # Note that we install transformers at run time. For production, it's advised to build your own 3-line Docker image with transformers pre-installed. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" # output: # ... # I0207 09:58:32.738831 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 09:58:32.739875 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 09:58:32.782066 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002 Query inference # Query ONNX models (replace transformer_onnx_inference by transformer_tensorrt_inference to query TensorRT engine): curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[1,2],\"data\":[-3.431640625,3.271484375]}]} Model output is at the end of the Json ( data field). More information about how to query the server from Python , and other languages . To get very low latency inference in your Python code (no inference server): click here Token-classification (NER) (encoder model) # Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization. Optimize existing model # This will optimize models, generate Triton configuration and Triton folder layout in a single command: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"kamalkraj/bert-base-cased-ner-conll2003\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128 \\ --task token-classification\" # output: # ... # Inference done on Tesla T4 # latencies: # [Pytorch (FP32)] mean=8.24ms, sd=0.46ms, min=7.66ms, max=13.91ms, median=8.20ms, 95p=8.38ms, 99p=10.01ms # [Pytorch (FP16)] mean=6.87ms, sd=0.44ms, min=6.69ms, max=13.05ms, median=6.78ms, 95p=7.33ms, 99p=8.86ms # [TensorRT (FP16)] mean=2.33ms, sd=0.32ms, min=2.19ms, max=4.18ms, median=2.24ms, 95p=3.00ms, 99p=4.04ms # [ONNX Runtime (FP32)] mean=8.08ms, sd=0.33ms, min=7.78ms, max=10.61ms, median=8.06ms, 95p=8.18ms, 99p=10.55ms # [ONNX Runtime (optimized)] mean=2.57ms, sd=0.04ms, min=2.38ms, max=2.83ms, median=2.56ms, 95p=2.68ms, 99p=2.73ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output It will output mean latency and other statistics. Usually Nvidia TensorRT is the fastest option and ONNX Runtime is usually a strong second option. On ONNX Runtime, optimized means that kernel fusion and mixed precision are enabled. Pytorch is never competitive on transformer inference, including mixed precision, whatever the model size. Run Nvidia Triton inference server # Note that we install transformers at run time. For production, it's advised to build your own 3-line Docker image with transformers pre-installed. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html && \\ tritonserver --model-repository=/models\" # output: # ... # I0207 09:58:32.738831 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 09:58:32.739875 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 09:58:32.782066 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002 Query inference # Query ONNX models (replace transformer_onnx_inference by transformer_tensorrt_inference to query TensorRT engine): curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"output\",\"datatype\":\"BYTES\",\"shape\":[],\"data\":[\"[{\\\"entity_group\\\": \\\"ORG\\\", \\\"score\\\": 0.9848777055740356, \\\"word\\\": \\\"Infinity\\\", \\\"start\\\": 45, \\\"end\\\": 53}]\"]}]} Feature extraction / dense embeddings # Feature extraction in NLP is the task to convert text to dense embeddings. It has gained some traction as a robust way to improve search engine relevancy (increase recall). This project supports models from sentence-transformers . Optimize existing model # docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"sentence-transformers/msmarco-distilbert-cos-v5\\\" \\ --backend tensorrt onnx \\ --task embedding \\ --seq-len 16 128 128\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=5.19ms, sd=0.45ms, min=4.74ms, max=6.64ms, median=5.03ms, 95p=6.14ms, 99p=6.26ms # [Pytorch (FP16)] mean=5.41ms, sd=0.18ms, min=5.26ms, max=8.15ms, median=5.36ms, 95p=5.62ms, 99p=5.72ms # [TensorRT (FP16)] mean=0.72ms, sd=0.04ms, min=0.69ms, max=1.33ms, median=0.70ms, 95p=0.78ms, 99p=0.81ms # [ONNX Runtime (FP32)] mean=1.69ms, sd=0.18ms, min=1.62ms, max=4.07ms, median=1.64ms, 95p=1.86ms, 99p=2.44ms # [ONNX Runtime (optimized)] mean=1.03ms, sd=0.09ms, min=0.98ms, max=2.30ms, median=1.00ms, 95p=1.15ms, 99p=1.41ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output Run Nvidia Triton inference server # docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" # output: # ... # I0207 11:04:33.761517 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 11:04:33.761844 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 11:04:33.803373 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002 Query inference # curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[1,768],\"data\":[0.06549072265625,-0.04327392578125,0.1103515625,-0.007320404052734375,... Generate text (decoder model) # Text generation seems to be the way to go for NLP. Unfortunately, they are slow to run, below we will accelerate the most famous of them: GPT-2. Optimize existing model # Like before, command below will prepare Triton inference server stuff. One point to have in mind is that Triton run: - inference engines ( ONNX Runtime and TensorRT ) - Python code in charge of the decoding part. Python code delegate to Triton server the model management. Python code is in ./triton_models/transformer_tensorrt_generate/1/model.py docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m gpt2 \\ --backend tensorrt onnx \\ --seq-len 6 256 256 \\ --task text-generation\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=9.43ms, sd=0.59ms, min=8.95ms, max=15.02ms, median=9.33ms, 95p=10.38ms, 99p=12.46ms # [Pytorch (FP16)] mean=9.92ms, sd=0.55ms, min=9.50ms, max=15.06ms, median=9.74ms, 95p=10.96ms, 99p=12.26ms # [TensorRT (FP16)] mean=2.19ms, sd=0.18ms, min=2.06ms, max=3.04ms, median=2.10ms, 95p=2.64ms, 99p=2.79ms # [ONNX Runtime (FP32)] mean=4.99ms, sd=0.38ms, min=4.68ms, max=9.09ms, median=4.78ms, 95p=5.72ms, 99p=5.95ms # [ONNX Runtime (optimized)] mean=3.93ms, sd=0.40ms, min=3.62ms, max=6.53ms, median=3.81ms, 95p=4.49ms, 99p=5.79ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output Two detailed notebooks are available: GPT-2: https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb T5: https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/t5.ipynb Run Nvidia Triton inference server # To run decoding algorithm server side, we need to install Pytorch on Triton docker image. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 8g \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html && \\ tritonserver --model-repository=/models\" # output: # ... # I0207 10:29:19.091191 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 10:29:19.091417 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 10:29:19.132902 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002 Query inference # Replace transformer_onnx_generate by transformer_tensorrt_generate to query TensorRT engine. curl -X POST http://localhost:8000/v2/models/transformer_onnx_generate/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_generate\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"output\",\"datatype\":\"BYTES\",\"shape\":[],\"data\":[\"This live event is great. I will sign-up for Infinity.\\n\\nI'm going to be doing a live stream of the event.\\n\\nI\"]}]} Ok, the output is not very interesting (\ud83d\udca9 in -> \ud83d\udca9 out) but you get the idea. Source code of the generative model is in ./triton_models/transformer_tensorrt_generate/1/model.py . You may want to tweak it regarding your needs (default is set for greedy search and output 64 tokens). Python code # You may be interested in running optimized text generation on Python directly, without using any inference server: docker run -p 8888 :8888 -v $PWD /demo/generative-model:/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser --allow-root\" Model quantization on GPU # Quantization is a generic method to get X2 speedup on top of other inference optimization. GPU quantization on transformers is almost never used because it requires to modify model source code. We have implemented in this library a mechanism which updates Hugging Face transformers library to support quantization. It makes it easy to use. To play with it, open this notebook: docker run -p 8888 :8888 -v $PWD /demo/quantization:/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser --allow-root\"","title":"Getting started"},{"location":"#features","text":"Heavily optimize transformer models for inference ( CPU and GPU ) -> between 5X and 10X speedup deploy models on Nvidia Triton inference servers (enterprise grade), 6X faster than FastAPI add quantization support for both CPU and GPU simple to use: optimization done in a single command line! supported model: any model that can be exported to ONNX (-> most of them) supported tasks: classification, feature extraction (aka sentence-transformers dense embeddings) Want to understand how it works under the hood? read \ud83e\udd17 Hugging Face Transformer inference UNDER 1 millisecond latency \ud83d\udcd6","title":"Features"},{"location":"#want-to-check-by-yourself-in-3-minutes","text":"To have a raw idea of what kind of acceleration you will get on your own model, you can try the docker only run below. For GPU run, you need to have installed on your machine Nvidia drivers and NVIDIA Container Toolkit . 3 tasks are covered below: Classification, feature extraction (text to dense embeddings) text generation (GPT-2 style). Moreover, we have added a GPU quantization notebook to open directly on Docker to play with. First, clone the repo as some commands below expect to find the demo folder: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy # docker image may take a few minutes docker pull ghcr.io/els-rd/transformer-deploy:0.4.0","title":"Want to check by yourself in 3 minutes?"},{"location":"#classificationreranking-encoder-model","text":"Classification is a common task in NLP, and large language models have shown great results. This task is also used for search engines to provide Google like relevancy (cf. arxiv )","title":"Classification/reranking (encoder model)"},{"location":"#optimize-existing-model","text":"This will optimize models, generate Triton configuration and Triton folder layout in a single command: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=5.43ms, sd=0.70ms, min=4.88ms, max=7.81ms, median=5.09ms, 95p=7.01ms, 99p=7.53ms # [Pytorch (FP16)] mean=6.55ms, sd=1.00ms, min=5.75ms, max=10.38ms, median=6.01ms, 95p=8.57ms, 99p=9.21ms # [TensorRT (FP16)] mean=0.53ms, sd=0.03ms, min=0.49ms, max=0.61ms, median=0.52ms, 95p=0.57ms, 99p=0.58ms # [ONNX Runtime (FP32)] mean=1.57ms, sd=0.05ms, min=1.49ms, max=1.90ms, median=1.57ms, 95p=1.63ms, 99p=1.76ms # [ONNX Runtime (optimized)] mean=0.90ms, sd=0.03ms, min=0.88ms, max=1.23ms, median=0.89ms, 95p=0.95ms, 99p=0.97ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output It will output mean latency and other statistics. Usually Nvidia TensorRT is the fastest option and ONNX Runtime is usually a strong second option. On ONNX Runtime, optimized means that kernel fusion and mixed precision are enabled. Pytorch is never competitive on transformer inference, including mixed precision, whatever the model size.","title":"Optimize existing model"},{"location":"#run-nvidia-triton-inference-server","text":"Note that we install transformers at run time. For production, it's advised to build your own 3-line Docker image with transformers pre-installed. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" # output: # ... # I0207 09:58:32.738831 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 09:58:32.739875 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 09:58:32.782066 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002","title":"Run Nvidia Triton inference server"},{"location":"#query-inference","text":"Query ONNX models (replace transformer_onnx_inference by transformer_tensorrt_inference to query TensorRT engine): curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[1,2],\"data\":[-3.431640625,3.271484375]}]} Model output is at the end of the Json ( data field). More information about how to query the server from Python , and other languages . To get very low latency inference in your Python code (no inference server): click here","title":"Query inference"},{"location":"#token-classification-ner-encoder-model","text":"Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.","title":"Token-classification (NER) (encoder model)"},{"location":"#optimize-existing-model_1","text":"This will optimize models, generate Triton configuration and Triton folder layout in a single command: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"kamalkraj/bert-base-cased-ner-conll2003\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128 \\ --task token-classification\" # output: # ... # Inference done on Tesla T4 # latencies: # [Pytorch (FP32)] mean=8.24ms, sd=0.46ms, min=7.66ms, max=13.91ms, median=8.20ms, 95p=8.38ms, 99p=10.01ms # [Pytorch (FP16)] mean=6.87ms, sd=0.44ms, min=6.69ms, max=13.05ms, median=6.78ms, 95p=7.33ms, 99p=8.86ms # [TensorRT (FP16)] mean=2.33ms, sd=0.32ms, min=2.19ms, max=4.18ms, median=2.24ms, 95p=3.00ms, 99p=4.04ms # [ONNX Runtime (FP32)] mean=8.08ms, sd=0.33ms, min=7.78ms, max=10.61ms, median=8.06ms, 95p=8.18ms, 99p=10.55ms # [ONNX Runtime (optimized)] mean=2.57ms, sd=0.04ms, min=2.38ms, max=2.83ms, median=2.56ms, 95p=2.68ms, 99p=2.73ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output It will output mean latency and other statistics. Usually Nvidia TensorRT is the fastest option and ONNX Runtime is usually a strong second option. On ONNX Runtime, optimized means that kernel fusion and mixed precision are enabled. Pytorch is never competitive on transformer inference, including mixed precision, whatever the model size.","title":"Optimize existing model"},{"location":"#run-nvidia-triton-inference-server_1","text":"Note that we install transformers at run time. For production, it's advised to build your own 3-line Docker image with transformers pre-installed. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html && \\ tritonserver --model-repository=/models\" # output: # ... # I0207 09:58:32.738831 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 09:58:32.739875 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 09:58:32.782066 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002","title":"Run Nvidia Triton inference server"},{"location":"#query-inference_1","text":"Query ONNX models (replace transformer_onnx_inference by transformer_tensorrt_inference to query TensorRT engine): curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"output\",\"datatype\":\"BYTES\",\"shape\":[],\"data\":[\"[{\\\"entity_group\\\": \\\"ORG\\\", \\\"score\\\": 0.9848777055740356, \\\"word\\\": \\\"Infinity\\\", \\\"start\\\": 45, \\\"end\\\": 53}]\"]}]}","title":"Query inference"},{"location":"#feature-extraction-dense-embeddings","text":"Feature extraction in NLP is the task to convert text to dense embeddings. It has gained some traction as a robust way to improve search engine relevancy (increase recall). This project supports models from sentence-transformers .","title":"Feature extraction / dense embeddings"},{"location":"#optimize-existing-model_2","text":"docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"sentence-transformers/msmarco-distilbert-cos-v5\\\" \\ --backend tensorrt onnx \\ --task embedding \\ --seq-len 16 128 128\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=5.19ms, sd=0.45ms, min=4.74ms, max=6.64ms, median=5.03ms, 95p=6.14ms, 99p=6.26ms # [Pytorch (FP16)] mean=5.41ms, sd=0.18ms, min=5.26ms, max=8.15ms, median=5.36ms, 95p=5.62ms, 99p=5.72ms # [TensorRT (FP16)] mean=0.72ms, sd=0.04ms, min=0.69ms, max=1.33ms, median=0.70ms, 95p=0.78ms, 99p=0.81ms # [ONNX Runtime (FP32)] mean=1.69ms, sd=0.18ms, min=1.62ms, max=4.07ms, median=1.64ms, 95p=1.86ms, 99p=2.44ms # [ONNX Runtime (optimized)] mean=1.03ms, sd=0.09ms, min=0.98ms, max=2.30ms, median=1.00ms, 95p=1.15ms, 99p=1.41ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output","title":"Optimize existing model"},{"location":"#run-nvidia-triton-inference-server_2","text":"docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" # output: # ... # I0207 11:04:33.761517 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 11:04:33.761844 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 11:04:33.803373 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002","title":"Run Nvidia Triton inference server"},{"location":"#query-inference_2","text":"curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_inference\",\"model_version\":\"1\",\"parameters\":{\"sequence_id\":0,\"sequence_start\":false,\"sequence_end\":false},\"outputs\":[{\"name\":\"output\",\"datatype\":\"FP32\",\"shape\":[1,768],\"data\":[0.06549072265625,-0.04327392578125,0.1103515625,-0.007320404052734375,...","title":"Query inference"},{"location":"#generate-text-decoder-model","text":"Text generation seems to be the way to go for NLP. Unfortunately, they are slow to run, below we will accelerate the most famous of them: GPT-2.","title":"Generate text (decoder model)"},{"location":"#optimize-existing-model_3","text":"Like before, command below will prepare Triton inference server stuff. One point to have in mind is that Triton run: - inference engines ( ONNX Runtime and TensorRT ) - Python code in charge of the decoding part. Python code delegate to Triton server the model management. Python code is in ./triton_models/transformer_tensorrt_generate/1/model.py docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m gpt2 \\ --backend tensorrt onnx \\ --seq-len 6 256 256 \\ --task text-generation\" # output: # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=9.43ms, sd=0.59ms, min=8.95ms, max=15.02ms, median=9.33ms, 95p=10.38ms, 99p=12.46ms # [Pytorch (FP16)] mean=9.92ms, sd=0.55ms, min=9.50ms, max=15.06ms, median=9.74ms, 95p=10.96ms, 99p=12.26ms # [TensorRT (FP16)] mean=2.19ms, sd=0.18ms, min=2.06ms, max=3.04ms, median=2.10ms, 95p=2.64ms, 99p=2.79ms # [ONNX Runtime (FP32)] mean=4.99ms, sd=0.38ms, min=4.68ms, max=9.09ms, median=4.78ms, 95p=5.72ms, 99p=5.95ms # [ONNX Runtime (optimized)] mean=3.93ms, sd=0.40ms, min=3.62ms, max=6.53ms, median=3.81ms, 95p=4.49ms, 99p=5.79ms # Each infence engine output is within 0.3 tolerance compared to Pytorch output Two detailed notebooks are available: GPT-2: https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/gpt2.ipynb T5: https://github.com/ELS-RD/transformer-deploy/blob/main/demo/generative-model/t5.ipynb","title":"Optimize existing model"},{"location":"#run-nvidia-triton-inference-server_3","text":"To run decoding algorithm server side, we need to install Pytorch on Triton docker image. docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 8g \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers torch==1.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html && \\ tritonserver --model-repository=/models\" # output: # ... # I0207 10:29:19.091191 1 grpc_server.cc:4195] Started GRPCInferenceService at 0.0.0.0:8001 # I0207 10:29:19.091417 1 http_server.cc:2857] Started HTTPService at 0.0.0.0:8000 # I0207 10:29:19.132902 1 http_server.cc:167] Started Metrics Service at 0.0.0.0:8002","title":"Run Nvidia Triton inference server"},{"location":"#query-inference_3","text":"Replace transformer_onnx_generate by transformer_tensorrt_generate to query TensorRT engine. curl -X POST http://localhost:8000/v2/models/transformer_onnx_generate/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" # output: # {\"model_name\":\"transformer_onnx_generate\",\"model_version\":\"1\",\"outputs\":[{\"name\":\"output\",\"datatype\":\"BYTES\",\"shape\":[],\"data\":[\"This live event is great. I will sign-up for Infinity.\\n\\nI'm going to be doing a live stream of the event.\\n\\nI\"]}]} Ok, the output is not very interesting (\ud83d\udca9 in -> \ud83d\udca9 out) but you get the idea. Source code of the generative model is in ./triton_models/transformer_tensorrt_generate/1/model.py . You may want to tweak it regarding your needs (default is set for greedy search and output 64 tokens).","title":"Query inference"},{"location":"#python-code","text":"You may be interested in running optimized text generation on Python directly, without using any inference server: docker run -p 8888 :8888 -v $PWD /demo/generative-model:/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser --allow-root\"","title":"Python code"},{"location":"#model-quantization-on-gpu","text":"Quantization is a generic method to get X2 speedup on top of other inference optimization. GPU quantization on transformers is almost never used because it requires to modify model source code. We have implemented in this library a mechanism which updates Hugging Face transformers library to support quantization. It makes it easy to use. To play with it, open this notebook: docker run -p 8888 :8888 -v $PWD /demo/quantization:/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && jupyter notebook --ip 0.0.0.0 --port 8888 --no-browser --allow-root\"","title":"Model quantization on GPU"},{"location":"benchmarks/","text":"Benchmarks # Most Transformer encoder based models are supported like Bert, Roberta, miniLM, Camembert, Albert, XLM-R, Distilbert, Electra, etc. Best results are obtained with TensorRT 8.2. Below examples are representative of the performance gain to expect from this library. Other improvements not shown here include GPU memory usage decrease, multi-stream, etc. Small architecture # batch 1, seq length 16 on T4/RTX 3090 GPUs (up to 10X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 16 16 16 --batch-size 1 1 1 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=0.65ms, sd=0.11ms, min=0.57ms, max=0.96ms, median=0.59ms, 95p=0.93ms, 99p=0.94ms [ONNX Runtime (vanilla)] mean=1.31ms, sd=0.05ms, min=1.27ms, max=1.48ms, median=1.30ms, 95p=1.44ms, 99p=1.45ms [ONNX Runtime (optimized)] mean=0.71ms, sd=0.01ms, min=0.69ms, max=0.74ms, median=0.70ms, 95p=0.73ms, 99p=0.74ms [Pytorch (FP32)] mean=5.01ms, sd=0.06ms, min=4.94ms, max=6.72ms, median=5.01ms, 95p=5.07ms, 99p=5.13ms [Pytorch (FP16)] mean=5.44ms, sd=0.07ms, min=5.36ms, max=6.80ms, median=5.43ms, 95p=5.49ms, 99p=5.55ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=0.45ms, sd=0.05ms, min=0.41ms, max=0.78ms, median=0.45ms, 95p=0.55ms, 99p=0.73ms [ONNX Runtime (vanilla)] mean=1.32ms, sd=0.11ms, min=1.24ms, max=2.36ms, median=1.30ms, 95p=1.50ms, 99p=1.74ms [ONNX Runtime (optimized)] mean=0.84ms, sd=0.11ms, min=0.76ms, max=2.03ms, median=0.81ms, 95p=1.10ms, 99p=1.25ms [Pytorch (FP32)] mean=4.68ms, sd=0.28ms, min=4.38ms, max=7.83ms, median=4.65ms, 95p=4.97ms, 99p=6.16ms [Pytorch (FP16)] mean=5.25ms, sd=0.60ms, min=4.83ms, max=8.54ms, median=5.03ms, 95p=6.54ms, 99p=7.77ms batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=16.38ms, sd=0.30ms, min=15.45ms, max=17.42ms, median=16.42ms, 95p=16.83ms, 99p=17.09ms [ONNX Runtime (vanilla)] mean=65.12ms, sd=1.53ms, min=61.74ms, max=68.51ms, median=65.21ms, 95p=67.46ms, 99p=67.90ms [ONNX Runtime (optimized)] mean=26.75ms, sd=0.30ms, min=25.96ms, max=27.71ms, median=26.73ms, 95p=27.23ms, 99p=27.52ms [Pytorch (FP32)] mean=82.22ms, sd=1.02ms, min=78.83ms, max=85.02ms, median=82.28ms, 95p=83.80ms, 99p=84.43ms [Pytorch (FP16)] mean=46.29ms, sd=0.41ms, min=45.23ms, max=47.56ms, median=46.30ms, 95p=46.98ms, 99p=47.37ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=5.44ms, sd=0.45ms, min=5.03ms, max=8.91ms, median=5.20ms, 95p=6.11ms, 99p=7.39ms [ONNX Runtime (vanilla)] mean=16.87ms, sd=2.15ms, min=15.38ms, max=26.03ms, median=15.82ms, 95p=22.63ms, 99p=24.20ms [ONNX Runtime (optimized)] mean=8.07ms, sd=0.58ms, min=7.59ms, max=13.63ms, median=7.93ms, 95p=8.71ms, 99p=11.45ms [Pytorch (FP32)] mean=17.09ms, sd=0.21ms, min=16.87ms, max=18.99ms, median=17.04ms, 95p=17.49ms, 99p=18.08ms [Pytorch (FP16)] mean=14.77ms, sd=1.83ms, min=13.50ms, max=20.97ms, median=13.87ms, 95p=19.15ms, 99p=20.01ms Base architecture # batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m cardiffnlp/twitter-roberta-base-sentiment --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=80.57ms, sd=1.00ms, min=76.23ms, max=83.16ms, median=80.53ms, 95p=82.14ms, 99p=82.53ms [ONNX Runtime (vanilla)] mean=353.81ms, sd=14.79ms, min=335.54ms, max=390.86ms, median=348.41ms, 95p=382.09ms, 99p=386.84ms [ONNX Runtime (optimized)] mean=97.94ms, sd=1.66ms, min=93.83ms, max=102.11ms, median=97.84ms, 95p=100.73ms, 99p=101.57ms [Pytorch (FP32)] mean=398.49ms, sd=25.76ms, min=369.81ms, max=454.55ms, median=387.17ms, 95p=445.52ms, 99p=450.81ms [Pytorch (FP16)] mean=134.18ms, sd=1.16ms, min=131.60ms, max=138.48ms, median=133.80ms, 95p=136.57ms, 99p=137.39ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=27.52ms, sd=1.61ms, min=24.49ms, max=33.78ms, median=28.01ms, 95p=30.33ms, 99p=31.22ms [ONNX Runtime (vanilla)] mean=65.95ms, sd=6.18ms, min=60.84ms, max=99.75ms, median=62.97ms, 95p=81.02ms, 99p=89.10ms [ONNX Runtime (optimized)] mean=32.73ms, sd=4.80ms, min=28.84ms, max=48.84ms, median=30.15ms, 95p=43.03ms, 99p=44.78ms [Pytorch (FP32)] mean=69.18ms, sd=4.79ms, min=65.97ms, max=97.74ms, median=67.16ms, 95p=77.88ms, 99p=92.43ms [Pytorch (FP16)] mean=48.78ms, sd=2.02ms, min=47.02ms, max=61.37ms, median=47.67ms, 95p=52.34ms, 99p=55.56ms Large architecture # batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m roberta-large-mnli --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=240.39ms, sd=11.01ms, min=217.59ms, max=259.57ms, median=242.68ms, 95p=255.03ms, 99p=257.04ms [ONNX Runtime (vanilla)] mean=1176.73ms, sd=63.51ms, min=1020.00ms, max=1225.03ms, median=1210.08ms, 95p=1217.54ms, 99p=1220.25ms [ONNX Runtime (optimized)] mean=295.03ms, sd=19.69ms, min=255.74ms, max=314.78ms, median=307.07ms, 95p=311.20ms, 99p=312.47ms [Pytorch (FP32)] mean=1220.41ms, sd=75.93ms, min=1119.93ms, max=1342.10ms, median=1216.23ms, 95p=1329.08ms, 99p=1336.47ms [Pytorch (FP16)] mean=438.26ms, sd=13.71ms, min=398.29ms, max=459.97ms, median=442.36ms, 95p=453.96ms, 99p=457.57ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=79.54ms, sd=5.99ms, min=74.47ms, max=113.25ms, median=76.87ms, 95p=88.02ms, 99p=104.48ms [ONNX Runtime (vanilla)] mean=202.88ms, sd=16.21ms, min=187.91ms, max=277.85ms, median=194.80ms, 95p=239.58ms, 99p=261.44ms [ONNX Runtime (optimized)] mean=97.04ms, sd=5.55ms, min=90.83ms, max=121.88ms, median=94.04ms, 95p=104.81ms, 99p=107.75ms [Pytorch (FP32)] mean=202.80ms, sd=11.16ms, min=194.47ms, max=284.70ms, median=198.46ms, 95p=221.72ms, 99p=257.31ms [Pytorch (FP16)] mean=142.63ms, sd=6.35ms, min=136.24ms, max=189.95ms, median=139.90ms, 95p=154.10ms, 99p=160.16ms","title":"Benchmarks run on AWS GPU instances"},{"location":"benchmarks/#benchmarks","text":"Most Transformer encoder based models are supported like Bert, Roberta, miniLM, Camembert, Albert, XLM-R, Distilbert, Electra, etc. Best results are obtained with TensorRT 8.2. Below examples are representative of the performance gain to expect from this library. Other improvements not shown here include GPU memory usage decrease, multi-stream, etc.","title":"Benchmarks"},{"location":"benchmarks/#small-architecture","text":"batch 1, seq length 16 on T4/RTX 3090 GPUs (up to 10X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 16 16 16 --batch-size 1 1 1 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=0.65ms, sd=0.11ms, min=0.57ms, max=0.96ms, median=0.59ms, 95p=0.93ms, 99p=0.94ms [ONNX Runtime (vanilla)] mean=1.31ms, sd=0.05ms, min=1.27ms, max=1.48ms, median=1.30ms, 95p=1.44ms, 99p=1.45ms [ONNX Runtime (optimized)] mean=0.71ms, sd=0.01ms, min=0.69ms, max=0.74ms, median=0.70ms, 95p=0.73ms, 99p=0.74ms [Pytorch (FP32)] mean=5.01ms, sd=0.06ms, min=4.94ms, max=6.72ms, median=5.01ms, 95p=5.07ms, 99p=5.13ms [Pytorch (FP16)] mean=5.44ms, sd=0.07ms, min=5.36ms, max=6.80ms, median=5.43ms, 95p=5.49ms, 99p=5.55ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=0.45ms, sd=0.05ms, min=0.41ms, max=0.78ms, median=0.45ms, 95p=0.55ms, 99p=0.73ms [ONNX Runtime (vanilla)] mean=1.32ms, sd=0.11ms, min=1.24ms, max=2.36ms, median=1.30ms, 95p=1.50ms, 99p=1.74ms [ONNX Runtime (optimized)] mean=0.84ms, sd=0.11ms, min=0.76ms, max=2.03ms, median=0.81ms, 95p=1.10ms, 99p=1.25ms [Pytorch (FP32)] mean=4.68ms, sd=0.28ms, min=4.38ms, max=7.83ms, median=4.65ms, 95p=4.97ms, 99p=6.16ms [Pytorch (FP16)] mean=5.25ms, sd=0.60ms, min=4.83ms, max=8.54ms, median=5.03ms, 95p=6.54ms, 99p=7.77ms batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=16.38ms, sd=0.30ms, min=15.45ms, max=17.42ms, median=16.42ms, 95p=16.83ms, 99p=17.09ms [ONNX Runtime (vanilla)] mean=65.12ms, sd=1.53ms, min=61.74ms, max=68.51ms, median=65.21ms, 95p=67.46ms, 99p=67.90ms [ONNX Runtime (optimized)] mean=26.75ms, sd=0.30ms, min=25.96ms, max=27.71ms, median=26.73ms, 95p=27.23ms, 99p=27.52ms [Pytorch (FP32)] mean=82.22ms, sd=1.02ms, min=78.83ms, max=85.02ms, median=82.28ms, 95p=83.80ms, 99p=84.43ms [Pytorch (FP16)] mean=46.29ms, sd=0.41ms, min=45.23ms, max=47.56ms, median=46.30ms, 95p=46.98ms, 99p=47.37ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=5.44ms, sd=0.45ms, min=5.03ms, max=8.91ms, median=5.20ms, 95p=6.11ms, 99p=7.39ms [ONNX Runtime (vanilla)] mean=16.87ms, sd=2.15ms, min=15.38ms, max=26.03ms, median=15.82ms, 95p=22.63ms, 99p=24.20ms [ONNX Runtime (optimized)] mean=8.07ms, sd=0.58ms, min=7.59ms, max=13.63ms, median=7.93ms, 95p=8.71ms, 99p=11.45ms [Pytorch (FP32)] mean=17.09ms, sd=0.21ms, min=16.87ms, max=18.99ms, median=17.04ms, 95p=17.49ms, 99p=18.08ms [Pytorch (FP16)] mean=14.77ms, sd=1.83ms, min=13.50ms, max=20.97ms, median=13.87ms, 95p=19.15ms, 99p=20.01ms","title":"Small architecture"},{"location":"benchmarks/#base-architecture","text":"batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m cardiffnlp/twitter-roberta-base-sentiment --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=80.57ms, sd=1.00ms, min=76.23ms, max=83.16ms, median=80.53ms, 95p=82.14ms, 99p=82.53ms [ONNX Runtime (vanilla)] mean=353.81ms, sd=14.79ms, min=335.54ms, max=390.86ms, median=348.41ms, 95p=382.09ms, 99p=386.84ms [ONNX Runtime (optimized)] mean=97.94ms, sd=1.66ms, min=93.83ms, max=102.11ms, median=97.84ms, 95p=100.73ms, 99p=101.57ms [Pytorch (FP32)] mean=398.49ms, sd=25.76ms, min=369.81ms, max=454.55ms, median=387.17ms, 95p=445.52ms, 99p=450.81ms [Pytorch (FP16)] mean=134.18ms, sd=1.16ms, min=131.60ms, max=138.48ms, median=133.80ms, 95p=136.57ms, 99p=137.39ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=27.52ms, sd=1.61ms, min=24.49ms, max=33.78ms, median=28.01ms, 95p=30.33ms, 99p=31.22ms [ONNX Runtime (vanilla)] mean=65.95ms, sd=6.18ms, min=60.84ms, max=99.75ms, median=62.97ms, 95p=81.02ms, 99p=89.10ms [ONNX Runtime (optimized)] mean=32.73ms, sd=4.80ms, min=28.84ms, max=48.84ms, median=30.15ms, 95p=43.03ms, 99p=44.78ms [Pytorch (FP32)] mean=69.18ms, sd=4.79ms, min=65.97ms, max=97.74ms, median=67.16ms, 95p=77.88ms, 99p=92.43ms [Pytorch (FP16)] mean=48.78ms, sd=2.02ms, min=47.02ms, max=61.37ms, median=47.67ms, 95p=52.34ms, 99p=55.56ms","title":"Base architecture"},{"location":"benchmarks/#large-architecture","text":"batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m roberta-large-mnli --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=240.39ms, sd=11.01ms, min=217.59ms, max=259.57ms, median=242.68ms, 95p=255.03ms, 99p=257.04ms [ONNX Runtime (vanilla)] mean=1176.73ms, sd=63.51ms, min=1020.00ms, max=1225.03ms, median=1210.08ms, 95p=1217.54ms, 99p=1220.25ms [ONNX Runtime (optimized)] mean=295.03ms, sd=19.69ms, min=255.74ms, max=314.78ms, median=307.07ms, 95p=311.20ms, 99p=312.47ms [Pytorch (FP32)] mean=1220.41ms, sd=75.93ms, min=1119.93ms, max=1342.10ms, median=1216.23ms, 95p=1329.08ms, 99p=1336.47ms [Pytorch (FP16)] mean=438.26ms, sd=13.71ms, min=398.29ms, max=459.97ms, median=442.36ms, 95p=453.96ms, 99p=457.57ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=79.54ms, sd=5.99ms, min=74.47ms, max=113.25ms, median=76.87ms, 95p=88.02ms, 99p=104.48ms [ONNX Runtime (vanilla)] mean=202.88ms, sd=16.21ms, min=187.91ms, max=277.85ms, median=194.80ms, 95p=239.58ms, 99p=261.44ms [ONNX Runtime (optimized)] mean=97.04ms, sd=5.55ms, min=90.83ms, max=121.88ms, median=94.04ms, 95p=104.81ms, 99p=107.75ms [Pytorch (FP32)] mean=202.80ms, sd=11.16ms, min=194.47ms, max=284.70ms, median=198.46ms, 95p=221.72ms, 99p=257.31ms [Pytorch (FP16)] mean=142.63ms, sd=6.35ms, min=136.24ms, max=189.95ms, median=139.90ms, 95p=154.10ms, 99p=160.16ms","title":"Large architecture"},{"location":"compare/","text":"High-level comparison # Inference engine # The inference engine performs the computation only, it doesn't manage the communication part (HTTP/GRPC API, etc.). Summary don't use Pytorch in production for inference ONNX Runtime is your good enough API for most inference jobs if you need best performances, use TensorRT Nvidia TensorRT Microsoft ONNX Runtime Meta Pytorch comments transformer-deploy support Licence Apache 2, optimization engine is closed source MIT Modified BSD ease of use (API) :fontawesome-regular-angry: Nvidia has chosen to not hide technical details + model is specific to a single hardware + model + data shapes association ease of use (documentation) (spread out, incomplete) (improving) (strong community) Hardware support GPU + Jetson CPU + GPU + IoT + Edge + Mobile CPU + GPU Performance TensorRT is usually 5 to 10X faster than Pytorch when you use quantization, etc. Accuracy TensorRT optimizations may be a bit too aggressive and decrease model accuracy. It requires manual modification to retrieve it. Inference HTTP/GRPC server # Nvidia Triton Meta TorchServe FastAPI comments transformer-deploy support Licence Modified BSD Apache 2 MIT ease of use (API) As a classic HTTP server, FastAPI may appear easier to use ease of use (documentation) FastAPI has one of the most beautiful documentation ever! Performance FastAPI is 6-10X slower to manage user query than Triton Support CPU GPU dynamic batching combine individual inference requests together to improve inference throughput concurrent model execution run multiple models (or multiple instances of the same model) pipeline one or more models and the connection of input and output tensors between those models native multiple backends* support *backends: Microsoft ONNX Runtime, Nvidia Triton, Meta Pytorch REST API GRPC API Inference metrics GPU utilization, server throughput, and server latency","title":"Which tool to choose for your inference?"},{"location":"compare/#high-level-comparison","text":"","title":"High-level comparison"},{"location":"compare/#inference-engine","text":"The inference engine performs the computation only, it doesn't manage the communication part (HTTP/GRPC API, etc.). Summary don't use Pytorch in production for inference ONNX Runtime is your good enough API for most inference jobs if you need best performances, use TensorRT Nvidia TensorRT Microsoft ONNX Runtime Meta Pytorch comments transformer-deploy support Licence Apache 2, optimization engine is closed source MIT Modified BSD ease of use (API) :fontawesome-regular-angry: Nvidia has chosen to not hide technical details + model is specific to a single hardware + model + data shapes association ease of use (documentation) (spread out, incomplete) (improving) (strong community) Hardware support GPU + Jetson CPU + GPU + IoT + Edge + Mobile CPU + GPU Performance TensorRT is usually 5 to 10X faster than Pytorch when you use quantization, etc. Accuracy TensorRT optimizations may be a bit too aggressive and decrease model accuracy. It requires manual modification to retrieve it.","title":"Inference engine"},{"location":"compare/#inference-httpgrpc-server","text":"Nvidia Triton Meta TorchServe FastAPI comments transformer-deploy support Licence Modified BSD Apache 2 MIT ease of use (API) As a classic HTTP server, FastAPI may appear easier to use ease of use (documentation) FastAPI has one of the most beautiful documentation ever! Performance FastAPI is 6-10X slower to manage user query than Triton Support CPU GPU dynamic batching combine individual inference requests together to improve inference throughput concurrent model execution run multiple models (or multiple instances of the same model) pipeline one or more models and the connection of input and output tensors between those models native multiple backends* support *backends: Microsoft ONNX Runtime, Nvidia Triton, Meta Pytorch REST API GRPC API Inference metrics GPU utilization, server throughput, and server latency","title":"Inference HTTP/GRPC server"},{"location":"demo/","text":"Optimize and deploy model on Nvidia Triton server # To better understand the context of this demo, check Hugging Face Transformer inference UNDER 1 millisecond latency This folder contains scripts to run different benchmarks: triton_client.py : query the model with a string triton_client_model.py : query the model directly (without using the tokenizer) with numpy arrays triton_client_requests.py : query the model directly (without using the tokenizer) with numpy arrays using only requests library triton_client_tokenizer.py : query the tokenizer only fast_api_server_onnx.py : FastAPI inference server to compare to Nvidia Triton Infinity demo information # In sept 2021, \ud83e\udd17 Hugging Face released a new product called Infinity . It\u2019s described as a server to perform inference at enterprise scale . The communication is around the promise that the product can perform Transformer inference at 1 millisecond latency on the GPU . There are very few information about its performances outside this YouTube video: demo video (Youtube) According to the demo presenter, Hugging Face Infinity server costs at least \ud83d\udcb020 000$/year for a single model deployed on a single machine (no information is publicly available on price scalability). In the next parts we will try to compare this open source library with the commercial solution from Hugging Face. Setup they used for their own demo: AWS instance GPU model seq len batch size latency g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 16 1 1.7ms g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 128 1 2.5ms The purpose of this tutorial is to explain how to heavily optimize a Transformer from Hugging Face and deploy it on a production-ready inference server, end to end. The performance improvement brought by this process applies to all scenarios, from short sequences to long ones, from a batch of size 1 to large batches. When the architecture is compliant with the expectations of the tools, the process always brings a significant performance boost compared to vanilla PyTorch. The process is in 3 steps: convert Pytorch model to a graph optimize the graph deploy the graph on a performant inference server At the end we will compare the performance of our inference server to the numbers shown by Hugging Face during the demo and will see that we are faster for both 16 and 128 tokens input sequences with batch size 1 (as far as I know, Hugging Face has not publicly shared information on other scenarios). Model optimization # We will optimize philschmid/MiniLM-L6-H384-uncased-sst2 model from the Hugging Face hub. We will use the 3 backends for that: ONNX Runtime, TensorRT and Pytorch. Usually, ONNX Runtime provide a good trade-off between simplicity and performance, TensorRT the best performances and Pytorch the simplest approach (at least it's the most well known tool). # add -v $PWD/src:/opt/tritonserver/src to apply source code modification to the container docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" 16 128 128 means that the TensorRT model will optimize for a sequence length between 16 and 128 tokens. Most of the time it's a bad idea to use dynamic axis on sequence length, it makes TensorRT slower. ONNX Runtime don't use this information and it has no impact on it. After a few minutes, it should display something like this: Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=1.00ms, sd=0.13ms, min=0.92ms, max=1.34ms, median=0.95ms, 95p=1.31ms, 99p=1.33ms [ONNX Runtime (vanilla)] mean=1.67ms, sd=0.08ms, min=1.59ms, max=3.48ms, median=1.65ms, 95p=1.85ms, 99p=1.87ms [ONNX Runtime (optimized)] mean=0.73ms, sd=0.01ms, min=0.71ms, max=0.87ms, median=0.73ms, 95p=0.75ms, 99p=0.76ms [Pytorch (FP32)] mean=5.13ms, sd=0.06ms, min=5.06ms, max=6.85ms, median=5.13ms, 95p=5.18ms, 99p=5.22ms [Pytorch (FP16)] mean=5.39ms, sd=0.10ms, min=5.31ms, max=8.39ms, median=5.39ms, 95p=5.45ms, 99p=5.48ms interesting to note that ONNX Runtime provide better performances than TensorRT for this setup, it's quite rare... Models are stored in newly generated ./triton_models/ folder. Subfolders contain templates for Nvidia Triton server. Launch Nvidia Triton inference server # \u26a0\ufe0f WARNING \u26a0\ufe0f: if you have generated the models outside the Docker container, check that your TensorRT version is the same than the Triton backend one. Launch Nvidia Triton inference server : # add --shm-size 256m -> to have up to 4 Python backends (tokenizer) at the same time (64Mb per instance) docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside. Performance analysis # Measures: 16 tokens + TensorRT: # need a local installation of the package # pip install .[GPU] ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/infinity/triton_client.py --length 16 --model tensorrt 10 /31/2021 12 :09:34 INFO timing [ triton transformers ] : mean = 1 .53ms, sd = 0 .06ms, min = 1 .48ms, max = 1 .78ms, median = 1 .51ms, 95p = 1 .66ms, 99p = 1 .74ms [[ -3.4355469 3 .2753906 ]] 128 tokens + TensorRT: ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/infinity/triton_client.py --length 128 --model tensorrt 10 /31/2021 12 :12:00 INFO timing [ triton transformers ] : mean = 1 .96ms, sd = 0 .08ms, min = 1 .88ms, max = 2 .24ms, median = 1 .93ms, 95p = 2 .17ms, 99p = 2 .23ms [[ -3.4589844 3 .3027344 ]] There is also a performance analysis tool provided by Nvidia called perf_analyzer # perf_analyzer needs this dependency sudo apt install libb64-dev # add -a for async measures, and -i grpc to use that protocol instead of http ~/.local/bin/perf_analyzer -m transformer_tensorrt_inference \\ --percentile = 95 \\ --string-data \"This live event is great. I will sign-up for Infinity.\" \\ --shape TEXT:1 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv # just test the model part (easier to get random input) ~/.local/bin/perf_analyzer --input-data zero -m transformer_tensorrt_model \\ --shape input_ids:1,128 \\ --shape attention_mask:1,128 \\ --shape token_type_ids:1,128 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv The tool need to be run on Ubuntu >= 20.04 (and won't work on Ubuntu 18.04 used for the AWS official Ubuntu deep learning image) Model analyzer # Model analyzer is a powerful tool to adjust the Triton server configuration. To run it: docker run -it --rm --gpus all -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.1.1 \\ bash -c \"model-analyzer profile -f /project/demo/infinity/config_analyzer.yaml\" FastAPI server baseline # This is our baseline, easy to run, but not very performant. # launch server, disable logging for best performances python3 -m uvicorn --log-level warning demo.infinity.fast_api_server_onnx:app --port 8000 --host 0 .0.0.0 # other variation, 1 worker per CPU for best latency (plus not a good idea to have several times the same model on a single GPU): python3 -m gunicorn -w 1 -k uvicorn.workers.UvicornWorker --log-level warning demo.infinity.fast_api_server_onnx --bind 0 .0.0.0:8000 # simple inference timing time curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict # slightly more serious measure sudo apt-get install linux-tools-common linux-tools-generic linux-tools- ` uname -r ` sudo perf stat -r 50 -d curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict -s > /dev/null It should produce: Performance counter stats for 'curl -G --data-urlencode query=This live event is great. I will sign-up for Infinity. localhost:8000/predict' ( 50 runs ) : 6 .14 msec task-clock # 0.494 CPUs utilized ( +- 0.59% ) 3 context-switches # 0.462 K/sec ( +- 1.84% ) 0 cpu-migrations # 0.000 K/sec 577 page-faults # 0.094 M/sec ( +- 0.06% ) <not supported> cycles <not supported> instructions <not supported> branches <not supported> branch-misses <not supported> L1-dcache-loads <not supported> L1-dcache-load-misses <not supported> LLC-loads <not supported> LLC-load-misses 0 .0124429 +- 0 .0000547 seconds time elapsed ( +- 0 .44% )","title":"From optimization to deployment: end to end demo"},{"location":"demo/#optimize-and-deploy-model-on-nvidia-triton-server","text":"To better understand the context of this demo, check Hugging Face Transformer inference UNDER 1 millisecond latency This folder contains scripts to run different benchmarks: triton_client.py : query the model with a string triton_client_model.py : query the model directly (without using the tokenizer) with numpy arrays triton_client_requests.py : query the model directly (without using the tokenizer) with numpy arrays using only requests library triton_client_tokenizer.py : query the tokenizer only fast_api_server_onnx.py : FastAPI inference server to compare to Nvidia Triton","title":"Optimize and deploy model on Nvidia Triton server"},{"location":"demo/#infinity-demo-information","text":"In sept 2021, \ud83e\udd17 Hugging Face released a new product called Infinity . It\u2019s described as a server to perform inference at enterprise scale . The communication is around the promise that the product can perform Transformer inference at 1 millisecond latency on the GPU . There are very few information about its performances outside this YouTube video: demo video (Youtube) According to the demo presenter, Hugging Face Infinity server costs at least \ud83d\udcb020 000$/year for a single model deployed on a single machine (no information is publicly available on price scalability). In the next parts we will try to compare this open source library with the commercial solution from Hugging Face. Setup they used for their own demo: AWS instance GPU model seq len batch size latency g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 16 1 1.7ms g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 128 1 2.5ms The purpose of this tutorial is to explain how to heavily optimize a Transformer from Hugging Face and deploy it on a production-ready inference server, end to end. The performance improvement brought by this process applies to all scenarios, from short sequences to long ones, from a batch of size 1 to large batches. When the architecture is compliant with the expectations of the tools, the process always brings a significant performance boost compared to vanilla PyTorch. The process is in 3 steps: convert Pytorch model to a graph optimize the graph deploy the graph on a performant inference server At the end we will compare the performance of our inference server to the numbers shown by Hugging Face during the demo and will see that we are faster for both 16 and 128 tokens input sequences with batch size 1 (as far as I know, Hugging Face has not publicly shared information on other scenarios).","title":"Infinity demo information"},{"location":"demo/#model-optimization","text":"We will optimize philschmid/MiniLM-L6-H384-uncased-sst2 model from the Hugging Face hub. We will use the 3 backends for that: ONNX Runtime, TensorRT and Pytorch. Usually, ONNX Runtime provide a good trade-off between simplicity and performance, TensorRT the best performances and Pytorch the simplest approach (at least it's the most well known tool). # add -v $PWD/src:/opt/tritonserver/src to apply source code modification to the container docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.4.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" 16 128 128 means that the TensorRT model will optimize for a sequence length between 16 and 128 tokens. Most of the time it's a bad idea to use dynamic axis on sequence length, it makes TensorRT slower. ONNX Runtime don't use this information and it has no impact on it. After a few minutes, it should display something like this: Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=1.00ms, sd=0.13ms, min=0.92ms, max=1.34ms, median=0.95ms, 95p=1.31ms, 99p=1.33ms [ONNX Runtime (vanilla)] mean=1.67ms, sd=0.08ms, min=1.59ms, max=3.48ms, median=1.65ms, 95p=1.85ms, 99p=1.87ms [ONNX Runtime (optimized)] mean=0.73ms, sd=0.01ms, min=0.71ms, max=0.87ms, median=0.73ms, 95p=0.75ms, 99p=0.76ms [Pytorch (FP32)] mean=5.13ms, sd=0.06ms, min=5.06ms, max=6.85ms, median=5.13ms, 95p=5.18ms, 99p=5.22ms [Pytorch (FP16)] mean=5.39ms, sd=0.10ms, min=5.31ms, max=8.39ms, median=5.39ms, 95p=5.45ms, 99p=5.48ms interesting to note that ONNX Runtime provide better performances than TensorRT for this setup, it's quite rare... Models are stored in newly generated ./triton_models/ folder. Subfolders contain templates for Nvidia Triton server.","title":"Model optimization"},{"location":"demo/#launch-nvidia-triton-inference-server","text":"\u26a0\ufe0f WARNING \u26a0\ufe0f: if you have generated the models outside the Docker container, check that your TensorRT version is the same than the Triton backend one. Launch Nvidia Triton inference server : # add --shm-size 256m -> to have up to 4 Python backends (tokenizer) at the same time (64Mb per instance) docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside.","title":"Launch Nvidia Triton inference server"},{"location":"demo/#performance-analysis","text":"Measures: 16 tokens + TensorRT: # need a local installation of the package # pip install .[GPU] ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/infinity/triton_client.py --length 16 --model tensorrt 10 /31/2021 12 :09:34 INFO timing [ triton transformers ] : mean = 1 .53ms, sd = 0 .06ms, min = 1 .48ms, max = 1 .78ms, median = 1 .51ms, 95p = 1 .66ms, 99p = 1 .74ms [[ -3.4355469 3 .2753906 ]] 128 tokens + TensorRT: ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/infinity/triton_client.py --length 128 --model tensorrt 10 /31/2021 12 :12:00 INFO timing [ triton transformers ] : mean = 1 .96ms, sd = 0 .08ms, min = 1 .88ms, max = 2 .24ms, median = 1 .93ms, 95p = 2 .17ms, 99p = 2 .23ms [[ -3.4589844 3 .3027344 ]] There is also a performance analysis tool provided by Nvidia called perf_analyzer # perf_analyzer needs this dependency sudo apt install libb64-dev # add -a for async measures, and -i grpc to use that protocol instead of http ~/.local/bin/perf_analyzer -m transformer_tensorrt_inference \\ --percentile = 95 \\ --string-data \"This live event is great. I will sign-up for Infinity.\" \\ --shape TEXT:1 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv # just test the model part (easier to get random input) ~/.local/bin/perf_analyzer --input-data zero -m transformer_tensorrt_model \\ --shape input_ids:1,128 \\ --shape attention_mask:1,128 \\ --shape token_type_ids:1,128 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv The tool need to be run on Ubuntu >= 20.04 (and won't work on Ubuntu 18.04 used for the AWS official Ubuntu deep learning image)","title":"Performance analysis"},{"location":"demo/#model-analyzer","text":"Model analyzer is a powerful tool to adjust the Triton server configuration. To run it: docker run -it --rm --gpus all -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.1.1 \\ bash -c \"model-analyzer profile -f /project/demo/infinity/config_analyzer.yaml\"","title":"Model analyzer"},{"location":"demo/#fastapi-server-baseline","text":"This is our baseline, easy to run, but not very performant. # launch server, disable logging for best performances python3 -m uvicorn --log-level warning demo.infinity.fast_api_server_onnx:app --port 8000 --host 0 .0.0.0 # other variation, 1 worker per CPU for best latency (plus not a good idea to have several times the same model on a single GPU): python3 -m gunicorn -w 1 -k uvicorn.workers.UvicornWorker --log-level warning demo.infinity.fast_api_server_onnx --bind 0 .0.0.0:8000 # simple inference timing time curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict # slightly more serious measure sudo apt-get install linux-tools-common linux-tools-generic linux-tools- ` uname -r ` sudo perf stat -r 50 -d curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict -s > /dev/null It should produce: Performance counter stats for 'curl -G --data-urlencode query=This live event is great. I will sign-up for Infinity. localhost:8000/predict' ( 50 runs ) : 6 .14 msec task-clock # 0.494 CPUs utilized ( +- 0.59% ) 3 context-switches # 0.462 K/sec ( +- 1.84% ) 0 cpu-migrations # 0.000 K/sec 577 page-faults # 0.094 M/sec ( +- 0.06% ) <not supported> cycles <not supported> instructions <not supported> branches <not supported> branch-misses <not supported> L1-dcache-loads <not supported> L1-dcache-load-misses <not supported> LLC-loads <not supported> LLC-load-misses 0 .0124429 +- 0 .0000547 seconds time elapsed ( +- 0 .44% )","title":"FastAPI server baseline"},{"location":"faq/","text":"FAQ # Is CPU deployment a viable option? # Let's start with the usual \"it depends\" It's a viable option if: your sequences are short or very short (<= 128 tokens) you use a distilled/small flavor model (like miniLM or XtremeDistil ) you don't use batching, as CPU are not very good at it, and using a batch size of 1 you can avoid padding you tune number of threads and number of inference engine instances: using all threads available won't provide best results, in many cases, you will get a better throughput by using multiple instances of the inference engine. Triton server has an option for that. you use dynamic quantization In most cases, Nvidia T4 GPU (the cheapest GPU option available on all clouds) will offer you the best perf / cost trade-off by a large margin. Compared to Intel Xeon of 2nd/3rd generation, they are cheaper for better results. What should I use to optimize GPU deployment? # on the hardware side, 1 or more T4 GPU TensorRT for the optimization INT-8 quantization ( QAT ) fixed sequence length and dynamic batch axis Can I use sparse tensors if my GPU supports it? # At the time of writing, sparse tensors can only be used if you implement your model with TensorRT operators manually. If you import your model from ONNX , you won't see any acceleration, it should improve in Q3 2022. Should I use quantization or distillation or both? # ... it depends, but usually quantization will bring a X2 speed up compared to an already optimized model with little cost in accuracy. Distillation can bring you more speed, but in many cases, will cost you more in accuracy (at least on hard NLP tasks). How this compares to the TensorFlow ecosystem? # Vanilla TensorFlow has a good ecosystem, it even has a basic integration of TensorRT (basic -> not all feature/optimization). If you need really good inference optimization, Nvidia advices in its official documentation to export the model on onnx and then follow the same optimization process than any PyTorch model. So, on a perf only side, there is no difference. For sure, the idea written everywhere that for production TensorFlow is a better choice is just wrong. For instance, Amazon and Microsoft use Nvidia Triton inference server on most of their products using ML (like Microsoft Office , or advertising on Amazon ), in 2021 at least. And Microsoft Bing is built over TensorRT . Should I use Torch-TensorRT? # Torch-TensorRT is not yet mature (few ATen -> trt op support), and has not yet Triton backend (they are working on it). It seems that the new Pytorch Fx interface is the right direction to go from PyTorch to TensorRT. What do I do if I have accuracy issues with mixed precision? # Most of the time, it's an operator which overflows (too big value for FP16 numbers). Polygraphy should help you to find the operator(s) to fix. Then, during engine building, you can fix the precision to FP32 for some operators. Check in the convert source code how we did it. What do I do if ONNX export fails? # Most of the time ONNX export fail because of unsupported operations. One way to workaround is to reimplement that part or override the module with symbolic() static function. More info on pytorch.org . And for TensorRT supported operations, you can check docs.nvidia.com . Why don't you support GPU quantization on ONNX Runtime instead of TensorRT? # There are few reasons why ONNX Runtime GPU quantization support is not supported: it doesn\u2019t support QAT , it \"just\" patches ONNX file and run them on TensorRT provider. Because ONNX file can\u2019t be retrained, you can't do a fine-tuning after quantization. Concretely, it means that if post-training quantization doesn't provide accuracy good enough for your use case, you won't use quantization at all. QAT library from Nvidia (the one used in this project) let you easily enable and disable each quantizer (on Python). You can see how useful it is in the quantization demo notebook, we used that feature to disable quantization on layernorm on specific layers to retrieve 1 point of accuracy without sacrificing perf at the post-training quantization step. However, the way it currently works on ONNX Runtime is all or nothing (for each operator). Of course, if you are well verse if ONNX things, you can manually parse your graph with a graph surgery tools and make your change but it would take a lot of time compared to just for loop in your Pytorch modules. and more important, in our own experiment, models that contain unsupported operators by TensorRT just crashed on ONNX Runtime\u2026 It was unexpected as ONNX Runtime is supposed to split graph and leverage several providers when one doesn't support an operation. I suppose in my case that the issue is that the operator exists but not with the right type. At the end, to make it work, I needed to patch the source code. So not a better user experience. Check this page for more questions.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#is-cpu-deployment-a-viable-option","text":"Let's start with the usual \"it depends\" It's a viable option if: your sequences are short or very short (<= 128 tokens) you use a distilled/small flavor model (like miniLM or XtremeDistil ) you don't use batching, as CPU are not very good at it, and using a batch size of 1 you can avoid padding you tune number of threads and number of inference engine instances: using all threads available won't provide best results, in many cases, you will get a better throughput by using multiple instances of the inference engine. Triton server has an option for that. you use dynamic quantization In most cases, Nvidia T4 GPU (the cheapest GPU option available on all clouds) will offer you the best perf / cost trade-off by a large margin. Compared to Intel Xeon of 2nd/3rd generation, they are cheaper for better results.","title":"Is CPU deployment a viable option?"},{"location":"faq/#what-should-i-use-to-optimize-gpu-deployment","text":"on the hardware side, 1 or more T4 GPU TensorRT for the optimization INT-8 quantization ( QAT ) fixed sequence length and dynamic batch axis","title":"What should I use to optimize GPU deployment?"},{"location":"faq/#can-i-use-sparse-tensors-if-my-gpu-supports-it","text":"At the time of writing, sparse tensors can only be used if you implement your model with TensorRT operators manually. If you import your model from ONNX , you won't see any acceleration, it should improve in Q3 2022.","title":"Can I use sparse tensors if my GPU supports it?"},{"location":"faq/#should-i-use-quantization-or-distillation-or-both","text":"... it depends, but usually quantization will bring a X2 speed up compared to an already optimized model with little cost in accuracy. Distillation can bring you more speed, but in many cases, will cost you more in accuracy (at least on hard NLP tasks).","title":"Should I use quantization or distillation or both?"},{"location":"faq/#how-this-compares-to-the-tensorflow-ecosystem","text":"Vanilla TensorFlow has a good ecosystem, it even has a basic integration of TensorRT (basic -> not all feature/optimization). If you need really good inference optimization, Nvidia advices in its official documentation to export the model on onnx and then follow the same optimization process than any PyTorch model. So, on a perf only side, there is no difference. For sure, the idea written everywhere that for production TensorFlow is a better choice is just wrong. For instance, Amazon and Microsoft use Nvidia Triton inference server on most of their products using ML (like Microsoft Office , or advertising on Amazon ), in 2021 at least. And Microsoft Bing is built over TensorRT .","title":"How this compares to the TensorFlow ecosystem?"},{"location":"faq/#should-i-use-torch-tensorrt","text":"Torch-TensorRT is not yet mature (few ATen -> trt op support), and has not yet Triton backend (they are working on it). It seems that the new Pytorch Fx interface is the right direction to go from PyTorch to TensorRT.","title":"Should I use Torch-TensorRT?"},{"location":"faq/#what-do-i-do-if-i-have-accuracy-issues-with-mixed-precision","text":"Most of the time, it's an operator which overflows (too big value for FP16 numbers). Polygraphy should help you to find the operator(s) to fix. Then, during engine building, you can fix the precision to FP32 for some operators. Check in the convert source code how we did it.","title":"What do I do if I have accuracy issues with mixed precision?"},{"location":"faq/#what-do-i-do-if-onnx-export-fails","text":"Most of the time ONNX export fail because of unsupported operations. One way to workaround is to reimplement that part or override the module with symbolic() static function. More info on pytorch.org . And for TensorRT supported operations, you can check docs.nvidia.com .","title":"What do I do if ONNX export fails?"},{"location":"faq/#why-dont-you-support-gpu-quantization-on-onnx-runtime-instead-of-tensorrt","text":"There are few reasons why ONNX Runtime GPU quantization support is not supported: it doesn\u2019t support QAT , it \"just\" patches ONNX file and run them on TensorRT provider. Because ONNX file can\u2019t be retrained, you can't do a fine-tuning after quantization. Concretely, it means that if post-training quantization doesn't provide accuracy good enough for your use case, you won't use quantization at all. QAT library from Nvidia (the one used in this project) let you easily enable and disable each quantizer (on Python). You can see how useful it is in the quantization demo notebook, we used that feature to disable quantization on layernorm on specific layers to retrieve 1 point of accuracy without sacrificing perf at the post-training quantization step. However, the way it currently works on ONNX Runtime is all or nothing (for each operator). Of course, if you are well verse if ONNX things, you can manually parse your graph with a graph surgery tools and make your change but it would take a lot of time compared to just for loop in your Pytorch modules. and more important, in our own experiment, models that contain unsupported operators by TensorRT just crashed on ONNX Runtime\u2026 It was unexpected as ONNX Runtime is supposed to split graph and leverage several providers when one doesn't support an operation. I suppose in my case that the issue is that the operator exists but not with the right type. At the end, to make it work, I needed to patch the source code. So not a better user experience. Check this page for more questions.","title":"Why don't you support GPU quantization on ONNX Runtime instead of TensorRT?"},{"location":"gpt2/","text":"(function (global, factory) { typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() : typeof define === 'function' && define.amd ? define(factory) : (global = global || self, global.ClipboardCopyElement = factory()); }(this, function () { 'use strict'; function createNode(text) { const node = document.createElement('pre'); node.style.width = '1px'; node.style.height = '1px'; node.style.position = 'fixed'; node.style.top = '5px'; node.textContent = text; return node; } function copyNode(node) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(node.textContent); } const selection = getSelection(); if (selection == null) { return Promise.reject(new Error()); } selection.removeAllRanges(); const range = document.createRange(); range.selectNodeContents(node); selection.addRange(range); document.execCommand('copy'); selection.removeAllRanges(); return Promise.resolve(); } function copyText(text) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(text); } const body = document.body; if (!body) { return Promise.reject(new Error()); } const node = createNode(text); body.appendChild(node); copyNode(node); body.removeChild(node); return Promise.resolve(); } function copy(button) { const id = button.getAttribute('for'); const text = button.getAttribute('value'); function trigger() { button.dispatchEvent(new CustomEvent('clipboard-copy', { bubbles: true })); } if (text) { copyText(text).then(trigger); } else if (id) { const root = 'getRootNode' in Element.prototype ? button.getRootNode() : button.ownerDocument; if (!(root instanceof Document || 'ShadowRoot' in window && root instanceof ShadowRoot)) return; const node = root.getElementById(id); if (node) copyTarget(node).then(trigger); } } function copyTarget(content) { if (content instanceof HTMLInputElement || content instanceof HTMLTextAreaElement) { return copyText(content.value); } else if (content instanceof HTMLAnchorElement && content.hasAttribute('href')) { return copyText(content.href); } else { return copyNode(content); } } function clicked(event) { const button = event.currentTarget; if (button instanceof HTMLElement) { copy(button); } } function keydown(event) { if (event.key === ' ' || event.key === 'Enter') { const button = event.currentTarget; if (button instanceof HTMLElement) { event.preventDefault(); copy(button); } } } function focused(event) { event.currentTarget.addEventListener('keydown', keydown); } function blurred(event) { event.currentTarget.removeEventListener('keydown', keydown); } class ClipboardCopyElement extends HTMLElement { constructor() { super(); this.addEventListener('click', clicked); this.addEventListener('focus', focused); this.addEventListener('blur', blurred); } connectedCallback() { if (!this.hasAttribute('tabindex')) { this.setAttribute('tabindex', '0'); } if (!this.hasAttribute('role')) { this.setAttribute('role', 'button'); } } get value() { return this.getAttribute('value') || ''; } set value(text) { this.setAttribute('value', text); } } if (!window.customElements.get('clipboard-copy')) { window.ClipboardCopyElement = ClipboardCopyElement; window.customElements.define('clipboard-copy', ClipboardCopyElement); } return ClipboardCopyElement; })); document.addEventListener('clipboard-copy', function(event) { const notice = event.target.querySelector('.notice') notice.hidden = false setTimeout(function() { notice.hidden = true }, 1000) }) pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight-ipynb .hll { background-color: var(--jp-cell-editor-active-background) } .highlight-ipynb { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight-ipynb .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight-ipynb .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight-ipynb .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight-ipynb .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight-ipynb .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight-ipynb .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight-ipynb .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight-ipynb .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight-ipynb .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight-ipynb .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight-ipynb .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight-ipynb .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight-ipynb .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight-ipynb .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight-ipynb .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight-ipynb .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight-ipynb .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight-ipynb .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight-ipynb .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /* This file is taken from the built JupyterLab theme.css Found on share/nbconvert/templates/lab/static Some changes have been made and marked with CHANGE */ .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-900); --jp-brand-color1: var(--md-blue-700); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-900); --jp-accent-color1: var(--md-green-700); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-900); --jp-warn-color1: var(--md-orange-700); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-900); --jp-error-color1: var(--md-red-700); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-900); --jp-success-color1: var(--md-green-700); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-900); --jp-info-color1: var(--md-cyan-700); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } [data-md-color-scheme=\"slate\"] .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ /* The dark theme shadows need a bit of work, but this will probably also require work on the core layout * colors used in the theme as well. */ --jp-shadow-base-lightness: 32; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-700); --jp-border-color1: var(--md-grey-700); --jp-border-color2: var(--md-grey-800); --jp-border-color3: var(--md-grey-900); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(255, 255, 255, 1); --jp-ui-font-color1: rgba(255, 255, 255, 0.87); --jp-ui-font-color2: rgba(255, 255, 255, 0.54); --jp-ui-font-color3: rgba(255, 255, 255, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(0, 0, 0, 1); --jp-ui-inverse-font-color1: rgba(0, 0, 0, 0.8); --jp-ui-inverse-font-color2: rgba(0, 0, 0, 0.5); --jp-ui-inverse-font-color3: rgba(0, 0, 0, 0.3); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(255, 255, 255, 1); --jp-content-font-color1: rgba(255, 255, 255, 1); --jp-content-font-color2: rgba(255, 255, 255, 0.7); --jp-content-font-color3: rgba(255, 255, 255, 0.5); --jp-content-link-color: var(--md-blue-300); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: #111111; --jp-layout-color1: var(--md-grey-900); --jp-layout-color2: var(--md-grey-800); --jp-layout-color3: var(--md-grey-700); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: white; --jp-inverse-layout-color1: white; --jp-inverse-layout-color2: var(--md-grey-200); --jp-inverse-layout-color3: var(--md-grey-400); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--jp-layout-color1); --jp-cell-editor-border-color: var(--md-grey-700); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 1; --jp-cell-prompt-not-active-font-color: var(--md-grey-300); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: rgba(33, 150, 243, 0.24); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: rgba(244, 67, 54, 0.28); --jp-rendermime-table-row-background: var(--md-grey-900); --jp-rendermime-table-row-hover-background: rgba(3, 169, 244, 0.2); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.6); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color2); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.8); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--jp-layout-color0); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color0); --jp-input-hover-background: var(--jp-layout-color2); --jp-input-background: var(--md-grey-800); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: var(--jp-layout-color2); --jp-editor-selected-focused-background: rgba(33, 150, 243, 0.24); --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: var(--md-green-500); --jp-mirror-editor-atom-color: var(--md-blue-300); --jp-mirror-editor-number-color: var(--md-green-400); --jp-mirror-editor-def-color: var(--md-blue-600); --jp-mirror-editor-variable-color: var(--md-grey-300); --jp-mirror-editor-variable-2-color: var(--md-blue-400); --jp-mirror-editor-variable-3-color: var(--md-green-600); --jp-mirror-editor-punctuation-color: var(--md-blue-400); --jp-mirror-editor-property-color: var(--md-blue-400); --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ff7070; --jp-mirror-editor-string-2-color: var(--md-purple-300); --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: var(--md-green-600); --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: var(--md-green-700); --jp-mirror-editor-attribute-color: var(--md-blue-700); --jp-mirror-editor-header-color: var(--md-blue-500); --jp-mirror-editor-quote-color: var(--md-green-300); --jp-mirror-editor-link-color: var(--md-blue-700); --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: var(--md-grey-400); /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.6; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(255, 225, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* scrollbar related styles. Supports every browser except Edge. */ /* colors based on JetBrain's Darcula theme */ --jp-scrollbar-background-color: #3f4244; --jp-scrollbar-thumb-color: 88, 96, 97; /* need to specify thumb color as an RGB triplet */ --jp-scrollbar-endpad: 3px; /* the minimum gap between the thumb and the ends of a scrollbar */ /* hacks for setting the thumb shape. These do nothing in Firefox */ --jp-scrollbar-thumb-margin: 3.5px; /* the space in between the sides of the thumb and the track */ --jp-scrollbar-thumb-radius: 9px; /* set to a large-ish value for rounded endcaps on the thumb */ /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper{/*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. *//*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */}.jupyter-wrapper [data-jp-theme-scrollbars=true]{scrollbar-color:rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar{scrollbar-color:rgba(var(--jp-scrollbar-thumb-color), 0.5) rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-corner{background:var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-thumb{background:rgb(var(--jp-scrollbar-thumb-color));border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-right:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-bottom:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-corner,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-corner{background-color:rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb{background:rgba(var(--jp-scrollbar-thumb-color), 0.5);border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-right:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-bottom:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{min-height:16px;max-height:16px;min-width:45px;border-top:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{min-width:16px;max-width:16px;min-height:45px;border-left:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar-button{background-color:#f0f0f0;background-position:center center;min-height:15px;max-height:15px;min-width:15px;max-width:15px}.jupyter-wrapper .lm-ScrollBar-button:hover{background-color:#dadada}.jupyter-wrapper .lm-ScrollBar-button.lm-mod-active{background-color:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-track{background:#f0f0f0}.jupyter-wrapper .lm-ScrollBar-thumb{background:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-thumb:hover{background:#bababa}.jupyter-wrapper .lm-ScrollBar-thumb.lm-mod-active{background:#a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-thumb{height:100%;min-width:15px;border-left:1px solid #a0a0a0;border-right:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-thumb{width:100%;min-height:15px;border-top:1px solid #a0a0a0;border-bottom:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-left);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-right);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-up);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-down);background-size:17px}.jupyter-wrapper .p-Widget,.jupyter-wrapper .lm-Widget{box-sizing:border-box;position:relative;overflow:hidden;cursor:default}.jupyter-wrapper .p-Widget.p-mod-hidden,.jupyter-wrapper .lm-Widget.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-CommandPalette,.jupyter-wrapper .lm-CommandPalette{display:flex;flex-direction:column;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-CommandPalette-search,.jupyter-wrapper .lm-CommandPalette-search{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-content,.jupyter-wrapper .lm-CommandPalette-content{flex:1 1 auto;margin:0;padding:0;min-height:0;overflow:auto;list-style-type:none}.jupyter-wrapper .p-CommandPalette-header,.jupyter-wrapper .lm-CommandPalette-header{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-CommandPalette-item,.jupyter-wrapper .lm-CommandPalette-item{display:flex;flex-direction:row}.jupyter-wrapper .p-CommandPalette-itemIcon,.jupyter-wrapper .lm-CommandPalette-itemIcon{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemContent,.jupyter-wrapper .lm-CommandPalette-itemContent{flex:1 1 auto;overflow:hidden}.jupyter-wrapper .p-CommandPalette-itemShortcut,.jupyter-wrapper .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemLabel,.jupyter-wrapper .lm-CommandPalette-itemLabel{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-DockPanel,.jupyter-wrapper .lm-DockPanel{z-index:0}.jupyter-wrapper .p-DockPanel-widget,.jupyter-wrapper .lm-DockPanel-widget{z-index:0}.jupyter-wrapper .p-DockPanel-tabBar,.jupyter-wrapper .lm-DockPanel-tabBar{z-index:1}.jupyter-wrapper .p-DockPanel-handle,.jupyter-wrapper .lm-DockPanel-handle{z-index:2}.jupyter-wrapper .p-DockPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-DockPanel-handle:after,.jupyter-wrapper .lm-DockPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]{cursor:ew-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]{cursor:ns-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-DockPanel-overlay,.jupyter-wrapper .lm-DockPanel-overlay{z-index:3;box-sizing:border-box;pointer-events:none}.jupyter-wrapper .p-DockPanel-overlay.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-overlay.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-Menu,.jupyter-wrapper .lm-Menu{z-index:10000;position:absolute;white-space:nowrap;overflow-x:hidden;overflow-y:auto;outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-Menu-content,.jupyter-wrapper .lm-Menu-content{margin:0;padding:0;display:table;list-style-type:none}.jupyter-wrapper .p-Menu-item,.jupyter-wrapper .lm-Menu-item{display:table-row}.jupyter-wrapper .p-Menu-item.p-mod-hidden,.jupyter-wrapper .p-Menu-item.p-mod-collapsed,.jupyter-wrapper .lm-Menu-item.lm-mod-hidden,.jupyter-wrapper .lm-Menu-item.lm-mod-collapsed{display:none !important}.jupyter-wrapper .p-Menu-itemIcon,.jupyter-wrapper .p-Menu-itemSubmenuIcon,.jupyter-wrapper .lm-Menu-itemIcon,.jupyter-wrapper .lm-Menu-itemSubmenuIcon{display:table-cell;text-align:center}.jupyter-wrapper .p-Menu-itemLabel,.jupyter-wrapper .lm-Menu-itemLabel{display:table-cell;text-align:left}.jupyter-wrapper .p-Menu-itemShortcut,.jupyter-wrapper .lm-Menu-itemShortcut{display:table-cell;text-align:right}.jupyter-wrapper .p-MenuBar,.jupyter-wrapper .lm-MenuBar{outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-MenuBar-content,.jupyter-wrapper .lm-MenuBar-content{margin:0;padding:0;display:flex;flex-direction:row;list-style-type:none}.jupyter-wrapper .p--MenuBar-item,.jupyter-wrapper .lm-MenuBar-item{box-sizing:border-box}.jupyter-wrapper .p-MenuBar-itemIcon,.jupyter-wrapper .p-MenuBar-itemLabel,.jupyter-wrapper .lm-MenuBar-itemIcon,.jupyter-wrapper .lm-MenuBar-itemLabel{display:inline-block}.jupyter-wrapper .p-ScrollBar,.jupyter-wrapper .lm-ScrollBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-ScrollBar[data-orientation=horizontal],.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-ScrollBar[data-orientation=vertical],.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-ScrollBar-button,.jupyter-wrapper .lm-ScrollBar-button{box-sizing:border-box;flex:0 0 auto}.jupyter-wrapper .p-ScrollBar-track,.jupyter-wrapper .lm-ScrollBar-track{box-sizing:border-box;position:relative;overflow:hidden;flex:1 1 auto}.jupyter-wrapper .p-ScrollBar-thumb,.jupyter-wrapper .lm-ScrollBar-thumb{box-sizing:border-box;position:absolute}.jupyter-wrapper .p-SplitPanel-child,.jupyter-wrapper .lm-SplitPanel-child{z-index:0}.jupyter-wrapper .p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel-handle{z-index:1}.jupyter-wrapper .p-SplitPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-SplitPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle{cursor:ew-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle{cursor:ns-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-TabBar,.jupyter-wrapper .lm-TabBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal],.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical],.jupyter-wrapper .lm-TabBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-TabBar-content,.jupyter-wrapper .lm-TabBar-content{margin:0;padding:0;display:flex;flex:1 1 auto;list-style-type:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]>.lm-TabBar-content{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=vertical]>.lm-TabBar-content{flex-direction:column}.jupyter-wrapper .p-TabBar-tab,.jupyter-wrapper .lm-TabBar-tab{display:flex;flex-direction:row;box-sizing:border-box;overflow:hidden}.jupyter-wrapper .p-TabBar-tabIcon,.jupyter-wrapper .p-TabBar-tabCloseIcon,.jupyter-wrapper .lm-TabBar-tabIcon,.jupyter-wrapper .lm-TabBar-tabCloseIcon{flex:0 0 auto}.jupyter-wrapper .p-TabBar-tabLabel,.jupyter-wrapper .lm-TabBar-tabLabel{flex:1 1 auto;overflow:hidden;white-space:nowrap}.jupyter-wrapper .p-TabBar-tab.p-mod-hidden,.jupyter-wrapper .lm-TabBar-tab.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging .lm-TabBar-tab{position:relative}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=horizontal] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=horizontal] .lm-TabBar-tab{left:0;transition:left 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=vertical] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=vertical] .lm-TabBar-tab{top:0;transition:top 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging{transition:none}.jupyter-wrapper .p-TabPanel-tabBar,.jupyter-wrapper .lm-TabPanel-tabBar{z-index:1}.jupyter-wrapper .p-TabPanel-stackedPanel,.jupyter-wrapper .lm-TabPanel-stackedPanel{z-index:0}.jupyter-wrapper ::-moz-selection{background:rgba(125,188,255,.6)}.jupyter-wrapper ::selection{background:rgba(125,188,255,.6)}.jupyter-wrapper .bp3-heading{color:#182026;font-weight:600;margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-dark .bp3-heading{color:#f5f8fa}.jupyter-wrapper h1.bp3-heading,.jupyter-wrapper .bp3-running-text h1{line-height:40px;font-size:36px}.jupyter-wrapper h2.bp3-heading,.jupyter-wrapper .bp3-running-text h2{line-height:32px;font-size:28px}.jupyter-wrapper h3.bp3-heading,.jupyter-wrapper .bp3-running-text h3{line-height:25px;font-size:22px}.jupyter-wrapper h4.bp3-heading,.jupyter-wrapper .bp3-running-text h4{line-height:21px;font-size:18px}.jupyter-wrapper h5.bp3-heading,.jupyter-wrapper .bp3-running-text h5{line-height:19px;font-size:16px}.jupyter-wrapper h6.bp3-heading,.jupyter-wrapper .bp3-running-text h6{line-height:16px;font-size:14px}.jupyter-wrapper .bp3-ui-text{text-transform:none;line-height:1.28581;letter-spacing:0;font-size:14px;font-weight:400}.jupyter-wrapper .bp3-monospace-text{text-transform:none;font-family:monospace}.jupyter-wrapper .bp3-text-muted{color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-text-muted{color:#a7b6c2}.jupyter-wrapper .bp3-text-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-text-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal}.jupyter-wrapper .bp3-running-text{line-height:1.5;font-size:14px}.jupyter-wrapper .bp3-running-text h1{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h1{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h2{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h2{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h3{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h3{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h4{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h4{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h5{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h5{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h6{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h6{color:#f5f8fa}.jupyter-wrapper .bp3-running-text hr{margin:20px 0;border:none;border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text hr{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-running-text p{margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-text-large{font-size:16px}.jupyter-wrapper .bp3-text-small{font-size:12px}.jupyter-wrapper a{text-decoration:none;color:#106ba3}.jupyter-wrapper a:hover{cursor:pointer;text-decoration:underline;color:#106ba3}.jupyter-wrapper a .bp3-icon,.jupyter-wrapper a .bp3-icon-standard,.jupyter-wrapper a .bp3-icon-large{color:inherit}.jupyter-wrapper a code,.jupyter-wrapper .bp3-dark a code{color:inherit}.jupyter-wrapper .bp3-dark a,.jupyter-wrapper .bp3-dark a:hover{color:#48aff0}.jupyter-wrapper .bp3-dark a .bp3-icon,.jupyter-wrapper .bp3-dark a .bp3-icon-standard,.jupyter-wrapper .bp3-dark a .bp3-icon-large,.jupyter-wrapper .bp3-dark a:hover .bp3-icon,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-standard,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-large{color:inherit}.jupyter-wrapper .bp3-running-text code,.jupyter-wrapper .bp3-code{text-transform:none;font-family:monospace;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);background:rgba(255,255,255,.7);padding:2px 5px;color:#5c7080;font-size:smaller}.jupyter-wrapper .bp3-dark .bp3-running-text code,.jupyter-wrapper .bp3-running-text .bp3-dark code,.jupyter-wrapper .bp3-dark .bp3-code{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#a7b6c2}.jupyter-wrapper .bp3-running-text a>code,.jupyter-wrapper a>.bp3-code{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-running-text a>code,.jupyter-wrapper .bp3-running-text .bp3-dark a>code,.jupyter-wrapper .bp3-dark a>.bp3-code{color:inherit}.jupyter-wrapper .bp3-running-text pre,.jupyter-wrapper .bp3-code-block{text-transform:none;font-family:monospace;display:block;margin:10px 0;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);background:rgba(255,255,255,.7);padding:13px 15px 12px;line-height:1.4;color:#182026;font-size:13px;word-break:break-all;word-wrap:break-word}.jupyter-wrapper .bp3-dark .bp3-running-text pre,.jupyter-wrapper .bp3-running-text .bp3-dark pre,.jupyter-wrapper .bp3-dark .bp3-code-block{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-running-text pre>code,.jupyter-wrapper .bp3-code-block>code{-webkit-box-shadow:none;box-shadow:none;background:none;padding:0;color:inherit;font-size:inherit}.jupyter-wrapper .bp3-running-text kbd,.jupyter-wrapper .bp3-key{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background:#fff;min-width:24px;height:24px;padding:3px 6px;vertical-align:middle;line-height:24px;color:#5c7080;font-family:inherit;font-size:12px}.jupyter-wrapper .bp3-running-text kbd .bp3-icon,.jupyter-wrapper .bp3-key .bp3-icon,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-standard,.jupyter-wrapper .bp3-key .bp3-icon-standard,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-large,.jupyter-wrapper .bp3-key .bp3-icon-large{margin-right:5px}.jupyter-wrapper .bp3-dark .bp3-running-text kbd,.jupyter-wrapper .bp3-running-text .bp3-dark kbd,.jupyter-wrapper .bp3-dark .bp3-key{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);background:#394b59;color:#a7b6c2}.jupyter-wrapper .bp3-running-text blockquote,.jupyter-wrapper .bp3-blockquote{margin:0 0 10px;border-left:solid 4px rgba(167,182,194,.5);padding:0 20px}.jupyter-wrapper .bp3-dark .bp3-running-text blockquote,.jupyter-wrapper .bp3-running-text .bp3-dark blockquote,.jupyter-wrapper .bp3-dark .bp3-blockquote{border-color:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-running-text ul,.jupyter-wrapper .bp3-running-text ol,.jupyter-wrapper .bp3-list{margin:10px 0;padding-left:30px}.jupyter-wrapper .bp3-running-text ul li:not(:last-child),.jupyter-wrapper .bp3-running-text ol li:not(:last-child),.jupyter-wrapper .bp3-list li:not(:last-child){margin-bottom:5px}.jupyter-wrapper .bp3-running-text ul ol,.jupyter-wrapper .bp3-running-text ol ol,.jupyter-wrapper .bp3-list ol,.jupyter-wrapper .bp3-running-text ul ul,.jupyter-wrapper .bp3-running-text ol ul,.jupyter-wrapper .bp3-list ul{margin-top:5px}.jupyter-wrapper .bp3-list-unstyled{margin:0;padding:0;list-style:none}.jupyter-wrapper .bp3-list-unstyled li{padding:0}.jupyter-wrapper .bp3-rtl{text-align:right}.jupyter-wrapper .bp3-dark{color:#f5f8fa}.jupyter-wrapper :focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-focus-disabled :focus{outline:none !important}.jupyter-wrapper .bp3-focus-disabled :focus~.bp3-control-indicator{outline:none !important}.jupyter-wrapper .bp3-alert{max-width:400px;padding:20px}.jupyter-wrapper .bp3-alert-body{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-alert-body .bp3-icon{margin-top:0;margin-right:20px;font-size:40px}.jupyter-wrapper .bp3-alert-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;margin-top:10px}.jupyter-wrapper .bp3-alert-footer .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-breadcrumbs{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;cursor:default;height:30px;padding:0;list-style:none}.jupyter-wrapper .bp3-breadcrumbs>li{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-breadcrumbs>li::after{display:block;margin:0 5px;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\");width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs>li:last-of-type::after{display:none}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumb-current,.jupyter-wrapper .bp3-breadcrumbs-collapsed{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumbs-collapsed{color:#5c7080}.jupyter-wrapper .bp3-breadcrumb:hover{text-decoration:none}.jupyter-wrapper .bp3-breadcrumb.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-breadcrumb .bp3-icon{margin-right:5px}.jupyter-wrapper .bp3-breadcrumb-current{color:inherit;font-weight:600}.jupyter-wrapper .bp3-breadcrumb-current .bp3-input{vertical-align:baseline;font-size:inherit;font-weight:inherit}.jupyter-wrapper .bp3-breadcrumbs-collapsed{margin-right:2px;border:none;border-radius:3px;background:#ced9e0;cursor:pointer;padding:1px 5px;vertical-align:text-bottom}.jupyter-wrapper .bp3-breadcrumbs-collapsed::before{display:block;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat;width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs-collapsed:hover{background:#bfccd6;text-decoration:none;color:#182026}.jupyter-wrapper .bp3-dark .bp3-breadcrumb,.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs>li::after{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumb.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-breadcrumb-current{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed:hover{background:rgba(16,22,26,.6);color:#f5f8fa}.jupyter-wrapper .bp3-button{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;min-width:30px;min-height:30px}.jupyter-wrapper .bp3-button>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-button>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-button::before,.jupyter-wrapper .bp3-button>*{margin-right:7px}.jupyter-wrapper .bp3-button:empty::before,.jupyter-wrapper .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button:empty{padding:0 !important}.jupyter-wrapper .bp3-button:disabled,.jupyter-wrapper .bp3-button.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-button.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button.bp3-align-right,.jupyter-wrapper .bp3-align-right .bp3-button{text-align:right}.jupyter-wrapper .bp3-button.bp3-align-left,.jupyter-wrapper .bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active:hover,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-button.bp3-intent-primary{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-success{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0f9960;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0d8050}.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0a6640;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(15,153,96,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-warning{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#d9822b;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#bf7326}.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a66321;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(217,130,43,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-danger{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#db3737;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#c23030}.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a82a2a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(219,55,55,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#fff}.jupyter-wrapper .bp3-button.bp3-large,.jupyter-wrapper .bp3-large .bp3-button{min-width:40px;min-height:40px;padding:5px 15px;font-size:16px}.jupyter-wrapper .bp3-button.bp3-large::before,.jupyter-wrapper .bp3-button.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-button::before,.jupyter-wrapper .bp3-large .bp3-button>*{margin-right:10px}.jupyter-wrapper .bp3-button.bp3-large:empty::before,.jupyter-wrapper .bp3-button.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-button:empty::before,.jupyter-wrapper .bp3-large .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button.bp3-small,.jupyter-wrapper .bp3-small .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-button.bp3-loading{position:relative}.jupyter-wrapper .bp3-button.bp3-loading[class*=bp3-icon-]::before{visibility:hidden}.jupyter-wrapper .bp3-button.bp3-loading .bp3-button-spinner{position:absolute;margin:0}.jupyter-wrapper .bp3-button.bp3-loading>:not(.bp3-button-spinner){visibility:hidden}.jupyter-wrapper .bp3-button[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon,.jupyter-wrapper .bp3-button .bp3-icon-standard,.jupyter-wrapper .bp3-button .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-standard.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-large.bp3-align-right{margin-left:7px}.jupyter-wrapper .bp3-button .bp3-icon:first-child:last-child,.jupyter-wrapper .bp3-button .bp3-spinner+.bp3-icon:last-child{margin:0 -7px}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-])[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-image:none;color:rgba(255,255,255,.3)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-button:disabled::before,.jupyter-wrapper .bp3-button:disabled .bp3-icon,.jupyter-wrapper .bp3-button:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button:disabled .bp3-icon-large,.jupyter-wrapper .bp3-button.bp3-disabled::before,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-large,.jupyter-wrapper .bp3-button[class*=bp3-intent-]::before,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-standard,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-large{color:inherit !important}.jupyter-wrapper .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button.bp3-minimal:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper a.bp3-button{text-align:center;text-decoration:none;-webkit-transition:none;transition:none}.jupyter-wrapper a.bp3-button,.jupyter-wrapper a.bp3-button:hover,.jupyter-wrapper a.bp3-button:active{color:#182026}.jupyter-wrapper a.bp3-button.bp3-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-text{-webkit-box-flex:0;-ms-flex:0 1 auto;flex:0 1 auto}.jupyter-wrapper .bp3-button.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button.bp3-align-right .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-right .bp3-button-text{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.jupyter-wrapper .bp3-button-group .bp3-button{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;z-index:4}.jupyter-wrapper .bp3-button-group .bp3-button:focus{z-index:5}.jupyter-wrapper .bp3-button-group .bp3-button:hover{z-index:6}.jupyter-wrapper .bp3-button-group .bp3-button:active,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-active{z-index:7}.jupyter-wrapper .bp3-button-group .bp3-button:disabled,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]{z-index:9}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:focus{z-index:10}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:hover{z-index:11}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-active{z-index:12}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:first-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:-1px;border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-button-group .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button-group .bp3-button.bp3-fill,.jupyter-wrapper .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;vertical-align:top}.jupyter-wrapper .bp3-button-group.bp3-vertical.bp3-fill{width:unset;height:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical .bp3-button{margin-right:0 !important;width:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:first-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:first-child{border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:last-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-bottom:-1px}.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:1px}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-button:not(:last-child){margin-bottom:1px}.jupyter-wrapper .bp3-callout{line-height:1.5;font-size:14px;position:relative;border-radius:3px;background-color:rgba(138,155,168,.15);width:100%;padding:10px 12px 9px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]{padding-left:40px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout.bp3-callout-icon{padding-left:40px}.jupyter-wrapper .bp3-callout.bp3-callout-icon>.bp3-icon:first-child{position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout .bp3-heading{margin-top:0;margin-bottom:5px;line-height:20px}.jupyter-wrapper .bp3-callout .bp3-heading:last-child{margin-bottom:0}.jupyter-wrapper .bp3-dark .bp3-callout{background-color:rgba(138,155,168,.2)}.jupyter-wrapper .bp3-dark .bp3-callout[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-primary .bp3-heading{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{color:#48aff0}.jupyter-wrapper .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-success .bp3-heading{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{color:#3dcc91}.jupyter-wrapper .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-warning .bp3-heading{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{color:#ffb366}.jupyter-wrapper .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-danger .bp3-heading{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{color:#ff7373}.jupyter-wrapper .bp3-running-text .bp3-callout{margin:20px 0}.jupyter-wrapper .bp3-card{border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#fff;padding:20px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-card.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#30404d}.jupyter-wrapper .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-0.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-1.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-2.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-3.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-4.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);cursor:pointer}.jupyter-wrapper .bp3-card.bp3-interactive:hover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:active{opacity:.9;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);-webkit-transition-duration:0;transition-duration:0}.jupyter-wrapper .bp3-card.bp3-interactive:active.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-collapse{height:0;overflow-y:hidden;-webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body{-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-context-menu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-context-menu-popover-target{position:fixed}.jupyter-wrapper .bp3-divider{margin:5px;border-right:1px solid rgba(16,22,26,.15);border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-divider{border-color:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dialog-container{opacity:1;-webkit-transform:scale(1);transform:scale(1);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;min-height:100%;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter-active>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear-active>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit-active>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:30px 0;border-radius:6px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#ebf1f5;width:500px;padding-bottom:20px;pointer-events:all;-webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text}.jupyter-wrapper .bp3-dialog:focus{outline:0}.jupyter-wrapper .bp3-dialog.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-dialog{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#293742;color:#f5f8fa}.jupyter-wrapper .bp3-dialog-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:6px 6px 0 0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);background:#fff;min-height:40px;padding-right:5px;padding-left:20px}.jupyter-wrapper .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dialog-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-dialog-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-dialog-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-dialog-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4);background:#30404d}.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dialog-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:20px;line-height:18px}.jupyter-wrapper .bp3-dialog-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin:0 20px}.jupyter-wrapper .bp3-dialog-footer-actions{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end}.jupyter-wrapper .bp3-dialog-footer-actions .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-drawer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#fff;padding:0}.jupyter-wrapper .bp3-drawer:focus{outline:0}.jupyter-wrapper .bp3-drawer.bp3-position-top{top:0;right:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{-webkit-transform:translateY(-100%);transform:translateY(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left{top:0;bottom:0;left:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear{-webkit-transform:translateX(-100%);transform:translateX(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{-webkit-transform:translateX(-100%);transform:translateX(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right{top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical){top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-drawer{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-drawer-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border-radius:0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);min-height:40px;padding:5px;padding-left:20px}.jupyter-wrapper .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-drawer-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-drawer-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-drawer-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-drawer-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-drawer-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;overflow:auto;line-height:18px}.jupyter-wrapper .bp3-drawer-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);padding:10px 20px}.jupyter-wrapper .bp3-dark .bp3-drawer-footer{-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.4);box-shadow:inset 0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text{display:inline-block;position:relative;cursor:text;max-width:100%;vertical-align:top;white-space:nowrap}.jupyter-wrapper .bp3-editable-text::before{position:absolute;top:-3px;right:-3px;bottom:-3px;left:-3px;border-radius:3px;content:\"\";-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#137cbd}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#0f9960}.jupyter-wrapper .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#d9822b}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#db3737}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4);box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4);box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4);box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4);box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text-content{display:inherit;position:relative;min-width:inherit;max-width:inherit;vertical-align:top;text-transform:inherit;letter-spacing:inherit;color:inherit;font:inherit;resize:none}.jupyter-wrapper .bp3-editable-text-input{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;width:100%;padding:0;white-space:pre-wrap}.jupyter-wrapper .bp3-editable-text-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:focus{outline:none}.jupyter-wrapper .bp3-editable-text-input::-ms-clear{display:none}.jupyter-wrapper .bp3-editable-text-content{overflow:hidden;padding-right:2px;text-overflow:ellipsis;white-space:pre}.jupyter-wrapper .bp3-editable-text-editing>.bp3-editable-text-content{position:absolute;left:0;visibility:hidden}.jupyter-wrapper .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-editable-text.bp3-multiline{display:block}.jupyter-wrapper .bp3-editable-text.bp3-multiline .bp3-editable-text-content{overflow:auto;white-space:pre-wrap;word-wrap:break-word}.jupyter-wrapper .bp3-control-group{-webkit-transform:translateZ(0);transform:translateZ(0);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-control-group>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select,.jupyter-wrapper .bp3-control-group .bp3-input,.jupyter-wrapper .bp3-control-group .bp3-select{position:relative}.jupyter-wrapper .bp3-control-group .bp3-input{z-index:2;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-input:focus{z-index:14;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-input[readonly],.jupyter-wrapper .bp3-control-group .bp3-input:disabled,.jupyter-wrapper .bp3-control-group .bp3-input.bp3-disabled{z-index:1}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select select,.jupyter-wrapper .bp3-control-group .bp3-select select{-webkit-transform:translateZ(0);transform:translateZ(0);z-index:4;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-button:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select:focus,.jupyter-wrapper .bp3-control-group .bp3-select select:focus{z-index:5}.jupyter-wrapper .bp3-control-group .bp3-button:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select:hover,.jupyter-wrapper .bp3-control-group .bp3-select select:hover{z-index:6}.jupyter-wrapper .bp3-control-group .bp3-button:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select:active,.jupyter-wrapper .bp3-control-group .bp3-select select:active{z-index:7}.jupyter-wrapper .bp3-control-group .bp3-button[readonly],.jupyter-wrapper .bp3-control-group .bp3-button:disabled,.jupyter-wrapper .bp3-control-group .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]{z-index:9}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:focus{z-index:10}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:hover{z-index:11}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:active{z-index:12}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-input-action{z-index:16}.jupyter-wrapper .bp3-control-group .bp3-select::after,.jupyter-wrapper .bp3-control-group .bp3-html-select::after,.jupyter-wrapper .bp3-control-group .bp3-select>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-html-select>.bp3-icon{z-index:17}.jupyter-wrapper .bp3-control-group:not(.bp3-vertical)>*{margin-right:-1px}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>*{margin-right:0}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>.bp3-button+.bp3-button{margin-left:1px}.jupyter-wrapper .bp3-control-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-control-group .bp3-popover-target{border-radius:inherit}.jupyter-wrapper .bp3-control-group>:first-child{border-radius:3px 0 0 3px}.jupyter-wrapper .bp3-control-group>:last-child{margin-right:0;border-radius:0 3px 3px 0}.jupyter-wrapper .bp3-control-group>:only-child{margin-right:0;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input-group .bp3-button{border-radius:3px}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-fill>*:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.jupyter-wrapper .bp3-control-group.bp3-vertical>*{margin-top:-1px}.jupyter-wrapper .bp3-control-group.bp3-vertical>:first-child{margin-top:0;border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-control-group.bp3-vertical>:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-control{display:block;position:relative;margin-bottom:10px;cursor:pointer;text-transform:none}.jupyter-wrapper .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control:not(.bp3-align-right){padding-left:26px}.jupyter-wrapper .bp3-control:not(.bp3-align-right) .bp3-control-indicator{margin-left:-26px}.jupyter-wrapper .bp3-control.bp3-align-right{padding-right:26px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{margin-right:-26px}.jupyter-wrapper .bp3-control.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-control.bp3-inline{display:inline-block;margin-right:20px}.jupyter-wrapper .bp3-control input{position:absolute;top:0;left:0;opacity:0;z-index:-1}.jupyter-wrapper .bp3-control .bp3-control-indicator{display:inline-block;position:relative;margin-top:-3px;margin-right:10px;border:none;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));cursor:pointer;width:1em;height:1em;vertical-align:middle;font-size:16px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-control .bp3-control-indicator::before{display:block;width:1em;height:1em;content:\"\"}.jupyter-wrapper .bp3-control:hover .bp3-control-indicator{background-color:#ebf1f5}.jupyter-wrapper .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background:#d8e1e8}.jupyter-wrapper .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed}.jupyter-wrapper .bp3-control input:focus~.bp3-control-indicator{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{float:right;margin-top:1px;margin-left:10px}.jupyter-wrapper .bp3-control.bp3-large{font-size:16px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right){padding-left:30px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right{padding-right:30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-30px}.jupyter-wrapper .bp3-control.bp3-large .bp3-control-indicator{font-size:20px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-top:0}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control.bp3-checkbox .bp3-control-indicator{border-radius:3px}.jupyter-wrapper .bp3-control.bp3-checkbox input:checked~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-radio .bp3-control-indicator{border-radius:50%}.jupyter-wrapper .bp3-control.bp3-radio input:checked~.bp3-control-indicator::before{background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%)}.jupyter-wrapper .bp3-control.bp3-radio input:checked:disabled~.bp3-control-indicator::before{opacity:.5}.jupyter-wrapper .bp3-control.bp3-radio input:focus~.bp3-control-indicator{-moz-outline-radius:16px}.jupyter-wrapper .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(167,182,194,.5)}.jupyter-wrapper .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(92,112,128,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(206,217,224,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right){padding-left:38px}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{margin-left:-38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right{padding-right:38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{margin-right:-38px}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator{border:none;border-radius:1.75em;-webkit-box-shadow:none !important;box-shadow:none !important;width:auto;min-width:1.75em;-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator::before{position:absolute;left:0;margin:2px;border-radius:50%;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);background:#fff;width:calc(1em - 4px);height:calc(1em - 4px);-webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{left:calc(100% - 1em)}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){padding-left:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right{padding-right:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-45px}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(16,22,26,.7)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(16,22,26,.9)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(57,75,89,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background:#394b59}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-switch-inner-text{text-align:center;font-size:.7em}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{visibility:hidden;margin-right:1.2em;margin-left:.5em;line-height:0}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{visibility:visible;margin-right:.5em;margin-left:1.2em;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:first-child{visibility:visible;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:last-child{visibility:hidden;line-height:0}.jupyter-wrapper .bp3-dark .bp3-control{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-control.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-control .bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0))}.jupyter-wrapper .bp3-dark .bp3-control:hover .bp3-control-indicator{background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background:#202b33}.jupyter-wrapper .bp3-dark .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);cursor:not-allowed}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked~.bp3-control-indicator,.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-file-input{display:inline-block;position:relative;cursor:pointer;height:30px}.jupyter-wrapper .bp3-file-input input{opacity:0;margin:0;min-width:200px}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active:hover,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#182026}.jupyter-wrapper .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#f5f8fa}.jupyter-wrapper .bp3-file-input.bp3-fill{width:100%}.jupyter-wrapper .bp3-file-input.bp3-large,.jupyter-wrapper .bp3-large .bp3-file-input{height:40px}.jupyter-wrapper .bp3-file-input .bp3-file-upload-input-custom-text::after{content:attr(bp3-button-text)}.jupyter-wrapper .bp3-file-upload-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;left:0;padding-right:80px;color:rgba(92,112,128,.6);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-file-upload-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:focus,.jupyter-wrapper .bp3-file-upload-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-file-upload-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;min-width:24px;min-height:24px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;margin:3px;border-radius:3px;width:70px;text-align:center;line-height:24px;content:\"Browse\"}.jupyter-wrapper .bp3-file-upload-input::after:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active:hover,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-file-upload-input:hover::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input:active::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-large .bp3-file-upload-input{height:40px;line-height:40px;font-size:16px;padding-right:95px}.jupyter-wrapper .bp3-large .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-large .bp3-file-upload-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-large .bp3-file-upload-input::after{min-width:30px;min-height:30px;margin:5px;width:85px;line-height:30px}.jupyter-wrapper .bp3-dark .bp3-file-upload-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:hover::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:active::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1)}.jupyter-wrapper .bp3-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0 0 15px}.jupyter-wrapper .bp3-form-group label.bp3-label{margin-bottom:5px}.jupyter-wrapper .bp3-form-group .bp3-control{margin-top:7px}.jupyter-wrapper .bp3-form-group .bp3-form-helper-text{margin-top:5px;color:#5c7080;font-size:12px}.jupyter-wrapper .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#106ba3}.jupyter-wrapper .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#0d8050}.jupyter-wrapper .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#bf7326}.jupyter-wrapper .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#c23030}.jupyter-wrapper .bp3-form-group.bp3-inline{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-form-group.bp3-inline.bp3-large label.bp3-label{margin:0 10px 0 0;line-height:40px}.jupyter-wrapper .bp3-form-group.bp3-inline label.bp3-label{margin:0 10px 0 0;line-height:30px}.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-form-group .bp3-form-helper-text{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-input-group{display:block;position:relative}.jupyter-wrapper .bp3-input-group .bp3-input{position:relative;width:100%}.jupyter-wrapper .bp3-input-group .bp3-input:not(:first-child){padding-left:30px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:last-child){padding-right:30px}.jupyter-wrapper .bp3-input-group .bp3-input-action,.jupyter-wrapper .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-input-group>.bp3-icon{position:absolute;top:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:first-child,.jupyter-wrapper .bp3-input-group>.bp3-button:first-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:first-child{left:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:last-child,.jupyter-wrapper .bp3-input-group>.bp3-button:last-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:last-child{right:0}.jupyter-wrapper .bp3-input-group .bp3-button{min-width:24px;min-height:24px;margin:3px;padding:0 7px}.jupyter-wrapper .bp3-input-group .bp3-button:empty{padding:0}.jupyter-wrapper .bp3-input-group>.bp3-icon{z-index:1;color:#5c7080}.jupyter-wrapper .bp3-input-group>.bp3-icon:empty{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input-action>.bp3-spinner{margin:7px}.jupyter-wrapper .bp3-input-group .bp3-tag{margin:5px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#a7b6c2}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-button{min-width:30px;min-height:30px;margin:5px}.jupyter-wrapper .bp3-input-group.bp3-large>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input-action>.bp3-spinner{margin:12px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:first-child){padding-left:40px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:last-child){padding-right:40px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-button{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-tag{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input-action>.bp3-spinner{margin:4px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:first-child){padding-left:24px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:last-child){padding-right:24px}.jupyter-wrapper .bp3-input-group.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-input-group.bp3-round .bp3-button,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-input,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-tag{border-radius:30px}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#48aff0}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-success>.bp3-icon{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-success>.bp3-icon{color:#3dcc91}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#ffb366}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#ff7373}.jupyter-wrapper .bp3-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none}.jupyter-wrapper .bp3-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:focus,.jupyter-wrapper .bp3-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input[type=search],.jupyter-wrapper .bp3-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-input:disabled,.jupyter-wrapper .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-input.bp3-large{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input.bp3-large[type=search],.jupyter-wrapper .bp3-input.bp3-large.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input.bp3-small{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input.bp3-small[type=search],.jupyter-wrapper .bp3-input.bp3-small.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-dark .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input::-ms-clear{display:none}.jupyter-wrapper textarea.bp3-input{max-width:100%;padding:10px}.jupyter-wrapper textarea.bp3-input,.jupyter-wrapper textarea.bp3-input.bp3-large,.jupyter-wrapper textarea.bp3-input.bp3-small{height:auto;line-height:inherit}.jupyter-wrapper textarea.bp3-input.bp3-small{padding:8px}.jupyter-wrapper .bp3-dark textarea.bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark textarea.bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input:disabled,.jupyter-wrapper .bp3-dark textarea.bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper label.bp3-label{display:block;margin-top:0;margin-bottom:15px}.jupyter-wrapper label.bp3-label .bp3-html-select,.jupyter-wrapper label.bp3-label .bp3-input,.jupyter-wrapper label.bp3-label .bp3-select,.jupyter-wrapper label.bp3-label .bp3-slider,.jupyter-wrapper label.bp3-label .bp3-popover-wrapper{display:block;margin-top:5px;text-transform:none}.jupyter-wrapper label.bp3-label .bp3-button-group{margin-top:5px}.jupyter-wrapper label.bp3-label .bp3-select select,.jupyter-wrapper label.bp3-label .bp3-html-select select{width:100%;vertical-align:top;font-weight:400}.jupyter-wrapper label.bp3-label.bp3-disabled,.jupyter-wrapper label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(92,112,128,.6)}.jupyter-wrapper label.bp3-label.bp3-inline{line-height:30px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-html-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-popover-wrapper{display:inline-block;margin:0 0 0 5px;vertical-align:top}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-button-group{margin:0 0 0 5px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group .bp3-input{margin-left:0}.jupyter-wrapper label.bp3-label.bp3-inline.bp3-large{line-height:40px}.jupyter-wrapper label.bp3-label:not(.bp3-inline) .bp3-popover-target{display:block}.jupyter-wrapper .bp3-dark label.bp3-label{color:#f5f8fa}.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled,.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button{-webkit-box-flex:1;-ms-flex:1 1 14px;flex:1 1 14px;width:30px;min-height:0;padding:0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:first-child{border-radius:0 3px 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:last-child{border-radius:0 0 3px 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:first-child{border-radius:3px 0 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:last-child{border-radius:0 0 0 3px}.jupyter-wrapper .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical>.bp3-button{width:40px}.jupyter-wrapper form{display:block}.jupyter-wrapper .bp3-html-select select,.jupyter-wrapper .bp3-select select{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;border-radius:3px;width:100%;height:30px;padding:0 25px 0 10px;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-html-select select>.bp3-fill,.jupyter-wrapper .bp3-select select>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-html-select select::before,.jupyter-wrapper .bp3-select select::before,.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{margin-right:7px}.jupyter-wrapper .bp3-html-select select:empty::before,.jupyter-wrapper .bp3-select select:empty::before,.jupyter-wrapper .bp3-html-select select>:last-child,.jupyter-wrapper .bp3-select select>:last-child{margin-right:0}.jupyter-wrapper .bp3-html-select select:hover,.jupyter-wrapper .bp3-select select:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-html-select select:active,.jupyter-wrapper .bp3-select select:active,.jupyter-wrapper .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-select select.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled,.jupyter-wrapper .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-select select.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal select{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-large select,.jupyter-wrapper .bp3-select.bp3-large select{height:40px;padding-right:35px;font-size:16px}.jupyter-wrapper .bp3-dark .bp3-html-select select,.jupyter-wrapper .bp3-dark .bp3-select select{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon,.jupyter-wrapper .bp3-select::after{position:absolute;top:7px;right:7px;color:#5c7080;pointer-events:none}.jupyter-wrapper .bp3-html-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-disabled.bp3-select::after{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select,.jupyter-wrapper .bp3-select{display:inline-block;position:relative;vertical-align:middle;letter-spacing:normal}.jupyter-wrapper .bp3-html-select select::-ms-expand,.jupyter-wrapper .bp3-select select::-ms-expand{display:none}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon{color:#5c7080}.jupyter-wrapper .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-select .bp3-icon:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon:hover{color:#f5f8fa}.jupyter-wrapper .bp3-html-select.bp3-large::after,.jupyter-wrapper .bp3-html-select.bp3-large .bp3-icon,.jupyter-wrapper .bp3-select.bp3-large::after,.jupyter-wrapper .bp3-select.bp3-large .bp3-icon{top:12px;right:12px}.jupyter-wrapper .bp3-html-select.bp3-fill,.jupyter-wrapper .bp3-html-select.bp3-fill select,.jupyter-wrapper .bp3-select.bp3-fill,.jupyter-wrapper .bp3-select.bp3-fill select{width:100%}.jupyter-wrapper .bp3-dark .bp3-html-select option,.jupyter-wrapper .bp3-dark .bp3-select option{background-color:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select::after,.jupyter-wrapper .bp3-dark .bp3-select::after{color:#a7b6c2}.jupyter-wrapper .bp3-select::after{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6c6\"}.jupyter-wrapper .bp3-running-text table,.jupyter-wrapper table.bp3-html-table{border-spacing:0;font-size:14px}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th,.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{padding:11px;vertical-align:top;text-align:left}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th{color:#182026;font-weight:600}.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{color:#182026}.jupyter-wrapper .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text table th,.jupyter-wrapper .bp3-running-text .bp3-dark table th,.jupyter-wrapper .bp3-dark table.bp3-html-table th{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table td,.jupyter-wrapper .bp3-running-text .bp3-dark table td,.jupyter-wrapper .bp3-dark table.bp3-html-table td{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child th,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child td,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed th,.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed td,.jupyter-wrapper table.bp3-html-table.bp3-small th,.jupyter-wrapper table.bp3-html-table.bp3-small td{padding-top:6px;padding-bottom:6px}.jupyter-wrapper table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(191,204,214,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(191,204,214,.3);cursor:pointer}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(92,112,128,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(92,112,128,.3);cursor:pointer}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-key-combo{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-key-combo>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-key-combo>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-key-combo::before,.jupyter-wrapper .bp3-key-combo>*{margin-right:5px}.jupyter-wrapper .bp3-key-combo:empty::before,.jupyter-wrapper .bp3-key-combo>:last-child{margin-right:0}.jupyter-wrapper .bp3-hotkey-dialog{top:40px;padding-bottom:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-dialog-body{margin:0;padding:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-hotkey-label{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.jupyter-wrapper .bp3-hotkey-column{margin:auto;max-height:80vh;overflow-y:auto;padding:30px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading{margin-bottom:20px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading:not(:first-child){margin-top:40px}.jupyter-wrapper .bp3-hotkey{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;margin-right:0;margin-left:0}.jupyter-wrapper .bp3-hotkey:not(:last-child){margin-bottom:10px}.jupyter-wrapper .bp3-icon{display:inline-block;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;vertical-align:text-bottom}.jupyter-wrapper .bp3-icon:not(:empty)::before{content:\"\" !important;content:unset !important}.jupyter-wrapper .bp3-icon>svg{display:block}.jupyter-wrapper .bp3-icon>svg:not([fill]){fill:currentColor}.jupyter-wrapper .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-icon-large.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-icon-large.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-icon-large.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-icon-large.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-danger{color:#ff7373}.jupyter-wrapper span.bp3-icon-standard{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon-large{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon:empty{line-height:1;font-family:\"Icons20\";font-size:inherit;font-weight:400;font-style:normal}.jupyter-wrapper span.bp3-icon:empty::before{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-icon-add::before{content:\"\ue63e\"}.jupyter-wrapper .bp3-icon-add-column-left::before{content:\"\ue6f9\"}.jupyter-wrapper .bp3-icon-add-column-right::before{content:\"\ue6fa\"}.jupyter-wrapper .bp3-icon-add-row-bottom::before{content:\"\ue6f8\"}.jupyter-wrapper .bp3-icon-add-row-top::before{content:\"\ue6f7\"}.jupyter-wrapper .bp3-icon-add-to-artifact::before{content:\"\ue67c\"}.jupyter-wrapper .bp3-icon-add-to-folder::before{content:\"\ue6d2\"}.jupyter-wrapper .bp3-icon-airplane::before{content:\"\ue74b\"}.jupyter-wrapper .bp3-icon-align-center::before{content:\"\ue603\"}.jupyter-wrapper .bp3-icon-align-justify::before{content:\"\ue605\"}.jupyter-wrapper .bp3-icon-align-left::before{content:\"\ue602\"}.jupyter-wrapper .bp3-icon-align-right::before{content:\"\ue604\"}.jupyter-wrapper .bp3-icon-alignment-bottom::before{content:\"\ue727\"}.jupyter-wrapper .bp3-icon-alignment-horizontal-center::before{content:\"\ue726\"}.jupyter-wrapper .bp3-icon-alignment-left::before{content:\"\ue722\"}.jupyter-wrapper .bp3-icon-alignment-right::before{content:\"\ue724\"}.jupyter-wrapper .bp3-icon-alignment-top::before{content:\"\ue725\"}.jupyter-wrapper .bp3-icon-alignment-vertical-center::before{content:\"\ue723\"}.jupyter-wrapper .bp3-icon-annotation::before{content:\"\ue6f0\"}.jupyter-wrapper .bp3-icon-application::before{content:\"\ue735\"}.jupyter-wrapper .bp3-icon-applications::before{content:\"\ue621\"}.jupyter-wrapper .bp3-icon-archive::before{content:\"\ue907\"}.jupyter-wrapper .bp3-icon-arrow-bottom-left::before{content:\"\u2199\"}.jupyter-wrapper .bp3-icon-arrow-bottom-right::before{content:\"\u2198\"}.jupyter-wrapper .bp3-icon-arrow-down::before{content:\"\u2193\"}.jupyter-wrapper .bp3-icon-arrow-left::before{content:\"\u2190\"}.jupyter-wrapper .bp3-icon-arrow-right::before{content:\"\u2192\"}.jupyter-wrapper .bp3-icon-arrow-top-left::before{content:\"\u2196\"}.jupyter-wrapper .bp3-icon-arrow-top-right::before{content:\"\u2197\"}.jupyter-wrapper .bp3-icon-arrow-up::before{content:\"\u2191\"}.jupyter-wrapper .bp3-icon-arrows-horizontal::before{content:\"\u2194\"}.jupyter-wrapper .bp3-icon-arrows-vertical::before{content:\"\u2195\"}.jupyter-wrapper .bp3-icon-asterisk::before{content:\"*\"}.jupyter-wrapper .bp3-icon-automatic-updates::before{content:\"\ue65f\"}.jupyter-wrapper .bp3-icon-badge::before{content:\"\ue6e3\"}.jupyter-wrapper .bp3-icon-ban-circle::before{content:\"\ue69d\"}.jupyter-wrapper .bp3-icon-bank-account::before{content:\"\ue76f\"}.jupyter-wrapper .bp3-icon-barcode::before{content:\"\ue676\"}.jupyter-wrapper .bp3-icon-blank::before{content:\"\ue900\"}.jupyter-wrapper .bp3-icon-blocked-person::before{content:\"\ue768\"}.jupyter-wrapper .bp3-icon-bold::before{content:\"\ue606\"}.jupyter-wrapper .bp3-icon-book::before{content:\"\ue6b8\"}.jupyter-wrapper .bp3-icon-bookmark::before{content:\"\ue61a\"}.jupyter-wrapper .bp3-icon-box::before{content:\"\ue6bf\"}.jupyter-wrapper .bp3-icon-briefcase::before{content:\"\ue674\"}.jupyter-wrapper .bp3-icon-bring-data::before{content:\"\ue90a\"}.jupyter-wrapper .bp3-icon-build::before{content:\"\ue72d\"}.jupyter-wrapper .bp3-icon-calculator::before{content:\"\ue70b\"}.jupyter-wrapper .bp3-icon-calendar::before{content:\"\ue62b\"}.jupyter-wrapper .bp3-icon-camera::before{content:\"\ue69e\"}.jupyter-wrapper .bp3-icon-caret-down::before{content:\"\u2304\"}.jupyter-wrapper .bp3-icon-caret-left::before{content:\"\u2329\"}.jupyter-wrapper .bp3-icon-caret-right::before{content:\"\u232a\"}.jupyter-wrapper .bp3-icon-caret-up::before{content:\"\u2303\"}.jupyter-wrapper .bp3-icon-cell-tower::before{content:\"\ue770\"}.jupyter-wrapper .bp3-icon-changes::before{content:\"\ue623\"}.jupyter-wrapper .bp3-icon-chart::before{content:\"\ue67e\"}.jupyter-wrapper .bp3-icon-chat::before{content:\"\ue689\"}.jupyter-wrapper .bp3-icon-chevron-backward::before{content:\"\ue6df\"}.jupyter-wrapper .bp3-icon-chevron-down::before{content:\"\ue697\"}.jupyter-wrapper .bp3-icon-chevron-forward::before{content:\"\ue6e0\"}.jupyter-wrapper .bp3-icon-chevron-left::before{content:\"\ue694\"}.jupyter-wrapper .bp3-icon-chevron-right::before{content:\"\ue695\"}.jupyter-wrapper .bp3-icon-chevron-up::before{content:\"\ue696\"}.jupyter-wrapper .bp3-icon-circle::before{content:\"\ue66a\"}.jupyter-wrapper .bp3-icon-circle-arrow-down::before{content:\"\ue68e\"}.jupyter-wrapper .bp3-icon-circle-arrow-left::before{content:\"\ue68c\"}.jupyter-wrapper .bp3-icon-circle-arrow-right::before{content:\"\ue68b\"}.jupyter-wrapper .bp3-icon-circle-arrow-up::before{content:\"\ue68d\"}.jupyter-wrapper .bp3-icon-citation::before{content:\"\ue61b\"}.jupyter-wrapper .bp3-icon-clean::before{content:\"\ue7c5\"}.jupyter-wrapper .bp3-icon-clipboard::before{content:\"\ue61d\"}.jupyter-wrapper .bp3-icon-cloud::before{content:\"\u2601\"}.jupyter-wrapper .bp3-icon-cloud-download::before{content:\"\ue690\"}.jupyter-wrapper .bp3-icon-cloud-upload::before{content:\"\ue691\"}.jupyter-wrapper .bp3-icon-code::before{content:\"\ue661\"}.jupyter-wrapper .bp3-icon-code-block::before{content:\"\ue6c5\"}.jupyter-wrapper .bp3-icon-cog::before{content:\"\ue645\"}.jupyter-wrapper .bp3-icon-collapse-all::before{content:\"\ue763\"}.jupyter-wrapper .bp3-icon-column-layout::before{content:\"\ue6da\"}.jupyter-wrapper .bp3-icon-comment::before{content:\"\ue68a\"}.jupyter-wrapper .bp3-icon-comparison::before{content:\"\ue637\"}.jupyter-wrapper .bp3-icon-compass::before{content:\"\ue79c\"}.jupyter-wrapper .bp3-icon-compressed::before{content:\"\ue6c0\"}.jupyter-wrapper .bp3-icon-confirm::before{content:\"\ue639\"}.jupyter-wrapper .bp3-icon-console::before{content:\"\ue79b\"}.jupyter-wrapper .bp3-icon-contrast::before{content:\"\ue6cb\"}.jupyter-wrapper .bp3-icon-control::before{content:\"\ue67f\"}.jupyter-wrapper .bp3-icon-credit-card::before{content:\"\ue649\"}.jupyter-wrapper .bp3-icon-cross::before{content:\"\u2717\"}.jupyter-wrapper .bp3-icon-crown::before{content:\"\ue7b4\"}.jupyter-wrapper .bp3-icon-cube::before{content:\"\ue7c8\"}.jupyter-wrapper .bp3-icon-cube-add::before{content:\"\ue7c9\"}.jupyter-wrapper .bp3-icon-cube-remove::before{content:\"\ue7d0\"}.jupyter-wrapper .bp3-icon-curved-range-chart::before{content:\"\ue71b\"}.jupyter-wrapper .bp3-icon-cut::before{content:\"\ue6ef\"}.jupyter-wrapper .bp3-icon-dashboard::before{content:\"\ue751\"}.jupyter-wrapper .bp3-icon-data-lineage::before{content:\"\ue908\"}.jupyter-wrapper .bp3-icon-database::before{content:\"\ue683\"}.jupyter-wrapper .bp3-icon-delete::before{content:\"\ue644\"}.jupyter-wrapper .bp3-icon-delta::before{content:\"\u0394\"}.jupyter-wrapper .bp3-icon-derive-column::before{content:\"\ue739\"}.jupyter-wrapper .bp3-icon-desktop::before{content:\"\ue6af\"}.jupyter-wrapper .bp3-icon-diagram-tree::before{content:\"\ue7b3\"}.jupyter-wrapper .bp3-icon-direction-left::before{content:\"\ue681\"}.jupyter-wrapper .bp3-icon-direction-right::before{content:\"\ue682\"}.jupyter-wrapper .bp3-icon-disable::before{content:\"\ue600\"}.jupyter-wrapper .bp3-icon-document::before{content:\"\ue630\"}.jupyter-wrapper .bp3-icon-document-open::before{content:\"\ue71e\"}.jupyter-wrapper .bp3-icon-document-share::before{content:\"\ue71f\"}.jupyter-wrapper .bp3-icon-dollar::before{content:\"$\"}.jupyter-wrapper .bp3-icon-dot::before{content:\"\u2022\"}.jupyter-wrapper .bp3-icon-double-caret-horizontal::before{content:\"\ue6c7\"}.jupyter-wrapper .bp3-icon-double-caret-vertical::before{content:\"\ue6c6\"}.jupyter-wrapper .bp3-icon-double-chevron-down::before{content:\"\ue703\"}.jupyter-wrapper .bp3-icon-double-chevron-left::before{content:\"\ue6ff\"}.jupyter-wrapper .bp3-icon-double-chevron-right::before{content:\"\ue701\"}.jupyter-wrapper .bp3-icon-double-chevron-up::before{content:\"\ue702\"}.jupyter-wrapper .bp3-icon-doughnut-chart::before{content:\"\ue6ce\"}.jupyter-wrapper .bp3-icon-download::before{content:\"\ue62f\"}.jupyter-wrapper .bp3-icon-drag-handle-horizontal::before{content:\"\ue716\"}.jupyter-wrapper .bp3-icon-drag-handle-vertical::before{content:\"\ue715\"}.jupyter-wrapper .bp3-icon-draw::before{content:\"\ue66b\"}.jupyter-wrapper .bp3-icon-drive-time::before{content:\"\ue615\"}.jupyter-wrapper .bp3-icon-duplicate::before{content:\"\ue69c\"}.jupyter-wrapper .bp3-icon-edit::before{content:\"\u270e\"}.jupyter-wrapper .bp3-icon-eject::before{content:\"\u23cf\"}.jupyter-wrapper .bp3-icon-endorsed::before{content:\"\ue75f\"}.jupyter-wrapper .bp3-icon-envelope::before{content:\"\u2709\"}.jupyter-wrapper .bp3-icon-equals::before{content:\"\ue7d9\"}.jupyter-wrapper .bp3-icon-eraser::before{content:\"\ue773\"}.jupyter-wrapper .bp3-icon-error::before{content:\"\ue648\"}.jupyter-wrapper .bp3-icon-euro::before{content:\"\u20ac\"}.jupyter-wrapper .bp3-icon-exchange::before{content:\"\ue636\"}.jupyter-wrapper .bp3-icon-exclude-row::before{content:\"\ue6ea\"}.jupyter-wrapper .bp3-icon-expand-all::before{content:\"\ue764\"}.jupyter-wrapper .bp3-icon-export::before{content:\"\ue633\"}.jupyter-wrapper .bp3-icon-eye-off::before{content:\"\ue6cc\"}.jupyter-wrapper .bp3-icon-eye-on::before{content:\"\ue75a\"}.jupyter-wrapper .bp3-icon-eye-open::before{content:\"\ue66f\"}.jupyter-wrapper .bp3-icon-fast-backward::before{content:\"\ue6a8\"}.jupyter-wrapper .bp3-icon-fast-forward::before{content:\"\ue6ac\"}.jupyter-wrapper .bp3-icon-feed::before{content:\"\ue656\"}.jupyter-wrapper .bp3-icon-feed-subscribed::before{content:\"\ue78f\"}.jupyter-wrapper .bp3-icon-film::before{content:\"\ue6a1\"}.jupyter-wrapper .bp3-icon-filter::before{content:\"\ue638\"}.jupyter-wrapper .bp3-icon-filter-keep::before{content:\"\ue78c\"}.jupyter-wrapper .bp3-icon-filter-list::before{content:\"\ue6ee\"}.jupyter-wrapper .bp3-icon-filter-open::before{content:\"\ue7d7\"}.jupyter-wrapper .bp3-icon-filter-remove::before{content:\"\ue78d\"}.jupyter-wrapper .bp3-icon-flag::before{content:\"\u2691\"}.jupyter-wrapper .bp3-icon-flame::before{content:\"\ue7a9\"}.jupyter-wrapper .bp3-icon-flash::before{content:\"\ue6b3\"}.jupyter-wrapper .bp3-icon-floppy-disk::before{content:\"\ue6b7\"}.jupyter-wrapper .bp3-icon-flow-branch::before{content:\"\ue7c1\"}.jupyter-wrapper .bp3-icon-flow-end::before{content:\"\ue7c4\"}.jupyter-wrapper .bp3-icon-flow-linear::before{content:\"\ue7c0\"}.jupyter-wrapper .bp3-icon-flow-review::before{content:\"\ue7c2\"}.jupyter-wrapper .bp3-icon-flow-review-branch::before{content:\"\ue7c3\"}.jupyter-wrapper .bp3-icon-flows::before{content:\"\ue659\"}.jupyter-wrapper .bp3-icon-folder-close::before{content:\"\ue652\"}.jupyter-wrapper .bp3-icon-folder-new::before{content:\"\ue7b0\"}.jupyter-wrapper .bp3-icon-folder-open::before{content:\"\ue651\"}.jupyter-wrapper .bp3-icon-folder-shared::before{content:\"\ue653\"}.jupyter-wrapper .bp3-icon-folder-shared-open::before{content:\"\ue670\"}.jupyter-wrapper .bp3-icon-follower::before{content:\"\ue760\"}.jupyter-wrapper .bp3-icon-following::before{content:\"\ue761\"}.jupyter-wrapper .bp3-icon-font::before{content:\"\ue6b4\"}.jupyter-wrapper .bp3-icon-fork::before{content:\"\ue63a\"}.jupyter-wrapper .bp3-icon-form::before{content:\"\ue795\"}.jupyter-wrapper .bp3-icon-full-circle::before{content:\"\ue685\"}.jupyter-wrapper .bp3-icon-full-stacked-chart::before{content:\"\ue75e\"}.jupyter-wrapper .bp3-icon-fullscreen::before{content:\"\ue699\"}.jupyter-wrapper .bp3-icon-function::before{content:\"\ue6e5\"}.jupyter-wrapper .bp3-icon-gantt-chart::before{content:\"\ue6f4\"}.jupyter-wrapper .bp3-icon-geolocation::before{content:\"\ue640\"}.jupyter-wrapper .bp3-icon-geosearch::before{content:\"\ue613\"}.jupyter-wrapper .bp3-icon-git-branch::before{content:\"\ue72a\"}.jupyter-wrapper .bp3-icon-git-commit::before{content:\"\ue72b\"}.jupyter-wrapper .bp3-icon-git-merge::before{content:\"\ue729\"}.jupyter-wrapper .bp3-icon-git-new-branch::before{content:\"\ue749\"}.jupyter-wrapper .bp3-icon-git-pull::before{content:\"\ue728\"}.jupyter-wrapper .bp3-icon-git-push::before{content:\"\ue72c\"}.jupyter-wrapper .bp3-icon-git-repo::before{content:\"\ue748\"}.jupyter-wrapper .bp3-icon-glass::before{content:\"\ue6b1\"}.jupyter-wrapper .bp3-icon-globe::before{content:\"\ue666\"}.jupyter-wrapper .bp3-icon-globe-network::before{content:\"\ue7b5\"}.jupyter-wrapper .bp3-icon-graph::before{content:\"\ue673\"}.jupyter-wrapper .bp3-icon-graph-remove::before{content:\"\ue609\"}.jupyter-wrapper .bp3-icon-greater-than::before{content:\"\ue7e1\"}.jupyter-wrapper .bp3-icon-greater-than-or-equal-to::before{content:\"\ue7e2\"}.jupyter-wrapper .bp3-icon-grid::before{content:\"\ue6d0\"}.jupyter-wrapper .bp3-icon-grid-view::before{content:\"\ue6e4\"}.jupyter-wrapper .bp3-icon-group-objects::before{content:\"\ue60a\"}.jupyter-wrapper .bp3-icon-grouped-bar-chart::before{content:\"\ue75d\"}.jupyter-wrapper .bp3-icon-hand::before{content:\"\ue6de\"}.jupyter-wrapper .bp3-icon-hand-down::before{content:\"\ue6bb\"}.jupyter-wrapper .bp3-icon-hand-left::before{content:\"\ue6bc\"}.jupyter-wrapper .bp3-icon-hand-right::before{content:\"\ue6b9\"}.jupyter-wrapper .bp3-icon-hand-up::before{content:\"\ue6ba\"}.jupyter-wrapper .bp3-icon-header::before{content:\"\ue6b5\"}.jupyter-wrapper .bp3-icon-header-one::before{content:\"\ue793\"}.jupyter-wrapper .bp3-icon-header-two::before{content:\"\ue794\"}.jupyter-wrapper .bp3-icon-headset::before{content:\"\ue6dc\"}.jupyter-wrapper .bp3-icon-heart::before{content:\"\u2665\"}.jupyter-wrapper .bp3-icon-heart-broken::before{content:\"\ue7a2\"}.jupyter-wrapper .bp3-icon-heat-grid::before{content:\"\ue6f3\"}.jupyter-wrapper .bp3-icon-heatmap::before{content:\"\ue614\"}.jupyter-wrapper .bp3-icon-help::before{content:\"?\"}.jupyter-wrapper .bp3-icon-helper-management::before{content:\"\ue66d\"}.jupyter-wrapper .bp3-icon-highlight::before{content:\"\ue6ed\"}.jupyter-wrapper .bp3-icon-history::before{content:\"\ue64a\"}.jupyter-wrapper .bp3-icon-home::before{content:\"\u2302\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart::before{content:\"\ue70c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-asc::before{content:\"\ue75c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-desc::before{content:\"\ue71d\"}.jupyter-wrapper .bp3-icon-horizontal-distribution::before{content:\"\ue720\"}.jupyter-wrapper .bp3-icon-id-number::before{content:\"\ue771\"}.jupyter-wrapper .bp3-icon-image-rotate-left::before{content:\"\ue73a\"}.jupyter-wrapper .bp3-icon-image-rotate-right::before{content:\"\ue73b\"}.jupyter-wrapper .bp3-icon-import::before{content:\"\ue632\"}.jupyter-wrapper .bp3-icon-inbox::before{content:\"\ue629\"}.jupyter-wrapper .bp3-icon-inbox-filtered::before{content:\"\ue7d1\"}.jupyter-wrapper .bp3-icon-inbox-geo::before{content:\"\ue7d2\"}.jupyter-wrapper .bp3-icon-inbox-search::before{content:\"\ue7d3\"}.jupyter-wrapper .bp3-icon-inbox-update::before{content:\"\ue7d4\"}.jupyter-wrapper .bp3-icon-info-sign::before{content:\"\u2139\"}.jupyter-wrapper .bp3-icon-inheritance::before{content:\"\ue7d5\"}.jupyter-wrapper .bp3-icon-inner-join::before{content:\"\ue7a3\"}.jupyter-wrapper .bp3-icon-insert::before{content:\"\ue66c\"}.jupyter-wrapper .bp3-icon-intersection::before{content:\"\ue765\"}.jupyter-wrapper .bp3-icon-ip-address::before{content:\"\ue772\"}.jupyter-wrapper .bp3-icon-issue::before{content:\"\ue774\"}.jupyter-wrapper .bp3-icon-issue-closed::before{content:\"\ue776\"}.jupyter-wrapper .bp3-icon-issue-new::before{content:\"\ue775\"}.jupyter-wrapper .bp3-icon-italic::before{content:\"\ue607\"}.jupyter-wrapper .bp3-icon-join-table::before{content:\"\ue738\"}.jupyter-wrapper .bp3-icon-key::before{content:\"\ue78e\"}.jupyter-wrapper .bp3-icon-key-backspace::before{content:\"\ue707\"}.jupyter-wrapper .bp3-icon-key-command::before{content:\"\ue705\"}.jupyter-wrapper .bp3-icon-key-control::before{content:\"\ue704\"}.jupyter-wrapper .bp3-icon-key-delete::before{content:\"\ue708\"}.jupyter-wrapper .bp3-icon-key-enter::before{content:\"\ue70a\"}.jupyter-wrapper .bp3-icon-key-escape::before{content:\"\ue709\"}.jupyter-wrapper .bp3-icon-key-option::before{content:\"\ue742\"}.jupyter-wrapper .bp3-icon-key-shift::before{content:\"\ue706\"}.jupyter-wrapper .bp3-icon-key-tab::before{content:\"\ue757\"}.jupyter-wrapper .bp3-icon-known-vehicle::before{content:\"\ue73c\"}.jupyter-wrapper .bp3-icon-label::before{content:\"\ue665\"}.jupyter-wrapper .bp3-icon-layer::before{content:\"\ue6cf\"}.jupyter-wrapper .bp3-icon-layers::before{content:\"\ue618\"}.jupyter-wrapper .bp3-icon-layout::before{content:\"\ue60c\"}.jupyter-wrapper .bp3-icon-layout-auto::before{content:\"\ue60d\"}.jupyter-wrapper .bp3-icon-layout-balloon::before{content:\"\ue6d3\"}.jupyter-wrapper .bp3-icon-layout-circle::before{content:\"\ue60e\"}.jupyter-wrapper .bp3-icon-layout-grid::before{content:\"\ue610\"}.jupyter-wrapper .bp3-icon-layout-group-by::before{content:\"\ue611\"}.jupyter-wrapper .bp3-icon-layout-hierarchy::before{content:\"\ue60f\"}.jupyter-wrapper .bp3-icon-layout-linear::before{content:\"\ue6c3\"}.jupyter-wrapper .bp3-icon-layout-skew-grid::before{content:\"\ue612\"}.jupyter-wrapper .bp3-icon-layout-sorted-clusters::before{content:\"\ue6d4\"}.jupyter-wrapper .bp3-icon-learning::before{content:\"\ue904\"}.jupyter-wrapper .bp3-icon-left-join::before{content:\"\ue7a4\"}.jupyter-wrapper .bp3-icon-less-than::before{content:\"\ue7e3\"}.jupyter-wrapper .bp3-icon-less-than-or-equal-to::before{content:\"\ue7e4\"}.jupyter-wrapper .bp3-icon-lifesaver::before{content:\"\ue7c7\"}.jupyter-wrapper .bp3-icon-lightbulb::before{content:\"\ue6b0\"}.jupyter-wrapper .bp3-icon-link::before{content:\"\ue62d\"}.jupyter-wrapper .bp3-icon-list::before{content:\"\u2630\"}.jupyter-wrapper .bp3-icon-list-columns::before{content:\"\ue7b9\"}.jupyter-wrapper .bp3-icon-list-detail-view::before{content:\"\ue743\"}.jupyter-wrapper .bp3-icon-locate::before{content:\"\ue619\"}.jupyter-wrapper .bp3-icon-lock::before{content:\"\ue625\"}.jupyter-wrapper .bp3-icon-log-in::before{content:\"\ue69a\"}.jupyter-wrapper .bp3-icon-log-out::before{content:\"\ue64c\"}.jupyter-wrapper .bp3-icon-manual::before{content:\"\ue6f6\"}.jupyter-wrapper .bp3-icon-manually-entered-data::before{content:\"\ue74a\"}.jupyter-wrapper .bp3-icon-map::before{content:\"\ue662\"}.jupyter-wrapper .bp3-icon-map-create::before{content:\"\ue741\"}.jupyter-wrapper .bp3-icon-map-marker::before{content:\"\ue67d\"}.jupyter-wrapper .bp3-icon-maximize::before{content:\"\ue635\"}.jupyter-wrapper .bp3-icon-media::before{content:\"\ue62c\"}.jupyter-wrapper .bp3-icon-menu::before{content:\"\ue762\"}.jupyter-wrapper .bp3-icon-menu-closed::before{content:\"\ue655\"}.jupyter-wrapper .bp3-icon-menu-open::before{content:\"\ue654\"}.jupyter-wrapper .bp3-icon-merge-columns::before{content:\"\ue74f\"}.jupyter-wrapper .bp3-icon-merge-links::before{content:\"\ue60b\"}.jupyter-wrapper .bp3-icon-minimize::before{content:\"\ue634\"}.jupyter-wrapper .bp3-icon-minus::before{content:\"\u2212\"}.jupyter-wrapper .bp3-icon-mobile-phone::before{content:\"\ue717\"}.jupyter-wrapper .bp3-icon-mobile-video::before{content:\"\ue69f\"}.jupyter-wrapper .bp3-icon-moon::before{content:\"\ue754\"}.jupyter-wrapper .bp3-icon-more::before{content:\"\ue62a\"}.jupyter-wrapper .bp3-icon-mountain::before{content:\"\ue7b1\"}.jupyter-wrapper .bp3-icon-move::before{content:\"\ue693\"}.jupyter-wrapper .bp3-icon-mugshot::before{content:\"\ue6db\"}.jupyter-wrapper .bp3-icon-multi-select::before{content:\"\ue680\"}.jupyter-wrapper .bp3-icon-music::before{content:\"\ue6a6\"}.jupyter-wrapper .bp3-icon-new-drawing::before{content:\"\ue905\"}.jupyter-wrapper .bp3-icon-new-grid-item::before{content:\"\ue747\"}.jupyter-wrapper .bp3-icon-new-layer::before{content:\"\ue902\"}.jupyter-wrapper .bp3-icon-new-layers::before{content:\"\ue903\"}.jupyter-wrapper .bp3-icon-new-link::before{content:\"\ue65c\"}.jupyter-wrapper .bp3-icon-new-object::before{content:\"\ue65d\"}.jupyter-wrapper .bp3-icon-new-person::before{content:\"\ue6e9\"}.jupyter-wrapper .bp3-icon-new-prescription::before{content:\"\ue78b\"}.jupyter-wrapper .bp3-icon-new-text-box::before{content:\"\ue65b\"}.jupyter-wrapper .bp3-icon-ninja::before{content:\"\ue675\"}.jupyter-wrapper .bp3-icon-not-equal-to::before{content:\"\ue7e0\"}.jupyter-wrapper .bp3-icon-notifications::before{content:\"\ue624\"}.jupyter-wrapper .bp3-icon-notifications-updated::before{content:\"\ue7b8\"}.jupyter-wrapper .bp3-icon-numbered-list::before{content:\"\ue746\"}.jupyter-wrapper .bp3-icon-numerical::before{content:\"\ue756\"}.jupyter-wrapper .bp3-icon-office::before{content:\"\ue69b\"}.jupyter-wrapper .bp3-icon-offline::before{content:\"\ue67a\"}.jupyter-wrapper .bp3-icon-oil-field::before{content:\"\ue73f\"}.jupyter-wrapper .bp3-icon-one-column::before{content:\"\ue658\"}.jupyter-wrapper .bp3-icon-outdated::before{content:\"\ue7a8\"}.jupyter-wrapper .bp3-icon-page-layout::before{content:\"\ue660\"}.jupyter-wrapper .bp3-icon-panel-stats::before{content:\"\ue777\"}.jupyter-wrapper .bp3-icon-panel-table::before{content:\"\ue778\"}.jupyter-wrapper .bp3-icon-paperclip::before{content:\"\ue664\"}.jupyter-wrapper .bp3-icon-paragraph::before{content:\"\ue76c\"}.jupyter-wrapper .bp3-icon-path::before{content:\"\ue753\"}.jupyter-wrapper .bp3-icon-path-search::before{content:\"\ue65e\"}.jupyter-wrapper .bp3-icon-pause::before{content:\"\ue6a9\"}.jupyter-wrapper .bp3-icon-people::before{content:\"\ue63d\"}.jupyter-wrapper .bp3-icon-percentage::before{content:\"\ue76a\"}.jupyter-wrapper .bp3-icon-person::before{content:\"\ue63c\"}.jupyter-wrapper .bp3-icon-phone::before{content:\"\u260e\"}.jupyter-wrapper .bp3-icon-pie-chart::before{content:\"\ue684\"}.jupyter-wrapper .bp3-icon-pin::before{content:\"\ue646\"}.jupyter-wrapper .bp3-icon-pivot::before{content:\"\ue6f1\"}.jupyter-wrapper .bp3-icon-pivot-table::before{content:\"\ue6eb\"}.jupyter-wrapper .bp3-icon-play::before{content:\"\ue6ab\"}.jupyter-wrapper .bp3-icon-plus::before{content:\"+\"}.jupyter-wrapper .bp3-icon-polygon-filter::before{content:\"\ue6d1\"}.jupyter-wrapper .bp3-icon-power::before{content:\"\ue6d9\"}.jupyter-wrapper .bp3-icon-predictive-analysis::before{content:\"\ue617\"}.jupyter-wrapper .bp3-icon-prescription::before{content:\"\ue78a\"}.jupyter-wrapper .bp3-icon-presentation::before{content:\"\ue687\"}.jupyter-wrapper .bp3-icon-print::before{content:\"\u2399\"}.jupyter-wrapper .bp3-icon-projects::before{content:\"\ue622\"}.jupyter-wrapper .bp3-icon-properties::before{content:\"\ue631\"}.jupyter-wrapper .bp3-icon-property::before{content:\"\ue65a\"}.jupyter-wrapper .bp3-icon-publish-function::before{content:\"\ue752\"}.jupyter-wrapper .bp3-icon-pulse::before{content:\"\ue6e8\"}.jupyter-wrapper .bp3-icon-random::before{content:\"\ue698\"}.jupyter-wrapper .bp3-icon-record::before{content:\"\ue6ae\"}.jupyter-wrapper .bp3-icon-redo::before{content:\"\ue6c4\"}.jupyter-wrapper .bp3-icon-refresh::before{content:\"\ue643\"}.jupyter-wrapper .bp3-icon-regression-chart::before{content:\"\ue758\"}.jupyter-wrapper .bp3-icon-remove::before{content:\"\ue63f\"}.jupyter-wrapper .bp3-icon-remove-column::before{content:\"\ue755\"}.jupyter-wrapper .bp3-icon-remove-column-left::before{content:\"\ue6fd\"}.jupyter-wrapper .bp3-icon-remove-column-right::before{content:\"\ue6fe\"}.jupyter-wrapper .bp3-icon-remove-row-bottom::before{content:\"\ue6fc\"}.jupyter-wrapper .bp3-icon-remove-row-top::before{content:\"\ue6fb\"}.jupyter-wrapper .bp3-icon-repeat::before{content:\"\ue692\"}.jupyter-wrapper .bp3-icon-reset::before{content:\"\ue7d6\"}.jupyter-wrapper .bp3-icon-resolve::before{content:\"\ue672\"}.jupyter-wrapper .bp3-icon-rig::before{content:\"\ue740\"}.jupyter-wrapper .bp3-icon-right-join::before{content:\"\ue7a5\"}.jupyter-wrapper .bp3-icon-ring::before{content:\"\ue6f2\"}.jupyter-wrapper .bp3-icon-rotate-document::before{content:\"\ue6e1\"}.jupyter-wrapper .bp3-icon-rotate-page::before{content:\"\ue6e2\"}.jupyter-wrapper .bp3-icon-satellite::before{content:\"\ue76b\"}.jupyter-wrapper .bp3-icon-saved::before{content:\"\ue6b6\"}.jupyter-wrapper .bp3-icon-scatter-plot::before{content:\"\ue73e\"}.jupyter-wrapper .bp3-icon-search::before{content:\"\ue64b\"}.jupyter-wrapper .bp3-icon-search-around::before{content:\"\ue608\"}.jupyter-wrapper .bp3-icon-search-template::before{content:\"\ue628\"}.jupyter-wrapper .bp3-icon-search-text::before{content:\"\ue663\"}.jupyter-wrapper .bp3-icon-segmented-control::before{content:\"\ue6ec\"}.jupyter-wrapper .bp3-icon-select::before{content:\"\ue616\"}.jupyter-wrapper .bp3-icon-selection::before{content:\"\u29bf\"}.jupyter-wrapper .bp3-icon-send-to::before{content:\"\ue66e\"}.jupyter-wrapper .bp3-icon-send-to-graph::before{content:\"\ue736\"}.jupyter-wrapper .bp3-icon-send-to-map::before{content:\"\ue737\"}.jupyter-wrapper .bp3-icon-series-add::before{content:\"\ue796\"}.jupyter-wrapper .bp3-icon-series-configuration::before{content:\"\ue79a\"}.jupyter-wrapper .bp3-icon-series-derived::before{content:\"\ue799\"}.jupyter-wrapper .bp3-icon-series-filtered::before{content:\"\ue798\"}.jupyter-wrapper .bp3-icon-series-search::before{content:\"\ue797\"}.jupyter-wrapper .bp3-icon-settings::before{content:\"\ue6a2\"}.jupyter-wrapper .bp3-icon-share::before{content:\"\ue62e\"}.jupyter-wrapper .bp3-icon-shield::before{content:\"\ue7b2\"}.jupyter-wrapper .bp3-icon-shop::before{content:\"\ue6c2\"}.jupyter-wrapper .bp3-icon-shopping-cart::before{content:\"\ue6c1\"}.jupyter-wrapper .bp3-icon-signal-search::before{content:\"\ue909\"}.jupyter-wrapper .bp3-icon-sim-card::before{content:\"\ue718\"}.jupyter-wrapper .bp3-icon-slash::before{content:\"\ue769\"}.jupyter-wrapper .bp3-icon-small-cross::before{content:\"\ue6d7\"}.jupyter-wrapper .bp3-icon-small-minus::before{content:\"\ue70e\"}.jupyter-wrapper .bp3-icon-small-plus::before{content:\"\ue70d\"}.jupyter-wrapper .bp3-icon-small-tick::before{content:\"\ue6d8\"}.jupyter-wrapper .bp3-icon-snowflake::before{content:\"\ue7b6\"}.jupyter-wrapper .bp3-icon-social-media::before{content:\"\ue671\"}.jupyter-wrapper .bp3-icon-sort::before{content:\"\ue64f\"}.jupyter-wrapper .bp3-icon-sort-alphabetical::before{content:\"\ue64d\"}.jupyter-wrapper .bp3-icon-sort-alphabetical-desc::before{content:\"\ue6c8\"}.jupyter-wrapper .bp3-icon-sort-asc::before{content:\"\ue6d5\"}.jupyter-wrapper .bp3-icon-sort-desc::before{content:\"\ue6d6\"}.jupyter-wrapper .bp3-icon-sort-numerical::before{content:\"\ue64e\"}.jupyter-wrapper .bp3-icon-sort-numerical-desc::before{content:\"\ue6c9\"}.jupyter-wrapper .bp3-icon-split-columns::before{content:\"\ue750\"}.jupyter-wrapper .bp3-icon-square::before{content:\"\ue686\"}.jupyter-wrapper .bp3-icon-stacked-chart::before{content:\"\ue6e7\"}.jupyter-wrapper .bp3-icon-star::before{content:\"\u2605\"}.jupyter-wrapper .bp3-icon-star-empty::before{content:\"\u2606\"}.jupyter-wrapper .bp3-icon-step-backward::before{content:\"\ue6a7\"}.jupyter-wrapper .bp3-icon-step-chart::before{content:\"\ue70f\"}.jupyter-wrapper .bp3-icon-step-forward::before{content:\"\ue6ad\"}.jupyter-wrapper .bp3-icon-stop::before{content:\"\ue6aa\"}.jupyter-wrapper .bp3-icon-stopwatch::before{content:\"\ue901\"}.jupyter-wrapper .bp3-icon-strikethrough::before{content:\"\ue7a6\"}.jupyter-wrapper .bp3-icon-style::before{content:\"\ue601\"}.jupyter-wrapper .bp3-icon-swap-horizontal::before{content:\"\ue745\"}.jupyter-wrapper .bp3-icon-swap-vertical::before{content:\"\ue744\"}.jupyter-wrapper .bp3-icon-symbol-circle::before{content:\"\ue72e\"}.jupyter-wrapper .bp3-icon-symbol-cross::before{content:\"\ue731\"}.jupyter-wrapper .bp3-icon-symbol-diamond::before{content:\"\ue730\"}.jupyter-wrapper .bp3-icon-symbol-square::before{content:\"\ue72f\"}.jupyter-wrapper .bp3-icon-symbol-triangle-down::before{content:\"\ue733\"}.jupyter-wrapper .bp3-icon-symbol-triangle-up::before{content:\"\ue732\"}.jupyter-wrapper .bp3-icon-tag::before{content:\"\ue61c\"}.jupyter-wrapper .bp3-icon-take-action::before{content:\"\ue6ca\"}.jupyter-wrapper .bp3-icon-taxi::before{content:\"\ue79e\"}.jupyter-wrapper .bp3-icon-text-highlight::before{content:\"\ue6dd\"}.jupyter-wrapper .bp3-icon-th::before{content:\"\ue667\"}.jupyter-wrapper .bp3-icon-th-derived::before{content:\"\ue669\"}.jupyter-wrapper .bp3-icon-th-disconnect::before{content:\"\ue7d8\"}.jupyter-wrapper .bp3-icon-th-filtered::before{content:\"\ue7c6\"}.jupyter-wrapper .bp3-icon-th-list::before{content:\"\ue668\"}.jupyter-wrapper .bp3-icon-thumbs-down::before{content:\"\ue6be\"}.jupyter-wrapper .bp3-icon-thumbs-up::before{content:\"\ue6bd\"}.jupyter-wrapper .bp3-icon-tick::before{content:\"\u2713\"}.jupyter-wrapper .bp3-icon-tick-circle::before{content:\"\ue779\"}.jupyter-wrapper .bp3-icon-time::before{content:\"\u23f2\"}.jupyter-wrapper .bp3-icon-timeline-area-chart::before{content:\"\ue6cd\"}.jupyter-wrapper .bp3-icon-timeline-bar-chart::before{content:\"\ue620\"}.jupyter-wrapper .bp3-icon-timeline-events::before{content:\"\ue61e\"}.jupyter-wrapper .bp3-icon-timeline-line-chart::before{content:\"\ue61f\"}.jupyter-wrapper .bp3-icon-tint::before{content:\"\ue6b2\"}.jupyter-wrapper .bp3-icon-torch::before{content:\"\ue677\"}.jupyter-wrapper .bp3-icon-tractor::before{content:\"\ue90c\"}.jupyter-wrapper .bp3-icon-train::before{content:\"\ue79f\"}.jupyter-wrapper .bp3-icon-translate::before{content:\"\ue759\"}.jupyter-wrapper .bp3-icon-trash::before{content:\"\ue63b\"}.jupyter-wrapper .bp3-icon-tree::before{content:\"\ue7b7\"}.jupyter-wrapper .bp3-icon-trending-down::before{content:\"\ue71a\"}.jupyter-wrapper .bp3-icon-trending-up::before{content:\"\ue719\"}.jupyter-wrapper .bp3-icon-truck::before{content:\"\ue90b\"}.jupyter-wrapper .bp3-icon-two-columns::before{content:\"\ue657\"}.jupyter-wrapper .bp3-icon-unarchive::before{content:\"\ue906\"}.jupyter-wrapper .bp3-icon-underline::before{content:\"\u2381\"}.jupyter-wrapper .bp3-icon-undo::before{content:\"\u238c\"}.jupyter-wrapper .bp3-icon-ungroup-objects::before{content:\"\ue688\"}.jupyter-wrapper .bp3-icon-unknown-vehicle::before{content:\"\ue73d\"}.jupyter-wrapper .bp3-icon-unlock::before{content:\"\ue626\"}.jupyter-wrapper .bp3-icon-unpin::before{content:\"\ue650\"}.jupyter-wrapper .bp3-icon-unresolve::before{content:\"\ue679\"}.jupyter-wrapper .bp3-icon-updated::before{content:\"\ue7a7\"}.jupyter-wrapper .bp3-icon-upload::before{content:\"\ue68f\"}.jupyter-wrapper .bp3-icon-user::before{content:\"\ue627\"}.jupyter-wrapper .bp3-icon-variable::before{content:\"\ue6f5\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-asc::before{content:\"\ue75b\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-desc::before{content:\"\ue71c\"}.jupyter-wrapper .bp3-icon-vertical-distribution::before{content:\"\ue721\"}.jupyter-wrapper .bp3-icon-video::before{content:\"\ue6a0\"}.jupyter-wrapper .bp3-icon-volume-down::before{content:\"\ue6a4\"}.jupyter-wrapper .bp3-icon-volume-off::before{content:\"\ue6a3\"}.jupyter-wrapper .bp3-icon-volume-up::before{content:\"\ue6a5\"}.jupyter-wrapper .bp3-icon-walk::before{content:\"\ue79d\"}.jupyter-wrapper .bp3-icon-warning-sign::before{content:\"\ue647\"}.jupyter-wrapper .bp3-icon-waterfall-chart::before{content:\"\ue6e6\"}.jupyter-wrapper .bp3-icon-widget::before{content:\"\ue678\"}.jupyter-wrapper .bp3-icon-widget-button::before{content:\"\ue790\"}.jupyter-wrapper .bp3-icon-widget-footer::before{content:\"\ue792\"}.jupyter-wrapper .bp3-icon-widget-header::before{content:\"\ue791\"}.jupyter-wrapper .bp3-icon-wrench::before{content:\"\ue734\"}.jupyter-wrapper .bp3-icon-zoom-in::before{content:\"\ue641\"}.jupyter-wrapper .bp3-icon-zoom-out::before{content:\"\ue642\"}.jupyter-wrapper .bp3-icon-zoom-to-fit::before{content:\"\ue67b\"}.jupyter-wrapper .bp3-submenu>.bp3-popover-wrapper{display:block}.jupyter-wrapper .bp3-submenu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-submenu.bp3-popover{-webkit-box-shadow:none;box-shadow:none;padding:0 5px}.jupyter-wrapper .bp3-submenu.bp3-popover>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover>.bp3-popover-content,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-menu{margin:0;border-radius:3px;background:#fff;min-width:180px;padding:5px;list-style:none;text-align:left;color:#182026}.jupyter-wrapper .bp3-menu-divider{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-divider{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;border-radius:2px;padding:5px 7px;text-decoration:none;line-height:20px;color:inherit;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-menu-item>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>*{margin-right:7px}.jupyter-wrapper .bp3-menu-item:empty::before,.jupyter-wrapper .bp3-menu-item>:last-child{margin-right:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{word-break:break-word}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(167,182,194,.3);cursor:pointer;text-decoration:none}.jupyter-wrapper .bp3-menu-item.bp3-disabled{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(138,155,168,.15);color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{background-color:inherit;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-right:7px}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>.bp3-icon{margin-top:2px;color:#5c7080}.jupyter-wrapper .bp3-menu-item .bp3-menu-item-label{color:#5c7080}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-menu-item:active{background-color:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-menu-item.bp3-disabled{outline:none !important;background-color:inherit !important;cursor:not-allowed !important;color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-large .bp3-menu-item{padding:9px 7px;line-height:22px;font-size:16px}.jupyter-wrapper .bp3-large .bp3-menu-item .bp3-icon{margin-top:3px}.jupyter-wrapper .bp3-large .bp3-menu-item::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-top:1px;margin-right:10px}.jupyter-wrapper button.bp3-menu-item{border:none;background:none;width:100%;text-align:left}.jupyter-wrapper .bp3-menu-header{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15);cursor:default;padding-left:2px}.jupyter-wrapper .bp3-dark .bp3-menu-header{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-header:first-of-type{border-top:none}.jupyter-wrapper .bp3-menu-header>h6{color:#182026;font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;margin:0;padding:10px 7px 0 1px;line-height:17px}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-large .bp3-menu-header>h6{padding-top:15px;padding-bottom:5px;font-size:18px}.jupyter-wrapper .bp3-large .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-dark .bp3-menu{background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item .bp3-menu-item-label{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item:active{background-color:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-divider,.jupyter-wrapper .bp3-dark .bp3-menu-header{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-label .bp3-menu{margin-top:5px}.jupyter-wrapper .bp3-navbar{position:relative;z-index:10;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background-color:#fff;width:100%;height:50px;padding:0 15px}.jupyter-wrapper .bp3-navbar.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-navbar{background-color:#394b59}.jupyter-wrapper .bp3-navbar.bp3-dark{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-navbar{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-navbar.bp3-fixed-top{position:fixed;top:0;right:0;left:0}.jupyter-wrapper .bp3-navbar-heading{margin-right:15px;font-size:16px}.jupyter-wrapper .bp3-navbar-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:50px}.jupyter-wrapper .bp3-navbar-group.bp3-align-left{float:left}.jupyter-wrapper .bp3-navbar-group.bp3-align-right{float:right}.jupyter-wrapper .bp3-navbar-divider{margin:0 10px;border-left:1px solid rgba(16,22,26,.15);height:20px}.jupyter-wrapper .bp3-dark .bp3-navbar-divider{border-left-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-non-ideal-state{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%;text-align:center}.jupyter-wrapper .bp3-non-ideal-state>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-non-ideal-state>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-non-ideal-state::before,.jupyter-wrapper .bp3-non-ideal-state>*{margin-bottom:20px}.jupyter-wrapper .bp3-non-ideal-state:empty::before,.jupyter-wrapper .bp3-non-ideal-state>:last-child{margin-bottom:0}.jupyter-wrapper .bp3-non-ideal-state>*{max-width:400px}.jupyter-wrapper .bp3-non-ideal-state-visual{color:rgba(92,112,128,.6);font-size:60px}.jupyter-wrapper .bp3-dark .bp3-non-ideal-state-visual{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-overflow-list{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;min-width:0}.jupyter-wrapper .bp3-overflow-list-spacer{-ms-flex-negative:1;flex-shrink:1;width:1px}.jupyter-wrapper body.bp3-overlay-open{overflow:hidden}.jupyter-wrapper .bp3-overlay{position:static;top:0;right:0;bottom:0;left:0;z-index:20}.jupyter-wrapper .bp3-overlay:not(.bp3-overlay-open){pointer-events:none}.jupyter-wrapper .bp3-overlay.bp3-overlay-container{position:fixed;overflow:hidden}.jupyter-wrapper .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container{position:fixed;overflow:auto}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-inline{display:inline;overflow:visible}.jupyter-wrapper .bp3-overlay-content{position:fixed;z-index:20}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-content,.jupyter-wrapper .bp3-overlay-scroll-container .bp3-overlay-content{position:absolute}.jupyter-wrapper .bp3-overlay-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;opacity:1;z-index:20;background-color:rgba(16,22,26,.7);overflow:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear{opacity:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter-active,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit{opacity:1}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop:focus{outline:none}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-backdrop{position:absolute}.jupyter-wrapper .bp3-panel-stack{position:relative;overflow:hidden}.jupyter-wrapper .bp3-panel-stack-header{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-negative:0;flex-shrink:0;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:1;-webkit-box-shadow:0 1px rgba(16,22,26,.15);box-shadow:0 1px rgba(16,22,26,.15);height:30px}.jupyter-wrapper .bp3-dark .bp3-panel-stack-header{-webkit-box-shadow:0 1px rgba(255,255,255,.15);box-shadow:0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-panel-stack-header>span{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1;flex:1;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-panel-stack-header .bp3-heading{margin:0 5px}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back{margin-left:5px;padding-left:0;white-space:nowrap}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back .bp3-icon{margin:0 2px}.jupyter-wrapper .bp3-panel-stack-view{position:absolute;top:0;right:0;bottom:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-right:-1px;border-right:1px solid rgba(16,22,26,.15);background-color:#fff;overflow-y:auto}.jupyter-wrapper .bp3-dark .bp3-panel-stack-view{background-color:#30404d}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit-active{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1);display:inline-block;z-index:20;border-radius:3px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow{position:absolute;width:30px;height:30px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{margin:5px;width:20px;height:20px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover{margin-top:-17px;margin-bottom:17px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{bottom:-11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover{margin-left:17px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{left:-11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover{margin-top:17px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{top:-11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover{margin-right:17px;margin-left:-17px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{right:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-popover>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-popover>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{top:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{right:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{left:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{bottom:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-popover .bp3-popover-content{background:#fff;color:inherit}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-fill{fill:#fff}.jupyter-wrapper .bp3-popover-enter>.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover .bp3-popover-content{position:relative;border-radius:3px}.jupyter-wrapper .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{max-width:350px;padding:20px}.jupyter-wrapper .bp3-popover-target+.bp3-overlay .bp3-popover.bp3-popover-content-sizing{width:350px}.jupyter-wrapper .bp3-popover.bp3-minimal{margin:0 !important}.jupyter-wrapper .bp3-popover.bp3-minimal .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-content{background:#30404d;color:inherit}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-fill{fill:#30404d}.jupyter-wrapper .bp3-popover-arrow::before{display:block;position:absolute;-webkit-transform:rotate(45deg);transform:rotate(45deg);border-radius:2px;content:\"\"}.jupyter-wrapper .bp3-tether-pinned .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover-backdrop{background:rgba(255,255,255,0)}.jupyter-wrapper .bp3-transition-container{opacity:1;display:-webkit-box;display:-ms-flexbox;display:flex;z-index:20}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear{opacity:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter-active,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit{opacity:1}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container:focus{outline:none}.jupyter-wrapper .bp3-transition-container.bp3-popover-leave .bp3-popover-content{pointer-events:none}.jupyter-wrapper .bp3-transition-container[data-x-out-of-boundaries]{display:none}.jupyter-wrapper span.bp3-popover-target{display:inline-block}.jupyter-wrapper .bp3-popover-wrapper.bp3-fill{width:100%}.jupyter-wrapper .bp3-portal{position:absolute;top:0;right:0;left:0}@-webkit-keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}@keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}.jupyter-wrapper .bp3-progress-bar{display:block;position:relative;border-radius:40px;background:rgba(92,112,128,.2);width:100%;height:8px;overflow:hidden}.jupyter-wrapper .bp3-progress-bar .bp3-progress-meter{position:absolute;border-radius:40px;background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);background-color:rgba(92,112,128,.8);background-size:30px 30px;width:100%;height:100%;-webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{animation:linear-progress-bar-stripes 300ms linear infinite reverse}.jupyter-wrapper .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{background-image:none}.jupyter-wrapper .bp3-dark .bp3-progress-bar{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-progress-bar .bp3-progress-meter{background-color:#8a9ba8}.jupyter-wrapper .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{background-color:#137cbd}.jupyter-wrapper .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{background-color:#0f9960}.jupyter-wrapper .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{background-color:#d9822b}.jupyter-wrapper .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{background-color:#db3737}@-webkit-keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}@keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}.jupyter-wrapper .bp3-skeleton{border-color:rgba(206,217,224,.2) !important;border-radius:2px;-webkit-box-shadow:none !important;box-shadow:none !important;background:rgba(206,217,224,.2);background-clip:padding-box !important;cursor:default;color:rgba(0,0,0,0) !important;-webkit-animation:1000ms linear infinite alternate skeleton-glow;animation:1000ms linear infinite alternate skeleton-glow;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-skeleton::before,.jupyter-wrapper .bp3-skeleton::after,.jupyter-wrapper .bp3-skeleton *{visibility:hidden !important}.jupyter-wrapper .bp3-slider{width:100%;min-width:150px;height:40px;position:relative;outline:none;cursor:default;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-slider:hover{cursor:pointer}.jupyter-wrapper .bp3-slider:active{cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-slider.bp3-disabled{opacity:.5;cursor:not-allowed}.jupyter-wrapper .bp3-slider.bp3-slider-unlabeled{height:16px}.jupyter-wrapper .bp3-slider-track,.jupyter-wrapper .bp3-slider-progress{top:5px;right:0;left:0;height:6px;position:absolute}.jupyter-wrapper .bp3-slider-track{border-radius:3px;overflow:hidden}.jupyter-wrapper .bp3-slider-progress{background:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-dark .bp3-slider-progress{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-slider-progress.bp3-intent-primary{background-color:#137cbd}.jupyter-wrapper .bp3-slider-progress.bp3-intent-success{background-color:#0f9960}.jupyter-wrapper .bp3-slider-progress.bp3-intent-warning{background-color:#d9822b}.jupyter-wrapper .bp3-slider-progress.bp3-intent-danger{background-color:#db3737}.jupyter-wrapper .bp3-slider-handle{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;position:absolute;top:0;left:0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:pointer;width:16px;height:16px}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-slider-handle:active,.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-slider-handle.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active:hover,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-slider-handle:focus{z-index:1}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5;z-index:2;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:-webkit-grab;cursor:grab}.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-disabled .bp3-slider-handle{-webkit-box-shadow:none;box-shadow:none;background:#bfccd6;pointer-events:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover,.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-slider-handle,.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{background-color:#394b59}.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{background-color:#293742}.jupyter-wrapper .bp3-dark .bp3-disabled .bp3-slider-handle{border-color:#5c7080;-webkit-box-shadow:none;box-shadow:none;background:#5c7080}.jupyter-wrapper .bp3-slider-handle .bp3-slider-label{margin-left:8px;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-disabled .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-slider-handle.bp3-start,.jupyter-wrapper .bp3-slider-handle.bp3-end{width:8px}.jupyter-wrapper .bp3-slider-handle.bp3-start{border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end{margin-left:8px;border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end .bp3-slider-label{margin-left:0}.jupyter-wrapper .bp3-slider-label{-webkit-transform:translate(-50%, 20px);transform:translate(-50%, 20px);display:inline-block;position:absolute;padding:2px 5px;vertical-align:top;line-height:1;font-size:12px}.jupyter-wrapper .bp3-slider.bp3-vertical{width:40px;min-width:40px;height:150px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-track,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:0;bottom:0;left:5px;width:6px;height:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-label{-webkit-transform:translate(20px, 50%);transform:translate(20px, 50%)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{margin-top:-8px;margin-left:0}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{margin-left:0;width:16px;height:8px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{border-top-left-radius:0;border-bottom-right-radius:3px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{-webkit-transform:translate(20px);transform:translate(20px)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{margin-bottom:8px;border-top-left-radius:3px;border-bottom-left-radius:0;border-bottom-right-radius:0}@-webkit-keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.jupyter-wrapper .bp3-spinner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;overflow:visible;vertical-align:middle}.jupyter-wrapper .bp3-spinner svg{display:block}.jupyter-wrapper .bp3-spinner path{fill-opacity:0}.jupyter-wrapper .bp3-spinner .bp3-spinner-head{-webkit-transform-origin:center;transform-origin:center;-webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);stroke:rgba(92,112,128,.8);stroke-linecap:round}.jupyter-wrapper .bp3-spinner .bp3-spinner-track{stroke:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-spinner-animation{-webkit-animation:pt-spinner-animation 500ms linear infinite;animation:pt-spinner-animation 500ms linear infinite}.jupyter-wrapper .bp3-no-spin>.bp3-spinner-animation{-webkit-animation:none;animation:none}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-track{stroke:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-spinner.bp3-intent-primary .bp3-spinner-head{stroke:#137cbd}.jupyter-wrapper .bp3-spinner.bp3-intent-success .bp3-spinner-head{stroke:#0f9960}.jupyter-wrapper .bp3-spinner.bp3-intent-warning .bp3-spinner-head{stroke:#d9822b}.jupyter-wrapper .bp3-spinner.bp3-intent-danger .bp3-spinner-head{stroke:#db3737}.jupyter-wrapper .bp3-tabs.bp3-vertical{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab{border-radius:3px;width:100%;padding:0 10px}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab[aria-selected=true]{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.2)}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{top:0;right:0;bottom:0;left:0;border-radius:3px;background-color:rgba(19,124,189,.2);height:auto}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-panel{margin-top:0;padding-left:20px}.jupyter-wrapper .bp3-tab-list{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:relative;margin:0;border:none;padding:0;list-style:none}.jupyter-wrapper .bp3-tab-list>*:not(:last-child){margin-right:20px}.jupyter-wrapper .bp3-tab{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;cursor:pointer;max-width:100%;vertical-align:top;line-height:30px;color:#182026;font-size:14px}.jupyter-wrapper .bp3-tab a{display:block;text-decoration:none;color:inherit}.jupyter-wrapper .bp3-tab-indicator-wrapper~.bp3-tab{-webkit-box-shadow:none !important;box-shadow:none !important;background-color:rgba(0,0,0,0) !important}.jupyter-wrapper .bp3-tab[aria-disabled=true]{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tab[aria-selected=true]{border-radius:0;-webkit-box-shadow:inset 0 -3px 0 #106ba3;box-shadow:inset 0 -3px 0 #106ba3}.jupyter-wrapper .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-tab:not([aria-disabled=true]):hover{color:#106ba3}.jupyter-wrapper .bp3-tab:focus{-moz-outline-radius:0}.jupyter-wrapper .bp3-large>.bp3-tab{line-height:40px;font-size:16px}.jupyter-wrapper .bp3-tab-panel{margin-top:20px}.jupyter-wrapper .bp3-tab-panel[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-tab-indicator-wrapper{position:absolute;top:0;left:0;-webkit-transform:translateX(0),translateY(0);transform:translateX(0),translateY(0);-webkit-transition:height,width,-webkit-transform;transition:height,width,-webkit-transform;transition:height,transform,width;transition:height,transform,width,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);pointer-events:none}.jupyter-wrapper .bp3-tab-indicator-wrapper .bp3-tab-indicator{position:absolute;right:0;bottom:0;left:0;background-color:#106ba3;height:3px}.jupyter-wrapper .bp3-tab-indicator-wrapper.bp3-no-animation{-webkit-transition:none;transition:none}.jupyter-wrapper .bp3-dark .bp3-tab{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tab[aria-disabled=true]{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true]{-webkit-box-shadow:inset 0 -3px 0 #48aff0;box-shadow:inset 0 -3px 0 #48aff0}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-dark .bp3-tab:not([aria-disabled=true]):hover{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tab-indicator{background-color:#48aff0}.jupyter-wrapper .bp3-flex-expander{-webkit-box-flex:1;-ms-flex:1 1;flex:1 1}.jupyter-wrapper .bp3-tag{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border:none;border-radius:3px;-webkit-box-shadow:none;box-shadow:none;background-color:#5c7080;min-width:20px;max-width:100%;min-height:20px;padding:2px 6px;line-height:16px;color:#f5f8fa;font-size:12px}.jupyter-wrapper .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-interactive:hover{background-color:rgba(92,112,128,.85)}.jupyter-wrapper .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-interactive:active{background-color:rgba(92,112,128,.7)}.jupyter-wrapper .bp3-tag>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag::before,.jupyter-wrapper .bp3-tag>*{margin-right:4px}.jupyter-wrapper .bp3-tag:empty::before,.jupyter-wrapper .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag:focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag.bp3-round{border-radius:30px;padding-right:8px;padding-left:8px}.jupyter-wrapper .bp3-dark .bp3-tag{background-color:#bfccd6;color:#182026}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:hover{background-color:rgba(191,204,214,.85)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:active{background-color:rgba(191,204,214,.7)}.jupyter-wrapper .bp3-dark .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-large{fill:currentColor}.jupyter-wrapper .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-tag .bp3-icon-large{fill:#fff}.jupyter-wrapper .bp3-tag.bp3-large,.jupyter-wrapper .bp3-large .bp3-tag{min-width:30px;min-height:30px;padding:0 10px;line-height:20px;font-size:14px}.jupyter-wrapper .bp3-tag.bp3-large::before,.jupyter-wrapper .bp3-tag.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-tag::before,.jupyter-wrapper .bp3-large .bp3-tag>*{margin-right:7px}.jupyter-wrapper .bp3-tag.bp3-large:empty::before,.jupyter-wrapper .bp3-tag.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-tag:empty::before,.jupyter-wrapper .bp3-large .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag.bp3-large.bp3-round,.jupyter-wrapper .bp3-large .bp3-tag.bp3-round{padding-right:12px;padding-left:12px}.jupyter-wrapper .bp3-tag.bp3-intent-primary{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-success{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-warning{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-danger{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.7)}.jupyter-wrapper .bp3-tag.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-tag.bp3-minimal>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-large{fill:#5c7080}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){background-color:rgba(138,155,168,.2);color:#182026}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(191,204,214,.3)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-])>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-large{fill:#a7b6c2}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{fill:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.25);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{fill:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.25);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{fill:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.25);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{fill:#db3737}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.25);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.45)}.jupyter-wrapper .bp3-tag-remove{display:-webkit-box;display:-ms-flexbox;display:flex;opacity:.5;margin-top:-2px;margin-right:-6px !important;margin-bottom:-2px;border:none;background:none;cursor:pointer;padding:2px;padding-left:0;color:inherit}.jupyter-wrapper .bp3-tag-remove:hover{opacity:.8;background:none;text-decoration:none}.jupyter-wrapper .bp3-tag-remove:active{opacity:1}.jupyter-wrapper .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6d7\"}.jupyter-wrapper .bp3-large .bp3-tag-remove{margin-right:-10px !important;padding:5px;padding-left:0}.jupyter-wrapper .bp3-large .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal}.jupyter-wrapper .bp3-tag-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;cursor:text;height:auto;min-height:30px;padding-right:0;padding-left:5px;line-height:inherit}.jupyter-wrapper .bp3-tag-input>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input>.bp3-tag-input-values{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-icon{margin-top:7px;margin-right:7px;margin-left:2px;color:#5c7080}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-item-align:stretch;align-self:stretch;margin-top:5px;margin-right:7px;min-width:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-right:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:empty::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{padding-left:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-bottom:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag{overflow-wrap:break-word}.jupyter-wrapper .bp3-tag-input .bp3-tag.bp3-active{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:80px;line-height:20px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost:disabled,.jupyter-wrapper .bp3-tag-input .bp3-input-ghost.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-tag-input .bp3-button,.jupyter-wrapper .bp3-tag-input .bp3-spinner{margin:3px;margin-left:0}.jupyter-wrapper .bp3-tag-input .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-tag-input.bp3-large{height:auto;min-height:40px}.jupyter-wrapper .bp3-tag-input.bp3-large::before,.jupyter-wrapper .bp3-tag-input.bp3-large>*{margin-right:10px}.jupyter-wrapper .bp3-tag-input.bp3-large:empty::before,.jupyter-wrapper .bp3-tag-input.bp3-large>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-tag-input-icon{margin-top:10px;margin-left:5px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-input-ghost{line-height:30px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-button{min-width:30px;min-height:30px;padding:5px 10px;margin:5px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-spinner{margin:8px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-tag-input-icon,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-tag-input-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-input-ghost{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;padding:0}.jupyter-wrapper .bp3-input-ghost::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:focus{outline:none !important}.jupyter-wrapper .bp3-toast{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;position:relative !important;margin:20px 0 0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background-color:#fff;min-width:300px;max-width:500px;pointer-events:all}.jupyter-wrapper .bp3-toast.bp3-toast-enter,.jupyter-wrapper .bp3-toast.bp3-toast-appear{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-enter~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit{opacity:1;-webkit-filter:blur(0);filter:blur(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active{opacity:0;-webkit-filter:blur(10px);filter:blur(10px);-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:opacity,filter;transition-property:opacity,filter,-webkit-filter;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:50ms;transition-delay:50ms}.jupyter-wrapper .bp3-toast .bp3-button-group{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;padding-left:0}.jupyter-wrapper .bp3-toast>.bp3-icon{margin:12px;margin-right:0;color:#5c7080}.jupyter-wrapper .bp3-toast.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-toast{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background-color:#394b59}.jupyter-wrapper .bp3-toast.bp3-dark>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-toast>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a:hover{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-]>.bp3-icon{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::before,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button .bp3-icon,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{color:rgba(255,255,255,.7) !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:focus{outline-color:rgba(255,255,255,.5)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:hover{background-color:rgba(255,255,255,.15) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{background-color:rgba(255,255,255,.3) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::after{background:rgba(255,255,255,.3) !important}.jupyter-wrapper .bp3-toast.bp3-intent-primary{background-color:#137cbd;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-success{background-color:#0f9960;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-warning{background-color:#d9822b;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-danger{background-color:#db3737;color:#fff}.jupyter-wrapper .bp3-toast-message{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;padding:11px;word-break:break-word}.jupyter-wrapper .bp3-toast-container{display:-webkit-box !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;right:0;left:0;z-index:40;overflow:hidden;padding:0 20px 20px;pointer-events:none}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-top{top:0;bottom:auto}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-bottom{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;top:auto;bottom:0}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-left{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-right{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active~.bp3-toast{-webkit-transform:translateY(60px);transform:translateY(60px)}.jupyter-wrapper .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow{position:absolute;width:22px;height:22px}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{margin:4px;width:14px;height:14px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip{margin-top:-11px;margin-bottom:11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{bottom:-8px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip{margin-left:11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{left:-8px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip{margin-top:11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{top:-8px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip{margin-right:11px;margin-left:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{right:-8px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-tooltip>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-tooltip>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{top:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{right:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{left:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{bottom:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-fill{fill:#394b59}.jupyter-wrapper .bp3-popover-enter>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear-active>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{padding:10px 12px}.jupyter-wrapper .bp3-tooltip.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-content{background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{fill:#e1e8ed}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-content{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{fill:#137cbd}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-content{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{fill:#0f9960}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-content{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{fill:#d9822b}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-content{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{fill:#db3737}.jupyter-wrapper .bp3-tooltip-indicator{border-bottom:dotted 1px;cursor:help}.jupyter-wrapper .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-tree .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-tree-node-list{margin:0;padding-left:0;list-style:none}.jupyter-wrapper .bp3-tree-root{position:relative;background-color:rgba(0,0,0,0);cursor:default;padding-left:0}.jupyter-wrapper .bp3-tree-node-content-0{padding-left:0px}.jupyter-wrapper .bp3-tree-node-content-1{padding-left:23px}.jupyter-wrapper .bp3-tree-node-content-2{padding-left:46px}.jupyter-wrapper .bp3-tree-node-content-3{padding-left:69px}.jupyter-wrapper .bp3-tree-node-content-4{padding-left:92px}.jupyter-wrapper .bp3-tree-node-content-5{padding-left:115px}.jupyter-wrapper .bp3-tree-node-content-6{padding-left:138px}.jupyter-wrapper .bp3-tree-node-content-7{padding-left:161px}.jupyter-wrapper .bp3-tree-node-content-8{padding-left:184px}.jupyter-wrapper .bp3-tree-node-content-9{padding-left:207px}.jupyter-wrapper .bp3-tree-node-content-10{padding-left:230px}.jupyter-wrapper .bp3-tree-node-content-11{padding-left:253px}.jupyter-wrapper .bp3-tree-node-content-12{padding-left:276px}.jupyter-wrapper .bp3-tree-node-content-13{padding-left:299px}.jupyter-wrapper .bp3-tree-node-content-14{padding-left:322px}.jupyter-wrapper .bp3-tree-node-content-15{padding-left:345px}.jupyter-wrapper .bp3-tree-node-content-16{padding-left:368px}.jupyter-wrapper .bp3-tree-node-content-17{padding-left:391px}.jupyter-wrapper .bp3-tree-node-content-18{padding-left:414px}.jupyter-wrapper .bp3-tree-node-content-19{padding-left:437px}.jupyter-wrapper .bp3-tree-node-content-20{padding-left:460px}.jupyter-wrapper .bp3-tree-node-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:30px;padding-right:5px}.jupyter-wrapper .bp3-tree-node-content:hover{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node-caret-none{min-width:30px}.jupyter-wrapper .bp3-tree-node-caret{color:#5c7080;-webkit-transform:rotate(0deg);transform:rotate(0deg);cursor:pointer;padding:7px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-tree-node-caret:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret:hover{color:#f5f8fa}.jupyter-wrapper .bp3-tree-node-caret.bp3-tree-node-caret-open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tree-node-caret.bp3-icon-standard::before{content:\"\ue695\"}.jupyter-wrapper .bp3-tree-node-icon{position:relative;margin-right:7px}.jupyter-wrapper .bp3-tree-node-label{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-label span{display:inline}.jupyter-wrapper .bp3-tree-node-secondary-label{padding:0 5px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-wrapper,.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-content{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-standard,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-large{color:#fff}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret::before{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret:hover::before{color:#fff}.jupyter-wrapper .bp3-dark .bp3-tree-node-content:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-dark .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-omnibar{-webkit-filter:blur(0);filter:blur(0);opacity:1;top:20vh;left:calc(50% - 250px);z-index:21;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background-color:#fff;width:500px}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter-active,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear-active{-webkit-filter:blur(0);filter:blur(0);opacity:1;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit{-webkit-filter:blur(0);filter:blur(0);opacity:1}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit-active{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar .bp3-input{border-radius:0;background-color:rgba(0,0,0,0)}.jupyter-wrapper .bp3-omnibar .bp3-input,.jupyter-wrapper .bp3-omnibar .bp3-input:focus{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-omnibar .bp3-menu{border-radius:0;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);background-color:rgba(0,0,0,0);max-height:calc(60vh - 40px);overflow:auto}.jupyter-wrapper .bp3-omnibar .bp3-menu:empty{display:none}.jupyter-wrapper .bp3-dark .bp3-omnibar,.jupyter-wrapper .bp3-omnibar.bp3-dark{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-omnibar-overlay .bp3-overlay-backdrop{background-color:rgba(16,22,26,.2)}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper .bp3-multi-select{min-width:150px}.jupyter-wrapper .bp3-multi-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper :root{--jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-build: url(data:image/svg+xml;base64,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);--jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);--jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);--jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);--jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);--jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);--jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter: url(data:image/svg+xml;base64,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);--jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);--jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);--jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);--jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);--jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);--jp-icon-listings-info: url(data:image/svg+xml;base64,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);--jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);--jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);--jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);--jp-icon-react: url(data:image/svg+xml;base64,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);--jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);--jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);--jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-terminal: url(data:image/svg+xml;base64,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);--jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);--jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);--jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K)}.jupyter-wrapper .jp-AddIcon{background-image:var(--jp-icon-add)}.jupyter-wrapper .jp-BugIcon{background-image:var(--jp-icon-bug)}.jupyter-wrapper .jp-BuildIcon{background-image:var(--jp-icon-build)}.jupyter-wrapper .jp-CaretDownEmptyIcon{background-image:var(--jp-icon-caret-down-empty)}.jupyter-wrapper .jp-CaretDownEmptyThinIcon{background-image:var(--jp-icon-caret-down-empty-thin)}.jupyter-wrapper .jp-CaretDownIcon{background-image:var(--jp-icon-caret-down)}.jupyter-wrapper .jp-CaretLeftIcon{background-image:var(--jp-icon-caret-left)}.jupyter-wrapper .jp-CaretRightIcon{background-image:var(--jp-icon-caret-right)}.jupyter-wrapper .jp-CaretUpEmptyThinIcon{background-image:var(--jp-icon-caret-up-empty-thin)}.jupyter-wrapper .jp-CaretUpIcon{background-image:var(--jp-icon-caret-up)}.jupyter-wrapper .jp-CaseSensitiveIcon{background-image:var(--jp-icon-case-sensitive)}.jupyter-wrapper .jp-CheckIcon{background-image:var(--jp-icon-check)}.jupyter-wrapper .jp-CircleEmptyIcon{background-image:var(--jp-icon-circle-empty)}.jupyter-wrapper .jp-CircleIcon{background-image:var(--jp-icon-circle)}.jupyter-wrapper .jp-ClearIcon{background-image:var(--jp-icon-clear)}.jupyter-wrapper .jp-CloseIcon{background-image:var(--jp-icon-close)}.jupyter-wrapper .jp-ConsoleIcon{background-image:var(--jp-icon-console)}.jupyter-wrapper .jp-CopyIcon{background-image:var(--jp-icon-copy)}.jupyter-wrapper .jp-CutIcon{background-image:var(--jp-icon-cut)}.jupyter-wrapper .jp-DownloadIcon{background-image:var(--jp-icon-download)}.jupyter-wrapper .jp-EditIcon{background-image:var(--jp-icon-edit)}.jupyter-wrapper .jp-EllipsesIcon{background-image:var(--jp-icon-ellipses)}.jupyter-wrapper .jp-ExtensionIcon{background-image:var(--jp-icon-extension)}.jupyter-wrapper .jp-FastForwardIcon{background-image:var(--jp-icon-fast-forward)}.jupyter-wrapper .jp-FileIcon{background-image:var(--jp-icon-file)}.jupyter-wrapper .jp-FileUploadIcon{background-image:var(--jp-icon-file-upload)}.jupyter-wrapper .jp-FilterListIcon{background-image:var(--jp-icon-filter-list)}.jupyter-wrapper .jp-FolderIcon{background-image:var(--jp-icon-folder)}.jupyter-wrapper .jp-Html5Icon{background-image:var(--jp-icon-html5)}.jupyter-wrapper .jp-ImageIcon{background-image:var(--jp-icon-image)}.jupyter-wrapper .jp-InspectorIcon{background-image:var(--jp-icon-inspector)}.jupyter-wrapper .jp-JsonIcon{background-image:var(--jp-icon-json)}.jupyter-wrapper .jp-JupyterFaviconIcon{background-image:var(--jp-icon-jupyter-favicon)}.jupyter-wrapper .jp-JupyterIcon{background-image:var(--jp-icon-jupyter)}.jupyter-wrapper .jp-JupyterlabWordmarkIcon{background-image:var(--jp-icon-jupyterlab-wordmark)}.jupyter-wrapper .jp-KernelIcon{background-image:var(--jp-icon-kernel)}.jupyter-wrapper .jp-KeyboardIcon{background-image:var(--jp-icon-keyboard)}.jupyter-wrapper .jp-LauncherIcon{background-image:var(--jp-icon-launcher)}.jupyter-wrapper .jp-LineFormIcon{background-image:var(--jp-icon-line-form)}.jupyter-wrapper .jp-LinkIcon{background-image:var(--jp-icon-link)}.jupyter-wrapper .jp-ListIcon{background-image:var(--jp-icon-list)}.jupyter-wrapper .jp-ListingsInfoIcon{background-image:var(--jp-icon-listings-info)}.jupyter-wrapper .jp-MarkdownIcon{background-image:var(--jp-icon-markdown)}.jupyter-wrapper .jp-NewFolderIcon{background-image:var(--jp-icon-new-folder)}.jupyter-wrapper .jp-NotTrustedIcon{background-image:var(--jp-icon-not-trusted)}.jupyter-wrapper .jp-NotebookIcon{background-image:var(--jp-icon-notebook)}.jupyter-wrapper .jp-PaletteIcon{background-image:var(--jp-icon-palette)}.jupyter-wrapper .jp-PasteIcon{background-image:var(--jp-icon-paste)}.jupyter-wrapper .jp-PythonIcon{background-image:var(--jp-icon-python)}.jupyter-wrapper .jp-RKernelIcon{background-image:var(--jp-icon-r-kernel)}.jupyter-wrapper .jp-ReactIcon{background-image:var(--jp-icon-react)}.jupyter-wrapper .jp-RefreshIcon{background-image:var(--jp-icon-refresh)}.jupyter-wrapper .jp-RegexIcon{background-image:var(--jp-icon-regex)}.jupyter-wrapper .jp-RunIcon{background-image:var(--jp-icon-run)}.jupyter-wrapper .jp-RunningIcon{background-image:var(--jp-icon-running)}.jupyter-wrapper .jp-SaveIcon{background-image:var(--jp-icon-save)}.jupyter-wrapper .jp-SearchIcon{background-image:var(--jp-icon-search)}.jupyter-wrapper .jp-SettingsIcon{background-image:var(--jp-icon-settings)}.jupyter-wrapper .jp-SpreadsheetIcon{background-image:var(--jp-icon-spreadsheet)}.jupyter-wrapper .jp-StopIcon{background-image:var(--jp-icon-stop)}.jupyter-wrapper .jp-TabIcon{background-image:var(--jp-icon-tab)}.jupyter-wrapper .jp-TerminalIcon{background-image:var(--jp-icon-terminal)}.jupyter-wrapper .jp-TextEditorIcon{background-image:var(--jp-icon-text-editor)}.jupyter-wrapper .jp-TrustedIcon{background-image:var(--jp-icon-trusted)}.jupyter-wrapper .jp-UndoIcon{background-image:var(--jp-icon-undo)}.jupyter-wrapper .jp-VegaIcon{background-image:var(--jp-icon-vega)}.jupyter-wrapper .jp-YamlIcon{background-image:var(--jp-icon-yaml)}.jupyter-wrapper :root{--jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==)}.jupyter-wrapper .jp-Icon,.jupyter-wrapper .jp-MaterialIcon{background-position:center;background-repeat:no-repeat;background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-cover{background-position:center;background-repeat:no-repeat;background-size:cover}.jupyter-wrapper .jp-Icon-16{background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-18{background-size:18px;min-width:18px;min-height:18px}.jupyter-wrapper .jp-Icon-20{background-size:20px;min-width:20px;min-height:20px}.jupyter-wrapper .jp-icon0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-accent0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-accent0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-none[fill]{fill:none}.jupyter-wrapper .jp-icon-none[stroke]{stroke:none}.jupyter-wrapper .jp-icon-brand0[fill]{fill:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[fill]{fill:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[fill]{fill:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[fill]{fill:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-brand0[stroke]{stroke:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[stroke]{stroke:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[stroke]{stroke:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[stroke]{stroke:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[stroke]{stroke:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-warn0[fill]{fill:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[fill]{fill:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[fill]{fill:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[fill]{fill:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-warn0[stroke]{stroke:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[stroke]{stroke:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[stroke]{stroke:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[stroke]{stroke:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-contrast0[fill]{fill:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[fill]{fill:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[fill]{fill:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[fill]{fill:var(--jp-icon-contrast-color3)}.jupyter-wrapper .jp-icon-contrast0[stroke]{stroke:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[stroke]{stroke:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[stroke]{stroke:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[stroke]{stroke:var(--jp-icon-contrast-color3)}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:#fff}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper :root{--jp-warn-color0: var(--md-orange-700)}.jupyter-wrapper .jp-DragIcon{margin-right:4px}.jupyter-wrapper .jp-icon-alt .jp-icon0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hoverShow:not(:hover) svg{display:none !important}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[fill]{fill:none}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[stroke]{stroke:none}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper :focus{outline:unset;outline-offset:unset;-moz-outline-radius:unset}.jupyter-wrapper .jp-Button{border-radius:var(--jp-border-radius);padding:0px 12px;font-size:var(--jp-ui-font-size1)}.jupyter-wrapper button.jp-Button.bp3-button.bp3-minimal:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Button.minimal{color:unset !important}.jupyter-wrapper .jp-Button.jp-ToolbarButtonComponent{text-transform:none}.jupyter-wrapper .jp-InputGroup input{box-sizing:border-box;border-radius:0;background-color:rgba(0,0,0,0);color:var(--jp-ui-font-color0);box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .jp-InputGroup input:focus{box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .jp-InputGroup input::placeholder,.jupyter-wrapper input::placeholder{color:var(--jp-ui-font-color3)}.jupyter-wrapper .jp-BPIcon{display:inline-block;vertical-align:middle;margin:auto}.jupyter-wrapper .bp3-icon.jp-BPIcon>svg:not([fill]){fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-InputGroupAction{padding:6px}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select{background-color:initial;border:none;border-radius:0;box-shadow:none;color:var(--jp-ui-font-color0);display:block;font-size:var(--jp-ui-font-size1);height:24px;line-height:14px;padding:0 25px 0 10px;text-align:left;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select:hover,.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select>option{background-color:var(--jp-layout-color2);color:var(--jp-ui-font-color0)}.jupyter-wrapper select{box-sizing:border-box}.jupyter-wrapper .jp-Collapse{display:flex;flex-direction:column;align-items:stretch;border-top:1px solid var(--jp-border-color2);border-bottom:1px solid var(--jp-border-color2)}.jupyter-wrapper .jp-Collapse-header{padding:1px 12px;color:var(--jp-ui-font-color1);background-color:var(--jp-layout-color1);font-size:var(--jp-ui-font-size2)}.jupyter-wrapper .jp-Collapse-header:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Collapse-contents{padding:0px 12px 0px 12px;background-color:var(--jp-layout-color1);color:var(--jp-ui-font-color1);overflow:auto}.jupyter-wrapper :root{--jp-private-commandpalette-search-height: 28px}.jupyter-wrapper .lm-CommandPalette{padding-bottom:0px;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-search{padding:4px;background-color:var(--jp-layout-color1);z-index:2}.jupyter-wrapper .lm-CommandPalette-wrapper{overflow:overlay;padding:0px 9px;background-color:var(--jp-input-active-background);height:30px;box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper{box-shadow:inset 0 0 0 1px var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .lm-CommandPalette-wrapper::after{content:\" \";color:#fff;background-color:var(--jp-brand-color1);position:absolute;top:4px;right:4px;height:30px;width:10px;padding:0px 10px;background-image:var(--jp-icon-search-white);background-size:20px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .lm-CommandPalette-input{background:rgba(0,0,0,0);width:calc(100% - 18px);float:left;border:none;outline:none;font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);line-height:var(--jp-private-commandpalette-search-height)}.jupyter-wrapper .lm-CommandPalette-input::-webkit-input-placeholder,.jupyter-wrapper .lm-CommandPalette-input::-moz-placeholder,.jupyter-wrapper .lm-CommandPalette-input:-ms-input-placeholder{color:var(--jp-ui-font-color3);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-header:first-child{margin-top:0px}.jupyter-wrapper .lm-CommandPalette-header{border-bottom:solid var(--jp-border-width) var(--jp-border-color2);color:var(--jp-ui-font-color1);cursor:pointer;display:flex;font-size:var(--jp-ui-font-size0);font-weight:600;letter-spacing:1px;margin-top:8px;padding:8px 0 8px 12px;text-transform:uppercase}.jupyter-wrapper .lm-CommandPalette-header.lm-mod-active{background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-header>mark{background-color:rgba(0,0,0,0);font-weight:bold;color:var(--jp-ui-font-color1)}.jupyter-wrapper .lm-CommandPalette-item{padding:4px 12px 4px 4px;color:var(--jp-ui-font-color1);font-size:var(--jp-ui-font-size1);font-weight:400;display:flex}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active{background:var(--jp-layout-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled){background:var(--jp-layout-color4)}.jupyter-wrapper .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled){background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-itemContent{overflow:hidden}.jupyter-wrapper .lm-CommandPalette-itemLabel>mark{color:var(--jp-ui-font-color0);background-color:rgba(0,0,0,0);font-weight:bold}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled mark{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemIcon{margin:0 4px 0 0;position:relative;width:16px;top:2px;flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon{opacity:.4}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-itemCaption{display:none}.jupyter-wrapper .lm-CommandPalette-content{background-color:var(--jp-layout-color1)}.jupyter-wrapper .lm-CommandPalette-content:empty:after{content:\"No results\";margin:auto;margin-top:20px;width:100px;display:block;font-size:var(--jp-ui-font-size2);font-family:var(--jp-ui-font-family);font-weight:lighter}.jupyter-wrapper .lm-CommandPalette-emptyMessage{text-align:center;margin-top:24px;line-height:1.32;padding:0px 8px;color:var(--jp-content-font-color3)}.jupyter-wrapper .jp-Dialog{position:absolute;z-index:10000;display:flex;flex-direction:column;align-items:center;justify-content:center;top:0px;left:0px;margin:0;padding:0;width:100%;height:100%;background:var(--jp-dialog-background)}.jupyter-wrapper .jp-Dialog-content{display:flex;flex-direction:column;margin-left:auto;margin-right:auto;background:var(--jp-layout-color1);padding:24px;padding-bottom:12px;min-width:300px;min-height:150px;max-width:1000px;max-height:500px;box-sizing:border-box;box-shadow:var(--jp-elevation-z20);word-wrap:break-word;border-radius:var(--jp-border-radius);font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color1)}.jupyter-wrapper .jp-Dialog-button{overflow:visible}.jupyter-wrapper button.jp-Dialog-button:focus{outline:1px solid var(--jp-brand-color1);outline-offset:4px;-moz-outline-radius:0px}.jupyter-wrapper button.jp-Dialog-button:focus::-moz-focus-inner{border:0}.jupyter-wrapper .jp-Dialog-header{flex:0 0 auto;padding-bottom:12px;font-size:var(--jp-ui-font-size3);font-weight:400;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-body{display:flex;flex-direction:column;flex:1 1 auto;font-size:var(--jp-ui-font-size1);background:var(--jp-layout-color1);overflow:auto}.jupyter-wrapper .jp-Dialog-footer{display:flex;flex-direction:row;justify-content:flex-end;flex:0 0 auto;margin-left:-12px;margin-right:-12px;padding:12px}.jupyter-wrapper .jp-Dialog-title{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .jp-Dialog-body>.jp-select-wrapper{width:100%}.jupyter-wrapper .jp-Dialog-body>button{padding:0px 16px}.jupyter-wrapper .jp-Dialog-body>label{line-height:1.4;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-button.jp-mod-styled:not(:last-child){margin-right:12px}.jupyter-wrapper .jp-HoverBox{position:fixed}.jupyter-wrapper .jp-HoverBox.jp-mod-outofview{display:none}.jupyter-wrapper .jp-IFrame{width:100%;height:100%}.jupyter-wrapper .jp-IFrame>iframe{border:none}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MainAreaWidget>:focus{outline:none}.jupyter-wrapper :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper .jp-Spinner{position:absolute;display:flex;justify-content:center;align-items:center;z-index:10;left:0;top:0;width:100%;height:100%;background:var(--jp-layout-color0);outline:none}.jupyter-wrapper .jp-SpinnerContent{font-size:10px;margin:50px auto;text-indent:-9999em;width:3em;height:3em;border-radius:50%;background:var(--jp-brand-color3);background:linear-gradient(to right, #f37626 10%, rgba(255, 255, 255, 0) 42%);position:relative;animation:load3 1s infinite linear,fadeIn 1s}.jupyter-wrapper .jp-SpinnerContent:before{width:50%;height:50%;background:#f37626;border-radius:100% 0 0 0;position:absolute;top:0;left:0;content:\"\"}.jupyter-wrapper .jp-SpinnerContent:after{background:var(--jp-layout-color0);width:75%;height:75%;border-radius:50%;content:\"\";margin:auto;position:absolute;top:0;left:0;bottom:0;right:0}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}@keyframes load3{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.jupyter-wrapper button.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:none;box-sizing:border-box;text-align:center;line-height:32px;height:32px;padding:0px 12px;letter-spacing:.8px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled{background:var(--jp-input-background);height:28px;box-sizing:border-box;border:var(--jp-border-width) solid var(--jp-border-color1);padding-left:7px;padding-right:7px;font-size:var(--jp-ui-font-size2);color:var(--jp-ui-font-color0);outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled:focus{border:var(--jp-border-width) solid var(--md-blue-500);box-shadow:inset 0 0 4px var(--md-blue-300)}.jupyter-wrapper .jp-select-wrapper{display:flex;position:relative;flex-direction:column;padding:1px;background-color:var(--jp-layout-color1);height:28px;box-sizing:border-box;margin-bottom:12px}.jupyter-wrapper .jp-select-wrapper.jp-mod-focused select.jp-mod-styled{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-input-active-background)}.jupyter-wrapper select.jp-mod-styled:hover{background-color:var(--jp-layout-color1);cursor:pointer;color:var(--jp-ui-font-color0);background-color:var(--jp-input-hover-background);box-shadow:inset 0 0px 1px rgba(0,0,0,.5)}.jupyter-wrapper select.jp-mod-styled{flex:1 1 auto;height:32px;width:100%;font-size:var(--jp-ui-font-size2);background:var(--jp-input-background);color:var(--jp-ui-font-color0);padding:0 25px 0 8px;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper :root{--jp-private-toolbar-height: calc( 28px + var(--jp-border-width) )}.jupyter-wrapper .jp-Toolbar{color:var(--jp-ui-font-color1);flex:0 0 auto;display:flex;flex-direction:row;border-bottom:var(--jp-border-width) solid var(--jp-toolbar-border-color);box-shadow:var(--jp-toolbar-box-shadow);background:var(--jp-toolbar-background);min-height:var(--jp-toolbar-micro-height);padding:2px;z-index:1}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item.jp-Toolbar-spacer{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-Toolbar-item.jp-Toolbar-kernelStatus{display:inline-block;width:32px;background-repeat:no-repeat;background-position:center;background-size:16px}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item{flex:0 0 auto;display:flex;padding-left:1px;padding-right:1px;font-size:var(--jp-ui-font-size1);line-height:var(--jp-private-toolbar-height);height:100%}.jupyter-wrapper div.jp-ToolbarButton{color:rgba(0,0,0,0);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px;margin:0px}.jupyter-wrapper button.jp-ToolbarButtonComponent{background:var(--jp-layout-color1);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px 6px;margin:0px;height:24px;border-radius:var(--jp-border-radius);display:flex;align-items:center;text-align:center;font-size:14px;min-width:unset;min-height:unset}.jupyter-wrapper button.jp-ToolbarButtonComponent:disabled{opacity:.4}.jupyter-wrapper button.jp-ToolbarButtonComponent span{padding:0px;flex:0 0 auto}.jupyter-wrapper button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label{font-size:var(--jp-ui-font-size1);line-height:100%;padding-left:2px;color:var(--jp-ui-font-color1)}.jupyter-wrapper body.p-mod-override-cursor *,.jupyter-wrapper body.lm-mod-override-cursor *{cursor:inherit !important}.jupyter-wrapper .jp-JSONEditor{display:flex;flex-direction:column;width:100%}.jupyter-wrapper .jp-JSONEditor-host{flex:1 1 auto;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;background:var(--jp-layout-color0);min-height:50px;padding:1px}.jupyter-wrapper .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host{border-color:red;outline-color:red}.jupyter-wrapper .jp-JSONEditor-header{display:flex;flex:1 0 auto;padding:0 0 0 12px}.jupyter-wrapper .jp-JSONEditor-header label{flex:0 0 auto}.jupyter-wrapper .jp-JSONEditor-commitButton{height:16px;width:16px;background-size:18px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .jp-JSONEditor-host.jp-mod-focused{background-color:var(--jp-input-active-background);border:1px solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .jp-Editor.jp-mod-dropTarget{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.jupyter-wrapper .CodeMirror-lines{padding:4px 0}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{padding:0 4px}.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{background-color:#fff}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.jupyter-wrapper .CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.jupyter-wrapper .CodeMirror-guttermarker{color:#000}.jupyter-wrapper .CodeMirror-guttermarker-subtle{color:#999}.jupyter-wrapper .CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.jupyter-wrapper .CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.jupyter-wrapper .cm-fat-cursor .CodeMirror-cursor{width:auto;border:0 !important;background:#7e7}.jupyter-wrapper .cm-fat-cursor div.CodeMirror-cursors{z-index:1}.jupyter-wrapper .cm-fat-cursor-mark{background-color:rgba(20,255,20,.5);-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite}.jupyter-wrapper .cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@-webkit-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@keyframes blink{50%{background-color:rgba(0,0,0,0)}}.jupyter-wrapper .cm-tab{display:inline-block;text-decoration:inherit}.jupyter-wrapper .CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:0;overflow:hidden}.jupyter-wrapper .CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.jupyter-wrapper .cm-s-default .cm-header{color:blue}.jupyter-wrapper .cm-s-default .cm-quote{color:#090}.jupyter-wrapper .cm-negative{color:#d44}.jupyter-wrapper .cm-positive{color:#292}.jupyter-wrapper .cm-header,.jupyter-wrapper .cm-strong{font-weight:bold}.jupyter-wrapper .cm-em{font-style:italic}.jupyter-wrapper .cm-link{text-decoration:underline}.jupyter-wrapper .cm-strikethrough{text-decoration:line-through}.jupyter-wrapper .cm-s-default .cm-keyword{color:#708}.jupyter-wrapper .cm-s-default .cm-atom{color:#219}.jupyter-wrapper .cm-s-default .cm-number{color:#164}.jupyter-wrapper .cm-s-default .cm-def{color:blue}.jupyter-wrapper .cm-s-default .cm-variable-2{color:#05a}.jupyter-wrapper .cm-s-default .cm-variable-3,.jupyter-wrapper .cm-s-default .cm-type{color:#085}.jupyter-wrapper .cm-s-default .cm-comment{color:#a50}.jupyter-wrapper .cm-s-default .cm-string{color:#a11}.jupyter-wrapper .cm-s-default .cm-string-2{color:#f50}.jupyter-wrapper .cm-s-default .cm-meta{color:#555}.jupyter-wrapper .cm-s-default .cm-qualifier{color:#555}.jupyter-wrapper .cm-s-default .cm-builtin{color:#30a}.jupyter-wrapper .cm-s-default .cm-bracket{color:#997}.jupyter-wrapper .cm-s-default .cm-tag{color:#170}.jupyter-wrapper .cm-s-default .cm-attribute{color:#00c}.jupyter-wrapper .cm-s-default .cm-hr{color:#999}.jupyter-wrapper .cm-s-default .cm-link{color:#00c}.jupyter-wrapper .cm-s-default .cm-error{color:red}.jupyter-wrapper .cm-invalidchar{color:red}.jupyter-wrapper .CodeMirror-composing{border-bottom:2px solid}.jupyter-wrapper div.CodeMirror span.CodeMirror-matchingbracket{color:#0b0}.jupyter-wrapper div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#a22}.jupyter-wrapper .CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.jupyter-wrapper .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .CodeMirror{position:relative;overflow:hidden;background:#fff}.jupyter-wrapper .CodeMirror-scroll{overflow:scroll !important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:none;position:relative}.jupyter-wrapper .CodeMirror-sizer{position:relative;border-right:30px solid rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-vscrollbar,.jupyter-wrapper .CodeMirror-hscrollbar,.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{position:absolute;z-index:6;display:none}.jupyter-wrapper .CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.jupyter-wrapper .CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.jupyter-wrapper .CodeMirror-scrollbar-filler{right:0;bottom:0}.jupyter-wrapper .CodeMirror-gutter-filler{left:0;bottom:0}.jupyter-wrapper .CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.jupyter-wrapper .CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.jupyter-wrapper .CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:none !important;border:none !important}.jupyter-wrapper .CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.jupyter-wrapper .CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.jupyter-wrapper .CodeMirror-gutter-wrapper ::selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-gutter-wrapper ::-moz-selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-lines{cursor:text;min-height:1px}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:rgba(0,0,0,0);font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line,.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line-like{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.jupyter-wrapper .CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.jupyter-wrapper .CodeMirror-linewidget{position:relative;z-index:2;padding:.1px}.jupyter-wrapper .CodeMirror-rtl pre{direction:rtl}.jupyter-wrapper .CodeMirror-code{outline:none}.jupyter-wrapper .CodeMirror-scroll,.jupyter-wrapper .CodeMirror-sizer,.jupyter-wrapper .CodeMirror-gutter,.jupyter-wrapper .CodeMirror-gutters,.jupyter-wrapper .CodeMirror-linenumber{-moz-box-sizing:content-box;box-sizing:content-box}.jupyter-wrapper .CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.jupyter-wrapper .CodeMirror-cursor{position:absolute;pointer-events:none}.jupyter-wrapper .CodeMirror-measure pre{position:static}.jupyter-wrapper div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}.jupyter-wrapper div.CodeMirror-dragcursors{visibility:visible}.jupyter-wrapper .CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.jupyter-wrapper .CodeMirror-selected{background:#d9d9d9}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.jupyter-wrapper .CodeMirror-crosshair{cursor:crosshair}.jupyter-wrapper .CodeMirror-line::selection,.jupyter-wrapper .CodeMirror-line>span::selection,.jupyter-wrapper .CodeMirror-line>span>span::selection{background:#d7d4f0}.jupyter-wrapper .CodeMirror-line::-moz-selection,.jupyter-wrapper .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.jupyter-wrapper .cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.jupyter-wrapper .cm-force-border{padding-right:.1px}@media print{.jupyter-wrapper .CodeMirror div.CodeMirror-cursors{visibility:hidden}}.jupyter-wrapper .cm-tab-wrap-hack:after{content:\"\"}.jupyter-wrapper span.CodeMirror-selectedtext{background:none}.jupyter-wrapper .CodeMirror-dialog{position:absolute;left:0;right:0;background:inherit;z-index:15;padding:.1em .8em;overflow:hidden;color:inherit}.jupyter-wrapper .CodeMirror-dialog-top{border-bottom:1px solid #eee;top:0}.jupyter-wrapper .CodeMirror-dialog-bottom{border-top:1px solid #eee;bottom:0}.jupyter-wrapper .CodeMirror-dialog input{border:none;outline:none;background:rgba(0,0,0,0);width:20em;color:inherit;font-family:monospace}.jupyter-wrapper .CodeMirror-dialog button{font-size:70%}.jupyter-wrapper .CodeMirror-foldmarker{color:blue;text-shadow:#b9f 1px 1px 2px,#b9f -1px -1px 2px,#b9f 1px -1px 2px,#b9f -1px 1px 2px;font-family:arial;line-height:.3;cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter{width:.7em}.jupyter-wrapper .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter-open:after{content:\"\u25be\"}.jupyter-wrapper .CodeMirror-foldgutter-folded:after{content:\"\u25b8\"}.jupyter-wrapper .cm-s-material.CodeMirror{background-color:#263238;color:#eff}.jupyter-wrapper .cm-s-material .CodeMirror-gutters{background:#263238;color:#546e7a;border:none}.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker,.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker-subtle,.jupyter-wrapper .cm-s-material .CodeMirror-linenumber{color:#546e7a}.jupyter-wrapper .cm-s-material .CodeMirror-cursor{border-left:1px solid #fc0}.jupyter-wrapper .cm-s-material div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material.CodeMirror-focused div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::-moz-selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-activeline-background{background:rgba(0,0,0,.5)}.jupyter-wrapper .cm-s-material .cm-keyword{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-operator{color:#89ddff}.jupyter-wrapper .cm-s-material .cm-variable-2{color:#eff}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#f07178}.jupyter-wrapper .cm-s-material .cm-builtin{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-atom{color:#f78c6c}.jupyter-wrapper .cm-s-material .cm-number{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-def{color:#82aaff}.jupyter-wrapper .cm-s-material .cm-string{color:#c3e88d}.jupyter-wrapper .cm-s-material .cm-string-2{color:#f07178}.jupyter-wrapper .cm-s-material .cm-comment{color:#546e7a}.jupyter-wrapper .cm-s-material .cm-variable{color:#f07178}.jupyter-wrapper .cm-s-material .cm-tag{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-meta{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-attribute{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-property{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-qualifier{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-error{color:#fff;background-color:#ff5370}.jupyter-wrapper .cm-s-material .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-gutters{background:#3f3f3f !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{color:#999}.jupyter-wrapper .cm-s-zenburn .CodeMirror-cursor{border-left:1px solid #fff}.jupyter-wrapper .cm-s-zenburn{background-color:#3f3f3f;color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-builtin{color:#dcdccc;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-comment{color:#7f9f7f}.jupyter-wrapper .cm-s-zenburn span.cm-keyword{color:#f0dfaf;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-atom{color:#bfebbf}.jupyter-wrapper .cm-s-zenburn span.cm-def{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-variable{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-variable-2{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-string{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-string-2{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-number{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-tag{color:#93e0e3}.jupyter-wrapper .cm-s-zenburn span.cm-property{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-attribute{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-qualifier{color:#7cb8bb}.jupyter-wrapper .cm-s-zenburn span.cm-meta{color:#f0dfaf}.jupyter-wrapper .cm-s-zenburn span.cm-header{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.cm-operator{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-matchingbracket{box-sizing:border-box;background:rgba(0,0,0,0);border-bottom:1px solid}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-nonmatchingbracket{border-bottom:1px solid;background:none}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline{background:#000}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline-background{background:#000}.jupyter-wrapper .cm-s-zenburn div.CodeMirror-selected{background:#545454}.jupyter-wrapper .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected{background:#4f4f4f}.jupyter-wrapper .cm-s-abcdef.CodeMirror{background:#0f0f0f;color:#defdef}.jupyter-wrapper .cm-s-abcdef div.CodeMirror-selected{background:#515151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::-moz-selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-gutters{background:#555;border-right:2px solid #314151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker{color:#222}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker-subtle{color:azure}.jupyter-wrapper .cm-s-abcdef .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-abcdef .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-abcdef span.cm-keyword{color:#b8860b;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-atom{color:#77f}.jupyter-wrapper .cm-s-abcdef span.cm-number{color:violet}.jupyter-wrapper .cm-s-abcdef span.cm-def{color:#fffabc}.jupyter-wrapper .cm-s-abcdef span.cm-variable{color:#abcdef}.jupyter-wrapper .cm-s-abcdef span.cm-variable-2{color:#cacbcc}.jupyter-wrapper .cm-s-abcdef span.cm-variable-3,.jupyter-wrapper .cm-s-abcdef span.cm-type{color:#def}.jupyter-wrapper .cm-s-abcdef span.cm-property{color:#fedcba}.jupyter-wrapper .cm-s-abcdef span.cm-operator{color:#ff0}.jupyter-wrapper .cm-s-abcdef span.cm-comment{color:#7a7b7c;font-style:italic}.jupyter-wrapper .cm-s-abcdef span.cm-string{color:#2b4}.jupyter-wrapper .cm-s-abcdef span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-abcdef span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-abcdef span.cm-builtin{color:#30aabc}.jupyter-wrapper .cm-s-abcdef span.cm-bracket{color:#8a8a8a}.jupyter-wrapper .cm-s-abcdef span.cm-tag{color:#fd4}.jupyter-wrapper .cm-s-abcdef span.cm-attribute{color:#df0}.jupyter-wrapper .cm-s-abcdef span.cm-error{color:red}.jupyter-wrapper .cm-s-abcdef span.cm-header{color:#7fffd4;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-link{color:#8a2be2}.jupyter-wrapper .cm-s-abcdef .CodeMirror-activeline-background{background:#314151}.jupyter-wrapper .cm-s-base16-light.CodeMirror{background:#f5f5f5;color:#202020}.jupyter-wrapper .cm-s-base16-light div.CodeMirror-selected{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::-moz-selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-gutters{background:#f5f5f5;border-right:0px}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker-subtle{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-linenumber{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-cursor{border-left:1px solid #505050}.jupyter-wrapper .cm-s-base16-light span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-light span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-property,.jupyter-wrapper .cm-s-base16-light span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-light span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-light span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-light span.cm-bracket{color:#202020}.jupyter-wrapper .cm-s-base16-light span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-error{background:#ac4142;color:#505050}.jupyter-wrapper .cm-s-base16-light .CodeMirror-activeline-background{background:#dddcdc}.jupyter-wrapper .cm-s-base16-light .CodeMirror-matchingbracket{color:#f5f5f5 !important;background-color:#6a9fb5 !important}.jupyter-wrapper .cm-s-base16-dark.CodeMirror{background:#151515;color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark div.CodeMirror-selected{background:#303030}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-gutters{background:#151515;border-right:0px}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker-subtle{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-linenumber{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-cursor{border-left:1px solid #b0b0b0}.jupyter-wrapper .cm-s-base16-dark span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-dark span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-property,.jupyter-wrapper .cm-s-base16-dark span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-dark span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-dark span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-dark span.cm-bracket{color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-error{background:#ac4142;color:#b0b0b0}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-activeline-background{background:#202020}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-dracula.CodeMirror,.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{background-color:#282a36 !important;color:#f8f8f2 !important;border:none}.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{color:#282a36}.jupyter-wrapper .cm-s-dracula .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-dracula .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-dracula .CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula span.cm-comment{color:#6272a4}.jupyter-wrapper .cm-s-dracula span.cm-string,.jupyter-wrapper .cm-s-dracula span.cm-string-2{color:#f1fa8c}.jupyter-wrapper .cm-s-dracula span.cm-number{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-variable{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-2{color:#fff}.jupyter-wrapper .cm-s-dracula span.cm-def{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-operator{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-atom{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-meta{color:#f8f8f2}.jupyter-wrapper .cm-s-dracula span.cm-tag{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-attribute{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-qualifier{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-property{color:#66d9ef}.jupyter-wrapper .cm-s-dracula span.cm-builtin{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-3,.jupyter-wrapper .cm-s-dracula span.cm-type{color:#ffb86c}.jupyter-wrapper .cm-s-dracula .CodeMirror-activeline-background{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch.CodeMirror{background:#322931;color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch div.CodeMirror-selected{background:#433b42 !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-gutters{background:#322931;border-right:0px}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-linenumber{color:#797379}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-cursor{border-left:1px solid #989498 !important}.jupyter-wrapper .cm-s-hopscotch span.cm-comment{color:#b33508}.jupyter-wrapper .cm-s-hopscotch span.cm-atom{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-number{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-property,.jupyter-wrapper .cm-s-hopscotch span.cm-attribute{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-keyword{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-string{color:#fdcc59}.jupyter-wrapper .cm-s-hopscotch span.cm-variable{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-variable-2{color:#1290bf}.jupyter-wrapper .cm-s-hopscotch span.cm-def{color:#fd8b19}.jupyter-wrapper .cm-s-hopscotch span.cm-error{background:#dd464c;color:#989498}.jupyter-wrapper .cm-s-hopscotch span.cm-bracket{color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch span.cm-tag{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-link{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-activeline-background{background:#302020}.jupyter-wrapper .cm-s-mbo.CodeMirror{background:#2c2c2c;color:#ffffec}.jupyter-wrapper .cm-s-mbo div.CodeMirror-selected{background:#716c62}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::-moz-selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-gutters{background:#4e4e4e;border-right:0px}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker{color:#fff}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker-subtle{color:gray}.jupyter-wrapper .cm-s-mbo .CodeMirror-linenumber{color:#dadada}.jupyter-wrapper .cm-s-mbo .CodeMirror-cursor{border-left:1px solid #ffffec}.jupyter-wrapper .cm-s-mbo span.cm-comment{color:#95958a}.jupyter-wrapper .cm-s-mbo span.cm-atom{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-number{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-property,.jupyter-wrapper .cm-s-mbo span.cm-attribute{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-keyword{color:#ffb928}.jupyter-wrapper .cm-s-mbo span.cm-string{color:#ffcf6c}.jupyter-wrapper .cm-s-mbo span.cm-string.cm-property{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable-2{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-def{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-bracket{color:#fffffc;font-weight:bold}.jupyter-wrapper .cm-s-mbo span.cm-tag{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-link{color:#f54b07}.jupyter-wrapper .cm-s-mbo span.cm-error{border-bottom:#636363;color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-qualifier{color:#ffffec}.jupyter-wrapper .cm-s-mbo .CodeMirror-activeline-background{background:#494b41}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingbracket{color:#ffb928 !important}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingtag{background:rgba(255,255,255,.37)}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{color:#999;background-color:#fff}.jupyter-wrapper .cm-s-mdn-like div.CodeMirror-selected{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::-moz-selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-gutters{background:#f8f8f8;border-left:6px solid rgba(0,83,159,.65);color:#333}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-linenumber{color:#aaa;padding-left:8px}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-cursor{border-left:2px solid #222}.jupyter-wrapper .cm-s-mdn-like .cm-keyword{color:#6262ff}.jupyter-wrapper .cm-s-mdn-like .cm-atom{color:#f90}.jupyter-wrapper .cm-s-mdn-like .cm-number{color:#ca7841}.jupyter-wrapper .cm-s-mdn-like .cm-def{color:#8da6ce}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-2,.jupyter-wrapper .cm-s-mdn-like span.cm-tag{color:#690}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-3,.jupyter-wrapper .cm-s-mdn-like span.cm-def,.jupyter-wrapper .cm-s-mdn-like span.cm-type{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-variable{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-property{color:#905}.jupyter-wrapper .cm-s-mdn-like .cm-qualifier{color:#690}.jupyter-wrapper .cm-s-mdn-like .cm-operator{color:#cda869}.jupyter-wrapper .cm-s-mdn-like .cm-comment{color:#777;font-weight:normal}.jupyter-wrapper .cm-s-mdn-like .cm-string{color:#07a;font-style:italic}.jupyter-wrapper .cm-s-mdn-like .cm-string-2{color:#bd6b18}.jupyter-wrapper .cm-s-mdn-like .cm-meta{color:#000}.jupyter-wrapper .cm-s-mdn-like .cm-builtin{color:#9b7536}.jupyter-wrapper .cm-s-mdn-like .cm-tag{color:#997643}.jupyter-wrapper .cm-s-mdn-like .cm-attribute{color:#d6bb6d}.jupyter-wrapper .cm-s-mdn-like .cm-header{color:#ff6400}.jupyter-wrapper .cm-s-mdn-like .cm-hr{color:#aeaeae}.jupyter-wrapper .cm-s-mdn-like .cm-link{color:#ad9361;font-style:italic;text-decoration:none}.jupyter-wrapper .cm-s-mdn-like .cm-error{border-bottom:1px solid red}.jupyter-wrapper div.cm-s-mdn-like .CodeMirror-activeline-background{background:#efefff}.jupyter-wrapper div.cm-s-mdn-like span.CodeMirror-matchingbracket{outline:1px solid gray;color:inherit}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{background-image:url(data:image/png;base64,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)}.jupyter-wrapper .cm-s-seti.CodeMirror{background-color:#151718 !important;color:#cfd2d1 !important;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-gutters{color:#404b53;background-color:#0e1112;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-seti .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-seti.CodeMirror-focused div.CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti span.cm-comment{color:#41535b}.jupyter-wrapper .cm-s-seti span.cm-string,.jupyter-wrapper .cm-s-seti span.cm-string-2{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-number{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-variable{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-variable-2{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-def{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-seti span.cm-operator{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#e6cd69}.jupyter-wrapper .cm-s-seti span.cm-atom{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-meta{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-tag{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-attribute{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-qualifier{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-property{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-variable-3,.jupyter-wrapper .cm-s-seti span.cm-type{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-builtin{color:#9fca56}.jupyter-wrapper .cm-s-seti .CodeMirror-activeline-background{background:#101213}.jupyter-wrapper .cm-s-seti .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .solarized.base03{color:#002b36}.jupyter-wrapper .solarized.base02{color:#073642}.jupyter-wrapper .solarized.base01{color:#586e75}.jupyter-wrapper .solarized.base00{color:#657b83}.jupyter-wrapper .solarized.base0{color:#839496}.jupyter-wrapper .solarized.base1{color:#93a1a1}.jupyter-wrapper .solarized.base2{color:#eee8d5}.jupyter-wrapper .solarized.base3{color:#fdf6e3}.jupyter-wrapper .solarized.solar-yellow{color:#b58900}.jupyter-wrapper .solarized.solar-orange{color:#cb4b16}.jupyter-wrapper .solarized.solar-red{color:#dc322f}.jupyter-wrapper .solarized.solar-magenta{color:#d33682}.jupyter-wrapper .solarized.solar-violet{color:#6c71c4}.jupyter-wrapper .solarized.solar-blue{color:#268bd2}.jupyter-wrapper .solarized.solar-cyan{color:#2aa198}.jupyter-wrapper .solarized.solar-green{color:#859900}.jupyter-wrapper .cm-s-solarized{line-height:1.45em;color-profile:sRGB;rendering-intent:auto}.jupyter-wrapper .cm-s-solarized.cm-s-dark{color:#839496;background-color:#002b36;text-shadow:#002b36 0 1px}.jupyter-wrapper .cm-s-solarized.cm-s-light{background-color:#fdf6e3;color:#657b83;text-shadow:#eee8d5 0 1px}.jupyter-wrapper .cm-s-solarized .CodeMirror-widget{text-shadow:none}.jupyter-wrapper .cm-s-solarized .cm-header{color:#586e75}.jupyter-wrapper .cm-s-solarized .cm-quote{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-keyword{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .cm-atom{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-number{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-def{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-variable{color:#839496}.jupyter-wrapper .cm-s-solarized .cm-variable-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-variable-3,.jupyter-wrapper .cm-s-solarized .cm-type{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-property{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-operator{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-comment{color:#586e75;font-style:italic}.jupyter-wrapper .cm-s-solarized .cm-string{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-string-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-meta{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-qualifier{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-builtin{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-bracket{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-matchingbracket{color:#859900}.jupyter-wrapper .cm-s-solarized .CodeMirror-nonmatchingbracket{color:#dc322f}.jupyter-wrapper .cm-s-solarized .cm-tag{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-attribute{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-hr{color:rgba(0,0,0,0);border-top:1px solid #586e75;display:block}.jupyter-wrapper .cm-s-solarized .cm-link{color:#93a1a1;cursor:pointer}.jupyter-wrapper .cm-s-solarized .cm-special{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-em{color:#999;text-decoration:underline;text-decoration-style:dotted}.jupyter-wrapper .cm-s-solarized .cm-error,.jupyter-wrapper .cm-s-solarized .cm-invalidchar{color:#586e75;border-bottom:1px dotted #dc322f}.jupyter-wrapper .cm-s-solarized.cm-s-dark div.CodeMirror-selected{background:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark.CodeMirror ::selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-light div.CodeMirror-selected{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span>span::selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span>span::-moz-selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.CodeMirror{-moz-box-shadow:inset 7px 0 12px -6px #000;-webkit-box-shadow:inset 7px 0 12px -6px #000;box-shadow:inset 7px 0 12px -6px #000}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutters{border-right:0}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-gutters{background-color:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-linenumber{color:#586e75;text-shadow:#021014 0 -1px}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-gutters{background-color:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-linenumber{color:#839496}.jupyter-wrapper .cm-s-solarized .CodeMirror-linenumber{padding:0 5px}.jupyter-wrapper .cm-s-solarized .CodeMirror-guttermarker-subtle{color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker{color:#ddd}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-guttermarker{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text{color:#586e75}.jupyter-wrapper .cm-s-solarized .CodeMirror-cursor{border-left:1px solid #819090}.jupyter-wrapper .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor{background:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-light .cm-animate-fat-cursor{background-color:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor{background:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor{background-color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background{background:rgba(255,255,255,.06)}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-activeline-background{background:rgba(0,0,0,.06)}.jupyter-wrapper .cm-s-the-matrix.CodeMirror{background:#000;color:lime}.jupyter-wrapper .cm-s-the-matrix div.CodeMirror-selected{background:#2d2d2d}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::-moz-selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-gutters{background:#060;border-right:2px solid lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker{color:lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker-subtle{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-the-matrix span.cm-keyword{color:#008803;font-weight:bold}.jupyter-wrapper .cm-s-the-matrix span.cm-atom{color:#3ff}.jupyter-wrapper .cm-s-the-matrix span.cm-number{color:#ffb94f}.jupyter-wrapper .cm-s-the-matrix span.cm-def{color:#99c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable{color:#f6c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-2{color:#c6f}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-3,.jupyter-wrapper .cm-s-the-matrix span.cm-type{color:#96f}.jupyter-wrapper .cm-s-the-matrix span.cm-property{color:#62ffa0}.jupyter-wrapper .cm-s-the-matrix span.cm-operator{color:#999}.jupyter-wrapper .cm-s-the-matrix span.cm-comment{color:#ccc}.jupyter-wrapper .cm-s-the-matrix span.cm-string{color:#39c}.jupyter-wrapper .cm-s-the-matrix span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-the-matrix span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-builtin{color:#30a}.jupyter-wrapper .cm-s-the-matrix span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-the-matrix span.cm-tag{color:#ffbd40}.jupyter-wrapper .cm-s-the-matrix span.cm-attribute{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-error{color:red}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-activeline-background{background:#040}.jupyter-wrapper .cm-s-xq-light span.cm-keyword{line-height:1em;font-weight:bold;color:#5a5cad}.jupyter-wrapper .cm-s-xq-light span.cm-atom{color:#6c8cd5}.jupyter-wrapper .cm-s-xq-light span.cm-number{color:#164}.jupyter-wrapper .cm-s-xq-light span.cm-def{text-decoration:underline}.jupyter-wrapper .cm-s-xq-light span.cm-variable{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-2{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-3,.jupyter-wrapper .cm-s-xq-light span.cm-type{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-comment{color:#0080ff;font-style:italic}.jupyter-wrapper .cm-s-xq-light span.cm-string{color:red}.jupyter-wrapper .cm-s-xq-light span.cm-meta{color:#ff0}.jupyter-wrapper .cm-s-xq-light span.cm-qualifier{color:gray}.jupyter-wrapper .cm-s-xq-light span.cm-builtin{color:#7ea656}.jupyter-wrapper .cm-s-xq-light span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-xq-light span.cm-tag{color:#3f7f7f}.jupyter-wrapper .cm-s-xq-light span.cm-attribute{color:#7f007f}.jupyter-wrapper .cm-s-xq-light span.cm-error{color:red}.jupyter-wrapper .cm-s-xq-light .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .cm-s-xq-light .CodeMirror-matchingbracket{outline:1px solid gray;color:#000 !important;background:#ff0}.jupyter-wrapper .CodeMirror{line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);border:0;border-radius:0;height:auto}.jupyter-wrapper .CodeMirror pre{padding:0 var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-dialog{background-color:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .CodeMirror-lines{padding:var(--jp-code-padding) 0}.jupyter-wrapper .CodeMirror-linenumber{padding:0 8px}.jupyter-wrapper .jp-CodeMirrorEditor-static{margin:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor,.jupyter-wrapper .jp-CodeMirrorEditor-static{cursor:text}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}@media screen and (min-width: 2138px)and (max-width: 4319px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color)}}@media screen and (min-width: 4320px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color)}}.jupyter-wrapper .CodeMirror.jp-mod-readOnly .CodeMirror-cursor{display:none}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid var(--jp-border-color2);background-color:var(--jp-layout-color0)}.jupyter-wrapper .jp-CollaboratorCursor{border-left:5px solid rgba(0,0,0,0);border-right:5px solid rgba(0,0,0,0);border-top:none;border-bottom:3px solid;background-clip:content-box;margin-left:-5px;margin-right:-5px}.jupyter-wrapper .CodeMirror-selectedtext.cm-searching{background-color:var(--jp-search-selected-match-background-color) !important;color:var(--jp-search-selected-match-color) !important}.jupyter-wrapper .cm-searching{background-color:var(--jp-search-unselected-match-background-color) !important;color:var(--jp-search-unselected-match-color) !important}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background-color:var(--jp-editor-selected-focused-background)}.jupyter-wrapper .CodeMirror-selected{background-color:var(--jp-editor-selected-background)}.jupyter-wrapper .jp-CollaboratorCursor-hover{position:absolute;z-index:1;transform:translateX(-50%);color:#fff;border-radius:3px;padding-left:4px;padding-right:4px;padding-top:1px;padding-bottom:1px;text-align:center;font-size:var(--jp-ui-font-size1);white-space:nowrap}.jupyter-wrapper .jp-CodeMirror-ruler{border-left:1px dashed var(--jp-border-color2)}.jupyter-wrapper .CodeMirror.cm-s-jupyter{background:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .jp-CodeConsole .CodeMirror.cm-s-jupyter,.jupyter-wrapper .jp-Notebook .CodeMirror.cm-s-jupyter{background:rgba(0,0,0,0)}.jupyter-wrapper .cm-s-jupyter .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}.jupyter-wrapper .cm-s-jupyter span.cm-keyword{color:var(--jp-mirror-editor-keyword-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-atom{color:var(--jp-mirror-editor-atom-color)}.jupyter-wrapper .cm-s-jupyter span.cm-number{color:var(--jp-mirror-editor-number-color)}.jupyter-wrapper .cm-s-jupyter span.cm-def{color:var(--jp-mirror-editor-def-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable{color:var(--jp-mirror-editor-variable-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-2{color:var(--jp-mirror-editor-variable-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-3{color:var(--jp-mirror-editor-variable-3-color)}.jupyter-wrapper .cm-s-jupyter span.cm-punctuation{color:var(--jp-mirror-editor-punctuation-color)}.jupyter-wrapper .cm-s-jupyter span.cm-property{color:var(--jp-mirror-editor-property-color)}.jupyter-wrapper .cm-s-jupyter span.cm-operator{color:var(--jp-mirror-editor-operator-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-comment{color:var(--jp-mirror-editor-comment-color);font-style:italic}.jupyter-wrapper .cm-s-jupyter span.cm-string{color:var(--jp-mirror-editor-string-color)}.jupyter-wrapper .cm-s-jupyter span.cm-string-2{color:var(--jp-mirror-editor-string-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-meta{color:var(--jp-mirror-editor-meta-color)}.jupyter-wrapper .cm-s-jupyter span.cm-qualifier{color:var(--jp-mirror-editor-qualifier-color)}.jupyter-wrapper .cm-s-jupyter span.cm-builtin{color:var(--jp-mirror-editor-builtin-color)}.jupyter-wrapper .cm-s-jupyter span.cm-bracket{color:var(--jp-mirror-editor-bracket-color)}.jupyter-wrapper .cm-s-jupyter span.cm-tag{color:var(--jp-mirror-editor-tag-color)}.jupyter-wrapper .cm-s-jupyter span.cm-attribute{color:var(--jp-mirror-editor-attribute-color)}.jupyter-wrapper .cm-s-jupyter span.cm-header{color:var(--jp-mirror-editor-header-color)}.jupyter-wrapper .cm-s-jupyter span.cm-quote{color:var(--jp-mirror-editor-quote-color)}.jupyter-wrapper .cm-s-jupyter span.cm-link{color:var(--jp-mirror-editor-link-color)}.jupyter-wrapper .cm-s-jupyter span.cm-error{color:var(--jp-mirror-editor-error-color)}.jupyter-wrapper .cm-s-jupyter span.cm-hr{color:#999}.jupyter-wrapper .cm-s-jupyter span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}.jupyter-wrapper .cm-s-jupyter .CodeMirror-activeline-background,.jupyter-wrapper .cm-s-jupyter .CodeMirror-gutter{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-RenderedLatex{color:var(--jp-content-font-color1);font-size:var(--jp-content-font-size1);line-height:var(--jp-content-line-height)}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedLatex{padding:var(--jp-code-padding);text-align:left}.jupyter-wrapper .jp-MimeDocument{outline:none}.jupyter-wrapper :root{--jp-private-filebrowser-button-height: 28px;--jp-private-filebrowser-button-width: 48px}.jupyter-wrapper .jp-FileBrowser{display:flex;flex-direction:column;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{border-bottom:none;height:auto;margin:var(--jp-toolbar-header-margin);box-shadow:none}.jupyter-wrapper .jp-BreadCrumbs{flex:0 0 auto;margin:4px 12px}.jupyter-wrapper .jp-BreadCrumbs-item{margin:0px 2px;padding:0px 2px;border-radius:var(--jp-border-radius);cursor:pointer}.jupyter-wrapper .jp-BreadCrumbs-item:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-BreadCrumbs-item:first-child{margin-left:0px}.jupyter-wrapper .jp-BreadCrumbs-item.jp-mod-dropTarget{background-color:var(--jp-brand-color2);opacity:.7}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{padding:0px}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{justify-content:space-evenly}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item{flex:1}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent{width:100%}.jupyter-wrapper .jp-DirListing{flex:1 1 auto;display:flex;flex-direction:column;outline:0}.jupyter-wrapper .jp-DirListing-header{flex:0 0 auto;display:flex;flex-direction:row;overflow:hidden;border-top:var(--jp-border-width) solid var(--jp-border-color2);border-bottom:var(--jp-border-width) solid var(--jp-border-color1);box-shadow:var(--jp-toolbar-box-shadow);z-index:2}.jupyter-wrapper .jp-DirListing-headerItem{padding:4px 12px 2px 12px;font-weight:500}.jupyter-wrapper .jp-DirListing-headerItem:hover{background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-name{flex:1 0 84px}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-modified{flex:0 0 112px;border-left:var(--jp-border-width) solid var(--jp-border-color2);text-align:right}.jupyter-wrapper .jp-DirListing-narrow .jp-id-modified,.jupyter-wrapper .jp-DirListing-narrow .jp-DirListing-itemModified{display:none}.jupyter-wrapper .jp-DirListing-headerItem.jp-mod-selected{font-weight:600}.jupyter-wrapper .jp-DirListing-content{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-DirListing.jp-mod-native-drop .jp-DirListing-content{outline:5px dashed rgba(128,128,128,.5);outline-offset:-10px;cursor:copy}.jupyter-wrapper .jp-DirListing-item{display:flex;flex-direction:row;padding:4px 12px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected{color:#fff;background:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-dropTarget{background:var(--jp-brand-color3)}.jupyter-wrapper .jp-DirListing-item:hover:not(.jp-mod-selected){background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-itemIcon{flex:0 0 20px;margin-right:4px}.jupyter-wrapper .jp-DirListing-itemText{flex:1 0 64px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;user-select:none}.jupyter-wrapper .jp-DirListing-itemModified{flex:0 0 125px;text-align:right}.jupyter-wrapper .jp-DirListing-editor{flex:1 0 64px;outline:none;border:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before{color:#32cd32;content:\"\u25cf\";font-size:8px;position:absolute;left:-8px}.jupyter-wrapper .jp-DirListing-item.lm-mod-drag-image,.jupyter-wrapper .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image{font-size:var(--jp-ui-font-size1);padding-left:4px;margin-left:4px;width:160px;background-color:var(--jp-ui-inverse-font-color2);box-shadow:var(--jp-elevation-z2);border-radius:0px;color:var(--jp-ui-font-color1);transform:translateX(-40%) translateY(-58%)}.jupyter-wrapper .jp-DirListing-deadSpace{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-Document{min-width:120px;min-height:120px;outline:none}.jupyter-wrapper .jp-FileDialog.jp-mod-conflict input{color:red}.jupyter-wrapper .jp-FileDialog .jp-new-name-title{margin-top:12px}.jupyter-wrapper .jp-OutputArea{overflow-y:auto}.jupyter-wrapper .jp-OutputArea-child{display:flex;flex-direction:row}.jupyter-wrapper .jp-OutputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-outprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-OutputArea-output{height:auto;overflow:auto;user-select:text;-moz-user-select:text;-webkit-user-select:text;-ms-user-select:text}.jupyter-wrapper .jp-OutputArea-child .jp-OutputArea-output{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-OutputArea-output.jp-mod-isolated{width:100%;display:block}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-OutputArea-output pre{border:none;margin:0px;padding:0px;overflow-x:auto;overflow-y:auto;word-break:break-all;word-wrap:break-word;white-space:pre-wrap}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedHTMLCommon table{margin-left:0;margin-right:0}.jupyter-wrapper .jp-OutputArea-output dl,.jupyter-wrapper .jp-OutputArea-output dt,.jupyter-wrapper .jp-OutputArea-output dd{display:block}.jupyter-wrapper .jp-OutputArea-output dl{width:100%;overflow:hidden;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dt{font-weight:bold;float:left;width:20%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dd{float:left;width:80%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt{display:none}.jupyter-wrapper .jp-OutputArea-output.jp-OutputArea-executeResult{margin-left:0px;flex:1 1 auto}.jupyter-wrapper .jp-OutputArea-executeResult.jp-RenderedText{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-OutputArea-stdin{line-height:var(--jp-code-line-height);padding-top:var(--jp-code-padding);display:flex}.jupyter-wrapper .jp-Stdin-prompt{color:var(--jp-content-font-color0);padding-right:var(--jp-code-padding);vertical-align:baseline;flex:0 0 auto}.jupyter-wrapper .jp-Stdin-input{font-family:var(--jp-code-font-family);font-size:inherit;color:inherit;background-color:inherit;width:42%;min-width:200px;vertical-align:baseline;padding:0em .25em;margin:0em .25em;flex:0 0 70%}.jupyter-wrapper .jp-Stdin-input:focus{box-shadow:none}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea{height:100%;display:block}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea-output:only-child{height:100%}.jupyter-wrapper .jp-Collapser{flex:0 0 var(--jp-cell-collapser-width);padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0);border-radius:var(--jp-border-radius);opacity:1}.jupyter-wrapper .jp-Collapser-child{display:block;width:100%;box-sizing:border-box;position:absolute;top:0px;bottom:0px}.jupyter-wrapper .jp-CellHeader,.jupyter-wrapper .jp-CellFooter{height:0px;width:100%;padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-InputArea{display:flex;flex-direction:row}.jupyter-wrapper .jp-InputArea-editor{flex:1 1 auto}.jupyter-wrapper .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);border-radius:0px;background:var(--jp-cell-editor-background)}.jupyter-wrapper .jp-InputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-inprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);opacity:var(--jp-cell-prompt-opacity);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-Placeholder{display:flex;flex-direction:row;flex:1 1 auto}.jupyter-wrapper .jp-Placeholder-prompt{box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content{flex:1 1 auto;border:none;background:rgba(0,0,0,0);height:20px;box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon{width:32px;height:16px;border:1px solid rgba(0,0,0,0);border-radius:var(--jp-border-radius)}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon:hover{border:1px solid var(--jp-border-color1);box-shadow:0px 0px 2px 0px rgba(0,0,0,.25);background-color:var(--jp-layout-color0)}.jupyter-wrapper :root{--jp-private-cell-scrolling-output-offset: 5px}.jupyter-wrapper .jp-Cell{padding:var(--jp-cell-padding);margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Cell-inputWrapper,.jupyter-wrapper .jp-Cell-outputWrapper{display:flex;flex-direction:row;padding:0px;margin:0px;overflow:visible}.jupyter-wrapper .jp-Cell-inputArea,.jupyter-wrapper .jp-Cell-outputArea{flex:1 1 auto}.jupyter-wrapper .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser{border:none !important;background:rgba(0,0,0,0) !important}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser{min-height:var(--jp-cell-collapser-min-height)}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper{margin-top:5px}.jupyter-wrapper .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea{overflow-y:auto;max-height:200px;box-shadow:inset 0 0 6px 2px rgba(0,0,0,.3);margin-left:var(--jp-private-cell-scrolling-output-offset)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt{flex:0 0 calc(var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset))}.jupyter-wrapper .jp-MarkdownOutput{flex:1 1 auto;margin-top:0;margin-bottom:0;padding-left:var(--jp-code-padding)}.jupyter-wrapper .jp-MarkdownOutput.jp-RenderedHTMLCommon{overflow:auto}.jupyter-wrapper .jp-NotebookPanel-toolbar{padding:2px}.jupyter-wrapper .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused{border:none;box-shadow:none}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown select{height:24px;font-size:var(--jp-ui-font-size1);line-height:14px;border-radius:0;display:block}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown span{top:5px !important}.jupyter-wrapper :root{--jp-private-notebook-dragImage-width: 304px;--jp-private-notebook-dragImage-height: 36px;--jp-private-notebook-selected-color: var(--md-blue-400);--jp-private-notebook-active-color: var(--md-green-400)}.jupyter-wrapper .jp-NotebookPanel{display:block;height:100%}.jupyter-wrapper .jp-NotebookPanel.jp-Document{min-width:240px;min-height:120px}.jupyter-wrapper .jp-Notebook{padding:var(--jp-notebook-padding);outline:none;overflow:auto;background:var(--jp-layout-color0)}.jupyter-wrapper .jp-Notebook.jp-mod-scrollPastEnd::after{display:block;content:\"\";min-height:var(--jp-notebook-scroll-padding)}.jupyter-wrapper .jp-Notebook .jp-Cell{overflow:visible}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-InputPrompt{cursor:move}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser{background:var(--jp-brand-color1)}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-Collapser:hover{box-shadow:var(--jp-elevation-z2);background:var(--jp-brand-color1);opacity:var(--jp-cell-collapser-not-active-hover-opacity)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover{background:var(--jp-brand-color0);opacity:1}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected{background:var(--jp-notebook-multiselected-color)}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected){background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-cell-editor-active-background)}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropSource{opacity:.5}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropTarget,.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget{border-top-color:var(--jp-private-notebook-selected-color);border-top-style:solid;border-top-width:2px}.jupyter-wrapper .jp-dragImage{display:flex;flex-direction:row;width:var(--jp-private-notebook-dragImage-width);height:var(--jp-private-notebook-dragImage-height);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background);overflow:visible}.jupyter-wrapper .jp-dragImage-singlePrompt{box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-dragImage .jp-dragImage-content{flex:1 1 auto;z-index:2;font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);line-height:var(--jp-code-line-height);padding:var(--jp-code-padding);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background-color);color:var(--jp-content-font-color3);text-align:left;margin:4px 4px 4px 0px}.jupyter-wrapper .jp-dragImage .jp-dragImage-prompt{flex:0 0 auto;min-width:36px;color:var(--jp-cell-inprompt-font-color);padding:var(--jp-code-padding);padding-left:12px;font-family:var(--jp-cell-prompt-font-family);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:1.9;font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0)}.jupyter-wrapper .jp-dragImage-multipleBack{z-index:-1;position:absolute;height:32px;width:300px;top:8px;left:8px;background:var(--jp-layout-color2);border:var(--jp-border-width) solid var(--jp-input-border-color);box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-NotebookTools{display:block;min-width:var(--jp-sidebar-min-width);color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1);overflow:auto}.jupyter-wrapper .jp-NotebookTools-tool{padding:0px 12px 0 12px}.jupyter-wrapper .jp-ActiveCellTool{padding:12px;background-color:var(--jp-layout-color1);border-top:none !important}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-prompt{flex:0 0 auto;padding-left:0px}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor{flex:1 1 auto;background:var(--jp-cell-editor-background);border-color:var(--jp-cell-editor-border-color)}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror{background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MetadataEditorTool{flex-direction:column;padding:12px 0px 12px 0px}.jupyter-wrapper .jp-RankedPanel>:not(:first-child){margin-top:12px}.jupyter-wrapper .jp-KeySelector select.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:var(--jp-border-width) solid var(--jp-border-color1)}.jupyter-wrapper .jp-KeySelector label,.jupyter-wrapper .jp-MetadataEditorTool label{line-height:1.4}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook{--jp-content-font-size1: var(--jp-content-presentation-font-size1);--jp-code-font-size: var(--jp-code-presentation-font-size)}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt{flex:0 0 110px}.jupyter-wrapper .md-typeset__scrollwrap{margin:0}.jupyter-wrapper .jp-MarkdownOutput{padding:0}.jupyter-wrapper h1 .anchor-link,.jupyter-wrapper h2 .anchor-link,.jupyter-wrapper h3 .anchor-link,.jupyter-wrapper h4 .anchor-link,.jupyter-wrapper h5 .anchor-link,.jupyter-wrapper h6 .anchor-link{display:none;margin-left:.5rem;color:var(--md-default-fg-color--lighter)}.jupyter-wrapper h1 .anchor-link:hover,.jupyter-wrapper h2 .anchor-link:hover,.jupyter-wrapper h3 .anchor-link:hover,.jupyter-wrapper h4 .anchor-link:hover,.jupyter-wrapper h5 .anchor-link:hover,.jupyter-wrapper h6 .anchor-link:hover{text-decoration:none;color:var(--md-accent-fg-color)}.jupyter-wrapper h1:hover .anchor-link,.jupyter-wrapper h2:hover .anchor-link,.jupyter-wrapper h3:hover .anchor-link,.jupyter-wrapper h4:hover .anchor-link,.jupyter-wrapper h5:hover .anchor-link,.jupyter-wrapper h6:hover .anchor-link{display:inline-block}.jupyter-wrapper .jp-InputArea{width:100%}.jupyter-wrapper .jp-Cell-inputArea{width:100%}.jupyter-wrapper .jp-RenderedHTMLCommon{width:100%}.jupyter-wrapper .jp-Cell-inputWrapper .jp-InputPrompt{display:none}.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt{display:block}.jupyter-wrapper .highlight pre{overflow:auto}.jupyter-wrapper .celltoolbar{border:none;background:#eee;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;box-pack:end;justify-content:flex-start;display:-webkit-flex}.jupyter-wrapper .celltoolbar .tags_button_container{display:flex}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container{display:flex;flex-direction:row;flex-grow:1;overflow:hidden;position:relative}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;box-shadow:none;width:inherit;font-size:11px;font-family:\"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace;height:22px;display:inline-block}.jupyter-wrapper .jp-InputArea-editor{width:1px}.jupyter-wrapper .jp-InputPrompt{overflow:unset}.jupyter-wrapper .jp-OutputPrompt{overflow:unset}.jupyter-wrapper .jp-RenderedText{font-size:var(--jp-code-font-size)}.jupyter-wrapper .highlight-ipynb{overflow:auto}.jupyter-wrapper .highlight-ipynb pre{margin:0;padding:5px 10px}.jupyter-wrapper table{width:max-content}.jupyter-wrapper table.dataframe{margin-left:auto;margin-right:auto;border:none;border-collapse:collapse;border-spacing:0;color:#000;font-size:12px;table-layout:fixed}.jupyter-wrapper table.dataframe thead{border-bottom:1px solid #000;vertical-align:bottom}.jupyter-wrapper table.dataframe tr,.jupyter-wrapper table.dataframe th,.jupyter-wrapper table.dataframe td{text-align:right;vertical-align:middle;padding:.5em .5em;line-height:normal;white-space:normal;max-width:none;border:none}.jupyter-wrapper table.dataframe th{font-weight:bold}.jupyter-wrapper table.dataframe tbody tr:nth-child(odd){background:#f5f5f5}.jupyter-wrapper table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}.jupyter-wrapper *+table{margin-top:1em}.jupyter-wrapper .jp-InputArea-editor{position:relative}.jupyter-wrapper .zeroclipboard-container{position:absolute;top:-3px;right:0;z-index:1000}.jupyter-wrapper .zeroclipboard-container clipboard-copy{-webkit-appearance:button;-moz-appearance:button;padding:7px 5px;font:11px system-ui,sans-serif;display:inline-block;cursor:default}.jupyter-wrapper .zeroclipboard-container .clipboard-copy-icon{padding:4px 4px 2px;color:#57606a;vertical-align:text-bottom}.jupyter-wrapper .clipboard-copy-txt{display:none}[data-md-color-scheme=slate] .clipboard-copy-icon{color:#fff !important}[data-md-color-scheme=slate] table.dataframe{color:#e9ebfc}[data-md-color-scheme=slate] table.dataframe thead{border-bottom:1px solid rgba(233,235,252,.12)}[data-md-color-scheme=slate] table.dataframe tbody tr:nth-child(odd){background:#222}[data-md-color-scheme=slate] table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}table{width:max-content} /*# sourceMappingURL=mkdocs-jupyter.css.map*/ init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Accelerating GPT-2 model \u00b6 (and any decoder based transformer models) \u00b6 Two trends ongoing in the NLP ecosystem: bigger language model and better text generation. Those trends are game changers (zero shot, etc.) and bring their own challenge: how to perform inference with them? At what cost? GPU or CPU ? etc. aka, how to leverage them in real life? That\u2019s what we worked on recently, and below you will find the main lessons learned : memory IO is by far the main perf bottleneck Standard API of ONNX Runtime should not be used but there is an undocumented way of using another ONNX Runtime API which works well Nvidia TensorRT is always the fastest option on GPU, by a large margin (expected) Caching K/V token representation does not bring any inference optimization (unexpected) First, let's remind how decoder models work... Generative text language models like GPT-2 produce text 1 token at a time. The model is auto regressive meaning that each produced token is part of the generation of the next token. There are mainly 2 blocks: the language model itself which produces big tensors, and the decoding algorithm which consumes the tensors and selects 1 or more tokens. Keep in mind that these blocks may live on different hardware\u2026 (spoiler: it\u2019s not a good idea) In [1]: Copied! from IPython.display import Image Image ( \"../../resources/img/gpt2.png\" ) from IPython.display import Image Image(\"../../resources/img/gpt2.png\") Out[1]: GPT-2 loading \u00b6 As a reminder: gpt2 : 117M parameters gpt2-large 774M parameters In [2]: Copied! ! nvidia - smi ! nvidia-smi Wed Feb 16 08:56:02 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 30% 29C P5 37W / 350W | 165MiB / 24576MiB | 15% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1851 G /usr/lib/xorg/Xorg 89MiB | | 0 N/A N/A 7363 G /usr/bin/gnome-shell 74MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import time from typing import Callable , Dict import numpy as np import tensorrt as trt import torch from tensorrt import ICudaEngine from tensorrt.tensorrt import Logger , Runtime from transformers import AutoTokenizer , BatchEncoding , GPT2LMHeadModel , AutoModelForCausalLM from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions from transformer_deploy.utils.generative_model import GPTModelWrapper import inspect from transformers import TensorType from transformer_deploy.backends.ort_utils import create_model_for_provider , inference_onnx_binding , optimize_onnx from transformer_deploy.backends.pytorch_utils import convert_to_onnx , get_model_size from transformer_deploy.backends.trt_utils import build_engine , load_engine , save_engine import logging import time from typing import Callable, Dict import numpy as np import tensorrt as trt import torch from tensorrt import ICudaEngine from tensorrt.tensorrt import Logger, Runtime from transformers import AutoTokenizer, BatchEncoding, GPT2LMHeadModel, AutoModelForCausalLM from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions from transformer_deploy.utils.generative_model import GPTModelWrapper import inspect from transformers import TensorType from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size from transformer_deploy.backends.trt_utils import build_engine, load_engine, save_engine In [4]: Copied! model_name = \"gpt2\" # choices: gpt2 | gpt2-large # use GPT2LMHeadModel and not AutoModel to export raw outputs to predict next token model : GPT2LMHeadModel = AutoModelForCausalLM . from_pretrained ( model_name ) model . eval () tokenizer = AutoTokenizer . from_pretrained ( model_name ) # to avoid error message or passing some args to each generate call model . config . pad_token_id = tokenizer . eos_token_id model_name = \"gpt2\" # choices: gpt2 | gpt2-large # use GPT2LMHeadModel and not AutoModel to export raw outputs to predict next token model: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(model_name) model.eval() tokenizer = AutoTokenizer.from_pretrained(model_name) # to avoid error message or passing some args to each generate call model.config.pad_token_id = tokenizer.eos_token_id Model output \u00b6 GPT-2 is trained to generate the probability for the next token. Then a decoding algorithm will consume this information and choose a token: greedy decoding will just choose the token getting the highest score beam search decoding will explore several possible sequences by keeping track of a limited set of tokens Below we output predictions for the next token. Output shape is: [batch size, nb input tokens, vocabulary size] In [5]: Copied! inputs = tokenizer ( \"Here is some text to encode Hello World\" , return_tensors = \"pt\" ) print ( \"input tensors\" ) print ( inputs ) print ( \"input tensor shape\" ) print ( inputs [ \"input_ids\" ] . size ()) with torch . no_grad (): outputs = model ( ** inputs ) logits = outputs . logits print ( \"output tensor\" ) print ( logits ) print ( \"output shape\" ) print ( logits . shape ) inputs = tokenizer(\"Here is some text to encode Hello World\", return_tensors=\"pt\") print(\"input tensors\") print(inputs) print(\"input tensor shape\") print(inputs[\"input_ids\"].size()) with torch.no_grad(): outputs = model(**inputs) logits = outputs.logits print(\"output tensor\") print(logits) print(\"output shape\") print(logits.shape) input tensors {'input_ids': tensor([[ 4342, 318, 617, 2420, 284, 37773, 18435, 2159]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} input tensor shape torch.Size([1, 8]) output tensor tensor([[[ -34.3027, -33.9891, -37.5683, ..., -42.6734, -42.0399, -34.6136], [ -83.3065, -82.9769, -86.1204, ..., -89.8062, -89.4546, -83.6084], [ -91.4901, -92.5655, -95.6423, ..., -96.6183, -98.1545, -91.5266], ..., [ -92.8820, -94.8433, -98.9224, ..., -101.4426, -103.2702, -95.7642], [ -72.6140, -76.3407, -79.7973, ..., -87.3300, -85.7930, -77.7521], [-103.6147, -108.7898, -109.6276, ..., -116.8557, -116.5565, -107.4467]]]) output shape torch.Size([1, 8, 50257]) Total tensor size (input+output) \u00b6 GPT-2 will generate a sequence 1 token at a time. So to generates 256 tokens from an 8 tokens prompt, it will perform 248 inferences. To simplify things, we assume that we are using a greedy decoding algorithm. If you want to know more about decoding algorithm check this article . Let's compute the total size of the output tensor. In [6]: Copied! size = 0 for i in range ( 8 , 256 , 1 ): # input sequence (input_ids) made of int-32 (4 bytes) size += np . prod ([ 1 , i ]) * 4 # output tensor made of float-32 (4 bytes) size += np . prod ([ 1 , i , 50257 ]) * 4 print ( f \"total size (input+output): { size / 1024 ** 3 : .2f } Gb\" ) # to manually check actual tensor size: # np.prod(logits.shape)*32/8/1024**2:.2f} # or # sys.getsizeof(logits.storage())/1024**2 size = 0 for i in range(8, 256, 1): # input sequence (input_ids) made of int-32 (4 bytes) size += np.prod([1, i]) * 4 # output tensor made of float-32 (4 bytes) size += np.prod([1, i, 50257]) * 4 print(f\"total size (input+output): {size / 1024**3:.2f} Gb\") # to manually check actual tensor size: # np.prod(logits.shape)*32/8/1024**2:.2f} # or # sys.getsizeof(logits.storage())/1024**2 total size (input+output): 6.11 Gb Over 6Gb is a lot of data. We will see later than naive approach makes these data move from GPU to host and then from host to GPU. Therefore we are moving 12Gb of data. Let's keep the order of magnitude in mind when we will try to optimize GPT-2 inference. Build ONNX graph \u00b6 Performant inference engines tend to consume graph instead of imperative Pytorch code. We use ONNX for that purpose. ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. https://onnx.ai/ In [7]: Copied! input_ids : BatchEncoding = tokenizer ( \"Here is some text to encode Hello World\" , add_special_tokens = True , return_attention_mask = False , return_tensors = \"pt\" ) # some inference engines don't support int64 tensor as inputs, we convert all input tensors to int32 type for k , v in input_ids . items (): # type: str, torch.Tensor input_ids [ k ] = v . type ( dtype = torch . int32 ) convert_to_onnx ( model_pytorch = model , output_path = \"test-gpt2.onnx\" , inputs_pytorch = dict ( input_ids ), quantization = False , var_output_seq = True , # we inform ONNX export tool that the output shape will vary with the input shape ) # model may switch to train mode for some unknown reasons, we force the eval mode. _ = model . eval () input_ids: BatchEncoding = tokenizer( \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\" ) # some inference engines don't support int64 tensor as inputs, we convert all input tensors to int32 type for k, v in input_ids.items(): # type: str, torch.Tensor input_ids[k] = v.type(dtype=torch.int32) convert_to_onnx( model_pytorch=model, output_path=\"test-gpt2.onnx\", inputs_pytorch=dict(input_ids), quantization=False, var_output_seq=True, # we inform ONNX export tool that the output shape will vary with the input shape ) # model may switch to train mode for some unknown reasons, we force the eval mode. _ = model.eval() /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! attn_weights = attn_weights / (float(value.size(-1)) ** 0.5) Optimize ONNX graph \u00b6 ONNX Runtime optimizations are offline operations to simplify the computation graph. Some make the computation itself faster (like approximating GELU) but most of them are targeting memory transfer (kernel fusion will perform a series of computations in a single pass, avoiding reading/writing several times the same array, etc.). In [8]: Copied! logging . basicConfig () logging . getLogger () . setLevel ( logging . INFO ) num_attention_heads , hidden_size = get_model_size ( path = model_name ) optimize_onnx ( onnx_path = \"test-gpt2.onnx\" , onnx_optim_model_path = \"test-gpt2-opt.onnx\" , fp16 = True , use_cuda = True , num_attention_heads = num_attention_heads , hidden_size = hidden_size , architecture = \"gpt2\" , ) logging.basicConfig() logging.getLogger().setLevel(logging.INFO) num_attention_heads, hidden_size = get_model_size(path=model_name) optimize_onnx( onnx_path=\"test-gpt2.onnx\", onnx_optim_model_path=\"test-gpt2-opt.onnx\", fp16=True, use_cuda=True, num_attention_heads=num_attention_heads, hidden_size=hidden_size, architecture=\"gpt2\", ) INFO:fusion_base:Fused LayerNormalization count: 25 INFO:fusion_base:Fused FastGelu count: 12 INFO:fusion_utils:Remove reshape node Reshape_9 since its input shape is same as output: ['batch_size', 'sequence'] INFO:fusion_utils:Remove reshape node Reshape_19 since its input shape is same as output: [1, 'sequence'] INFO:fusion_utils:Remove reshape node Reshape_2700 since its input shape is same as output: ['batch_size', 'sequence', 768] INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 23 nodes are removed INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 864 nodes are removed INFO:onnx_model_gpt2:postprocess: remove Reshape count:72 INFO:fusion_base:Fused FastGelu(add bias) count: 12 INFO:onnx_model_bert:opset verion: 13 INFO:onnx_model_bert:Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0} INFO:root:optimizations applied: {'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0} INFO:onnx_model:Sort graphs in topological order INFO:onnx_model:Output model to test-gpt2-opt.onnx Build TensorRT engine \u00b6 This operation will take the original ONNX graph (before ONNX Runtime optimizations) and optimize it the Nvidia way. The nature of the graph optimization is similar (approximation and kernel fusion) One of difference with ONNX Runtime is that these optimizations are benchmarked online on the real hardware, it's called kernel mapping (find the right kernel for specific data shape+model+hardware). It takes more time to optimize but at the end it is usually much faster. In [9]: Copied! from pathlib import Path trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) trt_model_name = \"test-gpt2.plan\" # create only of does not exist because it's slow to run... if not Path ( trt_model_name ) . exists (): engine : ICudaEngine = build_engine ( runtime = runtime , onnx_file_path = \"test-gpt2.onnx\" , logger = trt_logger , min_shape = ( 1 , 1 ), optimal_shape = ( 1 , 128 ), # num beam, batch size max_shape = ( 1 , 384 ), # num beam, batch size workspace_size = 10000 * 1024 ** 2 , fp16 = True , int8 = False , ) save_engine ( engine , trt_model_name ) from pathlib import Path trt_logger: Logger = trt.Logger(trt.Logger.ERROR) runtime: Runtime = trt.Runtime(trt_logger) trt_model_name = \"test-gpt2.plan\" # create only of does not exist because it's slow to run... if not Path(trt_model_name).exists(): engine: ICudaEngine = build_engine( runtime=runtime, onnx_file_path=\"test-gpt2.onnx\", logger=trt_logger, min_shape=(1, 1), optimal_shape=(1, 128), # num beam, batch size max_shape=(1, 384), # num beam, batch size workspace_size=10000 * 1024**2, fp16=True, int8=False, ) save_engine(engine, trt_model_name) End to end text generation benchmarks (inference+decoding) \u00b6 We will benchmark 3 inference engines: Pytorch : Hugging Face implementation ONNX Runtime : with optimized ONNX graph and standard API : tensors stored as numpy objects, on host RAM with optimized ONNX graph and binding IO API : tensors stored on CUDA, to limit IO Nvidia TensorRT : tensors stored on CUDA, to limit IO For each of them, we print the output of the result so we can check that it generates the same sequence. Then we measure inference latency. Generative model wrapper \u00b6 To replace Pytorch as inference engine and still use Hugging Face decoding algorithm, we use a wrapping class which separate those tasks. The most interesting thing in the class below is that we inherit from GenerationMixin (from Hugging Face transformers library), it will give our class some super-powers like a method to generate sequences (aka running some decoding algorithm on top of the model output). Note that the actual model inference is done by a function provided through the constructor. In [10]: Copied! print ( inspect . getsource ( GPTModelWrapper )) print(inspect.getsource(GPTModelWrapper)) class GPTModelWrapper(Module, GenerationMixin): def __init__( self, config: PretrainedConfig, device: torch.device, inference: Callable[[torch.Tensor], torch.Tensor] ): super().__init__() self.config: PretrainedConfig = config self.device: torch.device = device self.inference: Callable[[torch.Tensor], torch.Tensor] = inference self.main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 def prepare_inputs_for_generation(self, input_ids, **kwargs): return { self.main_input_name: input_ids, } def forward(self, input_ids, **_): logits = self.inference(input_ids) return CausalLMOutputWithCrossAttentions(logits=logits) In [11]: Copied! inputs = tokenizer ( \"Here is some text to encode Hello World\" , # Nvidia example prompt add_special_tokens = True , return_attention_mask = False , # Not used return_tensors = TensorType . PYTORCH , ) inputs inputs = tokenizer( \"Here is some text to encode Hello World\", # Nvidia example prompt add_special_tokens=True, return_attention_mask=False, # Not used return_tensors=TensorType.PYTORCH, ) inputs Out[11]: {'input_ids': tensor([[ 4342, 318, 617, 2420, 284, 37773, 18435, 2159]])} Pytorch inference \u00b6 We first measure vanilla Hugging face implementation with Pytorch backend. It will be our baseline. In [12]: Copied! def inference_torch ( input_ids : torch . Tensor ) -> torch . Tensor : transformer_outputs : BaseModelOutputWithPastAndCrossAttentions = model . transformer ( input_ids = input_ids ) return model . lm_head ( transformer_outputs . last_hidden_state ) model . cuda () model . eval () inputs . to ( \"cuda\" ) with torch . inference_mode (): gpt2_model = GPTModelWrapper ( config = model . config , device = model . device , inference = inference_torch ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = False )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) torch . cuda . synchronize () start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) torch . cuda . synchronize () print ( f \"---- \\n Pytorch: { ( time . time () - start ) / 10 : .2f } s/sequence\" ) _ = model . cpu () def inference_torch(input_ids: torch.Tensor) -> torch.Tensor: transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids) return model.lm_head(transformer_outputs.last_hidden_state) model.cuda() model.eval() inputs.to(\"cuda\") with torch.inference_mode(): gpt2_model = GPTModelWrapper(config=model.config, device=model.device, inference=inference_torch) sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=False)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) torch.cuda.synchronize() start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) torch.cuda.synchronize() print(f\"----\\nPytorch: {(time.time() - start)/10:.2f}s/sequence\") _ = model.cpu() Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- Pytorch: 2.51s/sequence Naive ONNX Runtime inference \u00b6 Below we use ONNX Runtime and its standard API. Inference is performed on GPU (CUDA provider in the ONNX Runtime ecosystem). It takes as input and return as output a numpy tensor. numpy tensor are by their nature always stored on host memory (RAM). It means that for each token the whole output tensor is moved from GPU memory to host. As measure above, it represents more than 2X6Gb of memory transfers between host and GPU memory. In our code, we convert the output numpy tensor to Pytorch one so it can be consumed by the Hugging Face decoding algorithm. ONNX model used is the optimized one (kernel fusion + fp16). In [13]: Copied! model_onnx = create_model_for_provider ( path = \"test-gpt2-opt.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) def inference_onnx_naive ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids . detach () . cpu () . numpy () . astype ( np . int32 )} logit = model_onnx . run ( None , data ) np_logit = np . array ( logit ) # convert list of numpy arrays to a numpy array # we convert numpy tensor to Pytorch tensor as it's the type expected by HF decoding algorithm return torch . squeeze ( torch . from_numpy ( np_logit ), dim = 0 ) gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cpu\" ), inference = inference_onnx_naive ) inputs . to ( \"cpu\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n ONNX Runtime (standard API): { ( time . time () - start ) / 10 : .2f } s/sequence\" ) del model_onnx model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\") def inference_onnx_naive(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids.detach().cpu().numpy().astype(np.int32)} logit = model_onnx.run(None, data) np_logit = np.array(logit) # convert list of numpy arrays to a numpy array # we convert numpy tensor to Pytorch tensor as it's the type expected by HF decoding algorithm return torch.squeeze(torch.from_numpy(np_logit), dim=0) gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cpu\"), inference=inference_onnx_naive) inputs.to(\"cpu\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nONNX Runtime (standard API): {(time.time() - start)/10:.2f}s/sequence\") del model_onnx Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- ONNX Runtime (standard API): 3.93s/sequence No surprise, the generation is slower than vanilla Pytorch, even after having optimized the ONNX model. Optimized ONNX Runtime inference (no GPU <-> host tensor move) \u00b6 In this benchmark, we use ONNX Runtime and its binding IO API . The way it works inside ( inference_onnx_binding code) is that we basically provide to the API a pointer to Pytorch tensor storage array. The storage array is a CUDA array (in our case) containing the tensor data themselves, the tensor abstraction adds some meta data to this array. Therefore ONNX Runtime can directly write output in the Pytorch tensor without using any numpy array (stored in host RAM). Because Hugging Face decoding algorithm can run on CUDA too, we do not move any data to host. By just removing memory movement, we reduce the inference time by a large margin. Warning if you want to use this ONNX Runtime API, don't forget to make your tensor contiguous in memory (not shown here because it is done by the transformer-deploy library). Another point to have in mind is that you need to reserve manually GPU memory... and unlike TensorRT, ONNX Runtime is not able to predict output shape, so you need to have the output size logic elsewhere. In [14]: Copied! model_onnx = create_model_for_provider ( path = \"test-gpt2-opt.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) def inference_onnx_optimized ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids } return inference_onnx_binding ( model_onnx = model_onnx , inputs = data , device = \"cuda\" )[ \"output\" ] gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cuda\" ), inference = inference_onnx_optimized ) inputs . to ( \"cuda\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n ONNX Runtime (binding io API): { ( time . time () - start ) / 10 : .2f } /sequence\" ) del model_onnx model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\") def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids} return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"] gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_onnx_optimized) inputs.to(\"cuda\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nONNX Runtime (binding io API): {(time.time() - start)/10:.2f}/sequence\") del model_onnx Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- ONNX Runtime (binding io API): 0.85/sequence Good news: the output is the same than Pytorch, and still inference time is much faster. TensorRT Inference \u00b6 To conclude, we use the Nvidia engine, all tensors are stored on CUDA. Unlike ONNX Runtime, each optimization applied has been checked on the target GPU with the expected tensor shape. It explains most of its performance compared to ONNX Runtime optimized performances. In [15]: Copied! tensorrt_model : Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ] = load_engine ( engine_file_path = \"test-gpt2.plan\" , runtime = runtime ) def inference_tensorrt ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids } return tensorrt_model ( data ) gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cuda\" ), inference = inference_tensorrt ) inputs . to ( \"cuda\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n TensorRT + CUDA tensors: { ( time . time () - start ) / 10 : .2f } /sequence\" ) del tensorrt_model tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine( engine_file_path=\"test-gpt2.plan\", runtime=runtime ) def inference_tensorrt(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids} return tensorrt_model(data) gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_tensorrt) inputs.to(\"cuda\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nTensorRT + CUDA tensors: {(time.time() - start)/10:.2f}/sequence\") del tensorrt_model Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- TensorRT + CUDA tensors: 0.50/sequence Is caching of Key / Values in self-attention block (Transformer architecture) a good strategy on GPU? \u00b6 In the self-attention block, the first step is to compute query, key and value vectors for each input token. For each token, we look up its embedding in the embedding matrix and adds to it a positional encoding so the model knows word order. Then we multiply each of these token representation by 3 matrices: query (Q), key (K), and value (V). These matrices are then transposed, with Q T and K T being used to compute the normalized dot-product attention values, before being combined with V T to produce the final output. This computation is done for each self-attention block (as each of them has its own \"memory\"). In a generative model, we need to recompute those values for each generated token. For a specific input token, the result won't change from one inference to the next one, it may be interesting to cache and reuse the results instead of recomputing it. However, this won't come for free as we would need 2 ONNX / TensorRT models, one generating the values for the first time (to boot the sequence generation), and one able to reuse the cached values. It means 2X bigger memory footprint on GPU :-( But first, let's measure the gain in performance of caching values, if any. In [16]: Copied! model . cuda () model . eval () inputs . to ( \"cuda\" ) with torch . inference_mode (): for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 64 , use_cache = True ) torch . cuda . synchronize () start = time . time () for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 256 , use_cache = True ) torch . cuda . synchronize () print ( f \"Pytorch with cache: { ( time . time () - start ) / 2 : .2f } s/sequence\" ) for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 64 , use_cache = False ) torch . cuda . synchronize () start = time . time () for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 256 , use_cache = False ) torch . cuda . synchronize () print ( f \"Pytorch without cache: { ( time . time () - start ) / 2 : .2f } s/sequence\" ) _ = model . cpu () model.cuda() model.eval() inputs.to(\"cuda\") with torch.inference_mode(): for _ in range(2): _ = model.generate(inputs.input_ids, max_length=64, use_cache=True) torch.cuda.synchronize() start = time.time() for _ in range(2): _ = model.generate(inputs.input_ids, max_length=256, use_cache=True) torch.cuda.synchronize() print(f\"Pytorch with cache: {(time.time() - start)/2:.2f}s/sequence\") for _ in range(2): _ = model.generate(inputs.input_ids, max_length=64, use_cache=False) torch.cuda.synchronize() start = time.time() for _ in range(2): _ = model.generate(inputs.input_ids, max_length=256, use_cache=False) torch.cuda.synchronize() print(f\"Pytorch without cache: {(time.time() - start)/2:.2f}s/sequence\") _ = model.cpu() Pytorch with cache: 2.35s/sequence Pytorch without cache: 2.32s/sequence Suprinsingly, cache support or not, on Pytorch, with an end-to-end test, for a 256 sequence length and when inference is performed on a last generation GPU (at the time of writing), latencies are very similar. Expect different results on older/slower GPU like Nvidia T4 (cache may help more). Not shown in this notebook to keep it \"light\", on fast GPU, for very long sequences (like 1024 tokens), on Pytorch, cache support provides better performances, but optimized ONNX Runtime / TensorRT engines without cache support are still faster. To dig a bit more, we will measure inference engines with fixed input sizes. Export an ONNX model able to reuse cache \u00b6 To simplify the code, we just use the ONNX exporter tool from Hugging Face library (it's a far from perfect tool, in this case it's okish, in general be prudent in using it). In [17]: Copied! from itertools import chain from torch.onnx import export from transformers.models.gpt2 import GPT2OnnxConfig from transformers.onnx.features import FeaturesManager model_name = \"gpt2\" feature = \"causal-lm-with-past\" seq_len = 256 atol = 0.2 tokenizer = AutoTokenizer . from_pretrained ( model_name ) model : GPT2LMHeadModel = FeaturesManager . get_model_from_feature ( feature , model_name ) model_kind , model_onnx_config = FeaturesManager . check_supported_model_or_raise ( model , feature = feature ) onnx_config : GPT2OnnxConfig = model_onnx_config ( model . config ) with torch . no_grad (): model . config . return_dict = True model . eval () # Check if we need to override certain configuration item if onnx_config . values_override is not None : for override_config_key , override_config_value in onnx_config . values_override . items (): setattr ( model . config , override_config_key , override_config_value ) # Ensure inputs match model_inputs = onnx_config . generate_dummy_inputs ( tokenizer , framework = TensorType . PYTORCH ) for k , v in model_inputs . items (): if isinstance ( v , torch . Tensor ): model_inputs [ k ] = model_inputs [ k ] . type ( torch . int32 ) onnx_outputs = list ( onnx_config . outputs . keys ()) onnx_config . patch_ops () # export can works with named args but the dict containing named args as to be last element of the args tuple export ( model , ( model_inputs ,), f = \"model-support-cache.onnx\" , input_names = list ( onnx_config . inputs . keys ()), output_names = onnx_outputs , dynamic_axes = { name : axes for name , axes in chain ( onnx_config . inputs . items (), onnx_config . outputs . items ())}, do_constant_folding = True , use_external_data_format = onnx_config . use_external_data_format ( model . num_parameters ()), enable_onnx_checker = True , opset_version = 13 , ) onnx_config . restore_ops () from itertools import chain from torch.onnx import export from transformers.models.gpt2 import GPT2OnnxConfig from transformers.onnx.features import FeaturesManager model_name = \"gpt2\" feature = \"causal-lm-with-past\" seq_len = 256 atol = 0.2 tokenizer = AutoTokenizer.from_pretrained(model_name) model: GPT2LMHeadModel = FeaturesManager.get_model_from_feature(feature, model_name) model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature) onnx_config: GPT2OnnxConfig = model_onnx_config(model.config) with torch.no_grad(): model.config.return_dict = True model.eval() # Check if we need to override certain configuration item if onnx_config.values_override is not None: for override_config_key, override_config_value in onnx_config.values_override.items(): setattr(model.config, override_config_key, override_config_value) # Ensure inputs match model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH) for k, v in model_inputs.items(): if isinstance(v, torch.Tensor): model_inputs[k] = model_inputs[k].type(torch.int32) onnx_outputs = list(onnx_config.outputs.keys()) onnx_config.patch_ops() # export can works with named args but the dict containing named args as to be last element of the args tuple export( model, (model_inputs,), f=\"model-support-cache.onnx\", input_names=list(onnx_config.inputs.keys()), output_names=onnx_outputs, dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())}, do_constant_folding=True, use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()), enable_onnx_checker=True, opset_version=13, ) onnx_config.restore_ops() /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError. warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \" /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers. warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \" /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:797: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! if batch_size <= 0: The ONNX graph with cache support expects as input: 1 tensor name input_ids of shape [batch, 1] 12 tensors named past_key_values.X.key (X replaced by layer ID) of shape [batch, nb heads (12), past sequence len, 64] 12 tensors named past_key_values.X.value (X replaced by layer ID) of shape [batch, nb heads (12), past sequence len, 64] 1 tensor named attention_mask of shape [batch, past sequence len + 1] To list model inputs (according to their ONNX graph): In [18]: Copied! import onnx model_cache = onnx . load ( \"model-support-cache.onnx\" ) # first 100 lines text = \" \\n \" . join ( str ( model_cache . graph . input ) . split ( \" \\n \" )[: 80 ]) print ( text ) del model_cache import onnx model_cache = onnx.load(\"model-support-cache.onnx\") # first 100 lines text = \"\\n\".join(str(model_cache.graph.input).split(\"\\n\")[:80]) print(text) del model_cache [name: \"input_ids\" type { tensor_type { elem_type: 6 shape { dim { dim_param: \"batch\" } dim { dim_param: \"sequence\" } } } } , name: \"past_key_values.0.key\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.0.value\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.1.key\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.1.value\" type { tensor_type { elem_type: 1 shape { dim { Benchmark ONNX Runtime, CUDA tensors, with cache \u00b6 When using cache, we only test non-optimized ONNX models (no kernel fusion, fp32) as optimization crash in this setup. It's possibly because ONNX Runtime doesn't find some patterns... In [19]: Copied! batch = 1 sequence = 256 # input with random values input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , 1 ), dtype = torch . int32 ) for i in range ( 12 ): # 12 layers input_cache [ f \"past_key_values. { i } .key\" ] = torch . empty (( batch , 12 , sequence , 64 ), dtype = torch . float32 ) input_cache [ f \"past_key_values. { i } .value\" ] = torch . empty (( batch , 12 , sequence , 64 ), dtype = torch . float32 ) input_cache [ \"attention_mask\" ] = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 ) print ( f \"for { sequence } tokens WITH cache:\" ) for nb_inference , provider , device in [( 10 , \"CPUExecutionProvider\" , \"cpu\" ), ( 100 , \"CUDAExecutionProvider\" , \"cuda\" )]: model_onnx = create_model_for_provider ( path = \"model-support-cache.onnx\" , provider_to_use = provider ) # Pytorch model = model . to ( device = device , non_blocking = False ) output_pytorch = model ( input_ids = torch . ones (( batch , sequence ), dtype = torch . int32 , device = device ), past_key_values = None ) pytorch_input = torch . ones (( batch , 1 ), dtype = torch . int32 , device = device ) for i in range ( nb_inference ): _ = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) torch . cuda . synchronize () start = time . time () for i in range ( nb_inference ): _ = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) torch . cuda . synchronize () print ( f \"[Pytorch / { device . upper () } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # naive implementation (tensor copies) inputs_ort_np = { k : v . cpu () . numpy () for k , v in input_cache . items ()} # warmup for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_np ) start = time . time () for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_np ) print ( f \"[ONNX Runtime / { device . upper () } - with copy (naive)] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # ONNX Runtime optimized (no tensor copy) inputs = { k : v . to ( device ) for k , v in input_cache . items ()} for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) start = time . time () for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) print ( f \"[ONNX Runtime / { device . upper () } - no copy] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) batch = 1 sequence = 256 # input with random values input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32) for i in range(12): # 12 layers input_cache[f\"past_key_values.{i}.key\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32) input_cache[f\"past_key_values.{i}.value\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32) input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32) print(f\"for {sequence} tokens WITH cache:\") for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]: model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=provider) # Pytorch model = model.to(device=device, non_blocking=False) output_pytorch = model( input_ids=torch.ones((batch, sequence), dtype=torch.int32, device=device), past_key_values=None ) pytorch_input = torch.ones((batch, 1), dtype=torch.int32, device=device) for i in range(nb_inference): _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) torch.cuda.synchronize() start = time.time() for i in range(nb_inference): _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) torch.cuda.synchronize() print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\") # naive implementation (tensor copies) inputs_ort_np = {k: v.cpu().numpy() for k, v in input_cache.items()} # warmup for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_np) start = time.time() for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_np) print(f\"[ONNX Runtime / {device.upper()} - with copy (naive)] {1e3*(time.time() - start)/nb_inference:.2f}ms\") # ONNX Runtime optimized (no tensor copy) inputs = {k: v.to(device) for k, v in input_cache.items()} for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) start = time.time() for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) print(f\"[ONNX Runtime / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\") for 256 tokens WITH cache: [Pytorch / CPU] 23.87ms [ONNX Runtime / CPU - with copy (naive)] 27.76ms [ONNX Runtime / CPU - no copy] 39.61ms [Pytorch / CUDA] 11.00ms [ONNX Runtime / CUDA - with copy (naive)] 14.02ms [ONNX Runtime / CUDA - no copy] 4.32ms First we note that ONNX Runtime with tensor copy is equivalent to Pytorch which keeps all its tensors on GPU. When we remove the overhead of tensor copies, ONNX Runtime is 1/3 faster compared to Pytorch. On CPU no copy latency is similar to tensor copy implementation... because tensors are all on host RAM (no host <-> GPU memory transfer). On GPU, no copy latency is 3 times faster than tensor copies implementation which shows that IO is crucial in this scenario. We compare outputs of Pytorch and ONNX Runtime, with and without cache: In [20]: Copied! device = \"cuda\" model = model . to ( device ) a = model ( input_ids = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 , device = device ), past_key_values = None ) print ( \"pytorch - do not use cache output\" ) print ( a . logits [:, - 1 , :]) b = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) print ( \"pytorch - use cache output\" ) print ( b . logits [:, - 1 , :]) input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , 1 ), dtype = torch . int32 , device = device ) input_cache [ \"attention_mask\" ] = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 , device = device ) for index , ( k , v ) in enumerate ( output_pytorch . past_key_values ): # type: int, (torch.Tensor, torch.Tensor) input_cache [ f \"past_key_values. { index } .key\" ] = k input_cache [ f \"past_key_values. { index } .value\" ] = v model_onnx = create_model_for_provider ( path = \"model-support-cache.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) print ( \"ONNX Runtime - use cache output\" ) print ( inference_onnx_binding ( model_onnx = model_onnx , inputs = input_cache , device = device )[ \"logits\" ]) device = \"cuda\" model = model.to(device) a = model(input_ids=torch.ones((batch, sequence + 1), dtype=torch.int32, device=device), past_key_values=None) print(\"pytorch - do not use cache output\") print(a.logits[:, -1, :]) b = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) print(\"pytorch - use cache output\") print(b.logits[:, -1, :]) input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32, device=device) input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32, device=device) for index, (k, v) in enumerate(output_pytorch.past_key_values): # type: int, (torch.Tensor, torch.Tensor) input_cache[f\"past_key_values.{index}.key\"] = k input_cache[f\"past_key_values.{index}.value\"] = v model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=\"CUDAExecutionProvider\") print(\"ONNX Runtime - use cache output\") print(inference_onnx_binding(model_onnx=model_onnx, inputs=input_cache, device=device)[\"logits\"]) pytorch - do not use cache output tensor([[-252.7095, -233.3065, -248.4932, ..., -274.1753, -281.5232, -249.9053]], device='cuda:0', grad_fn=<SliceBackward0>) pytorch - use cache output tensor([[-252.6975, -233.2973, -248.4842, ..., -274.1697, -281.5168, -249.8984]], device='cuda:0', grad_fn=<SliceBackward0>) ONNX Runtime - use cache output tensor([[[-252.6790, -233.2568, -248.4554, ..., -274.1343, -281.4962, -249.8820]]], device='cuda:0') Benchmark ONNX Runtime, CUDA tensors, without cache \u00b6 We compare the following setup, on both CPU and GPU: Pytorch ONNX Runtime (FP32): test-gpt2.onnx ONNX Runtime (kernel fusion + FP16): test-gpt2-opt.onnx For ONNX Runtime, we test 2 models, one without kernel fusion and one with it and FP16 precision. In [21]: Copied! input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , sequence ), dtype = torch . int32 , device = \"cuda\" ) print ( f \"for { sequence } tokens WITHOUT cache:\" ) for nb_inference , provider , device in [( 10 , \"CPUExecutionProvider\" , \"cpu\" ), ( 100 , \"CUDAExecutionProvider\" , \"cuda\" )]: inputs = { k : v . to ( device ) for k , v in input_cache . items ()} for _ in range ( nb_inference ): _ = model ( ** input_cache ) torch . cuda . synchronize () start = time . time () for _ in range ( nb_inference ): _ = model ( ** input_cache ) torch . cuda . synchronize () print ( f \"[Pytorch / { device . upper () } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) for model_path in [ \"test-gpt2.onnx\" , \"test-gpt2-opt.onnx\" ]: model_onnx = create_model_for_provider ( path = model_path , provider_to_use = provider ) # naive implementation inputs_ort_numpy = { k : v . cpu () . numpy () for k , v in input_cache . items ()} # warmup for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_numpy ) start = time . time () for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_numpy ) print ( f \"[ONNX Runtime / { device . upper () } - with copy (naive) - { model_path } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # no copy for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) start = time . time () for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) print ( f \"[ONNX Runtime / { device . upper () } - no copy - { model_path } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, sequence), dtype=torch.int32, device=\"cuda\") print(f\"for {sequence} tokens WITHOUT cache:\") for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]: inputs = {k: v.to(device) for k, v in input_cache.items()} for _ in range(nb_inference): _ = model(**input_cache) torch.cuda.synchronize() start = time.time() for _ in range(nb_inference): _ = model(**input_cache) torch.cuda.synchronize() print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\") for model_path in [\"test-gpt2.onnx\", \"test-gpt2-opt.onnx\"]: model_onnx = create_model_for_provider(path=model_path, provider_to_use=provider) # naive implementation inputs_ort_numpy = {k: v.cpu().numpy() for k, v in input_cache.items()} # warmup for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_numpy) start = time.time() for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_numpy) print( f\"[ONNX Runtime / {device.upper()} - with copy (naive) - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\" ) # no copy for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) start = time.time() for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) print( f\"[ONNX Runtime / {device.upper()} - no copy - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\" ) for 256 tokens WITHOUT cache: [Pytorch / CPU] 12.43ms [ONNX Runtime / CPU - with copy (naive) - test-gpt2.onnx] 166.56ms [ONNX Runtime / CPU - no copy - test-gpt2.onnx] 159.60ms [ONNX Runtime / CPU - with copy (naive) - test-gpt2-opt.onnx] 363.27ms [ONNX Runtime / CPU - no copy - test-gpt2-opt.onnx] 362.63ms [Pytorch / CUDA] 11.41ms [ONNX Runtime / CUDA - with copy (naive) - test-gpt2.onnx] 13.49ms [ONNX Runtime / CUDA - no copy - test-gpt2.onnx] 4.82ms [ONNX Runtime / CUDA - with copy (naive) - test-gpt2-opt.onnx] 12.25ms [ONNX Runtime / CUDA - no copy - test-gpt2-opt.onnx] 3.36ms In [22]: Copied! trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) trt_model_name = \"test-gpt2.plan\" tensorrt_model : Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ] = load_engine ( engine_file_path = \"test-gpt2.plan\" , runtime = runtime ) nb_inference = 100 for _ in range ( nb_inference ): tensorrt_model ( input_cache ) start = time . time () for _ in range ( nb_inference ): tensorrt_model ( input_cache ) print ( f \"[TensorRT / { device . upper () } - no copy] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) trt_logger: Logger = trt.Logger(trt.Logger.ERROR) runtime: Runtime = trt.Runtime(trt_logger) trt_model_name = \"test-gpt2.plan\" tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine( engine_file_path=\"test-gpt2.plan\", runtime=runtime ) nb_inference = 100 for _ in range(nb_inference): tensorrt_model(input_cache) start = time.time() for _ in range(nb_inference): tensorrt_model(input_cache) print(f\"[TensorRT / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\") [TensorRT / CUDA - no copy] 2.14ms The most interesting thing to note is that on ONNX Runtime, without cache and kernel fusion we get similar latency than with cache. It means that cache overhead is bigger than recomputing the tensors! It's probably the most unexpected result of this experience. As usual, FP16 models on CPU are slower than FP32. And to conclude, surprise, TensorRT is the fastest option by a large margin. In [23]: Copied!","title":"Accelerate text generation with GPT-2"},{"location":"gpt2/#accelerating-gpt-2-model","text":"","title":"Accelerating GPT-2 model"},{"location":"gpt2/#and-any-decoder-based-transformer-models","text":"Two trends ongoing in the NLP ecosystem: bigger language model and better text generation. Those trends are game changers (zero shot, etc.) and bring their own challenge: how to perform inference with them? At what cost? GPU or CPU ? etc. aka, how to leverage them in real life? That\u2019s what we worked on recently, and below you will find the main lessons learned : memory IO is by far the main perf bottleneck Standard API of ONNX Runtime should not be used but there is an undocumented way of using another ONNX Runtime API which works well Nvidia TensorRT is always the fastest option on GPU, by a large margin (expected) Caching K/V token representation does not bring any inference optimization (unexpected) First, let's remind how decoder models work... Generative text language models like GPT-2 produce text 1 token at a time. The model is auto regressive meaning that each produced token is part of the generation of the next token. There are mainly 2 blocks: the language model itself which produces big tensors, and the decoding algorithm which consumes the tensors and selects 1 or more tokens. Keep in mind that these blocks may live on different hardware\u2026 (spoiler: it\u2019s not a good idea) In [1]: Copied! from IPython.display import Image Image ( \"../../resources/img/gpt2.png\" ) from IPython.display import Image Image(\"../../resources/img/gpt2.png\") Out[1]:","title":"(and any decoder based transformer models)"},{"location":"gpt2/#gpt-2-loading","text":"As a reminder: gpt2 : 117M parameters gpt2-large 774M parameters In [2]: Copied! ! nvidia - smi ! nvidia-smi Wed Feb 16 08:56:02 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 30% 29C P5 37W / 350W | 165MiB / 24576MiB | 15% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1851 G /usr/lib/xorg/Xorg 89MiB | | 0 N/A N/A 7363 G /usr/bin/gnome-shell 74MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import time from typing import Callable , Dict import numpy as np import tensorrt as trt import torch from tensorrt import ICudaEngine from tensorrt.tensorrt import Logger , Runtime from transformers import AutoTokenizer , BatchEncoding , GPT2LMHeadModel , AutoModelForCausalLM from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions from transformer_deploy.utils.generative_model import GPTModelWrapper import inspect from transformers import TensorType from transformer_deploy.backends.ort_utils import create_model_for_provider , inference_onnx_binding , optimize_onnx from transformer_deploy.backends.pytorch_utils import convert_to_onnx , get_model_size from transformer_deploy.backends.trt_utils import build_engine , load_engine , save_engine import logging import time from typing import Callable, Dict import numpy as np import tensorrt as trt import torch from tensorrt import ICudaEngine from tensorrt.tensorrt import Logger, Runtime from transformers import AutoTokenizer, BatchEncoding, GPT2LMHeadModel, AutoModelForCausalLM from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions from transformer_deploy.utils.generative_model import GPTModelWrapper import inspect from transformers import TensorType from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size from transformer_deploy.backends.trt_utils import build_engine, load_engine, save_engine In [4]: Copied! model_name = \"gpt2\" # choices: gpt2 | gpt2-large # use GPT2LMHeadModel and not AutoModel to export raw outputs to predict next token model : GPT2LMHeadModel = AutoModelForCausalLM . from_pretrained ( model_name ) model . eval () tokenizer = AutoTokenizer . from_pretrained ( model_name ) # to avoid error message or passing some args to each generate call model . config . pad_token_id = tokenizer . eos_token_id model_name = \"gpt2\" # choices: gpt2 | gpt2-large # use GPT2LMHeadModel and not AutoModel to export raw outputs to predict next token model: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(model_name) model.eval() tokenizer = AutoTokenizer.from_pretrained(model_name) # to avoid error message or passing some args to each generate call model.config.pad_token_id = tokenizer.eos_token_id","title":"GPT-2 loading"},{"location":"gpt2/#model-output","text":"GPT-2 is trained to generate the probability for the next token. Then a decoding algorithm will consume this information and choose a token: greedy decoding will just choose the token getting the highest score beam search decoding will explore several possible sequences by keeping track of a limited set of tokens Below we output predictions for the next token. Output shape is: [batch size, nb input tokens, vocabulary size] In [5]: Copied! inputs = tokenizer ( \"Here is some text to encode Hello World\" , return_tensors = \"pt\" ) print ( \"input tensors\" ) print ( inputs ) print ( \"input tensor shape\" ) print ( inputs [ \"input_ids\" ] . size ()) with torch . no_grad (): outputs = model ( ** inputs ) logits = outputs . logits print ( \"output tensor\" ) print ( logits ) print ( \"output shape\" ) print ( logits . shape ) inputs = tokenizer(\"Here is some text to encode Hello World\", return_tensors=\"pt\") print(\"input tensors\") print(inputs) print(\"input tensor shape\") print(inputs[\"input_ids\"].size()) with torch.no_grad(): outputs = model(**inputs) logits = outputs.logits print(\"output tensor\") print(logits) print(\"output shape\") print(logits.shape) input tensors {'input_ids': tensor([[ 4342, 318, 617, 2420, 284, 37773, 18435, 2159]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} input tensor shape torch.Size([1, 8]) output tensor tensor([[[ -34.3027, -33.9891, -37.5683, ..., -42.6734, -42.0399, -34.6136], [ -83.3065, -82.9769, -86.1204, ..., -89.8062, -89.4546, -83.6084], [ -91.4901, -92.5655, -95.6423, ..., -96.6183, -98.1545, -91.5266], ..., [ -92.8820, -94.8433, -98.9224, ..., -101.4426, -103.2702, -95.7642], [ -72.6140, -76.3407, -79.7973, ..., -87.3300, -85.7930, -77.7521], [-103.6147, -108.7898, -109.6276, ..., -116.8557, -116.5565, -107.4467]]]) output shape torch.Size([1, 8, 50257])","title":"Model output"},{"location":"gpt2/#total-tensor-size-inputoutput","text":"GPT-2 will generate a sequence 1 token at a time. So to generates 256 tokens from an 8 tokens prompt, it will perform 248 inferences. To simplify things, we assume that we are using a greedy decoding algorithm. If you want to know more about decoding algorithm check this article . Let's compute the total size of the output tensor. In [6]: Copied! size = 0 for i in range ( 8 , 256 , 1 ): # input sequence (input_ids) made of int-32 (4 bytes) size += np . prod ([ 1 , i ]) * 4 # output tensor made of float-32 (4 bytes) size += np . prod ([ 1 , i , 50257 ]) * 4 print ( f \"total size (input+output): { size / 1024 ** 3 : .2f } Gb\" ) # to manually check actual tensor size: # np.prod(logits.shape)*32/8/1024**2:.2f} # or # sys.getsizeof(logits.storage())/1024**2 size = 0 for i in range(8, 256, 1): # input sequence (input_ids) made of int-32 (4 bytes) size += np.prod([1, i]) * 4 # output tensor made of float-32 (4 bytes) size += np.prod([1, i, 50257]) * 4 print(f\"total size (input+output): {size / 1024**3:.2f} Gb\") # to manually check actual tensor size: # np.prod(logits.shape)*32/8/1024**2:.2f} # or # sys.getsizeof(logits.storage())/1024**2 total size (input+output): 6.11 Gb Over 6Gb is a lot of data. We will see later than naive approach makes these data move from GPU to host and then from host to GPU. Therefore we are moving 12Gb of data. Let's keep the order of magnitude in mind when we will try to optimize GPT-2 inference.","title":"Total tensor size (input+output)"},{"location":"gpt2/#build-onnx-graph","text":"Performant inference engines tend to consume graph instead of imperative Pytorch code. We use ONNX for that purpose. ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. https://onnx.ai/ In [7]: Copied! input_ids : BatchEncoding = tokenizer ( \"Here is some text to encode Hello World\" , add_special_tokens = True , return_attention_mask = False , return_tensors = \"pt\" ) # some inference engines don't support int64 tensor as inputs, we convert all input tensors to int32 type for k , v in input_ids . items (): # type: str, torch.Tensor input_ids [ k ] = v . type ( dtype = torch . int32 ) convert_to_onnx ( model_pytorch = model , output_path = \"test-gpt2.onnx\" , inputs_pytorch = dict ( input_ids ), quantization = False , var_output_seq = True , # we inform ONNX export tool that the output shape will vary with the input shape ) # model may switch to train mode for some unknown reasons, we force the eval mode. _ = model . eval () input_ids: BatchEncoding = tokenizer( \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\" ) # some inference engines don't support int64 tensor as inputs, we convert all input tensors to int32 type for k, v in input_ids.items(): # type: str, torch.Tensor input_ids[k] = v.type(dtype=torch.int32) convert_to_onnx( model_pytorch=model, output_path=\"test-gpt2.onnx\", inputs_pytorch=dict(input_ids), quantization=False, var_output_seq=True, # we inform ONNX export tool that the output shape will vary with the input shape ) # model may switch to train mode for some unknown reasons, we force the eval mode. _ = model.eval() /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)","title":"Build ONNX graph"},{"location":"gpt2/#optimize-onnx-graph","text":"ONNX Runtime optimizations are offline operations to simplify the computation graph. Some make the computation itself faster (like approximating GELU) but most of them are targeting memory transfer (kernel fusion will perform a series of computations in a single pass, avoiding reading/writing several times the same array, etc.). In [8]: Copied! logging . basicConfig () logging . getLogger () . setLevel ( logging . INFO ) num_attention_heads , hidden_size = get_model_size ( path = model_name ) optimize_onnx ( onnx_path = \"test-gpt2.onnx\" , onnx_optim_model_path = \"test-gpt2-opt.onnx\" , fp16 = True , use_cuda = True , num_attention_heads = num_attention_heads , hidden_size = hidden_size , architecture = \"gpt2\" , ) logging.basicConfig() logging.getLogger().setLevel(logging.INFO) num_attention_heads, hidden_size = get_model_size(path=model_name) optimize_onnx( onnx_path=\"test-gpt2.onnx\", onnx_optim_model_path=\"test-gpt2-opt.onnx\", fp16=True, use_cuda=True, num_attention_heads=num_attention_heads, hidden_size=hidden_size, architecture=\"gpt2\", ) INFO:fusion_base:Fused LayerNormalization count: 25 INFO:fusion_base:Fused FastGelu count: 12 INFO:fusion_utils:Remove reshape node Reshape_9 since its input shape is same as output: ['batch_size', 'sequence'] INFO:fusion_utils:Remove reshape node Reshape_19 since its input shape is same as output: [1, 'sequence'] INFO:fusion_utils:Remove reshape node Reshape_2700 since its input shape is same as output: ['batch_size', 'sequence', 768] INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 23 nodes are removed INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 864 nodes are removed INFO:onnx_model_gpt2:postprocess: remove Reshape count:72 INFO:fusion_base:Fused FastGelu(add bias) count: 12 INFO:onnx_model_bert:opset verion: 13 INFO:onnx_model_bert:Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0} INFO:root:optimizations applied: {'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0} INFO:onnx_model:Sort graphs in topological order INFO:onnx_model:Output model to test-gpt2-opt.onnx","title":"Optimize ONNX graph"},{"location":"gpt2/#build-tensorrt-engine","text":"This operation will take the original ONNX graph (before ONNX Runtime optimizations) and optimize it the Nvidia way. The nature of the graph optimization is similar (approximation and kernel fusion) One of difference with ONNX Runtime is that these optimizations are benchmarked online on the real hardware, it's called kernel mapping (find the right kernel for specific data shape+model+hardware). It takes more time to optimize but at the end it is usually much faster. In [9]: Copied! from pathlib import Path trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) trt_model_name = \"test-gpt2.plan\" # create only of does not exist because it's slow to run... if not Path ( trt_model_name ) . exists (): engine : ICudaEngine = build_engine ( runtime = runtime , onnx_file_path = \"test-gpt2.onnx\" , logger = trt_logger , min_shape = ( 1 , 1 ), optimal_shape = ( 1 , 128 ), # num beam, batch size max_shape = ( 1 , 384 ), # num beam, batch size workspace_size = 10000 * 1024 ** 2 , fp16 = True , int8 = False , ) save_engine ( engine , trt_model_name ) from pathlib import Path trt_logger: Logger = trt.Logger(trt.Logger.ERROR) runtime: Runtime = trt.Runtime(trt_logger) trt_model_name = \"test-gpt2.plan\" # create only of does not exist because it's slow to run... if not Path(trt_model_name).exists(): engine: ICudaEngine = build_engine( runtime=runtime, onnx_file_path=\"test-gpt2.onnx\", logger=trt_logger, min_shape=(1, 1), optimal_shape=(1, 128), # num beam, batch size max_shape=(1, 384), # num beam, batch size workspace_size=10000 * 1024**2, fp16=True, int8=False, ) save_engine(engine, trt_model_name)","title":"Build TensorRT engine"},{"location":"gpt2/#end-to-end-text-generation-benchmarks-inferencedecoding","text":"We will benchmark 3 inference engines: Pytorch : Hugging Face implementation ONNX Runtime : with optimized ONNX graph and standard API : tensors stored as numpy objects, on host RAM with optimized ONNX graph and binding IO API : tensors stored on CUDA, to limit IO Nvidia TensorRT : tensors stored on CUDA, to limit IO For each of them, we print the output of the result so we can check that it generates the same sequence. Then we measure inference latency.","title":"End to end text generation benchmarks (inference+decoding)"},{"location":"gpt2/#generative-model-wrapper","text":"To replace Pytorch as inference engine and still use Hugging Face decoding algorithm, we use a wrapping class which separate those tasks. The most interesting thing in the class below is that we inherit from GenerationMixin (from Hugging Face transformers library), it will give our class some super-powers like a method to generate sequences (aka running some decoding algorithm on top of the model output). Note that the actual model inference is done by a function provided through the constructor. In [10]: Copied! print ( inspect . getsource ( GPTModelWrapper )) print(inspect.getsource(GPTModelWrapper)) class GPTModelWrapper(Module, GenerationMixin): def __init__( self, config: PretrainedConfig, device: torch.device, inference: Callable[[torch.Tensor], torch.Tensor] ): super().__init__() self.config: PretrainedConfig = config self.device: torch.device = device self.inference: Callable[[torch.Tensor], torch.Tensor] = inference self.main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 def prepare_inputs_for_generation(self, input_ids, **kwargs): return { self.main_input_name: input_ids, } def forward(self, input_ids, **_): logits = self.inference(input_ids) return CausalLMOutputWithCrossAttentions(logits=logits) In [11]: Copied! inputs = tokenizer ( \"Here is some text to encode Hello World\" , # Nvidia example prompt add_special_tokens = True , return_attention_mask = False , # Not used return_tensors = TensorType . PYTORCH , ) inputs inputs = tokenizer( \"Here is some text to encode Hello World\", # Nvidia example prompt add_special_tokens=True, return_attention_mask=False, # Not used return_tensors=TensorType.PYTORCH, ) inputs Out[11]: {'input_ids': tensor([[ 4342, 318, 617, 2420, 284, 37773, 18435, 2159]])}","title":"Generative model wrapper"},{"location":"gpt2/#pytorch-inference","text":"We first measure vanilla Hugging face implementation with Pytorch backend. It will be our baseline. In [12]: Copied! def inference_torch ( input_ids : torch . Tensor ) -> torch . Tensor : transformer_outputs : BaseModelOutputWithPastAndCrossAttentions = model . transformer ( input_ids = input_ids ) return model . lm_head ( transformer_outputs . last_hidden_state ) model . cuda () model . eval () inputs . to ( \"cuda\" ) with torch . inference_mode (): gpt2_model = GPTModelWrapper ( config = model . config , device = model . device , inference = inference_torch ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = False )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) torch . cuda . synchronize () start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) torch . cuda . synchronize () print ( f \"---- \\n Pytorch: { ( time . time () - start ) / 10 : .2f } s/sequence\" ) _ = model . cpu () def inference_torch(input_ids: torch.Tensor) -> torch.Tensor: transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids) return model.lm_head(transformer_outputs.last_hidden_state) model.cuda() model.eval() inputs.to(\"cuda\") with torch.inference_mode(): gpt2_model = GPTModelWrapper(config=model.config, device=model.device, inference=inference_torch) sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=False)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) torch.cuda.synchronize() start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) torch.cuda.synchronize() print(f\"----\\nPytorch: {(time.time() - start)/10:.2f}s/sequence\") _ = model.cpu() Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- Pytorch: 2.51s/sequence","title":"Pytorch inference"},{"location":"gpt2/#naive-onnx-runtime-inference","text":"Below we use ONNX Runtime and its standard API. Inference is performed on GPU (CUDA provider in the ONNX Runtime ecosystem). It takes as input and return as output a numpy tensor. numpy tensor are by their nature always stored on host memory (RAM). It means that for each token the whole output tensor is moved from GPU memory to host. As measure above, it represents more than 2X6Gb of memory transfers between host and GPU memory. In our code, we convert the output numpy tensor to Pytorch one so it can be consumed by the Hugging Face decoding algorithm. ONNX model used is the optimized one (kernel fusion + fp16). In [13]: Copied! model_onnx = create_model_for_provider ( path = \"test-gpt2-opt.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) def inference_onnx_naive ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids . detach () . cpu () . numpy () . astype ( np . int32 )} logit = model_onnx . run ( None , data ) np_logit = np . array ( logit ) # convert list of numpy arrays to a numpy array # we convert numpy tensor to Pytorch tensor as it's the type expected by HF decoding algorithm return torch . squeeze ( torch . from_numpy ( np_logit ), dim = 0 ) gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cpu\" ), inference = inference_onnx_naive ) inputs . to ( \"cpu\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n ONNX Runtime (standard API): { ( time . time () - start ) / 10 : .2f } s/sequence\" ) del model_onnx model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\") def inference_onnx_naive(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids.detach().cpu().numpy().astype(np.int32)} logit = model_onnx.run(None, data) np_logit = np.array(logit) # convert list of numpy arrays to a numpy array # we convert numpy tensor to Pytorch tensor as it's the type expected by HF decoding algorithm return torch.squeeze(torch.from_numpy(np_logit), dim=0) gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cpu\"), inference=inference_onnx_naive) inputs.to(\"cpu\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nONNX Runtime (standard API): {(time.time() - start)/10:.2f}s/sequence\") del model_onnx Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- ONNX Runtime (standard API): 3.93s/sequence No surprise, the generation is slower than vanilla Pytorch, even after having optimized the ONNX model.","title":"Naive ONNX Runtime inference"},{"location":"gpt2/#optimized-onnx-runtime-inference-no-gpu-host-tensor-move","text":"In this benchmark, we use ONNX Runtime and its binding IO API . The way it works inside ( inference_onnx_binding code) is that we basically provide to the API a pointer to Pytorch tensor storage array. The storage array is a CUDA array (in our case) containing the tensor data themselves, the tensor abstraction adds some meta data to this array. Therefore ONNX Runtime can directly write output in the Pytorch tensor without using any numpy array (stored in host RAM). Because Hugging Face decoding algorithm can run on CUDA too, we do not move any data to host. By just removing memory movement, we reduce the inference time by a large margin. Warning if you want to use this ONNX Runtime API, don't forget to make your tensor contiguous in memory (not shown here because it is done by the transformer-deploy library). Another point to have in mind is that you need to reserve manually GPU memory... and unlike TensorRT, ONNX Runtime is not able to predict output shape, so you need to have the output size logic elsewhere. In [14]: Copied! model_onnx = create_model_for_provider ( path = \"test-gpt2-opt.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) def inference_onnx_optimized ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids } return inference_onnx_binding ( model_onnx = model_onnx , inputs = data , device = \"cuda\" )[ \"output\" ] gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cuda\" ), inference = inference_onnx_optimized ) inputs . to ( \"cuda\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n ONNX Runtime (binding io API): { ( time . time () - start ) / 10 : .2f } /sequence\" ) del model_onnx model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\") def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids} return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"] gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_onnx_optimized) inputs.to(\"cuda\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nONNX Runtime (binding io API): {(time.time() - start)/10:.2f}/sequence\") del model_onnx Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- ONNX Runtime (binding io API): 0.85/sequence Good news: the output is the same than Pytorch, and still inference time is much faster.","title":"Optimized ONNX Runtime inference (no GPU &lt;-&gt; host tensor move)"},{"location":"gpt2/#tensorrt-inference","text":"To conclude, we use the Nvidia engine, all tensors are stored on CUDA. Unlike ONNX Runtime, each optimization applied has been checked on the target GPU with the expected tensor shape. It explains most of its performance compared to ONNX Runtime optimized performances. In [15]: Copied! tensorrt_model : Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ] = load_engine ( engine_file_path = \"test-gpt2.plan\" , runtime = runtime ) def inference_tensorrt ( input_ids : torch . Tensor ) -> torch . Tensor : data = { \"input_ids\" : input_ids } return tensorrt_model ( data ) gpt2_model = GPTModelWrapper ( config = model . config , device = torch . device ( \"cuda\" ), inference = inference_tensorrt ) inputs . to ( \"cuda\" ) sample_output = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) print ( tokenizer . decode ( sample_output [ 0 ], skip_special_tokens = True )) for _ in range ( 2 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 64 ) start = time . time () for _ in range ( 10 ): _ = gpt2_model . generate ( inputs . input_ids , max_length = 256 ) print ( f \"---- \\n TensorRT + CUDA tensors: { ( time . time () - start ) / 10 : .2f } /sequence\" ) del tensorrt_model tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine( engine_file_path=\"test-gpt2.plan\", runtime=runtime ) def inference_tensorrt(input_ids: torch.Tensor) -> torch.Tensor: data = {\"input_ids\": input_ids} return tensorrt_model(data) gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_tensorrt) inputs.to(\"cuda\") sample_output = gpt2_model.generate(inputs.input_ids, max_length=64) print(tokenizer.decode(sample_output[0], skip_special_tokens=True)) for _ in range(2): _ = gpt2_model.generate(inputs.input_ids, max_length=64) start = time.time() for _ in range(10): _ = gpt2_model.generate(inputs.input_ids, max_length=256) print(f\"----\\nTensorRT + CUDA tensors: {(time.time() - start)/10:.2f}/sequence\") del tensorrt_model Here is some text to encode Hello World. Hello World Hello World is a simple program that takes a string and returns a string. The program is written in C. The program is written in C. The program is written in C. The program is written in C. The program ---- TensorRT + CUDA tensors: 0.50/sequence","title":"TensorRT Inference"},{"location":"gpt2/#is-caching-of-key-values-in-self-attention-block-transformer-architecture-a-good-strategy-on-gpu","text":"In the self-attention block, the first step is to compute query, key and value vectors for each input token. For each token, we look up its embedding in the embedding matrix and adds to it a positional encoding so the model knows word order. Then we multiply each of these token representation by 3 matrices: query (Q), key (K), and value (V). These matrices are then transposed, with Q T and K T being used to compute the normalized dot-product attention values, before being combined with V T to produce the final output. This computation is done for each self-attention block (as each of them has its own \"memory\"). In a generative model, we need to recompute those values for each generated token. For a specific input token, the result won't change from one inference to the next one, it may be interesting to cache and reuse the results instead of recomputing it. However, this won't come for free as we would need 2 ONNX / TensorRT models, one generating the values for the first time (to boot the sequence generation), and one able to reuse the cached values. It means 2X bigger memory footprint on GPU :-( But first, let's measure the gain in performance of caching values, if any. In [16]: Copied! model . cuda () model . eval () inputs . to ( \"cuda\" ) with torch . inference_mode (): for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 64 , use_cache = True ) torch . cuda . synchronize () start = time . time () for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 256 , use_cache = True ) torch . cuda . synchronize () print ( f \"Pytorch with cache: { ( time . time () - start ) / 2 : .2f } s/sequence\" ) for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 64 , use_cache = False ) torch . cuda . synchronize () start = time . time () for _ in range ( 2 ): _ = model . generate ( inputs . input_ids , max_length = 256 , use_cache = False ) torch . cuda . synchronize () print ( f \"Pytorch without cache: { ( time . time () - start ) / 2 : .2f } s/sequence\" ) _ = model . cpu () model.cuda() model.eval() inputs.to(\"cuda\") with torch.inference_mode(): for _ in range(2): _ = model.generate(inputs.input_ids, max_length=64, use_cache=True) torch.cuda.synchronize() start = time.time() for _ in range(2): _ = model.generate(inputs.input_ids, max_length=256, use_cache=True) torch.cuda.synchronize() print(f\"Pytorch with cache: {(time.time() - start)/2:.2f}s/sequence\") for _ in range(2): _ = model.generate(inputs.input_ids, max_length=64, use_cache=False) torch.cuda.synchronize() start = time.time() for _ in range(2): _ = model.generate(inputs.input_ids, max_length=256, use_cache=False) torch.cuda.synchronize() print(f\"Pytorch without cache: {(time.time() - start)/2:.2f}s/sequence\") _ = model.cpu() Pytorch with cache: 2.35s/sequence Pytorch without cache: 2.32s/sequence Suprinsingly, cache support or not, on Pytorch, with an end-to-end test, for a 256 sequence length and when inference is performed on a last generation GPU (at the time of writing), latencies are very similar. Expect different results on older/slower GPU like Nvidia T4 (cache may help more). Not shown in this notebook to keep it \"light\", on fast GPU, for very long sequences (like 1024 tokens), on Pytorch, cache support provides better performances, but optimized ONNX Runtime / TensorRT engines without cache support are still faster. To dig a bit more, we will measure inference engines with fixed input sizes.","title":"Is caching of Key / Values in self-attention block (Transformer architecture) a good strategy on GPU?"},{"location":"gpt2/#export-an-onnx-model-able-to-reuse-cache","text":"To simplify the code, we just use the ONNX exporter tool from Hugging Face library (it's a far from perfect tool, in this case it's okish, in general be prudent in using it). In [17]: Copied! from itertools import chain from torch.onnx import export from transformers.models.gpt2 import GPT2OnnxConfig from transformers.onnx.features import FeaturesManager model_name = \"gpt2\" feature = \"causal-lm-with-past\" seq_len = 256 atol = 0.2 tokenizer = AutoTokenizer . from_pretrained ( model_name ) model : GPT2LMHeadModel = FeaturesManager . get_model_from_feature ( feature , model_name ) model_kind , model_onnx_config = FeaturesManager . check_supported_model_or_raise ( model , feature = feature ) onnx_config : GPT2OnnxConfig = model_onnx_config ( model . config ) with torch . no_grad (): model . config . return_dict = True model . eval () # Check if we need to override certain configuration item if onnx_config . values_override is not None : for override_config_key , override_config_value in onnx_config . values_override . items (): setattr ( model . config , override_config_key , override_config_value ) # Ensure inputs match model_inputs = onnx_config . generate_dummy_inputs ( tokenizer , framework = TensorType . PYTORCH ) for k , v in model_inputs . items (): if isinstance ( v , torch . Tensor ): model_inputs [ k ] = model_inputs [ k ] . type ( torch . int32 ) onnx_outputs = list ( onnx_config . outputs . keys ()) onnx_config . patch_ops () # export can works with named args but the dict containing named args as to be last element of the args tuple export ( model , ( model_inputs ,), f = \"model-support-cache.onnx\" , input_names = list ( onnx_config . inputs . keys ()), output_names = onnx_outputs , dynamic_axes = { name : axes for name , axes in chain ( onnx_config . inputs . items (), onnx_config . outputs . items ())}, do_constant_folding = True , use_external_data_format = onnx_config . use_external_data_format ( model . num_parameters ()), enable_onnx_checker = True , opset_version = 13 , ) onnx_config . restore_ops () from itertools import chain from torch.onnx import export from transformers.models.gpt2 import GPT2OnnxConfig from transformers.onnx.features import FeaturesManager model_name = \"gpt2\" feature = \"causal-lm-with-past\" seq_len = 256 atol = 0.2 tokenizer = AutoTokenizer.from_pretrained(model_name) model: GPT2LMHeadModel = FeaturesManager.get_model_from_feature(feature, model_name) model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature) onnx_config: GPT2OnnxConfig = model_onnx_config(model.config) with torch.no_grad(): model.config.return_dict = True model.eval() # Check if we need to override certain configuration item if onnx_config.values_override is not None: for override_config_key, override_config_value in onnx_config.values_override.items(): setattr(model.config, override_config_key, override_config_value) # Ensure inputs match model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH) for k, v in model_inputs.items(): if isinstance(v, torch.Tensor): model_inputs[k] = model_inputs[k].type(torch.int32) onnx_outputs = list(onnx_config.outputs.keys()) onnx_config.patch_ops() # export can works with named args but the dict containing named args as to be last element of the args tuple export( model, (model_inputs,), f=\"model-support-cache.onnx\", input_names=list(onnx_config.inputs.keys()), output_names=onnx_outputs, dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())}, do_constant_folding=True, use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()), enable_onnx_checker=True, opset_version=13, ) onnx_config.restore_ops() /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError. warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \" /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers. warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \" /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:797: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! if batch_size <= 0: The ONNX graph with cache support expects as input: 1 tensor name input_ids of shape [batch, 1] 12 tensors named past_key_values.X.key (X replaced by layer ID) of shape [batch, nb heads (12), past sequence len, 64] 12 tensors named past_key_values.X.value (X replaced by layer ID) of shape [batch, nb heads (12), past sequence len, 64] 1 tensor named attention_mask of shape [batch, past sequence len + 1] To list model inputs (according to their ONNX graph): In [18]: Copied! import onnx model_cache = onnx . load ( \"model-support-cache.onnx\" ) # first 100 lines text = \" \\n \" . join ( str ( model_cache . graph . input ) . split ( \" \\n \" )[: 80 ]) print ( text ) del model_cache import onnx model_cache = onnx.load(\"model-support-cache.onnx\") # first 100 lines text = \"\\n\".join(str(model_cache.graph.input).split(\"\\n\")[:80]) print(text) del model_cache [name: \"input_ids\" type { tensor_type { elem_type: 6 shape { dim { dim_param: \"batch\" } dim { dim_param: \"sequence\" } } } } , name: \"past_key_values.0.key\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.0.value\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.1.key\" type { tensor_type { elem_type: 1 shape { dim { dim_param: \"batch\" } dim { dim_value: 12 } dim { dim_param: \"past_sequence + sequence\" } dim { dim_value: 64 } } } } , name: \"past_key_values.1.value\" type { tensor_type { elem_type: 1 shape { dim {","title":"Export an ONNX model able to reuse cache"},{"location":"gpt2/#benchmark-onnx-runtime-cuda-tensors-with-cache","text":"When using cache, we only test non-optimized ONNX models (no kernel fusion, fp32) as optimization crash in this setup. It's possibly because ONNX Runtime doesn't find some patterns... In [19]: Copied! batch = 1 sequence = 256 # input with random values input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , 1 ), dtype = torch . int32 ) for i in range ( 12 ): # 12 layers input_cache [ f \"past_key_values. { i } .key\" ] = torch . empty (( batch , 12 , sequence , 64 ), dtype = torch . float32 ) input_cache [ f \"past_key_values. { i } .value\" ] = torch . empty (( batch , 12 , sequence , 64 ), dtype = torch . float32 ) input_cache [ \"attention_mask\" ] = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 ) print ( f \"for { sequence } tokens WITH cache:\" ) for nb_inference , provider , device in [( 10 , \"CPUExecutionProvider\" , \"cpu\" ), ( 100 , \"CUDAExecutionProvider\" , \"cuda\" )]: model_onnx = create_model_for_provider ( path = \"model-support-cache.onnx\" , provider_to_use = provider ) # Pytorch model = model . to ( device = device , non_blocking = False ) output_pytorch = model ( input_ids = torch . ones (( batch , sequence ), dtype = torch . int32 , device = device ), past_key_values = None ) pytorch_input = torch . ones (( batch , 1 ), dtype = torch . int32 , device = device ) for i in range ( nb_inference ): _ = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) torch . cuda . synchronize () start = time . time () for i in range ( nb_inference ): _ = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) torch . cuda . synchronize () print ( f \"[Pytorch / { device . upper () } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # naive implementation (tensor copies) inputs_ort_np = { k : v . cpu () . numpy () for k , v in input_cache . items ()} # warmup for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_np ) start = time . time () for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_np ) print ( f \"[ONNX Runtime / { device . upper () } - with copy (naive)] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # ONNX Runtime optimized (no tensor copy) inputs = { k : v . to ( device ) for k , v in input_cache . items ()} for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) start = time . time () for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) print ( f \"[ONNX Runtime / { device . upper () } - no copy] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) batch = 1 sequence = 256 # input with random values input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32) for i in range(12): # 12 layers input_cache[f\"past_key_values.{i}.key\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32) input_cache[f\"past_key_values.{i}.value\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32) input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32) print(f\"for {sequence} tokens WITH cache:\") for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]: model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=provider) # Pytorch model = model.to(device=device, non_blocking=False) output_pytorch = model( input_ids=torch.ones((batch, sequence), dtype=torch.int32, device=device), past_key_values=None ) pytorch_input = torch.ones((batch, 1), dtype=torch.int32, device=device) for i in range(nb_inference): _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) torch.cuda.synchronize() start = time.time() for i in range(nb_inference): _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) torch.cuda.synchronize() print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\") # naive implementation (tensor copies) inputs_ort_np = {k: v.cpu().numpy() for k, v in input_cache.items()} # warmup for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_np) start = time.time() for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_np) print(f\"[ONNX Runtime / {device.upper()} - with copy (naive)] {1e3*(time.time() - start)/nb_inference:.2f}ms\") # ONNX Runtime optimized (no tensor copy) inputs = {k: v.to(device) for k, v in input_cache.items()} for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) start = time.time() for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) print(f\"[ONNX Runtime / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\") for 256 tokens WITH cache: [Pytorch / CPU] 23.87ms [ONNX Runtime / CPU - with copy (naive)] 27.76ms [ONNX Runtime / CPU - no copy] 39.61ms [Pytorch / CUDA] 11.00ms [ONNX Runtime / CUDA - with copy (naive)] 14.02ms [ONNX Runtime / CUDA - no copy] 4.32ms First we note that ONNX Runtime with tensor copy is equivalent to Pytorch which keeps all its tensors on GPU. When we remove the overhead of tensor copies, ONNX Runtime is 1/3 faster compared to Pytorch. On CPU no copy latency is similar to tensor copy implementation... because tensors are all on host RAM (no host <-> GPU memory transfer). On GPU, no copy latency is 3 times faster than tensor copies implementation which shows that IO is crucial in this scenario. We compare outputs of Pytorch and ONNX Runtime, with and without cache: In [20]: Copied! device = \"cuda\" model = model . to ( device ) a = model ( input_ids = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 , device = device ), past_key_values = None ) print ( \"pytorch - do not use cache output\" ) print ( a . logits [:, - 1 , :]) b = model ( input_ids = pytorch_input , past_key_values = output_pytorch . past_key_values ) print ( \"pytorch - use cache output\" ) print ( b . logits [:, - 1 , :]) input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , 1 ), dtype = torch . int32 , device = device ) input_cache [ \"attention_mask\" ] = torch . ones (( batch , sequence + 1 ), dtype = torch . int32 , device = device ) for index , ( k , v ) in enumerate ( output_pytorch . past_key_values ): # type: int, (torch.Tensor, torch.Tensor) input_cache [ f \"past_key_values. { index } .key\" ] = k input_cache [ f \"past_key_values. { index } .value\" ] = v model_onnx = create_model_for_provider ( path = \"model-support-cache.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) print ( \"ONNX Runtime - use cache output\" ) print ( inference_onnx_binding ( model_onnx = model_onnx , inputs = input_cache , device = device )[ \"logits\" ]) device = \"cuda\" model = model.to(device) a = model(input_ids=torch.ones((batch, sequence + 1), dtype=torch.int32, device=device), past_key_values=None) print(\"pytorch - do not use cache output\") print(a.logits[:, -1, :]) b = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values) print(\"pytorch - use cache output\") print(b.logits[:, -1, :]) input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32, device=device) input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32, device=device) for index, (k, v) in enumerate(output_pytorch.past_key_values): # type: int, (torch.Tensor, torch.Tensor) input_cache[f\"past_key_values.{index}.key\"] = k input_cache[f\"past_key_values.{index}.value\"] = v model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=\"CUDAExecutionProvider\") print(\"ONNX Runtime - use cache output\") print(inference_onnx_binding(model_onnx=model_onnx, inputs=input_cache, device=device)[\"logits\"]) pytorch - do not use cache output tensor([[-252.7095, -233.3065, -248.4932, ..., -274.1753, -281.5232, -249.9053]], device='cuda:0', grad_fn=<SliceBackward0>) pytorch - use cache output tensor([[-252.6975, -233.2973, -248.4842, ..., -274.1697, -281.5168, -249.8984]], device='cuda:0', grad_fn=<SliceBackward0>) ONNX Runtime - use cache output tensor([[[-252.6790, -233.2568, -248.4554, ..., -274.1343, -281.4962, -249.8820]]], device='cuda:0')","title":"Benchmark ONNX Runtime, CUDA tensors, with cache"},{"location":"gpt2/#benchmark-onnx-runtime-cuda-tensors-without-cache","text":"We compare the following setup, on both CPU and GPU: Pytorch ONNX Runtime (FP32): test-gpt2.onnx ONNX Runtime (kernel fusion + FP16): test-gpt2-opt.onnx For ONNX Runtime, we test 2 models, one without kernel fusion and one with it and FP16 precision. In [21]: Copied! input_cache = dict () input_cache [ \"input_ids\" ] = torch . ones (( batch , sequence ), dtype = torch . int32 , device = \"cuda\" ) print ( f \"for { sequence } tokens WITHOUT cache:\" ) for nb_inference , provider , device in [( 10 , \"CPUExecutionProvider\" , \"cpu\" ), ( 100 , \"CUDAExecutionProvider\" , \"cuda\" )]: inputs = { k : v . to ( device ) for k , v in input_cache . items ()} for _ in range ( nb_inference ): _ = model ( ** input_cache ) torch . cuda . synchronize () start = time . time () for _ in range ( nb_inference ): _ = model ( ** input_cache ) torch . cuda . synchronize () print ( f \"[Pytorch / { device . upper () } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) for model_path in [ \"test-gpt2.onnx\" , \"test-gpt2-opt.onnx\" ]: model_onnx = create_model_for_provider ( path = model_path , provider_to_use = provider ) # naive implementation inputs_ort_numpy = { k : v . cpu () . numpy () for k , v in input_cache . items ()} # warmup for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_numpy ) start = time . time () for _ in range ( nb_inference ): _ = model_onnx . run ( None , inputs_ort_numpy ) print ( f \"[ONNX Runtime / { device . upper () } - with copy (naive) - { model_path } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) # no copy for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) start = time . time () for _ in range ( nb_inference ): inference_onnx_binding ( model_onnx = model_onnx , inputs = inputs , device = device ) print ( f \"[ONNX Runtime / { device . upper () } - no copy - { model_path } ] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) input_cache = dict() input_cache[\"input_ids\"] = torch.ones((batch, sequence), dtype=torch.int32, device=\"cuda\") print(f\"for {sequence} tokens WITHOUT cache:\") for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]: inputs = {k: v.to(device) for k, v in input_cache.items()} for _ in range(nb_inference): _ = model(**input_cache) torch.cuda.synchronize() start = time.time() for _ in range(nb_inference): _ = model(**input_cache) torch.cuda.synchronize() print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\") for model_path in [\"test-gpt2.onnx\", \"test-gpt2-opt.onnx\"]: model_onnx = create_model_for_provider(path=model_path, provider_to_use=provider) # naive implementation inputs_ort_numpy = {k: v.cpu().numpy() for k, v in input_cache.items()} # warmup for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_numpy) start = time.time() for _ in range(nb_inference): _ = model_onnx.run(None, inputs_ort_numpy) print( f\"[ONNX Runtime / {device.upper()} - with copy (naive) - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\" ) # no copy for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) start = time.time() for _ in range(nb_inference): inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device) print( f\"[ONNX Runtime / {device.upper()} - no copy - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\" ) for 256 tokens WITHOUT cache: [Pytorch / CPU] 12.43ms [ONNX Runtime / CPU - with copy (naive) - test-gpt2.onnx] 166.56ms [ONNX Runtime / CPU - no copy - test-gpt2.onnx] 159.60ms [ONNX Runtime / CPU - with copy (naive) - test-gpt2-opt.onnx] 363.27ms [ONNX Runtime / CPU - no copy - test-gpt2-opt.onnx] 362.63ms [Pytorch / CUDA] 11.41ms [ONNX Runtime / CUDA - with copy (naive) - test-gpt2.onnx] 13.49ms [ONNX Runtime / CUDA - no copy - test-gpt2.onnx] 4.82ms [ONNX Runtime / CUDA - with copy (naive) - test-gpt2-opt.onnx] 12.25ms [ONNX Runtime / CUDA - no copy - test-gpt2-opt.onnx] 3.36ms In [22]: Copied! trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) trt_model_name = \"test-gpt2.plan\" tensorrt_model : Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ] = load_engine ( engine_file_path = \"test-gpt2.plan\" , runtime = runtime ) nb_inference = 100 for _ in range ( nb_inference ): tensorrt_model ( input_cache ) start = time . time () for _ in range ( nb_inference ): tensorrt_model ( input_cache ) print ( f \"[TensorRT / { device . upper () } - no copy] { 1e3 * ( time . time () - start ) / nb_inference : .2f } ms\" ) trt_logger: Logger = trt.Logger(trt.Logger.ERROR) runtime: Runtime = trt.Runtime(trt_logger) trt_model_name = \"test-gpt2.plan\" tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine( engine_file_path=\"test-gpt2.plan\", runtime=runtime ) nb_inference = 100 for _ in range(nb_inference): tensorrt_model(input_cache) start = time.time() for _ in range(nb_inference): tensorrt_model(input_cache) print(f\"[TensorRT / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\") [TensorRT / CUDA - no copy] 2.14ms The most interesting thing to note is that on ONNX Runtime, without cache and kernel fusion we get similar latency than with cache. It means that cache overhead is bigger than recomputing the tensors! It's probably the most unexpected result of this experience. As usual, FP16 models on CPU are slower than FP32. And to conclude, surprise, TensorRT is the fastest option by a large margin. In [23]: Copied!","title":"Benchmark ONNX Runtime, CUDA tensors, without cache"},{"location":"onnx_convert/","text":"Convert Pytorch model to ONNX # To ease optimization we need to convert our Pytorch model written in imperative code in a mostly static graph. Therefore, optimization tooling will be able to run static analysis and search for some pattern to optimize. The target graph format is ONNX . from https://onnx.ai/ ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators \u2014 the building blocks of machine learning and deep learning models \u2014 and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\u201d (https://onnx.ai/). The format has initially been created by Facebook and Microsoft to have a bridge between Pytorch (research) and Caffee2 (production). There are 2 ways to perform an export from Pytorch: tracing mode : send some (dummy) data to the model, and the tool will trace them inside the model, that way it will guess what the graph looks like; scripting : requires the models to be written in a certain way to work, its main advantage is that the dynamic logic is kept intact but adds many constraints in the way models are written. Attention Tracing mode is not magic, for instance it can\u2019t see operations you are doing in numpy (if any), the graph will be static, some if/else code is fixed forever, for loop will be unrolled, etc. Hugging Face and model authors took care that main/most models are tracing mode compatible. Following commented code performs the ONNX conversion: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from collections import OrderedDict import torch from torch.onnx import TrainingMode def convert_to_onnx ( model_pytorch , output_path : str , inputs_pytorch , opset : int = 12 ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. :param model_pytorch: Pytorch model :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param opset: version of ONNX protocol to use, usually 12, or 13 if you use per channel quantized model \"\"\" # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = opset , # the ONNX version to use, 13 if quantized model, 12 for not quantized ones do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) Note One particular point is that we declare some axis as dynamic. If we were not doing that, the graph would only accept tensors with the exact same shape that the ones we are using to build it (the dummy data), so sequence length or batch size would be fixed. The name we have given to input and output fields will be reused in other tools. A complete conversion process in real life (including TensorRT engine step) looks like that:","title":"How ONNX conversion works?"},{"location":"onnx_convert/#convert-pytorch-model-to-onnx","text":"To ease optimization we need to convert our Pytorch model written in imperative code in a mostly static graph. Therefore, optimization tooling will be able to run static analysis and search for some pattern to optimize. The target graph format is ONNX . from https://onnx.ai/ ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators \u2014 the building blocks of machine learning and deep learning models \u2014 and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\u201d (https://onnx.ai/). The format has initially been created by Facebook and Microsoft to have a bridge between Pytorch (research) and Caffee2 (production). There are 2 ways to perform an export from Pytorch: tracing mode : send some (dummy) data to the model, and the tool will trace them inside the model, that way it will guess what the graph looks like; scripting : requires the models to be written in a certain way to work, its main advantage is that the dynamic logic is kept intact but adds many constraints in the way models are written. Attention Tracing mode is not magic, for instance it can\u2019t see operations you are doing in numpy (if any), the graph will be static, some if/else code is fixed forever, for loop will be unrolled, etc. Hugging Face and model authors took care that main/most models are tracing mode compatible. Following commented code performs the ONNX conversion: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from collections import OrderedDict import torch from torch.onnx import TrainingMode def convert_to_onnx ( model_pytorch , output_path : str , inputs_pytorch , opset : int = 12 ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. :param model_pytorch: Pytorch model :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param opset: version of ONNX protocol to use, usually 12, or 13 if you use per channel quantized model \"\"\" # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = opset , # the ONNX version to use, 13 if quantized model, 12 for not quantized ones do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) Note One particular point is that we declare some axis as dynamic. If we were not doing that, the graph would only accept tensors with the exact same shape that the ones we are using to build it (the dummy data), so sequence length or batch size would be fixed. The name we have given to input and output fields will be reused in other tools. A complete conversion process in real life (including TensorRT engine step) looks like that:","title":"Convert Pytorch model to ONNX"},{"location":"optimizations/","text":"Optimizing a model for inference # There are few ways to optimize a model for inference, some of them are basically a simplification of its graph: find and remove redundant operations : for instance dropout has no use outside the training loop, it can be removed without any impact on inference; perform constant folding : meaning find some parts of the graph made of constant expressions, and compute the results at compile time instead of runtime (similar to most programming language compiler); kernel fusion : to avoid 1/ loading time, 2/ share memory to avoid back and forth transfers with the global memory and 3/ use optimal implementation of a series of operations. Obviously, it will mainly benefit to memory bound operations (like multiply and add operations, a very common pattern in deep learning), it\u2019s called \u201ckernel fusion\u201d; Another orthogonal approach is to use lower precision tensors, it may be FP16 float number or INT-8 quantization. Attention Mixed precision and INT-8 quantization may have an accuracy cost. The reason is that you can't encode as much information in FP16 or INT-8 tensor that you can in FP32 tensor. Sometimes you do not have enough granularity, some other times the range is not big enough. When it happens, you need to modify the computation graph to keep some operators in full precision. This library does it for mixed precision (for most models) and provide you with a simple way to do it for INT-8 quantization","title":"Understanding model optimization"},{"location":"optimizations/#optimizing-a-model-for-inference","text":"There are few ways to optimize a model for inference, some of them are basically a simplification of its graph: find and remove redundant operations : for instance dropout has no use outside the training loop, it can be removed without any impact on inference; perform constant folding : meaning find some parts of the graph made of constant expressions, and compute the results at compile time instead of runtime (similar to most programming language compiler); kernel fusion : to avoid 1/ loading time, 2/ share memory to avoid back and forth transfers with the global memory and 3/ use optimal implementation of a series of operations. Obviously, it will mainly benefit to memory bound operations (like multiply and add operations, a very common pattern in deep learning), it\u2019s called \u201ckernel fusion\u201d; Another orthogonal approach is to use lower precision tensors, it may be FP16 float number or INT-8 quantization. Attention Mixed precision and INT-8 quantization may have an accuracy cost. The reason is that you can't encode as much information in FP16 or INT-8 tensor that you can in FP32 tensor. Sometimes you do not have enough granularity, some other times the range is not big enough. When it happens, you need to modify the computation graph to keep some operators in full precision. This library does it for mixed precision (for most models) and provide you with a simple way to do it for INT-8 quantization","title":"Optimizing a model for inference"},{"location":"python/","text":"TensorRT usage in Python script # There are 2 ways to use a TensorRT optimized model: deploy it on Triton server use it directly in Python This document is about the second option. High-level explanations # call load_engine() to parse an existing TensorRT engine or build_engine() to convert an ONNX file setup a CUDA stream (for async call), a TensorRT runtime and a context load your profile (s) call infer_tensorrt() Build engine # We assume that you have already prepared your ONNX file. Now we need to convert to TensorRT: import tensorrt as trt from tensorrt.tensorrt import Logger , Runtime from transformer_deploy.backends.trt_utils import build_engine trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 max_seq_len = 256 batch_size = 32 engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) Prepare inference # Now the engine is ready, we can prepare the inference: import torch from tensorrt.tensorrt import IExecutionContext from transformer_deploy.backends.trt_utils import get_binding_idxs context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] Inference # from transformer_deploy.backends.trt_utils import infer_tensorrt input_np = ... tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print ( tensorrt_output ) ... and you are done! \ud83c\udf89 Tip To go deeper, check in the API: Convert Backends/Trt utils ... and if you are looking for inspiration, check onnx-tensorrt","title":"Direct use TensorRT in Python script (no server)"},{"location":"python/#tensorrt-usage-in-python-script","text":"There are 2 ways to use a TensorRT optimized model: deploy it on Triton server use it directly in Python This document is about the second option.","title":"TensorRT usage in Python script"},{"location":"python/#high-level-explanations","text":"call load_engine() to parse an existing TensorRT engine or build_engine() to convert an ONNX file setup a CUDA stream (for async call), a TensorRT runtime and a context load your profile (s) call infer_tensorrt()","title":"High-level explanations"},{"location":"python/#build-engine","text":"We assume that you have already prepared your ONNX file. Now we need to convert to TensorRT: import tensorrt as trt from tensorrt.tensorrt import Logger , Runtime from transformer_deploy.backends.trt_utils import build_engine trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 max_seq_len = 256 batch_size = 32 engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , )","title":"Build engine"},{"location":"python/#prepare-inference","text":"Now the engine is ready, we can prepare the inference: import torch from tensorrt.tensorrt import IExecutionContext from transformer_deploy.backends.trt_utils import get_binding_idxs context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int]","title":"Prepare inference"},{"location":"python/#inference","text":"from transformer_deploy.backends.trt_utils import infer_tensorrt input_np = ... tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print ( tensorrt_output ) ... and you are done! \ud83c\udf89 Tip To go deeper, check in the API: Convert Backends/Trt utils ... and if you are looking for inspiration, check onnx-tensorrt","title":"Inference"},{"location":"run/","text":"Command line # Tip You can run commands below from Docker directly (no need to install Nvidia dependencies outside Docker one), like: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:latest \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend onnx \\ --seq-len 128 128 128\" With the single command below, you will: download the model and its tokenizer from Hugging Face hub, convert the model to ONNX graph, optimize the model with ONNX Runtime and save artefact ( model.onnx ), the model with TensorRT and save artefact ( model.plan ), benchmark each backend (including Pytorch), generate configuration files for Triton inference server convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend onnx --seq-len 128 128 128 --batch-size 1 32 32 # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=8.75ms, sd=0.30ms, min=8.60ms, max=11.20ms, median=8.68ms, 95p=9.15ms, 99p=10.77ms # [Pytorch (FP16)] mean=6.75ms, sd=0.22ms, min=6.66ms, max=8.99ms, median=6.71ms, 95p=6.88ms, 99p=7.95ms # [ONNX Runtime (FP32)] mean=8.10ms, sd=0.43ms, min=7.93ms, max=11.76ms, median=8.02ms, 95p=8.39ms, 99p=11.30ms # [ONNX Runtime (optimized)] mean=3.66ms, sd=0.23ms, min=3.57ms, max=6.46ms, median=3.62ms, 95p=3.70ms, 99p=4.95ms Info 128 128 128 -> minimum, optimal, maximum sequence length, to help TensorRT better optimize your model. Better to have the same value for seq len to get best performances from TensorRT ( ONNX Runtime has not this limitation). 1 32 32 -> batch size, same as above. Good idea to get 1 as minimum value. No impact on TensorRT performance. Launch Nvidia Triton inference server to play with both ONNX and TensorRT models: docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers sentencepiece && tritonserver --model-repository=/models\" As you can see, we install Transformers and then launch the server itself. This is, of course, a bad practice, you should make your own 2 lines Dockerfile with Transformers inside. Query the inference server # First you need to convert your strings to a binary file following the format of demo/infinity/query_body.bin . The Json part is straightforward, the second part a bit less. Please follow the code below for the recipe for a single string. If you have several strings, just concatenate the results. import struct text : str = \"This live event is great. I will sign-up for Infinity. \\n \" text_b : bytes = text . encode ( \"UTF-8\" ) print ( struct . pack ( \"<I\" , len ( text_b )) + text_b ) # <I means little-endian unsigned integers, followed by the number of elements Tip check Nvidia implementation from https://github.com/triton-inference-server/client/blob/530bcac5f1574aa2222930076200544eb274245c/src/python/library/tritonclient/utils/ init .py#L187 for more information. # https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md # @ means no data conversion (curl feature) curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" check demo folder to discover more performant ways to query the server from Python or elsewhere.","title":"Run (1 command)"},{"location":"run/#command-line","text":"Tip You can run commands below from Docker directly (no need to install Nvidia dependencies outside Docker one), like: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:latest \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend onnx \\ --seq-len 128 128 128\" With the single command below, you will: download the model and its tokenizer from Hugging Face hub, convert the model to ONNX graph, optimize the model with ONNX Runtime and save artefact ( model.onnx ), the model with TensorRT and save artefact ( model.plan ), benchmark each backend (including Pytorch), generate configuration files for Triton inference server convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend onnx --seq-len 128 128 128 --batch-size 1 32 32 # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=8.75ms, sd=0.30ms, min=8.60ms, max=11.20ms, median=8.68ms, 95p=9.15ms, 99p=10.77ms # [Pytorch (FP16)] mean=6.75ms, sd=0.22ms, min=6.66ms, max=8.99ms, median=6.71ms, 95p=6.88ms, 99p=7.95ms # [ONNX Runtime (FP32)] mean=8.10ms, sd=0.43ms, min=7.93ms, max=11.76ms, median=8.02ms, 95p=8.39ms, 99p=11.30ms # [ONNX Runtime (optimized)] mean=3.66ms, sd=0.23ms, min=3.57ms, max=6.46ms, median=3.62ms, 95p=3.70ms, 99p=4.95ms Info 128 128 128 -> minimum, optimal, maximum sequence length, to help TensorRT better optimize your model. Better to have the same value for seq len to get best performances from TensorRT ( ONNX Runtime has not this limitation). 1 32 32 -> batch size, same as above. Good idea to get 1 as minimum value. No impact on TensorRT performance. Launch Nvidia Triton inference server to play with both ONNX and TensorRT models: docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:22.01-py3 \\ bash -c \"pip install transformers sentencepiece && tritonserver --model-repository=/models\" As you can see, we install Transformers and then launch the server itself. This is, of course, a bad practice, you should make your own 2 lines Dockerfile with Transformers inside.","title":"Command line"},{"location":"run/#query-the-inference-server","text":"First you need to convert your strings to a binary file following the format of demo/infinity/query_body.bin . The Json part is straightforward, the second part a bit less. Please follow the code below for the recipe for a single string. If you have several strings, just concatenate the results. import struct text : str = \"This live event is great. I will sign-up for Infinity. \\n \" text_b : bytes = text . encode ( \"UTF-8\" ) print ( struct . pack ( \"<I\" , len ( text_b )) + text_b ) # <I means little-endian unsigned integers, followed by the number of elements Tip check Nvidia implementation from https://github.com/triton-inference-server/client/blob/530bcac5f1574aa2222930076200544eb274245c/src/python/library/tritonclient/utils/ init .py#L187 for more information. # https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md # @ means no data conversion (curl feature) curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/infinity/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" check demo folder to discover more performant ways to query the server from Python or elsewhere.","title":"Query the inference server"},{"location":"setup_local/","text":"Installation # Tip Use Docker if you don't want to install all Nvidia dependencies (for a first try for instance). In the long term, local install is probably a better idea. 6 rules to install locally Nvidia dependencies You may have heard or experienced difficulties in installing Nvidia dependencies, or making them detected by your system. If you are on Debian / Ubuntu, it should be easy . 1st rule : don't follow install guides found on reddit, blogs, etc. they are never up to date 2nd rule : don't follow install guides from Nvidia dependency manual, they are not always up to date 3rd rule : only follow install guides from Nvidia downlad pages , they are the only ones with updated instructions 4th rule : uninstall all your Nvidia dependencies not coming directly from a Nvidia repo (including the Ubuntu driver) and reinstall them from Nvidia repositories 5th rule : if your OS version is recent and not listed in compatible/tested OS of a dependency, just take the dependency tested latest OS version, it will work otherwise Twitter/forums would be full of complaints. 6th rule : choose the network .deb option when possible (meaning add a repo to get updates). Local .deb means manual update. The list of dependencies you will need to run this library locally: CUDA >= 11.4.x cuDNN 8.2 TensorRT 8.2.1 (GA) Optional, to run this library from Docker (so you don't have to install all other dependencies): nvidia-docker You may need to login with a free Nvidia account to download some dependencies. Then, it's the usual git clone: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy for CPU / GPU support: pip3 install \".[GPU]\" -f https://download.pytorch.org/whl/cu113/torch_stable.html --extra-index-url https://pypi.ngc.nvidia.com # if you want to perform GPU quantization (recommended): pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg = pytorch-quantization \\& subdirectory = tools/pytorch-quantization/ # if you want to accelerate dense embeddings extraction: pip install sentence-transformers for CPU only support: pip3 install \".[CPU]\" -f https://download.pytorch.org/whl/cpu/torch_stable.html # if you want to accelerate dence embeddings extraction: pip install sentence-transformers To build your own version of the Docker image: make docker_build","title":"Installation (local or Docker only)"},{"location":"setup_local/#installation","text":"Tip Use Docker if you don't want to install all Nvidia dependencies (for a first try for instance). In the long term, local install is probably a better idea. 6 rules to install locally Nvidia dependencies You may have heard or experienced difficulties in installing Nvidia dependencies, or making them detected by your system. If you are on Debian / Ubuntu, it should be easy . 1st rule : don't follow install guides found on reddit, blogs, etc. they are never up to date 2nd rule : don't follow install guides from Nvidia dependency manual, they are not always up to date 3rd rule : only follow install guides from Nvidia downlad pages , they are the only ones with updated instructions 4th rule : uninstall all your Nvidia dependencies not coming directly from a Nvidia repo (including the Ubuntu driver) and reinstall them from Nvidia repositories 5th rule : if your OS version is recent and not listed in compatible/tested OS of a dependency, just take the dependency tested latest OS version, it will work otherwise Twitter/forums would be full of complaints. 6th rule : choose the network .deb option when possible (meaning add a repo to get updates). Local .deb means manual update. The list of dependencies you will need to run this library locally: CUDA >= 11.4.x cuDNN 8.2 TensorRT 8.2.1 (GA) Optional, to run this library from Docker (so you don't have to install all other dependencies): nvidia-docker You may need to login with a free Nvidia account to download some dependencies. Then, it's the usual git clone: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy for CPU / GPU support: pip3 install \".[GPU]\" -f https://download.pytorch.org/whl/cu113/torch_stable.html --extra-index-url https://pypi.ngc.nvidia.com # if you want to perform GPU quantization (recommended): pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg = pytorch-quantization \\& subdirectory = tools/pytorch-quantization/ # if you want to accelerate dense embeddings extraction: pip install sentence-transformers for CPU only support: pip3 install \".[CPU]\" -f https://download.pytorch.org/whl/cpu/torch_stable.html # if you want to accelerate dence embeddings extraction: pip install sentence-transformers To build your own version of the Docker image: make docker_build","title":"Installation"},{"location":"t5/","text":"(function (global, factory) { typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() : typeof define === 'function' && define.amd ? define(factory) : (global = global || self, global.ClipboardCopyElement = factory()); }(this, function () { 'use strict'; function createNode(text) { const node = document.createElement('pre'); node.style.width = '1px'; node.style.height = '1px'; node.style.position = 'fixed'; node.style.top = '5px'; node.textContent = text; return node; } function copyNode(node) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(node.textContent); } const selection = getSelection(); if (selection == null) { return Promise.reject(new Error()); } selection.removeAllRanges(); const range = document.createRange(); range.selectNodeContents(node); selection.addRange(range); document.execCommand('copy'); selection.removeAllRanges(); return Promise.resolve(); } function copyText(text) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(text); } const body = document.body; if (!body) { return Promise.reject(new Error()); } const node = createNode(text); body.appendChild(node); copyNode(node); body.removeChild(node); return Promise.resolve(); } function copy(button) { const id = button.getAttribute('for'); const text = button.getAttribute('value'); function trigger() { button.dispatchEvent(new CustomEvent('clipboard-copy', { bubbles: true })); } if (text) { copyText(text).then(trigger); } else if (id) { const root = 'getRootNode' in Element.prototype ? button.getRootNode() : button.ownerDocument; if (!(root instanceof Document || 'ShadowRoot' in window && root instanceof ShadowRoot)) return; const node = root.getElementById(id); if (node) copyTarget(node).then(trigger); } } function copyTarget(content) { if (content instanceof HTMLInputElement || content instanceof HTMLTextAreaElement) { return copyText(content.value); } else if (content instanceof HTMLAnchorElement && content.hasAttribute('href')) { return copyText(content.href); } else { return copyNode(content); } } function clicked(event) { const button = event.currentTarget; if (button instanceof HTMLElement) { copy(button); } } function keydown(event) { if (event.key === ' ' || event.key === 'Enter') { const button = event.currentTarget; if (button instanceof HTMLElement) { event.preventDefault(); copy(button); } } } function focused(event) { event.currentTarget.addEventListener('keydown', keydown); } function blurred(event) { event.currentTarget.removeEventListener('keydown', keydown); } class ClipboardCopyElement extends HTMLElement { constructor() { super(); this.addEventListener('click', clicked); this.addEventListener('focus', focused); this.addEventListener('blur', blurred); } connectedCallback() { if (!this.hasAttribute('tabindex')) { this.setAttribute('tabindex', '0'); } if (!this.hasAttribute('role')) { this.setAttribute('role', 'button'); } } get value() { return this.getAttribute('value') || ''; } set value(text) { this.setAttribute('value', text); } } if (!window.customElements.get('clipboard-copy')) { window.ClipboardCopyElement = ClipboardCopyElement; window.customElements.define('clipboard-copy', ClipboardCopyElement); } return ClipboardCopyElement; })); document.addEventListener('clipboard-copy', function(event) { const notice = event.target.querySelector('.notice') notice.hidden = false setTimeout(function() { notice.hidden = true }, 1000) }) pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight-ipynb .hll { background-color: var(--jp-cell-editor-active-background) } .highlight-ipynb { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight-ipynb .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight-ipynb .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight-ipynb .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight-ipynb .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight-ipynb .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight-ipynb .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight-ipynb .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight-ipynb .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight-ipynb .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight-ipynb .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight-ipynb .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight-ipynb .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight-ipynb .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight-ipynb .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight-ipynb .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight-ipynb .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight-ipynb .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight-ipynb .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight-ipynb .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /* This file is taken from the built JupyterLab theme.css Found on share/nbconvert/templates/lab/static Some changes have been made and marked with CHANGE */ .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-900); --jp-brand-color1: var(--md-blue-700); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-900); --jp-accent-color1: var(--md-green-700); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-900); --jp-warn-color1: var(--md-orange-700); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-900); --jp-error-color1: var(--md-red-700); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-900); --jp-success-color1: var(--md-green-700); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-900); --jp-info-color1: var(--md-cyan-700); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } [data-md-color-scheme=\"slate\"] .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ /* The dark theme shadows need a bit of work, but this will probably also require work on the core layout * colors used in the theme as well. */ --jp-shadow-base-lightness: 32; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-700); --jp-border-color1: var(--md-grey-700); --jp-border-color2: var(--md-grey-800); --jp-border-color3: var(--md-grey-900); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(255, 255, 255, 1); --jp-ui-font-color1: rgba(255, 255, 255, 0.87); --jp-ui-font-color2: rgba(255, 255, 255, 0.54); --jp-ui-font-color3: rgba(255, 255, 255, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(0, 0, 0, 1); --jp-ui-inverse-font-color1: rgba(0, 0, 0, 0.8); --jp-ui-inverse-font-color2: rgba(0, 0, 0, 0.5); --jp-ui-inverse-font-color3: rgba(0, 0, 0, 0.3); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(255, 255, 255, 1); --jp-content-font-color1: rgba(255, 255, 255, 1); --jp-content-font-color2: rgba(255, 255, 255, 0.7); --jp-content-font-color3: rgba(255, 255, 255, 0.5); --jp-content-link-color: var(--md-blue-300); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: #111111; --jp-layout-color1: var(--md-grey-900); --jp-layout-color2: var(--md-grey-800); --jp-layout-color3: var(--md-grey-700); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: white; --jp-inverse-layout-color1: white; --jp-inverse-layout-color2: var(--md-grey-200); --jp-inverse-layout-color3: var(--md-grey-400); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--jp-layout-color1); --jp-cell-editor-border-color: var(--md-grey-700); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 1; --jp-cell-prompt-not-active-font-color: var(--md-grey-300); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: rgba(33, 150, 243, 0.24); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: rgba(244, 67, 54, 0.28); --jp-rendermime-table-row-background: var(--md-grey-900); --jp-rendermime-table-row-hover-background: rgba(3, 169, 244, 0.2); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.6); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color2); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.8); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--jp-layout-color0); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color0); --jp-input-hover-background: var(--jp-layout-color2); --jp-input-background: var(--md-grey-800); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: var(--jp-layout-color2); --jp-editor-selected-focused-background: rgba(33, 150, 243, 0.24); --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: var(--md-green-500); --jp-mirror-editor-atom-color: var(--md-blue-300); --jp-mirror-editor-number-color: var(--md-green-400); --jp-mirror-editor-def-color: var(--md-blue-600); --jp-mirror-editor-variable-color: var(--md-grey-300); --jp-mirror-editor-variable-2-color: var(--md-blue-400); --jp-mirror-editor-variable-3-color: var(--md-green-600); --jp-mirror-editor-punctuation-color: var(--md-blue-400); --jp-mirror-editor-property-color: var(--md-blue-400); --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ff7070; --jp-mirror-editor-string-2-color: var(--md-purple-300); --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: var(--md-green-600); --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: var(--md-green-700); --jp-mirror-editor-attribute-color: var(--md-blue-700); --jp-mirror-editor-header-color: var(--md-blue-500); --jp-mirror-editor-quote-color: var(--md-green-300); --jp-mirror-editor-link-color: var(--md-blue-700); --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: var(--md-grey-400); /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.6; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(255, 225, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* scrollbar related styles. Supports every browser except Edge. */ /* colors based on JetBrain's Darcula theme */ --jp-scrollbar-background-color: #3f4244; --jp-scrollbar-thumb-color: 88, 96, 97; /* need to specify thumb color as an RGB triplet */ --jp-scrollbar-endpad: 3px; /* the minimum gap between the thumb and the ends of a scrollbar */ /* hacks for setting the thumb shape. These do nothing in Firefox */ --jp-scrollbar-thumb-margin: 3.5px; /* the space in between the sides of the thumb and the track */ --jp-scrollbar-thumb-radius: 9px; /* set to a large-ish value for rounded endcaps on the thumb */ /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper{/*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. *//*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */}.jupyter-wrapper [data-jp-theme-scrollbars=true]{scrollbar-color:rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar{scrollbar-color:rgba(var(--jp-scrollbar-thumb-color), 0.5) rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-corner{background:var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-thumb{background:rgb(var(--jp-scrollbar-thumb-color));border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-right:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-bottom:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-corner,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-corner{background-color:rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb{background:rgba(var(--jp-scrollbar-thumb-color), 0.5);border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-right:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-bottom:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{min-height:16px;max-height:16px;min-width:45px;border-top:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{min-width:16px;max-width:16px;min-height:45px;border-left:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar-button{background-color:#f0f0f0;background-position:center center;min-height:15px;max-height:15px;min-width:15px;max-width:15px}.jupyter-wrapper .lm-ScrollBar-button:hover{background-color:#dadada}.jupyter-wrapper .lm-ScrollBar-button.lm-mod-active{background-color:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-track{background:#f0f0f0}.jupyter-wrapper .lm-ScrollBar-thumb{background:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-thumb:hover{background:#bababa}.jupyter-wrapper .lm-ScrollBar-thumb.lm-mod-active{background:#a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-thumb{height:100%;min-width:15px;border-left:1px solid #a0a0a0;border-right:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-thumb{width:100%;min-height:15px;border-top:1px solid #a0a0a0;border-bottom:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-left);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-right);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-up);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-down);background-size:17px}.jupyter-wrapper .p-Widget,.jupyter-wrapper .lm-Widget{box-sizing:border-box;position:relative;overflow:hidden;cursor:default}.jupyter-wrapper .p-Widget.p-mod-hidden,.jupyter-wrapper .lm-Widget.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-CommandPalette,.jupyter-wrapper .lm-CommandPalette{display:flex;flex-direction:column;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-CommandPalette-search,.jupyter-wrapper .lm-CommandPalette-search{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-content,.jupyter-wrapper .lm-CommandPalette-content{flex:1 1 auto;margin:0;padding:0;min-height:0;overflow:auto;list-style-type:none}.jupyter-wrapper .p-CommandPalette-header,.jupyter-wrapper .lm-CommandPalette-header{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-CommandPalette-item,.jupyter-wrapper .lm-CommandPalette-item{display:flex;flex-direction:row}.jupyter-wrapper .p-CommandPalette-itemIcon,.jupyter-wrapper .lm-CommandPalette-itemIcon{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemContent,.jupyter-wrapper .lm-CommandPalette-itemContent{flex:1 1 auto;overflow:hidden}.jupyter-wrapper .p-CommandPalette-itemShortcut,.jupyter-wrapper .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemLabel,.jupyter-wrapper .lm-CommandPalette-itemLabel{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-DockPanel,.jupyter-wrapper .lm-DockPanel{z-index:0}.jupyter-wrapper .p-DockPanel-widget,.jupyter-wrapper .lm-DockPanel-widget{z-index:0}.jupyter-wrapper .p-DockPanel-tabBar,.jupyter-wrapper .lm-DockPanel-tabBar{z-index:1}.jupyter-wrapper .p-DockPanel-handle,.jupyter-wrapper .lm-DockPanel-handle{z-index:2}.jupyter-wrapper .p-DockPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-DockPanel-handle:after,.jupyter-wrapper .lm-DockPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]{cursor:ew-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]{cursor:ns-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-DockPanel-overlay,.jupyter-wrapper .lm-DockPanel-overlay{z-index:3;box-sizing:border-box;pointer-events:none}.jupyter-wrapper .p-DockPanel-overlay.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-overlay.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-Menu,.jupyter-wrapper .lm-Menu{z-index:10000;position:absolute;white-space:nowrap;overflow-x:hidden;overflow-y:auto;outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-Menu-content,.jupyter-wrapper .lm-Menu-content{margin:0;padding:0;display:table;list-style-type:none}.jupyter-wrapper .p-Menu-item,.jupyter-wrapper .lm-Menu-item{display:table-row}.jupyter-wrapper .p-Menu-item.p-mod-hidden,.jupyter-wrapper .p-Menu-item.p-mod-collapsed,.jupyter-wrapper .lm-Menu-item.lm-mod-hidden,.jupyter-wrapper .lm-Menu-item.lm-mod-collapsed{display:none !important}.jupyter-wrapper .p-Menu-itemIcon,.jupyter-wrapper .p-Menu-itemSubmenuIcon,.jupyter-wrapper .lm-Menu-itemIcon,.jupyter-wrapper .lm-Menu-itemSubmenuIcon{display:table-cell;text-align:center}.jupyter-wrapper .p-Menu-itemLabel,.jupyter-wrapper .lm-Menu-itemLabel{display:table-cell;text-align:left}.jupyter-wrapper .p-Menu-itemShortcut,.jupyter-wrapper .lm-Menu-itemShortcut{display:table-cell;text-align:right}.jupyter-wrapper .p-MenuBar,.jupyter-wrapper .lm-MenuBar{outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-MenuBar-content,.jupyter-wrapper .lm-MenuBar-content{margin:0;padding:0;display:flex;flex-direction:row;list-style-type:none}.jupyter-wrapper .p--MenuBar-item,.jupyter-wrapper .lm-MenuBar-item{box-sizing:border-box}.jupyter-wrapper .p-MenuBar-itemIcon,.jupyter-wrapper .p-MenuBar-itemLabel,.jupyter-wrapper .lm-MenuBar-itemIcon,.jupyter-wrapper .lm-MenuBar-itemLabel{display:inline-block}.jupyter-wrapper .p-ScrollBar,.jupyter-wrapper .lm-ScrollBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-ScrollBar[data-orientation=horizontal],.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-ScrollBar[data-orientation=vertical],.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-ScrollBar-button,.jupyter-wrapper .lm-ScrollBar-button{box-sizing:border-box;flex:0 0 auto}.jupyter-wrapper .p-ScrollBar-track,.jupyter-wrapper .lm-ScrollBar-track{box-sizing:border-box;position:relative;overflow:hidden;flex:1 1 auto}.jupyter-wrapper .p-ScrollBar-thumb,.jupyter-wrapper .lm-ScrollBar-thumb{box-sizing:border-box;position:absolute}.jupyter-wrapper .p-SplitPanel-child,.jupyter-wrapper .lm-SplitPanel-child{z-index:0}.jupyter-wrapper .p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel-handle{z-index:1}.jupyter-wrapper .p-SplitPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-SplitPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle{cursor:ew-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle{cursor:ns-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-TabBar,.jupyter-wrapper .lm-TabBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal],.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical],.jupyter-wrapper .lm-TabBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-TabBar-content,.jupyter-wrapper .lm-TabBar-content{margin:0;padding:0;display:flex;flex:1 1 auto;list-style-type:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]>.lm-TabBar-content{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=vertical]>.lm-TabBar-content{flex-direction:column}.jupyter-wrapper .p-TabBar-tab,.jupyter-wrapper .lm-TabBar-tab{display:flex;flex-direction:row;box-sizing:border-box;overflow:hidden}.jupyter-wrapper .p-TabBar-tabIcon,.jupyter-wrapper .p-TabBar-tabCloseIcon,.jupyter-wrapper .lm-TabBar-tabIcon,.jupyter-wrapper .lm-TabBar-tabCloseIcon{flex:0 0 auto}.jupyter-wrapper .p-TabBar-tabLabel,.jupyter-wrapper .lm-TabBar-tabLabel{flex:1 1 auto;overflow:hidden;white-space:nowrap}.jupyter-wrapper .p-TabBar-tab.p-mod-hidden,.jupyter-wrapper .lm-TabBar-tab.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging .lm-TabBar-tab{position:relative}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=horizontal] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=horizontal] .lm-TabBar-tab{left:0;transition:left 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=vertical] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=vertical] .lm-TabBar-tab{top:0;transition:top 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging{transition:none}.jupyter-wrapper .p-TabPanel-tabBar,.jupyter-wrapper .lm-TabPanel-tabBar{z-index:1}.jupyter-wrapper .p-TabPanel-stackedPanel,.jupyter-wrapper .lm-TabPanel-stackedPanel{z-index:0}.jupyter-wrapper ::-moz-selection{background:rgba(125,188,255,.6)}.jupyter-wrapper ::selection{background:rgba(125,188,255,.6)}.jupyter-wrapper .bp3-heading{color:#182026;font-weight:600;margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-dark .bp3-heading{color:#f5f8fa}.jupyter-wrapper h1.bp3-heading,.jupyter-wrapper .bp3-running-text h1{line-height:40px;font-size:36px}.jupyter-wrapper h2.bp3-heading,.jupyter-wrapper .bp3-running-text h2{line-height:32px;font-size:28px}.jupyter-wrapper h3.bp3-heading,.jupyter-wrapper .bp3-running-text h3{line-height:25px;font-size:22px}.jupyter-wrapper h4.bp3-heading,.jupyter-wrapper .bp3-running-text h4{line-height:21px;font-size:18px}.jupyter-wrapper h5.bp3-heading,.jupyter-wrapper .bp3-running-text h5{line-height:19px;font-size:16px}.jupyter-wrapper h6.bp3-heading,.jupyter-wrapper .bp3-running-text h6{line-height:16px;font-size:14px}.jupyter-wrapper .bp3-ui-text{text-transform:none;line-height:1.28581;letter-spacing:0;font-size:14px;font-weight:400}.jupyter-wrapper .bp3-monospace-text{text-transform:none;font-family:monospace}.jupyter-wrapper .bp3-text-muted{color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-text-muted{color:#a7b6c2}.jupyter-wrapper .bp3-text-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-text-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal}.jupyter-wrapper .bp3-running-text{line-height:1.5;font-size:14px}.jupyter-wrapper .bp3-running-text h1{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h1{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h2{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h2{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h3{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h3{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h4{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h4{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h5{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h5{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h6{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h6{color:#f5f8fa}.jupyter-wrapper .bp3-running-text hr{margin:20px 0;border:none;border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text hr{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-running-text p{margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-text-large{font-size:16px}.jupyter-wrapper .bp3-text-small{font-size:12px}.jupyter-wrapper a{text-decoration:none;color:#106ba3}.jupyter-wrapper a:hover{cursor:pointer;text-decoration:underline;color:#106ba3}.jupyter-wrapper a .bp3-icon,.jupyter-wrapper a .bp3-icon-standard,.jupyter-wrapper a .bp3-icon-large{color:inherit}.jupyter-wrapper a code,.jupyter-wrapper .bp3-dark a code{color:inherit}.jupyter-wrapper .bp3-dark a,.jupyter-wrapper .bp3-dark a:hover{color:#48aff0}.jupyter-wrapper .bp3-dark a .bp3-icon,.jupyter-wrapper .bp3-dark a .bp3-icon-standard,.jupyter-wrapper .bp3-dark a .bp3-icon-large,.jupyter-wrapper .bp3-dark a:hover .bp3-icon,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-standard,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-large{color:inherit}.jupyter-wrapper .bp3-running-text code,.jupyter-wrapper .bp3-code{text-transform:none;font-family:monospace;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);background:rgba(255,255,255,.7);padding:2px 5px;color:#5c7080;font-size:smaller}.jupyter-wrapper .bp3-dark .bp3-running-text code,.jupyter-wrapper .bp3-running-text .bp3-dark code,.jupyter-wrapper .bp3-dark .bp3-code{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#a7b6c2}.jupyter-wrapper .bp3-running-text a>code,.jupyter-wrapper a>.bp3-code{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-running-text a>code,.jupyter-wrapper .bp3-running-text .bp3-dark a>code,.jupyter-wrapper .bp3-dark a>.bp3-code{color:inherit}.jupyter-wrapper .bp3-running-text pre,.jupyter-wrapper .bp3-code-block{text-transform:none;font-family:monospace;display:block;margin:10px 0;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);background:rgba(255,255,255,.7);padding:13px 15px 12px;line-height:1.4;color:#182026;font-size:13px;word-break:break-all;word-wrap:break-word}.jupyter-wrapper .bp3-dark .bp3-running-text pre,.jupyter-wrapper .bp3-running-text .bp3-dark pre,.jupyter-wrapper .bp3-dark .bp3-code-block{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-running-text pre>code,.jupyter-wrapper .bp3-code-block>code{-webkit-box-shadow:none;box-shadow:none;background:none;padding:0;color:inherit;font-size:inherit}.jupyter-wrapper .bp3-running-text kbd,.jupyter-wrapper .bp3-key{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background:#fff;min-width:24px;height:24px;padding:3px 6px;vertical-align:middle;line-height:24px;color:#5c7080;font-family:inherit;font-size:12px}.jupyter-wrapper .bp3-running-text kbd .bp3-icon,.jupyter-wrapper .bp3-key .bp3-icon,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-standard,.jupyter-wrapper .bp3-key .bp3-icon-standard,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-large,.jupyter-wrapper .bp3-key .bp3-icon-large{margin-right:5px}.jupyter-wrapper .bp3-dark .bp3-running-text kbd,.jupyter-wrapper .bp3-running-text .bp3-dark kbd,.jupyter-wrapper .bp3-dark .bp3-key{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);background:#394b59;color:#a7b6c2}.jupyter-wrapper .bp3-running-text blockquote,.jupyter-wrapper .bp3-blockquote{margin:0 0 10px;border-left:solid 4px rgba(167,182,194,.5);padding:0 20px}.jupyter-wrapper .bp3-dark .bp3-running-text blockquote,.jupyter-wrapper .bp3-running-text .bp3-dark blockquote,.jupyter-wrapper .bp3-dark .bp3-blockquote{border-color:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-running-text ul,.jupyter-wrapper .bp3-running-text ol,.jupyter-wrapper .bp3-list{margin:10px 0;padding-left:30px}.jupyter-wrapper .bp3-running-text ul li:not(:last-child),.jupyter-wrapper .bp3-running-text ol li:not(:last-child),.jupyter-wrapper .bp3-list li:not(:last-child){margin-bottom:5px}.jupyter-wrapper .bp3-running-text ul ol,.jupyter-wrapper .bp3-running-text ol ol,.jupyter-wrapper .bp3-list ol,.jupyter-wrapper .bp3-running-text ul ul,.jupyter-wrapper .bp3-running-text ol ul,.jupyter-wrapper .bp3-list ul{margin-top:5px}.jupyter-wrapper .bp3-list-unstyled{margin:0;padding:0;list-style:none}.jupyter-wrapper .bp3-list-unstyled li{padding:0}.jupyter-wrapper .bp3-rtl{text-align:right}.jupyter-wrapper .bp3-dark{color:#f5f8fa}.jupyter-wrapper :focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-focus-disabled :focus{outline:none !important}.jupyter-wrapper .bp3-focus-disabled :focus~.bp3-control-indicator{outline:none !important}.jupyter-wrapper .bp3-alert{max-width:400px;padding:20px}.jupyter-wrapper .bp3-alert-body{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-alert-body .bp3-icon{margin-top:0;margin-right:20px;font-size:40px}.jupyter-wrapper .bp3-alert-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;margin-top:10px}.jupyter-wrapper .bp3-alert-footer .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-breadcrumbs{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;cursor:default;height:30px;padding:0;list-style:none}.jupyter-wrapper .bp3-breadcrumbs>li{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-breadcrumbs>li::after{display:block;margin:0 5px;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\");width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs>li:last-of-type::after{display:none}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumb-current,.jupyter-wrapper .bp3-breadcrumbs-collapsed{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumbs-collapsed{color:#5c7080}.jupyter-wrapper .bp3-breadcrumb:hover{text-decoration:none}.jupyter-wrapper .bp3-breadcrumb.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-breadcrumb .bp3-icon{margin-right:5px}.jupyter-wrapper .bp3-breadcrumb-current{color:inherit;font-weight:600}.jupyter-wrapper .bp3-breadcrumb-current .bp3-input{vertical-align:baseline;font-size:inherit;font-weight:inherit}.jupyter-wrapper .bp3-breadcrumbs-collapsed{margin-right:2px;border:none;border-radius:3px;background:#ced9e0;cursor:pointer;padding:1px 5px;vertical-align:text-bottom}.jupyter-wrapper .bp3-breadcrumbs-collapsed::before{display:block;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat;width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs-collapsed:hover{background:#bfccd6;text-decoration:none;color:#182026}.jupyter-wrapper .bp3-dark .bp3-breadcrumb,.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs>li::after{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumb.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-breadcrumb-current{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed:hover{background:rgba(16,22,26,.6);color:#f5f8fa}.jupyter-wrapper .bp3-button{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;min-width:30px;min-height:30px}.jupyter-wrapper .bp3-button>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-button>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-button::before,.jupyter-wrapper .bp3-button>*{margin-right:7px}.jupyter-wrapper .bp3-button:empty::before,.jupyter-wrapper .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button:empty{padding:0 !important}.jupyter-wrapper .bp3-button:disabled,.jupyter-wrapper .bp3-button.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-button.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button.bp3-align-right,.jupyter-wrapper .bp3-align-right .bp3-button{text-align:right}.jupyter-wrapper .bp3-button.bp3-align-left,.jupyter-wrapper .bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active:hover,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-button.bp3-intent-primary{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-success{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0f9960;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0d8050}.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0a6640;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(15,153,96,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-warning{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#d9822b;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#bf7326}.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a66321;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(217,130,43,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-danger{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#db3737;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#c23030}.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a82a2a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(219,55,55,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#fff}.jupyter-wrapper .bp3-button.bp3-large,.jupyter-wrapper .bp3-large .bp3-button{min-width:40px;min-height:40px;padding:5px 15px;font-size:16px}.jupyter-wrapper .bp3-button.bp3-large::before,.jupyter-wrapper .bp3-button.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-button::before,.jupyter-wrapper .bp3-large .bp3-button>*{margin-right:10px}.jupyter-wrapper .bp3-button.bp3-large:empty::before,.jupyter-wrapper .bp3-button.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-button:empty::before,.jupyter-wrapper .bp3-large .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button.bp3-small,.jupyter-wrapper .bp3-small .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-button.bp3-loading{position:relative}.jupyter-wrapper .bp3-button.bp3-loading[class*=bp3-icon-]::before{visibility:hidden}.jupyter-wrapper .bp3-button.bp3-loading .bp3-button-spinner{position:absolute;margin:0}.jupyter-wrapper .bp3-button.bp3-loading>:not(.bp3-button-spinner){visibility:hidden}.jupyter-wrapper .bp3-button[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon,.jupyter-wrapper .bp3-button .bp3-icon-standard,.jupyter-wrapper .bp3-button .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-standard.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-large.bp3-align-right{margin-left:7px}.jupyter-wrapper .bp3-button .bp3-icon:first-child:last-child,.jupyter-wrapper .bp3-button .bp3-spinner+.bp3-icon:last-child{margin:0 -7px}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-])[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-image:none;color:rgba(255,255,255,.3)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-button:disabled::before,.jupyter-wrapper .bp3-button:disabled .bp3-icon,.jupyter-wrapper .bp3-button:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button:disabled .bp3-icon-large,.jupyter-wrapper .bp3-button.bp3-disabled::before,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-large,.jupyter-wrapper .bp3-button[class*=bp3-intent-]::before,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-standard,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-large{color:inherit !important}.jupyter-wrapper .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button.bp3-minimal:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper a.bp3-button{text-align:center;text-decoration:none;-webkit-transition:none;transition:none}.jupyter-wrapper a.bp3-button,.jupyter-wrapper a.bp3-button:hover,.jupyter-wrapper a.bp3-button:active{color:#182026}.jupyter-wrapper a.bp3-button.bp3-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-text{-webkit-box-flex:0;-ms-flex:0 1 auto;flex:0 1 auto}.jupyter-wrapper .bp3-button.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button.bp3-align-right .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-right .bp3-button-text{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.jupyter-wrapper .bp3-button-group .bp3-button{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;z-index:4}.jupyter-wrapper .bp3-button-group .bp3-button:focus{z-index:5}.jupyter-wrapper .bp3-button-group .bp3-button:hover{z-index:6}.jupyter-wrapper .bp3-button-group .bp3-button:active,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-active{z-index:7}.jupyter-wrapper .bp3-button-group .bp3-button:disabled,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]{z-index:9}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:focus{z-index:10}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:hover{z-index:11}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-active{z-index:12}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:first-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:-1px;border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-button-group .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button-group .bp3-button.bp3-fill,.jupyter-wrapper .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;vertical-align:top}.jupyter-wrapper .bp3-button-group.bp3-vertical.bp3-fill{width:unset;height:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical .bp3-button{margin-right:0 !important;width:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:first-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:first-child{border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:last-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-bottom:-1px}.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:1px}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-button:not(:last-child){margin-bottom:1px}.jupyter-wrapper .bp3-callout{line-height:1.5;font-size:14px;position:relative;border-radius:3px;background-color:rgba(138,155,168,.15);width:100%;padding:10px 12px 9px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]{padding-left:40px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout.bp3-callout-icon{padding-left:40px}.jupyter-wrapper .bp3-callout.bp3-callout-icon>.bp3-icon:first-child{position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout .bp3-heading{margin-top:0;margin-bottom:5px;line-height:20px}.jupyter-wrapper .bp3-callout .bp3-heading:last-child{margin-bottom:0}.jupyter-wrapper .bp3-dark .bp3-callout{background-color:rgba(138,155,168,.2)}.jupyter-wrapper .bp3-dark .bp3-callout[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-primary .bp3-heading{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{color:#48aff0}.jupyter-wrapper .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-success .bp3-heading{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{color:#3dcc91}.jupyter-wrapper .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-warning .bp3-heading{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{color:#ffb366}.jupyter-wrapper .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-danger .bp3-heading{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{color:#ff7373}.jupyter-wrapper .bp3-running-text .bp3-callout{margin:20px 0}.jupyter-wrapper .bp3-card{border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#fff;padding:20px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-card.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#30404d}.jupyter-wrapper .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-0.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-1.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-2.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-3.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-4.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);cursor:pointer}.jupyter-wrapper .bp3-card.bp3-interactive:hover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:active{opacity:.9;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);-webkit-transition-duration:0;transition-duration:0}.jupyter-wrapper .bp3-card.bp3-interactive:active.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-collapse{height:0;overflow-y:hidden;-webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body{-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-context-menu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-context-menu-popover-target{position:fixed}.jupyter-wrapper .bp3-divider{margin:5px;border-right:1px solid rgba(16,22,26,.15);border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-divider{border-color:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dialog-container{opacity:1;-webkit-transform:scale(1);transform:scale(1);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;min-height:100%;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter-active>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear-active>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit-active>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:30px 0;border-radius:6px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#ebf1f5;width:500px;padding-bottom:20px;pointer-events:all;-webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text}.jupyter-wrapper .bp3-dialog:focus{outline:0}.jupyter-wrapper .bp3-dialog.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-dialog{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#293742;color:#f5f8fa}.jupyter-wrapper .bp3-dialog-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:6px 6px 0 0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);background:#fff;min-height:40px;padding-right:5px;padding-left:20px}.jupyter-wrapper .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dialog-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-dialog-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-dialog-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-dialog-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4);background:#30404d}.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dialog-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:20px;line-height:18px}.jupyter-wrapper .bp3-dialog-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin:0 20px}.jupyter-wrapper .bp3-dialog-footer-actions{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end}.jupyter-wrapper .bp3-dialog-footer-actions .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-drawer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#fff;padding:0}.jupyter-wrapper .bp3-drawer:focus{outline:0}.jupyter-wrapper .bp3-drawer.bp3-position-top{top:0;right:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{-webkit-transform:translateY(-100%);transform:translateY(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left{top:0;bottom:0;left:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear{-webkit-transform:translateX(-100%);transform:translateX(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{-webkit-transform:translateX(-100%);transform:translateX(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right{top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical){top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-drawer{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-drawer-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border-radius:0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);min-height:40px;padding:5px;padding-left:20px}.jupyter-wrapper .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-drawer-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-drawer-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-drawer-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-drawer-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-drawer-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;overflow:auto;line-height:18px}.jupyter-wrapper .bp3-drawer-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);padding:10px 20px}.jupyter-wrapper .bp3-dark .bp3-drawer-footer{-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.4);box-shadow:inset 0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text{display:inline-block;position:relative;cursor:text;max-width:100%;vertical-align:top;white-space:nowrap}.jupyter-wrapper .bp3-editable-text::before{position:absolute;top:-3px;right:-3px;bottom:-3px;left:-3px;border-radius:3px;content:\"\";-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#137cbd}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#0f9960}.jupyter-wrapper .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#d9822b}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#db3737}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4);box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4);box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4);box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4);box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text-content{display:inherit;position:relative;min-width:inherit;max-width:inherit;vertical-align:top;text-transform:inherit;letter-spacing:inherit;color:inherit;font:inherit;resize:none}.jupyter-wrapper .bp3-editable-text-input{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;width:100%;padding:0;white-space:pre-wrap}.jupyter-wrapper .bp3-editable-text-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:focus{outline:none}.jupyter-wrapper .bp3-editable-text-input::-ms-clear{display:none}.jupyter-wrapper .bp3-editable-text-content{overflow:hidden;padding-right:2px;text-overflow:ellipsis;white-space:pre}.jupyter-wrapper .bp3-editable-text-editing>.bp3-editable-text-content{position:absolute;left:0;visibility:hidden}.jupyter-wrapper .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-editable-text.bp3-multiline{display:block}.jupyter-wrapper .bp3-editable-text.bp3-multiline .bp3-editable-text-content{overflow:auto;white-space:pre-wrap;word-wrap:break-word}.jupyter-wrapper .bp3-control-group{-webkit-transform:translateZ(0);transform:translateZ(0);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-control-group>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select,.jupyter-wrapper .bp3-control-group .bp3-input,.jupyter-wrapper .bp3-control-group .bp3-select{position:relative}.jupyter-wrapper .bp3-control-group .bp3-input{z-index:2;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-input:focus{z-index:14;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-input[readonly],.jupyter-wrapper .bp3-control-group .bp3-input:disabled,.jupyter-wrapper .bp3-control-group .bp3-input.bp3-disabled{z-index:1}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select select,.jupyter-wrapper .bp3-control-group .bp3-select select{-webkit-transform:translateZ(0);transform:translateZ(0);z-index:4;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-button:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select:focus,.jupyter-wrapper .bp3-control-group .bp3-select select:focus{z-index:5}.jupyter-wrapper .bp3-control-group .bp3-button:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select:hover,.jupyter-wrapper .bp3-control-group .bp3-select select:hover{z-index:6}.jupyter-wrapper .bp3-control-group .bp3-button:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select:active,.jupyter-wrapper .bp3-control-group .bp3-select select:active{z-index:7}.jupyter-wrapper .bp3-control-group .bp3-button[readonly],.jupyter-wrapper .bp3-control-group .bp3-button:disabled,.jupyter-wrapper .bp3-control-group .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]{z-index:9}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:focus{z-index:10}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:hover{z-index:11}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:active{z-index:12}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-input-action{z-index:16}.jupyter-wrapper .bp3-control-group .bp3-select::after,.jupyter-wrapper .bp3-control-group .bp3-html-select::after,.jupyter-wrapper .bp3-control-group .bp3-select>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-html-select>.bp3-icon{z-index:17}.jupyter-wrapper .bp3-control-group:not(.bp3-vertical)>*{margin-right:-1px}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>*{margin-right:0}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>.bp3-button+.bp3-button{margin-left:1px}.jupyter-wrapper .bp3-control-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-control-group .bp3-popover-target{border-radius:inherit}.jupyter-wrapper .bp3-control-group>:first-child{border-radius:3px 0 0 3px}.jupyter-wrapper .bp3-control-group>:last-child{margin-right:0;border-radius:0 3px 3px 0}.jupyter-wrapper .bp3-control-group>:only-child{margin-right:0;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input-group .bp3-button{border-radius:3px}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-fill>*:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.jupyter-wrapper .bp3-control-group.bp3-vertical>*{margin-top:-1px}.jupyter-wrapper .bp3-control-group.bp3-vertical>:first-child{margin-top:0;border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-control-group.bp3-vertical>:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-control{display:block;position:relative;margin-bottom:10px;cursor:pointer;text-transform:none}.jupyter-wrapper .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control:not(.bp3-align-right){padding-left:26px}.jupyter-wrapper .bp3-control:not(.bp3-align-right) .bp3-control-indicator{margin-left:-26px}.jupyter-wrapper .bp3-control.bp3-align-right{padding-right:26px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{margin-right:-26px}.jupyter-wrapper .bp3-control.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-control.bp3-inline{display:inline-block;margin-right:20px}.jupyter-wrapper .bp3-control input{position:absolute;top:0;left:0;opacity:0;z-index:-1}.jupyter-wrapper .bp3-control .bp3-control-indicator{display:inline-block;position:relative;margin-top:-3px;margin-right:10px;border:none;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));cursor:pointer;width:1em;height:1em;vertical-align:middle;font-size:16px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-control .bp3-control-indicator::before{display:block;width:1em;height:1em;content:\"\"}.jupyter-wrapper .bp3-control:hover .bp3-control-indicator{background-color:#ebf1f5}.jupyter-wrapper .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background:#d8e1e8}.jupyter-wrapper .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed}.jupyter-wrapper .bp3-control input:focus~.bp3-control-indicator{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{float:right;margin-top:1px;margin-left:10px}.jupyter-wrapper .bp3-control.bp3-large{font-size:16px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right){padding-left:30px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right{padding-right:30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-30px}.jupyter-wrapper .bp3-control.bp3-large .bp3-control-indicator{font-size:20px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-top:0}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control.bp3-checkbox .bp3-control-indicator{border-radius:3px}.jupyter-wrapper .bp3-control.bp3-checkbox input:checked~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-radio .bp3-control-indicator{border-radius:50%}.jupyter-wrapper .bp3-control.bp3-radio input:checked~.bp3-control-indicator::before{background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%)}.jupyter-wrapper .bp3-control.bp3-radio input:checked:disabled~.bp3-control-indicator::before{opacity:.5}.jupyter-wrapper .bp3-control.bp3-radio input:focus~.bp3-control-indicator{-moz-outline-radius:16px}.jupyter-wrapper .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(167,182,194,.5)}.jupyter-wrapper .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(92,112,128,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(206,217,224,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right){padding-left:38px}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{margin-left:-38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right{padding-right:38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{margin-right:-38px}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator{border:none;border-radius:1.75em;-webkit-box-shadow:none !important;box-shadow:none !important;width:auto;min-width:1.75em;-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator::before{position:absolute;left:0;margin:2px;border-radius:50%;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);background:#fff;width:calc(1em - 4px);height:calc(1em - 4px);-webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{left:calc(100% - 1em)}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){padding-left:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right{padding-right:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-45px}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(16,22,26,.7)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(16,22,26,.9)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(57,75,89,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background:#394b59}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-switch-inner-text{text-align:center;font-size:.7em}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{visibility:hidden;margin-right:1.2em;margin-left:.5em;line-height:0}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{visibility:visible;margin-right:.5em;margin-left:1.2em;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:first-child{visibility:visible;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:last-child{visibility:hidden;line-height:0}.jupyter-wrapper .bp3-dark .bp3-control{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-control.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-control .bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0))}.jupyter-wrapper .bp3-dark .bp3-control:hover .bp3-control-indicator{background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background:#202b33}.jupyter-wrapper .bp3-dark .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);cursor:not-allowed}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked~.bp3-control-indicator,.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-file-input{display:inline-block;position:relative;cursor:pointer;height:30px}.jupyter-wrapper .bp3-file-input input{opacity:0;margin:0;min-width:200px}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active:hover,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#182026}.jupyter-wrapper .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#f5f8fa}.jupyter-wrapper .bp3-file-input.bp3-fill{width:100%}.jupyter-wrapper .bp3-file-input.bp3-large,.jupyter-wrapper .bp3-large .bp3-file-input{height:40px}.jupyter-wrapper .bp3-file-input .bp3-file-upload-input-custom-text::after{content:attr(bp3-button-text)}.jupyter-wrapper .bp3-file-upload-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;left:0;padding-right:80px;color:rgba(92,112,128,.6);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-file-upload-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:focus,.jupyter-wrapper .bp3-file-upload-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-file-upload-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;min-width:24px;min-height:24px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;margin:3px;border-radius:3px;width:70px;text-align:center;line-height:24px;content:\"Browse\"}.jupyter-wrapper .bp3-file-upload-input::after:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active:hover,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-file-upload-input:hover::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input:active::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-large .bp3-file-upload-input{height:40px;line-height:40px;font-size:16px;padding-right:95px}.jupyter-wrapper .bp3-large .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-large .bp3-file-upload-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-large .bp3-file-upload-input::after{min-width:30px;min-height:30px;margin:5px;width:85px;line-height:30px}.jupyter-wrapper .bp3-dark .bp3-file-upload-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:hover::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:active::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1)}.jupyter-wrapper .bp3-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0 0 15px}.jupyter-wrapper .bp3-form-group label.bp3-label{margin-bottom:5px}.jupyter-wrapper .bp3-form-group .bp3-control{margin-top:7px}.jupyter-wrapper .bp3-form-group .bp3-form-helper-text{margin-top:5px;color:#5c7080;font-size:12px}.jupyter-wrapper .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#106ba3}.jupyter-wrapper .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#0d8050}.jupyter-wrapper .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#bf7326}.jupyter-wrapper .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#c23030}.jupyter-wrapper .bp3-form-group.bp3-inline{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-form-group.bp3-inline.bp3-large label.bp3-label{margin:0 10px 0 0;line-height:40px}.jupyter-wrapper .bp3-form-group.bp3-inline label.bp3-label{margin:0 10px 0 0;line-height:30px}.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-form-group .bp3-form-helper-text{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-input-group{display:block;position:relative}.jupyter-wrapper .bp3-input-group .bp3-input{position:relative;width:100%}.jupyter-wrapper .bp3-input-group .bp3-input:not(:first-child){padding-left:30px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:last-child){padding-right:30px}.jupyter-wrapper .bp3-input-group .bp3-input-action,.jupyter-wrapper .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-input-group>.bp3-icon{position:absolute;top:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:first-child,.jupyter-wrapper .bp3-input-group>.bp3-button:first-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:first-child{left:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:last-child,.jupyter-wrapper .bp3-input-group>.bp3-button:last-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:last-child{right:0}.jupyter-wrapper .bp3-input-group .bp3-button{min-width:24px;min-height:24px;margin:3px;padding:0 7px}.jupyter-wrapper .bp3-input-group .bp3-button:empty{padding:0}.jupyter-wrapper .bp3-input-group>.bp3-icon{z-index:1;color:#5c7080}.jupyter-wrapper .bp3-input-group>.bp3-icon:empty{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input-action>.bp3-spinner{margin:7px}.jupyter-wrapper .bp3-input-group .bp3-tag{margin:5px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#a7b6c2}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-button{min-width:30px;min-height:30px;margin:5px}.jupyter-wrapper .bp3-input-group.bp3-large>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input-action>.bp3-spinner{margin:12px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:first-child){padding-left:40px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:last-child){padding-right:40px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-button{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-tag{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input-action>.bp3-spinner{margin:4px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:first-child){padding-left:24px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:last-child){padding-right:24px}.jupyter-wrapper .bp3-input-group.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-input-group.bp3-round .bp3-button,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-input,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-tag{border-radius:30px}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#48aff0}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-success>.bp3-icon{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-success>.bp3-icon{color:#3dcc91}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#ffb366}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#ff7373}.jupyter-wrapper .bp3-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none}.jupyter-wrapper .bp3-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:focus,.jupyter-wrapper .bp3-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input[type=search],.jupyter-wrapper .bp3-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-input:disabled,.jupyter-wrapper .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-input.bp3-large{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input.bp3-large[type=search],.jupyter-wrapper .bp3-input.bp3-large.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input.bp3-small{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input.bp3-small[type=search],.jupyter-wrapper .bp3-input.bp3-small.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-dark .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input::-ms-clear{display:none}.jupyter-wrapper textarea.bp3-input{max-width:100%;padding:10px}.jupyter-wrapper textarea.bp3-input,.jupyter-wrapper textarea.bp3-input.bp3-large,.jupyter-wrapper textarea.bp3-input.bp3-small{height:auto;line-height:inherit}.jupyter-wrapper textarea.bp3-input.bp3-small{padding:8px}.jupyter-wrapper .bp3-dark textarea.bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark textarea.bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input:disabled,.jupyter-wrapper .bp3-dark textarea.bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper label.bp3-label{display:block;margin-top:0;margin-bottom:15px}.jupyter-wrapper label.bp3-label .bp3-html-select,.jupyter-wrapper label.bp3-label .bp3-input,.jupyter-wrapper label.bp3-label .bp3-select,.jupyter-wrapper label.bp3-label .bp3-slider,.jupyter-wrapper label.bp3-label .bp3-popover-wrapper{display:block;margin-top:5px;text-transform:none}.jupyter-wrapper label.bp3-label .bp3-button-group{margin-top:5px}.jupyter-wrapper label.bp3-label .bp3-select select,.jupyter-wrapper label.bp3-label .bp3-html-select select{width:100%;vertical-align:top;font-weight:400}.jupyter-wrapper label.bp3-label.bp3-disabled,.jupyter-wrapper label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(92,112,128,.6)}.jupyter-wrapper label.bp3-label.bp3-inline{line-height:30px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-html-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-popover-wrapper{display:inline-block;margin:0 0 0 5px;vertical-align:top}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-button-group{margin:0 0 0 5px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group .bp3-input{margin-left:0}.jupyter-wrapper label.bp3-label.bp3-inline.bp3-large{line-height:40px}.jupyter-wrapper label.bp3-label:not(.bp3-inline) .bp3-popover-target{display:block}.jupyter-wrapper .bp3-dark label.bp3-label{color:#f5f8fa}.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled,.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button{-webkit-box-flex:1;-ms-flex:1 1 14px;flex:1 1 14px;width:30px;min-height:0;padding:0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:first-child{border-radius:0 3px 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:last-child{border-radius:0 0 3px 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:first-child{border-radius:3px 0 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:last-child{border-radius:0 0 0 3px}.jupyter-wrapper .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical>.bp3-button{width:40px}.jupyter-wrapper form{display:block}.jupyter-wrapper .bp3-html-select select,.jupyter-wrapper .bp3-select select{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;border-radius:3px;width:100%;height:30px;padding:0 25px 0 10px;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-html-select select>.bp3-fill,.jupyter-wrapper .bp3-select select>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-html-select select::before,.jupyter-wrapper .bp3-select select::before,.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{margin-right:7px}.jupyter-wrapper .bp3-html-select select:empty::before,.jupyter-wrapper .bp3-select select:empty::before,.jupyter-wrapper .bp3-html-select select>:last-child,.jupyter-wrapper .bp3-select select>:last-child{margin-right:0}.jupyter-wrapper .bp3-html-select select:hover,.jupyter-wrapper .bp3-select select:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-html-select select:active,.jupyter-wrapper .bp3-select select:active,.jupyter-wrapper .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-select select.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled,.jupyter-wrapper .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-select select.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal select{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-large select,.jupyter-wrapper .bp3-select.bp3-large select{height:40px;padding-right:35px;font-size:16px}.jupyter-wrapper .bp3-dark .bp3-html-select select,.jupyter-wrapper .bp3-dark .bp3-select select{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon,.jupyter-wrapper .bp3-select::after{position:absolute;top:7px;right:7px;color:#5c7080;pointer-events:none}.jupyter-wrapper .bp3-html-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-disabled.bp3-select::after{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select,.jupyter-wrapper .bp3-select{display:inline-block;position:relative;vertical-align:middle;letter-spacing:normal}.jupyter-wrapper .bp3-html-select select::-ms-expand,.jupyter-wrapper .bp3-select select::-ms-expand{display:none}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon{color:#5c7080}.jupyter-wrapper .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-select .bp3-icon:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon:hover{color:#f5f8fa}.jupyter-wrapper .bp3-html-select.bp3-large::after,.jupyter-wrapper .bp3-html-select.bp3-large .bp3-icon,.jupyter-wrapper .bp3-select.bp3-large::after,.jupyter-wrapper .bp3-select.bp3-large .bp3-icon{top:12px;right:12px}.jupyter-wrapper .bp3-html-select.bp3-fill,.jupyter-wrapper .bp3-html-select.bp3-fill select,.jupyter-wrapper .bp3-select.bp3-fill,.jupyter-wrapper .bp3-select.bp3-fill select{width:100%}.jupyter-wrapper .bp3-dark .bp3-html-select option,.jupyter-wrapper .bp3-dark .bp3-select option{background-color:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select::after,.jupyter-wrapper .bp3-dark .bp3-select::after{color:#a7b6c2}.jupyter-wrapper .bp3-select::after{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6c6\"}.jupyter-wrapper .bp3-running-text table,.jupyter-wrapper table.bp3-html-table{border-spacing:0;font-size:14px}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th,.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{padding:11px;vertical-align:top;text-align:left}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th{color:#182026;font-weight:600}.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{color:#182026}.jupyter-wrapper .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text table th,.jupyter-wrapper .bp3-running-text .bp3-dark table th,.jupyter-wrapper .bp3-dark table.bp3-html-table th{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table td,.jupyter-wrapper .bp3-running-text .bp3-dark table td,.jupyter-wrapper .bp3-dark table.bp3-html-table td{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child th,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child td,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed th,.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed td,.jupyter-wrapper table.bp3-html-table.bp3-small th,.jupyter-wrapper table.bp3-html-table.bp3-small td{padding-top:6px;padding-bottom:6px}.jupyter-wrapper table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(191,204,214,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(191,204,214,.3);cursor:pointer}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(92,112,128,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(92,112,128,.3);cursor:pointer}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-key-combo{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-key-combo>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-key-combo>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-key-combo::before,.jupyter-wrapper .bp3-key-combo>*{margin-right:5px}.jupyter-wrapper .bp3-key-combo:empty::before,.jupyter-wrapper .bp3-key-combo>:last-child{margin-right:0}.jupyter-wrapper .bp3-hotkey-dialog{top:40px;padding-bottom:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-dialog-body{margin:0;padding:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-hotkey-label{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.jupyter-wrapper .bp3-hotkey-column{margin:auto;max-height:80vh;overflow-y:auto;padding:30px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading{margin-bottom:20px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading:not(:first-child){margin-top:40px}.jupyter-wrapper .bp3-hotkey{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;margin-right:0;margin-left:0}.jupyter-wrapper .bp3-hotkey:not(:last-child){margin-bottom:10px}.jupyter-wrapper .bp3-icon{display:inline-block;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;vertical-align:text-bottom}.jupyter-wrapper .bp3-icon:not(:empty)::before{content:\"\" !important;content:unset !important}.jupyter-wrapper .bp3-icon>svg{display:block}.jupyter-wrapper .bp3-icon>svg:not([fill]){fill:currentColor}.jupyter-wrapper .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-icon-large.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-icon-large.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-icon-large.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-icon-large.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-danger{color:#ff7373}.jupyter-wrapper span.bp3-icon-standard{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon-large{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon:empty{line-height:1;font-family:\"Icons20\";font-size:inherit;font-weight:400;font-style:normal}.jupyter-wrapper span.bp3-icon:empty::before{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-icon-add::before{content:\"\ue63e\"}.jupyter-wrapper .bp3-icon-add-column-left::before{content:\"\ue6f9\"}.jupyter-wrapper .bp3-icon-add-column-right::before{content:\"\ue6fa\"}.jupyter-wrapper .bp3-icon-add-row-bottom::before{content:\"\ue6f8\"}.jupyter-wrapper .bp3-icon-add-row-top::before{content:\"\ue6f7\"}.jupyter-wrapper .bp3-icon-add-to-artifact::before{content:\"\ue67c\"}.jupyter-wrapper .bp3-icon-add-to-folder::before{content:\"\ue6d2\"}.jupyter-wrapper .bp3-icon-airplane::before{content:\"\ue74b\"}.jupyter-wrapper .bp3-icon-align-center::before{content:\"\ue603\"}.jupyter-wrapper .bp3-icon-align-justify::before{content:\"\ue605\"}.jupyter-wrapper .bp3-icon-align-left::before{content:\"\ue602\"}.jupyter-wrapper .bp3-icon-align-right::before{content:\"\ue604\"}.jupyter-wrapper .bp3-icon-alignment-bottom::before{content:\"\ue727\"}.jupyter-wrapper .bp3-icon-alignment-horizontal-center::before{content:\"\ue726\"}.jupyter-wrapper .bp3-icon-alignment-left::before{content:\"\ue722\"}.jupyter-wrapper .bp3-icon-alignment-right::before{content:\"\ue724\"}.jupyter-wrapper .bp3-icon-alignment-top::before{content:\"\ue725\"}.jupyter-wrapper .bp3-icon-alignment-vertical-center::before{content:\"\ue723\"}.jupyter-wrapper .bp3-icon-annotation::before{content:\"\ue6f0\"}.jupyter-wrapper .bp3-icon-application::before{content:\"\ue735\"}.jupyter-wrapper .bp3-icon-applications::before{content:\"\ue621\"}.jupyter-wrapper .bp3-icon-archive::before{content:\"\ue907\"}.jupyter-wrapper .bp3-icon-arrow-bottom-left::before{content:\"\u2199\"}.jupyter-wrapper .bp3-icon-arrow-bottom-right::before{content:\"\u2198\"}.jupyter-wrapper .bp3-icon-arrow-down::before{content:\"\u2193\"}.jupyter-wrapper .bp3-icon-arrow-left::before{content:\"\u2190\"}.jupyter-wrapper .bp3-icon-arrow-right::before{content:\"\u2192\"}.jupyter-wrapper .bp3-icon-arrow-top-left::before{content:\"\u2196\"}.jupyter-wrapper .bp3-icon-arrow-top-right::before{content:\"\u2197\"}.jupyter-wrapper .bp3-icon-arrow-up::before{content:\"\u2191\"}.jupyter-wrapper .bp3-icon-arrows-horizontal::before{content:\"\u2194\"}.jupyter-wrapper .bp3-icon-arrows-vertical::before{content:\"\u2195\"}.jupyter-wrapper .bp3-icon-asterisk::before{content:\"*\"}.jupyter-wrapper .bp3-icon-automatic-updates::before{content:\"\ue65f\"}.jupyter-wrapper .bp3-icon-badge::before{content:\"\ue6e3\"}.jupyter-wrapper .bp3-icon-ban-circle::before{content:\"\ue69d\"}.jupyter-wrapper .bp3-icon-bank-account::before{content:\"\ue76f\"}.jupyter-wrapper .bp3-icon-barcode::before{content:\"\ue676\"}.jupyter-wrapper .bp3-icon-blank::before{content:\"\ue900\"}.jupyter-wrapper .bp3-icon-blocked-person::before{content:\"\ue768\"}.jupyter-wrapper .bp3-icon-bold::before{content:\"\ue606\"}.jupyter-wrapper .bp3-icon-book::before{content:\"\ue6b8\"}.jupyter-wrapper .bp3-icon-bookmark::before{content:\"\ue61a\"}.jupyter-wrapper .bp3-icon-box::before{content:\"\ue6bf\"}.jupyter-wrapper .bp3-icon-briefcase::before{content:\"\ue674\"}.jupyter-wrapper .bp3-icon-bring-data::before{content:\"\ue90a\"}.jupyter-wrapper .bp3-icon-build::before{content:\"\ue72d\"}.jupyter-wrapper .bp3-icon-calculator::before{content:\"\ue70b\"}.jupyter-wrapper .bp3-icon-calendar::before{content:\"\ue62b\"}.jupyter-wrapper .bp3-icon-camera::before{content:\"\ue69e\"}.jupyter-wrapper .bp3-icon-caret-down::before{content:\"\u2304\"}.jupyter-wrapper .bp3-icon-caret-left::before{content:\"\u2329\"}.jupyter-wrapper .bp3-icon-caret-right::before{content:\"\u232a\"}.jupyter-wrapper .bp3-icon-caret-up::before{content:\"\u2303\"}.jupyter-wrapper .bp3-icon-cell-tower::before{content:\"\ue770\"}.jupyter-wrapper .bp3-icon-changes::before{content:\"\ue623\"}.jupyter-wrapper .bp3-icon-chart::before{content:\"\ue67e\"}.jupyter-wrapper .bp3-icon-chat::before{content:\"\ue689\"}.jupyter-wrapper .bp3-icon-chevron-backward::before{content:\"\ue6df\"}.jupyter-wrapper .bp3-icon-chevron-down::before{content:\"\ue697\"}.jupyter-wrapper .bp3-icon-chevron-forward::before{content:\"\ue6e0\"}.jupyter-wrapper .bp3-icon-chevron-left::before{content:\"\ue694\"}.jupyter-wrapper .bp3-icon-chevron-right::before{content:\"\ue695\"}.jupyter-wrapper .bp3-icon-chevron-up::before{content:\"\ue696\"}.jupyter-wrapper .bp3-icon-circle::before{content:\"\ue66a\"}.jupyter-wrapper .bp3-icon-circle-arrow-down::before{content:\"\ue68e\"}.jupyter-wrapper .bp3-icon-circle-arrow-left::before{content:\"\ue68c\"}.jupyter-wrapper .bp3-icon-circle-arrow-right::before{content:\"\ue68b\"}.jupyter-wrapper .bp3-icon-circle-arrow-up::before{content:\"\ue68d\"}.jupyter-wrapper .bp3-icon-citation::before{content:\"\ue61b\"}.jupyter-wrapper .bp3-icon-clean::before{content:\"\ue7c5\"}.jupyter-wrapper .bp3-icon-clipboard::before{content:\"\ue61d\"}.jupyter-wrapper .bp3-icon-cloud::before{content:\"\u2601\"}.jupyter-wrapper .bp3-icon-cloud-download::before{content:\"\ue690\"}.jupyter-wrapper .bp3-icon-cloud-upload::before{content:\"\ue691\"}.jupyter-wrapper .bp3-icon-code::before{content:\"\ue661\"}.jupyter-wrapper .bp3-icon-code-block::before{content:\"\ue6c5\"}.jupyter-wrapper .bp3-icon-cog::before{content:\"\ue645\"}.jupyter-wrapper .bp3-icon-collapse-all::before{content:\"\ue763\"}.jupyter-wrapper .bp3-icon-column-layout::before{content:\"\ue6da\"}.jupyter-wrapper .bp3-icon-comment::before{content:\"\ue68a\"}.jupyter-wrapper .bp3-icon-comparison::before{content:\"\ue637\"}.jupyter-wrapper .bp3-icon-compass::before{content:\"\ue79c\"}.jupyter-wrapper .bp3-icon-compressed::before{content:\"\ue6c0\"}.jupyter-wrapper .bp3-icon-confirm::before{content:\"\ue639\"}.jupyter-wrapper .bp3-icon-console::before{content:\"\ue79b\"}.jupyter-wrapper .bp3-icon-contrast::before{content:\"\ue6cb\"}.jupyter-wrapper .bp3-icon-control::before{content:\"\ue67f\"}.jupyter-wrapper .bp3-icon-credit-card::before{content:\"\ue649\"}.jupyter-wrapper .bp3-icon-cross::before{content:\"\u2717\"}.jupyter-wrapper .bp3-icon-crown::before{content:\"\ue7b4\"}.jupyter-wrapper .bp3-icon-cube::before{content:\"\ue7c8\"}.jupyter-wrapper .bp3-icon-cube-add::before{content:\"\ue7c9\"}.jupyter-wrapper .bp3-icon-cube-remove::before{content:\"\ue7d0\"}.jupyter-wrapper .bp3-icon-curved-range-chart::before{content:\"\ue71b\"}.jupyter-wrapper .bp3-icon-cut::before{content:\"\ue6ef\"}.jupyter-wrapper .bp3-icon-dashboard::before{content:\"\ue751\"}.jupyter-wrapper .bp3-icon-data-lineage::before{content:\"\ue908\"}.jupyter-wrapper .bp3-icon-database::before{content:\"\ue683\"}.jupyter-wrapper .bp3-icon-delete::before{content:\"\ue644\"}.jupyter-wrapper .bp3-icon-delta::before{content:\"\u0394\"}.jupyter-wrapper .bp3-icon-derive-column::before{content:\"\ue739\"}.jupyter-wrapper .bp3-icon-desktop::before{content:\"\ue6af\"}.jupyter-wrapper .bp3-icon-diagram-tree::before{content:\"\ue7b3\"}.jupyter-wrapper .bp3-icon-direction-left::before{content:\"\ue681\"}.jupyter-wrapper .bp3-icon-direction-right::before{content:\"\ue682\"}.jupyter-wrapper .bp3-icon-disable::before{content:\"\ue600\"}.jupyter-wrapper .bp3-icon-document::before{content:\"\ue630\"}.jupyter-wrapper .bp3-icon-document-open::before{content:\"\ue71e\"}.jupyter-wrapper .bp3-icon-document-share::before{content:\"\ue71f\"}.jupyter-wrapper .bp3-icon-dollar::before{content:\"$\"}.jupyter-wrapper .bp3-icon-dot::before{content:\"\u2022\"}.jupyter-wrapper .bp3-icon-double-caret-horizontal::before{content:\"\ue6c7\"}.jupyter-wrapper .bp3-icon-double-caret-vertical::before{content:\"\ue6c6\"}.jupyter-wrapper .bp3-icon-double-chevron-down::before{content:\"\ue703\"}.jupyter-wrapper .bp3-icon-double-chevron-left::before{content:\"\ue6ff\"}.jupyter-wrapper .bp3-icon-double-chevron-right::before{content:\"\ue701\"}.jupyter-wrapper .bp3-icon-double-chevron-up::before{content:\"\ue702\"}.jupyter-wrapper .bp3-icon-doughnut-chart::before{content:\"\ue6ce\"}.jupyter-wrapper .bp3-icon-download::before{content:\"\ue62f\"}.jupyter-wrapper .bp3-icon-drag-handle-horizontal::before{content:\"\ue716\"}.jupyter-wrapper .bp3-icon-drag-handle-vertical::before{content:\"\ue715\"}.jupyter-wrapper .bp3-icon-draw::before{content:\"\ue66b\"}.jupyter-wrapper .bp3-icon-drive-time::before{content:\"\ue615\"}.jupyter-wrapper .bp3-icon-duplicate::before{content:\"\ue69c\"}.jupyter-wrapper .bp3-icon-edit::before{content:\"\u270e\"}.jupyter-wrapper .bp3-icon-eject::before{content:\"\u23cf\"}.jupyter-wrapper .bp3-icon-endorsed::before{content:\"\ue75f\"}.jupyter-wrapper .bp3-icon-envelope::before{content:\"\u2709\"}.jupyter-wrapper .bp3-icon-equals::before{content:\"\ue7d9\"}.jupyter-wrapper .bp3-icon-eraser::before{content:\"\ue773\"}.jupyter-wrapper .bp3-icon-error::before{content:\"\ue648\"}.jupyter-wrapper .bp3-icon-euro::before{content:\"\u20ac\"}.jupyter-wrapper .bp3-icon-exchange::before{content:\"\ue636\"}.jupyter-wrapper .bp3-icon-exclude-row::before{content:\"\ue6ea\"}.jupyter-wrapper .bp3-icon-expand-all::before{content:\"\ue764\"}.jupyter-wrapper .bp3-icon-export::before{content:\"\ue633\"}.jupyter-wrapper .bp3-icon-eye-off::before{content:\"\ue6cc\"}.jupyter-wrapper .bp3-icon-eye-on::before{content:\"\ue75a\"}.jupyter-wrapper .bp3-icon-eye-open::before{content:\"\ue66f\"}.jupyter-wrapper .bp3-icon-fast-backward::before{content:\"\ue6a8\"}.jupyter-wrapper .bp3-icon-fast-forward::before{content:\"\ue6ac\"}.jupyter-wrapper .bp3-icon-feed::before{content:\"\ue656\"}.jupyter-wrapper .bp3-icon-feed-subscribed::before{content:\"\ue78f\"}.jupyter-wrapper .bp3-icon-film::before{content:\"\ue6a1\"}.jupyter-wrapper .bp3-icon-filter::before{content:\"\ue638\"}.jupyter-wrapper .bp3-icon-filter-keep::before{content:\"\ue78c\"}.jupyter-wrapper .bp3-icon-filter-list::before{content:\"\ue6ee\"}.jupyter-wrapper .bp3-icon-filter-open::before{content:\"\ue7d7\"}.jupyter-wrapper .bp3-icon-filter-remove::before{content:\"\ue78d\"}.jupyter-wrapper .bp3-icon-flag::before{content:\"\u2691\"}.jupyter-wrapper .bp3-icon-flame::before{content:\"\ue7a9\"}.jupyter-wrapper .bp3-icon-flash::before{content:\"\ue6b3\"}.jupyter-wrapper .bp3-icon-floppy-disk::before{content:\"\ue6b7\"}.jupyter-wrapper .bp3-icon-flow-branch::before{content:\"\ue7c1\"}.jupyter-wrapper .bp3-icon-flow-end::before{content:\"\ue7c4\"}.jupyter-wrapper .bp3-icon-flow-linear::before{content:\"\ue7c0\"}.jupyter-wrapper .bp3-icon-flow-review::before{content:\"\ue7c2\"}.jupyter-wrapper .bp3-icon-flow-review-branch::before{content:\"\ue7c3\"}.jupyter-wrapper .bp3-icon-flows::before{content:\"\ue659\"}.jupyter-wrapper .bp3-icon-folder-close::before{content:\"\ue652\"}.jupyter-wrapper .bp3-icon-folder-new::before{content:\"\ue7b0\"}.jupyter-wrapper .bp3-icon-folder-open::before{content:\"\ue651\"}.jupyter-wrapper .bp3-icon-folder-shared::before{content:\"\ue653\"}.jupyter-wrapper .bp3-icon-folder-shared-open::before{content:\"\ue670\"}.jupyter-wrapper .bp3-icon-follower::before{content:\"\ue760\"}.jupyter-wrapper .bp3-icon-following::before{content:\"\ue761\"}.jupyter-wrapper .bp3-icon-font::before{content:\"\ue6b4\"}.jupyter-wrapper .bp3-icon-fork::before{content:\"\ue63a\"}.jupyter-wrapper .bp3-icon-form::before{content:\"\ue795\"}.jupyter-wrapper .bp3-icon-full-circle::before{content:\"\ue685\"}.jupyter-wrapper .bp3-icon-full-stacked-chart::before{content:\"\ue75e\"}.jupyter-wrapper .bp3-icon-fullscreen::before{content:\"\ue699\"}.jupyter-wrapper .bp3-icon-function::before{content:\"\ue6e5\"}.jupyter-wrapper .bp3-icon-gantt-chart::before{content:\"\ue6f4\"}.jupyter-wrapper .bp3-icon-geolocation::before{content:\"\ue640\"}.jupyter-wrapper .bp3-icon-geosearch::before{content:\"\ue613\"}.jupyter-wrapper .bp3-icon-git-branch::before{content:\"\ue72a\"}.jupyter-wrapper .bp3-icon-git-commit::before{content:\"\ue72b\"}.jupyter-wrapper .bp3-icon-git-merge::before{content:\"\ue729\"}.jupyter-wrapper .bp3-icon-git-new-branch::before{content:\"\ue749\"}.jupyter-wrapper .bp3-icon-git-pull::before{content:\"\ue728\"}.jupyter-wrapper .bp3-icon-git-push::before{content:\"\ue72c\"}.jupyter-wrapper .bp3-icon-git-repo::before{content:\"\ue748\"}.jupyter-wrapper .bp3-icon-glass::before{content:\"\ue6b1\"}.jupyter-wrapper .bp3-icon-globe::before{content:\"\ue666\"}.jupyter-wrapper .bp3-icon-globe-network::before{content:\"\ue7b5\"}.jupyter-wrapper .bp3-icon-graph::before{content:\"\ue673\"}.jupyter-wrapper .bp3-icon-graph-remove::before{content:\"\ue609\"}.jupyter-wrapper .bp3-icon-greater-than::before{content:\"\ue7e1\"}.jupyter-wrapper .bp3-icon-greater-than-or-equal-to::before{content:\"\ue7e2\"}.jupyter-wrapper .bp3-icon-grid::before{content:\"\ue6d0\"}.jupyter-wrapper .bp3-icon-grid-view::before{content:\"\ue6e4\"}.jupyter-wrapper .bp3-icon-group-objects::before{content:\"\ue60a\"}.jupyter-wrapper .bp3-icon-grouped-bar-chart::before{content:\"\ue75d\"}.jupyter-wrapper .bp3-icon-hand::before{content:\"\ue6de\"}.jupyter-wrapper .bp3-icon-hand-down::before{content:\"\ue6bb\"}.jupyter-wrapper .bp3-icon-hand-left::before{content:\"\ue6bc\"}.jupyter-wrapper .bp3-icon-hand-right::before{content:\"\ue6b9\"}.jupyter-wrapper .bp3-icon-hand-up::before{content:\"\ue6ba\"}.jupyter-wrapper .bp3-icon-header::before{content:\"\ue6b5\"}.jupyter-wrapper .bp3-icon-header-one::before{content:\"\ue793\"}.jupyter-wrapper .bp3-icon-header-two::before{content:\"\ue794\"}.jupyter-wrapper .bp3-icon-headset::before{content:\"\ue6dc\"}.jupyter-wrapper .bp3-icon-heart::before{content:\"\u2665\"}.jupyter-wrapper .bp3-icon-heart-broken::before{content:\"\ue7a2\"}.jupyter-wrapper .bp3-icon-heat-grid::before{content:\"\ue6f3\"}.jupyter-wrapper .bp3-icon-heatmap::before{content:\"\ue614\"}.jupyter-wrapper .bp3-icon-help::before{content:\"?\"}.jupyter-wrapper .bp3-icon-helper-management::before{content:\"\ue66d\"}.jupyter-wrapper .bp3-icon-highlight::before{content:\"\ue6ed\"}.jupyter-wrapper .bp3-icon-history::before{content:\"\ue64a\"}.jupyter-wrapper .bp3-icon-home::before{content:\"\u2302\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart::before{content:\"\ue70c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-asc::before{content:\"\ue75c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-desc::before{content:\"\ue71d\"}.jupyter-wrapper .bp3-icon-horizontal-distribution::before{content:\"\ue720\"}.jupyter-wrapper .bp3-icon-id-number::before{content:\"\ue771\"}.jupyter-wrapper .bp3-icon-image-rotate-left::before{content:\"\ue73a\"}.jupyter-wrapper .bp3-icon-image-rotate-right::before{content:\"\ue73b\"}.jupyter-wrapper .bp3-icon-import::before{content:\"\ue632\"}.jupyter-wrapper .bp3-icon-inbox::before{content:\"\ue629\"}.jupyter-wrapper .bp3-icon-inbox-filtered::before{content:\"\ue7d1\"}.jupyter-wrapper .bp3-icon-inbox-geo::before{content:\"\ue7d2\"}.jupyter-wrapper .bp3-icon-inbox-search::before{content:\"\ue7d3\"}.jupyter-wrapper .bp3-icon-inbox-update::before{content:\"\ue7d4\"}.jupyter-wrapper .bp3-icon-info-sign::before{content:\"\u2139\"}.jupyter-wrapper .bp3-icon-inheritance::before{content:\"\ue7d5\"}.jupyter-wrapper .bp3-icon-inner-join::before{content:\"\ue7a3\"}.jupyter-wrapper .bp3-icon-insert::before{content:\"\ue66c\"}.jupyter-wrapper .bp3-icon-intersection::before{content:\"\ue765\"}.jupyter-wrapper .bp3-icon-ip-address::before{content:\"\ue772\"}.jupyter-wrapper .bp3-icon-issue::before{content:\"\ue774\"}.jupyter-wrapper .bp3-icon-issue-closed::before{content:\"\ue776\"}.jupyter-wrapper .bp3-icon-issue-new::before{content:\"\ue775\"}.jupyter-wrapper .bp3-icon-italic::before{content:\"\ue607\"}.jupyter-wrapper .bp3-icon-join-table::before{content:\"\ue738\"}.jupyter-wrapper .bp3-icon-key::before{content:\"\ue78e\"}.jupyter-wrapper .bp3-icon-key-backspace::before{content:\"\ue707\"}.jupyter-wrapper .bp3-icon-key-command::before{content:\"\ue705\"}.jupyter-wrapper .bp3-icon-key-control::before{content:\"\ue704\"}.jupyter-wrapper .bp3-icon-key-delete::before{content:\"\ue708\"}.jupyter-wrapper .bp3-icon-key-enter::before{content:\"\ue70a\"}.jupyter-wrapper .bp3-icon-key-escape::before{content:\"\ue709\"}.jupyter-wrapper .bp3-icon-key-option::before{content:\"\ue742\"}.jupyter-wrapper .bp3-icon-key-shift::before{content:\"\ue706\"}.jupyter-wrapper .bp3-icon-key-tab::before{content:\"\ue757\"}.jupyter-wrapper .bp3-icon-known-vehicle::before{content:\"\ue73c\"}.jupyter-wrapper .bp3-icon-label::before{content:\"\ue665\"}.jupyter-wrapper .bp3-icon-layer::before{content:\"\ue6cf\"}.jupyter-wrapper .bp3-icon-layers::before{content:\"\ue618\"}.jupyter-wrapper .bp3-icon-layout::before{content:\"\ue60c\"}.jupyter-wrapper .bp3-icon-layout-auto::before{content:\"\ue60d\"}.jupyter-wrapper .bp3-icon-layout-balloon::before{content:\"\ue6d3\"}.jupyter-wrapper .bp3-icon-layout-circle::before{content:\"\ue60e\"}.jupyter-wrapper .bp3-icon-layout-grid::before{content:\"\ue610\"}.jupyter-wrapper .bp3-icon-layout-group-by::before{content:\"\ue611\"}.jupyter-wrapper .bp3-icon-layout-hierarchy::before{content:\"\ue60f\"}.jupyter-wrapper .bp3-icon-layout-linear::before{content:\"\ue6c3\"}.jupyter-wrapper .bp3-icon-layout-skew-grid::before{content:\"\ue612\"}.jupyter-wrapper .bp3-icon-layout-sorted-clusters::before{content:\"\ue6d4\"}.jupyter-wrapper .bp3-icon-learning::before{content:\"\ue904\"}.jupyter-wrapper .bp3-icon-left-join::before{content:\"\ue7a4\"}.jupyter-wrapper .bp3-icon-less-than::before{content:\"\ue7e3\"}.jupyter-wrapper .bp3-icon-less-than-or-equal-to::before{content:\"\ue7e4\"}.jupyter-wrapper .bp3-icon-lifesaver::before{content:\"\ue7c7\"}.jupyter-wrapper .bp3-icon-lightbulb::before{content:\"\ue6b0\"}.jupyter-wrapper .bp3-icon-link::before{content:\"\ue62d\"}.jupyter-wrapper .bp3-icon-list::before{content:\"\u2630\"}.jupyter-wrapper .bp3-icon-list-columns::before{content:\"\ue7b9\"}.jupyter-wrapper .bp3-icon-list-detail-view::before{content:\"\ue743\"}.jupyter-wrapper .bp3-icon-locate::before{content:\"\ue619\"}.jupyter-wrapper .bp3-icon-lock::before{content:\"\ue625\"}.jupyter-wrapper .bp3-icon-log-in::before{content:\"\ue69a\"}.jupyter-wrapper .bp3-icon-log-out::before{content:\"\ue64c\"}.jupyter-wrapper .bp3-icon-manual::before{content:\"\ue6f6\"}.jupyter-wrapper .bp3-icon-manually-entered-data::before{content:\"\ue74a\"}.jupyter-wrapper .bp3-icon-map::before{content:\"\ue662\"}.jupyter-wrapper .bp3-icon-map-create::before{content:\"\ue741\"}.jupyter-wrapper .bp3-icon-map-marker::before{content:\"\ue67d\"}.jupyter-wrapper .bp3-icon-maximize::before{content:\"\ue635\"}.jupyter-wrapper .bp3-icon-media::before{content:\"\ue62c\"}.jupyter-wrapper .bp3-icon-menu::before{content:\"\ue762\"}.jupyter-wrapper .bp3-icon-menu-closed::before{content:\"\ue655\"}.jupyter-wrapper .bp3-icon-menu-open::before{content:\"\ue654\"}.jupyter-wrapper .bp3-icon-merge-columns::before{content:\"\ue74f\"}.jupyter-wrapper .bp3-icon-merge-links::before{content:\"\ue60b\"}.jupyter-wrapper .bp3-icon-minimize::before{content:\"\ue634\"}.jupyter-wrapper .bp3-icon-minus::before{content:\"\u2212\"}.jupyter-wrapper .bp3-icon-mobile-phone::before{content:\"\ue717\"}.jupyter-wrapper .bp3-icon-mobile-video::before{content:\"\ue69f\"}.jupyter-wrapper .bp3-icon-moon::before{content:\"\ue754\"}.jupyter-wrapper .bp3-icon-more::before{content:\"\ue62a\"}.jupyter-wrapper .bp3-icon-mountain::before{content:\"\ue7b1\"}.jupyter-wrapper .bp3-icon-move::before{content:\"\ue693\"}.jupyter-wrapper .bp3-icon-mugshot::before{content:\"\ue6db\"}.jupyter-wrapper .bp3-icon-multi-select::before{content:\"\ue680\"}.jupyter-wrapper .bp3-icon-music::before{content:\"\ue6a6\"}.jupyter-wrapper .bp3-icon-new-drawing::before{content:\"\ue905\"}.jupyter-wrapper .bp3-icon-new-grid-item::before{content:\"\ue747\"}.jupyter-wrapper .bp3-icon-new-layer::before{content:\"\ue902\"}.jupyter-wrapper .bp3-icon-new-layers::before{content:\"\ue903\"}.jupyter-wrapper .bp3-icon-new-link::before{content:\"\ue65c\"}.jupyter-wrapper .bp3-icon-new-object::before{content:\"\ue65d\"}.jupyter-wrapper .bp3-icon-new-person::before{content:\"\ue6e9\"}.jupyter-wrapper .bp3-icon-new-prescription::before{content:\"\ue78b\"}.jupyter-wrapper .bp3-icon-new-text-box::before{content:\"\ue65b\"}.jupyter-wrapper .bp3-icon-ninja::before{content:\"\ue675\"}.jupyter-wrapper .bp3-icon-not-equal-to::before{content:\"\ue7e0\"}.jupyter-wrapper .bp3-icon-notifications::before{content:\"\ue624\"}.jupyter-wrapper .bp3-icon-notifications-updated::before{content:\"\ue7b8\"}.jupyter-wrapper .bp3-icon-numbered-list::before{content:\"\ue746\"}.jupyter-wrapper .bp3-icon-numerical::before{content:\"\ue756\"}.jupyter-wrapper .bp3-icon-office::before{content:\"\ue69b\"}.jupyter-wrapper .bp3-icon-offline::before{content:\"\ue67a\"}.jupyter-wrapper .bp3-icon-oil-field::before{content:\"\ue73f\"}.jupyter-wrapper .bp3-icon-one-column::before{content:\"\ue658\"}.jupyter-wrapper .bp3-icon-outdated::before{content:\"\ue7a8\"}.jupyter-wrapper .bp3-icon-page-layout::before{content:\"\ue660\"}.jupyter-wrapper .bp3-icon-panel-stats::before{content:\"\ue777\"}.jupyter-wrapper .bp3-icon-panel-table::before{content:\"\ue778\"}.jupyter-wrapper .bp3-icon-paperclip::before{content:\"\ue664\"}.jupyter-wrapper .bp3-icon-paragraph::before{content:\"\ue76c\"}.jupyter-wrapper .bp3-icon-path::before{content:\"\ue753\"}.jupyter-wrapper .bp3-icon-path-search::before{content:\"\ue65e\"}.jupyter-wrapper .bp3-icon-pause::before{content:\"\ue6a9\"}.jupyter-wrapper .bp3-icon-people::before{content:\"\ue63d\"}.jupyter-wrapper .bp3-icon-percentage::before{content:\"\ue76a\"}.jupyter-wrapper .bp3-icon-person::before{content:\"\ue63c\"}.jupyter-wrapper .bp3-icon-phone::before{content:\"\u260e\"}.jupyter-wrapper .bp3-icon-pie-chart::before{content:\"\ue684\"}.jupyter-wrapper .bp3-icon-pin::before{content:\"\ue646\"}.jupyter-wrapper .bp3-icon-pivot::before{content:\"\ue6f1\"}.jupyter-wrapper .bp3-icon-pivot-table::before{content:\"\ue6eb\"}.jupyter-wrapper .bp3-icon-play::before{content:\"\ue6ab\"}.jupyter-wrapper .bp3-icon-plus::before{content:\"+\"}.jupyter-wrapper .bp3-icon-polygon-filter::before{content:\"\ue6d1\"}.jupyter-wrapper .bp3-icon-power::before{content:\"\ue6d9\"}.jupyter-wrapper .bp3-icon-predictive-analysis::before{content:\"\ue617\"}.jupyter-wrapper .bp3-icon-prescription::before{content:\"\ue78a\"}.jupyter-wrapper .bp3-icon-presentation::before{content:\"\ue687\"}.jupyter-wrapper .bp3-icon-print::before{content:\"\u2399\"}.jupyter-wrapper .bp3-icon-projects::before{content:\"\ue622\"}.jupyter-wrapper .bp3-icon-properties::before{content:\"\ue631\"}.jupyter-wrapper .bp3-icon-property::before{content:\"\ue65a\"}.jupyter-wrapper .bp3-icon-publish-function::before{content:\"\ue752\"}.jupyter-wrapper .bp3-icon-pulse::before{content:\"\ue6e8\"}.jupyter-wrapper .bp3-icon-random::before{content:\"\ue698\"}.jupyter-wrapper .bp3-icon-record::before{content:\"\ue6ae\"}.jupyter-wrapper .bp3-icon-redo::before{content:\"\ue6c4\"}.jupyter-wrapper .bp3-icon-refresh::before{content:\"\ue643\"}.jupyter-wrapper .bp3-icon-regression-chart::before{content:\"\ue758\"}.jupyter-wrapper .bp3-icon-remove::before{content:\"\ue63f\"}.jupyter-wrapper .bp3-icon-remove-column::before{content:\"\ue755\"}.jupyter-wrapper .bp3-icon-remove-column-left::before{content:\"\ue6fd\"}.jupyter-wrapper .bp3-icon-remove-column-right::before{content:\"\ue6fe\"}.jupyter-wrapper .bp3-icon-remove-row-bottom::before{content:\"\ue6fc\"}.jupyter-wrapper .bp3-icon-remove-row-top::before{content:\"\ue6fb\"}.jupyter-wrapper .bp3-icon-repeat::before{content:\"\ue692\"}.jupyter-wrapper .bp3-icon-reset::before{content:\"\ue7d6\"}.jupyter-wrapper .bp3-icon-resolve::before{content:\"\ue672\"}.jupyter-wrapper .bp3-icon-rig::before{content:\"\ue740\"}.jupyter-wrapper .bp3-icon-right-join::before{content:\"\ue7a5\"}.jupyter-wrapper .bp3-icon-ring::before{content:\"\ue6f2\"}.jupyter-wrapper .bp3-icon-rotate-document::before{content:\"\ue6e1\"}.jupyter-wrapper .bp3-icon-rotate-page::before{content:\"\ue6e2\"}.jupyter-wrapper .bp3-icon-satellite::before{content:\"\ue76b\"}.jupyter-wrapper .bp3-icon-saved::before{content:\"\ue6b6\"}.jupyter-wrapper .bp3-icon-scatter-plot::before{content:\"\ue73e\"}.jupyter-wrapper .bp3-icon-search::before{content:\"\ue64b\"}.jupyter-wrapper .bp3-icon-search-around::before{content:\"\ue608\"}.jupyter-wrapper .bp3-icon-search-template::before{content:\"\ue628\"}.jupyter-wrapper .bp3-icon-search-text::before{content:\"\ue663\"}.jupyter-wrapper .bp3-icon-segmented-control::before{content:\"\ue6ec\"}.jupyter-wrapper .bp3-icon-select::before{content:\"\ue616\"}.jupyter-wrapper .bp3-icon-selection::before{content:\"\u29bf\"}.jupyter-wrapper .bp3-icon-send-to::before{content:\"\ue66e\"}.jupyter-wrapper .bp3-icon-send-to-graph::before{content:\"\ue736\"}.jupyter-wrapper .bp3-icon-send-to-map::before{content:\"\ue737\"}.jupyter-wrapper .bp3-icon-series-add::before{content:\"\ue796\"}.jupyter-wrapper .bp3-icon-series-configuration::before{content:\"\ue79a\"}.jupyter-wrapper .bp3-icon-series-derived::before{content:\"\ue799\"}.jupyter-wrapper .bp3-icon-series-filtered::before{content:\"\ue798\"}.jupyter-wrapper .bp3-icon-series-search::before{content:\"\ue797\"}.jupyter-wrapper .bp3-icon-settings::before{content:\"\ue6a2\"}.jupyter-wrapper .bp3-icon-share::before{content:\"\ue62e\"}.jupyter-wrapper .bp3-icon-shield::before{content:\"\ue7b2\"}.jupyter-wrapper .bp3-icon-shop::before{content:\"\ue6c2\"}.jupyter-wrapper .bp3-icon-shopping-cart::before{content:\"\ue6c1\"}.jupyter-wrapper .bp3-icon-signal-search::before{content:\"\ue909\"}.jupyter-wrapper .bp3-icon-sim-card::before{content:\"\ue718\"}.jupyter-wrapper .bp3-icon-slash::before{content:\"\ue769\"}.jupyter-wrapper .bp3-icon-small-cross::before{content:\"\ue6d7\"}.jupyter-wrapper .bp3-icon-small-minus::before{content:\"\ue70e\"}.jupyter-wrapper .bp3-icon-small-plus::before{content:\"\ue70d\"}.jupyter-wrapper .bp3-icon-small-tick::before{content:\"\ue6d8\"}.jupyter-wrapper .bp3-icon-snowflake::before{content:\"\ue7b6\"}.jupyter-wrapper .bp3-icon-social-media::before{content:\"\ue671\"}.jupyter-wrapper .bp3-icon-sort::before{content:\"\ue64f\"}.jupyter-wrapper .bp3-icon-sort-alphabetical::before{content:\"\ue64d\"}.jupyter-wrapper .bp3-icon-sort-alphabetical-desc::before{content:\"\ue6c8\"}.jupyter-wrapper .bp3-icon-sort-asc::before{content:\"\ue6d5\"}.jupyter-wrapper .bp3-icon-sort-desc::before{content:\"\ue6d6\"}.jupyter-wrapper .bp3-icon-sort-numerical::before{content:\"\ue64e\"}.jupyter-wrapper .bp3-icon-sort-numerical-desc::before{content:\"\ue6c9\"}.jupyter-wrapper .bp3-icon-split-columns::before{content:\"\ue750\"}.jupyter-wrapper .bp3-icon-square::before{content:\"\ue686\"}.jupyter-wrapper .bp3-icon-stacked-chart::before{content:\"\ue6e7\"}.jupyter-wrapper .bp3-icon-star::before{content:\"\u2605\"}.jupyter-wrapper .bp3-icon-star-empty::before{content:\"\u2606\"}.jupyter-wrapper .bp3-icon-step-backward::before{content:\"\ue6a7\"}.jupyter-wrapper .bp3-icon-step-chart::before{content:\"\ue70f\"}.jupyter-wrapper .bp3-icon-step-forward::before{content:\"\ue6ad\"}.jupyter-wrapper .bp3-icon-stop::before{content:\"\ue6aa\"}.jupyter-wrapper .bp3-icon-stopwatch::before{content:\"\ue901\"}.jupyter-wrapper .bp3-icon-strikethrough::before{content:\"\ue7a6\"}.jupyter-wrapper .bp3-icon-style::before{content:\"\ue601\"}.jupyter-wrapper .bp3-icon-swap-horizontal::before{content:\"\ue745\"}.jupyter-wrapper .bp3-icon-swap-vertical::before{content:\"\ue744\"}.jupyter-wrapper .bp3-icon-symbol-circle::before{content:\"\ue72e\"}.jupyter-wrapper .bp3-icon-symbol-cross::before{content:\"\ue731\"}.jupyter-wrapper .bp3-icon-symbol-diamond::before{content:\"\ue730\"}.jupyter-wrapper .bp3-icon-symbol-square::before{content:\"\ue72f\"}.jupyter-wrapper .bp3-icon-symbol-triangle-down::before{content:\"\ue733\"}.jupyter-wrapper .bp3-icon-symbol-triangle-up::before{content:\"\ue732\"}.jupyter-wrapper .bp3-icon-tag::before{content:\"\ue61c\"}.jupyter-wrapper .bp3-icon-take-action::before{content:\"\ue6ca\"}.jupyter-wrapper .bp3-icon-taxi::before{content:\"\ue79e\"}.jupyter-wrapper .bp3-icon-text-highlight::before{content:\"\ue6dd\"}.jupyter-wrapper .bp3-icon-th::before{content:\"\ue667\"}.jupyter-wrapper .bp3-icon-th-derived::before{content:\"\ue669\"}.jupyter-wrapper .bp3-icon-th-disconnect::before{content:\"\ue7d8\"}.jupyter-wrapper .bp3-icon-th-filtered::before{content:\"\ue7c6\"}.jupyter-wrapper .bp3-icon-th-list::before{content:\"\ue668\"}.jupyter-wrapper .bp3-icon-thumbs-down::before{content:\"\ue6be\"}.jupyter-wrapper .bp3-icon-thumbs-up::before{content:\"\ue6bd\"}.jupyter-wrapper .bp3-icon-tick::before{content:\"\u2713\"}.jupyter-wrapper .bp3-icon-tick-circle::before{content:\"\ue779\"}.jupyter-wrapper .bp3-icon-time::before{content:\"\u23f2\"}.jupyter-wrapper .bp3-icon-timeline-area-chart::before{content:\"\ue6cd\"}.jupyter-wrapper .bp3-icon-timeline-bar-chart::before{content:\"\ue620\"}.jupyter-wrapper .bp3-icon-timeline-events::before{content:\"\ue61e\"}.jupyter-wrapper .bp3-icon-timeline-line-chart::before{content:\"\ue61f\"}.jupyter-wrapper .bp3-icon-tint::before{content:\"\ue6b2\"}.jupyter-wrapper .bp3-icon-torch::before{content:\"\ue677\"}.jupyter-wrapper .bp3-icon-tractor::before{content:\"\ue90c\"}.jupyter-wrapper .bp3-icon-train::before{content:\"\ue79f\"}.jupyter-wrapper .bp3-icon-translate::before{content:\"\ue759\"}.jupyter-wrapper .bp3-icon-trash::before{content:\"\ue63b\"}.jupyter-wrapper .bp3-icon-tree::before{content:\"\ue7b7\"}.jupyter-wrapper .bp3-icon-trending-down::before{content:\"\ue71a\"}.jupyter-wrapper .bp3-icon-trending-up::before{content:\"\ue719\"}.jupyter-wrapper .bp3-icon-truck::before{content:\"\ue90b\"}.jupyter-wrapper .bp3-icon-two-columns::before{content:\"\ue657\"}.jupyter-wrapper .bp3-icon-unarchive::before{content:\"\ue906\"}.jupyter-wrapper .bp3-icon-underline::before{content:\"\u2381\"}.jupyter-wrapper .bp3-icon-undo::before{content:\"\u238c\"}.jupyter-wrapper .bp3-icon-ungroup-objects::before{content:\"\ue688\"}.jupyter-wrapper .bp3-icon-unknown-vehicle::before{content:\"\ue73d\"}.jupyter-wrapper .bp3-icon-unlock::before{content:\"\ue626\"}.jupyter-wrapper .bp3-icon-unpin::before{content:\"\ue650\"}.jupyter-wrapper .bp3-icon-unresolve::before{content:\"\ue679\"}.jupyter-wrapper .bp3-icon-updated::before{content:\"\ue7a7\"}.jupyter-wrapper .bp3-icon-upload::before{content:\"\ue68f\"}.jupyter-wrapper .bp3-icon-user::before{content:\"\ue627\"}.jupyter-wrapper .bp3-icon-variable::before{content:\"\ue6f5\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-asc::before{content:\"\ue75b\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-desc::before{content:\"\ue71c\"}.jupyter-wrapper .bp3-icon-vertical-distribution::before{content:\"\ue721\"}.jupyter-wrapper .bp3-icon-video::before{content:\"\ue6a0\"}.jupyter-wrapper .bp3-icon-volume-down::before{content:\"\ue6a4\"}.jupyter-wrapper .bp3-icon-volume-off::before{content:\"\ue6a3\"}.jupyter-wrapper .bp3-icon-volume-up::before{content:\"\ue6a5\"}.jupyter-wrapper .bp3-icon-walk::before{content:\"\ue79d\"}.jupyter-wrapper .bp3-icon-warning-sign::before{content:\"\ue647\"}.jupyter-wrapper .bp3-icon-waterfall-chart::before{content:\"\ue6e6\"}.jupyter-wrapper .bp3-icon-widget::before{content:\"\ue678\"}.jupyter-wrapper .bp3-icon-widget-button::before{content:\"\ue790\"}.jupyter-wrapper .bp3-icon-widget-footer::before{content:\"\ue792\"}.jupyter-wrapper .bp3-icon-widget-header::before{content:\"\ue791\"}.jupyter-wrapper .bp3-icon-wrench::before{content:\"\ue734\"}.jupyter-wrapper .bp3-icon-zoom-in::before{content:\"\ue641\"}.jupyter-wrapper .bp3-icon-zoom-out::before{content:\"\ue642\"}.jupyter-wrapper .bp3-icon-zoom-to-fit::before{content:\"\ue67b\"}.jupyter-wrapper .bp3-submenu>.bp3-popover-wrapper{display:block}.jupyter-wrapper .bp3-submenu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-submenu.bp3-popover{-webkit-box-shadow:none;box-shadow:none;padding:0 5px}.jupyter-wrapper .bp3-submenu.bp3-popover>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover>.bp3-popover-content,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-menu{margin:0;border-radius:3px;background:#fff;min-width:180px;padding:5px;list-style:none;text-align:left;color:#182026}.jupyter-wrapper .bp3-menu-divider{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-divider{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;border-radius:2px;padding:5px 7px;text-decoration:none;line-height:20px;color:inherit;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-menu-item>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>*{margin-right:7px}.jupyter-wrapper .bp3-menu-item:empty::before,.jupyter-wrapper .bp3-menu-item>:last-child{margin-right:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{word-break:break-word}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(167,182,194,.3);cursor:pointer;text-decoration:none}.jupyter-wrapper .bp3-menu-item.bp3-disabled{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(138,155,168,.15);color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{background-color:inherit;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-right:7px}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>.bp3-icon{margin-top:2px;color:#5c7080}.jupyter-wrapper .bp3-menu-item .bp3-menu-item-label{color:#5c7080}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-menu-item:active{background-color:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-menu-item.bp3-disabled{outline:none !important;background-color:inherit !important;cursor:not-allowed !important;color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-large .bp3-menu-item{padding:9px 7px;line-height:22px;font-size:16px}.jupyter-wrapper .bp3-large .bp3-menu-item .bp3-icon{margin-top:3px}.jupyter-wrapper .bp3-large .bp3-menu-item::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-top:1px;margin-right:10px}.jupyter-wrapper button.bp3-menu-item{border:none;background:none;width:100%;text-align:left}.jupyter-wrapper .bp3-menu-header{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15);cursor:default;padding-left:2px}.jupyter-wrapper .bp3-dark .bp3-menu-header{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-header:first-of-type{border-top:none}.jupyter-wrapper .bp3-menu-header>h6{color:#182026;font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;margin:0;padding:10px 7px 0 1px;line-height:17px}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-large .bp3-menu-header>h6{padding-top:15px;padding-bottom:5px;font-size:18px}.jupyter-wrapper .bp3-large .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-dark .bp3-menu{background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item .bp3-menu-item-label{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item:active{background-color:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-divider,.jupyter-wrapper .bp3-dark .bp3-menu-header{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-label .bp3-menu{margin-top:5px}.jupyter-wrapper .bp3-navbar{position:relative;z-index:10;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background-color:#fff;width:100%;height:50px;padding:0 15px}.jupyter-wrapper .bp3-navbar.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-navbar{background-color:#394b59}.jupyter-wrapper .bp3-navbar.bp3-dark{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-navbar{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-navbar.bp3-fixed-top{position:fixed;top:0;right:0;left:0}.jupyter-wrapper .bp3-navbar-heading{margin-right:15px;font-size:16px}.jupyter-wrapper .bp3-navbar-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:50px}.jupyter-wrapper .bp3-navbar-group.bp3-align-left{float:left}.jupyter-wrapper .bp3-navbar-group.bp3-align-right{float:right}.jupyter-wrapper .bp3-navbar-divider{margin:0 10px;border-left:1px solid rgba(16,22,26,.15);height:20px}.jupyter-wrapper .bp3-dark .bp3-navbar-divider{border-left-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-non-ideal-state{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%;text-align:center}.jupyter-wrapper .bp3-non-ideal-state>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-non-ideal-state>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-non-ideal-state::before,.jupyter-wrapper .bp3-non-ideal-state>*{margin-bottom:20px}.jupyter-wrapper .bp3-non-ideal-state:empty::before,.jupyter-wrapper .bp3-non-ideal-state>:last-child{margin-bottom:0}.jupyter-wrapper .bp3-non-ideal-state>*{max-width:400px}.jupyter-wrapper .bp3-non-ideal-state-visual{color:rgba(92,112,128,.6);font-size:60px}.jupyter-wrapper .bp3-dark .bp3-non-ideal-state-visual{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-overflow-list{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;min-width:0}.jupyter-wrapper .bp3-overflow-list-spacer{-ms-flex-negative:1;flex-shrink:1;width:1px}.jupyter-wrapper body.bp3-overlay-open{overflow:hidden}.jupyter-wrapper .bp3-overlay{position:static;top:0;right:0;bottom:0;left:0;z-index:20}.jupyter-wrapper .bp3-overlay:not(.bp3-overlay-open){pointer-events:none}.jupyter-wrapper .bp3-overlay.bp3-overlay-container{position:fixed;overflow:hidden}.jupyter-wrapper .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container{position:fixed;overflow:auto}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-inline{display:inline;overflow:visible}.jupyter-wrapper .bp3-overlay-content{position:fixed;z-index:20}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-content,.jupyter-wrapper .bp3-overlay-scroll-container .bp3-overlay-content{position:absolute}.jupyter-wrapper .bp3-overlay-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;opacity:1;z-index:20;background-color:rgba(16,22,26,.7);overflow:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear{opacity:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter-active,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit{opacity:1}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop:focus{outline:none}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-backdrop{position:absolute}.jupyter-wrapper .bp3-panel-stack{position:relative;overflow:hidden}.jupyter-wrapper .bp3-panel-stack-header{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-negative:0;flex-shrink:0;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:1;-webkit-box-shadow:0 1px rgba(16,22,26,.15);box-shadow:0 1px rgba(16,22,26,.15);height:30px}.jupyter-wrapper .bp3-dark .bp3-panel-stack-header{-webkit-box-shadow:0 1px rgba(255,255,255,.15);box-shadow:0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-panel-stack-header>span{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1;flex:1;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-panel-stack-header .bp3-heading{margin:0 5px}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back{margin-left:5px;padding-left:0;white-space:nowrap}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back .bp3-icon{margin:0 2px}.jupyter-wrapper .bp3-panel-stack-view{position:absolute;top:0;right:0;bottom:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-right:-1px;border-right:1px solid rgba(16,22,26,.15);background-color:#fff;overflow-y:auto}.jupyter-wrapper .bp3-dark .bp3-panel-stack-view{background-color:#30404d}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit-active{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1);display:inline-block;z-index:20;border-radius:3px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow{position:absolute;width:30px;height:30px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{margin:5px;width:20px;height:20px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover{margin-top:-17px;margin-bottom:17px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{bottom:-11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover{margin-left:17px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{left:-11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover{margin-top:17px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{top:-11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover{margin-right:17px;margin-left:-17px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{right:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-popover>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-popover>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{top:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{right:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{left:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{bottom:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-popover .bp3-popover-content{background:#fff;color:inherit}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-fill{fill:#fff}.jupyter-wrapper .bp3-popover-enter>.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover .bp3-popover-content{position:relative;border-radius:3px}.jupyter-wrapper .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{max-width:350px;padding:20px}.jupyter-wrapper .bp3-popover-target+.bp3-overlay .bp3-popover.bp3-popover-content-sizing{width:350px}.jupyter-wrapper .bp3-popover.bp3-minimal{margin:0 !important}.jupyter-wrapper .bp3-popover.bp3-minimal .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-content{background:#30404d;color:inherit}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-fill{fill:#30404d}.jupyter-wrapper .bp3-popover-arrow::before{display:block;position:absolute;-webkit-transform:rotate(45deg);transform:rotate(45deg);border-radius:2px;content:\"\"}.jupyter-wrapper .bp3-tether-pinned .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover-backdrop{background:rgba(255,255,255,0)}.jupyter-wrapper .bp3-transition-container{opacity:1;display:-webkit-box;display:-ms-flexbox;display:flex;z-index:20}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear{opacity:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter-active,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit{opacity:1}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container:focus{outline:none}.jupyter-wrapper .bp3-transition-container.bp3-popover-leave .bp3-popover-content{pointer-events:none}.jupyter-wrapper .bp3-transition-container[data-x-out-of-boundaries]{display:none}.jupyter-wrapper span.bp3-popover-target{display:inline-block}.jupyter-wrapper .bp3-popover-wrapper.bp3-fill{width:100%}.jupyter-wrapper .bp3-portal{position:absolute;top:0;right:0;left:0}@-webkit-keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}@keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}.jupyter-wrapper .bp3-progress-bar{display:block;position:relative;border-radius:40px;background:rgba(92,112,128,.2);width:100%;height:8px;overflow:hidden}.jupyter-wrapper .bp3-progress-bar .bp3-progress-meter{position:absolute;border-radius:40px;background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);background-color:rgba(92,112,128,.8);background-size:30px 30px;width:100%;height:100%;-webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{animation:linear-progress-bar-stripes 300ms linear infinite reverse}.jupyter-wrapper .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{background-image:none}.jupyter-wrapper .bp3-dark .bp3-progress-bar{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-progress-bar .bp3-progress-meter{background-color:#8a9ba8}.jupyter-wrapper .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{background-color:#137cbd}.jupyter-wrapper .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{background-color:#0f9960}.jupyter-wrapper .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{background-color:#d9822b}.jupyter-wrapper .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{background-color:#db3737}@-webkit-keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}@keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}.jupyter-wrapper .bp3-skeleton{border-color:rgba(206,217,224,.2) !important;border-radius:2px;-webkit-box-shadow:none !important;box-shadow:none !important;background:rgba(206,217,224,.2);background-clip:padding-box !important;cursor:default;color:rgba(0,0,0,0) !important;-webkit-animation:1000ms linear infinite alternate skeleton-glow;animation:1000ms linear infinite alternate skeleton-glow;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-skeleton::before,.jupyter-wrapper .bp3-skeleton::after,.jupyter-wrapper .bp3-skeleton *{visibility:hidden !important}.jupyter-wrapper .bp3-slider{width:100%;min-width:150px;height:40px;position:relative;outline:none;cursor:default;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-slider:hover{cursor:pointer}.jupyter-wrapper .bp3-slider:active{cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-slider.bp3-disabled{opacity:.5;cursor:not-allowed}.jupyter-wrapper .bp3-slider.bp3-slider-unlabeled{height:16px}.jupyter-wrapper .bp3-slider-track,.jupyter-wrapper .bp3-slider-progress{top:5px;right:0;left:0;height:6px;position:absolute}.jupyter-wrapper .bp3-slider-track{border-radius:3px;overflow:hidden}.jupyter-wrapper .bp3-slider-progress{background:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-dark .bp3-slider-progress{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-slider-progress.bp3-intent-primary{background-color:#137cbd}.jupyter-wrapper .bp3-slider-progress.bp3-intent-success{background-color:#0f9960}.jupyter-wrapper .bp3-slider-progress.bp3-intent-warning{background-color:#d9822b}.jupyter-wrapper .bp3-slider-progress.bp3-intent-danger{background-color:#db3737}.jupyter-wrapper .bp3-slider-handle{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;position:absolute;top:0;left:0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:pointer;width:16px;height:16px}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-slider-handle:active,.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-slider-handle.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active:hover,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-slider-handle:focus{z-index:1}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5;z-index:2;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:-webkit-grab;cursor:grab}.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-disabled .bp3-slider-handle{-webkit-box-shadow:none;box-shadow:none;background:#bfccd6;pointer-events:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover,.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-slider-handle,.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{background-color:#394b59}.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{background-color:#293742}.jupyter-wrapper .bp3-dark .bp3-disabled .bp3-slider-handle{border-color:#5c7080;-webkit-box-shadow:none;box-shadow:none;background:#5c7080}.jupyter-wrapper .bp3-slider-handle .bp3-slider-label{margin-left:8px;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-disabled .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-slider-handle.bp3-start,.jupyter-wrapper .bp3-slider-handle.bp3-end{width:8px}.jupyter-wrapper .bp3-slider-handle.bp3-start{border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end{margin-left:8px;border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end .bp3-slider-label{margin-left:0}.jupyter-wrapper .bp3-slider-label{-webkit-transform:translate(-50%, 20px);transform:translate(-50%, 20px);display:inline-block;position:absolute;padding:2px 5px;vertical-align:top;line-height:1;font-size:12px}.jupyter-wrapper .bp3-slider.bp3-vertical{width:40px;min-width:40px;height:150px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-track,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:0;bottom:0;left:5px;width:6px;height:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-label{-webkit-transform:translate(20px, 50%);transform:translate(20px, 50%)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{margin-top:-8px;margin-left:0}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{margin-left:0;width:16px;height:8px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{border-top-left-radius:0;border-bottom-right-radius:3px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{-webkit-transform:translate(20px);transform:translate(20px)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{margin-bottom:8px;border-top-left-radius:3px;border-bottom-left-radius:0;border-bottom-right-radius:0}@-webkit-keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.jupyter-wrapper .bp3-spinner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;overflow:visible;vertical-align:middle}.jupyter-wrapper .bp3-spinner svg{display:block}.jupyter-wrapper .bp3-spinner path{fill-opacity:0}.jupyter-wrapper .bp3-spinner .bp3-spinner-head{-webkit-transform-origin:center;transform-origin:center;-webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);stroke:rgba(92,112,128,.8);stroke-linecap:round}.jupyter-wrapper .bp3-spinner .bp3-spinner-track{stroke:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-spinner-animation{-webkit-animation:pt-spinner-animation 500ms linear infinite;animation:pt-spinner-animation 500ms linear infinite}.jupyter-wrapper .bp3-no-spin>.bp3-spinner-animation{-webkit-animation:none;animation:none}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-track{stroke:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-spinner.bp3-intent-primary .bp3-spinner-head{stroke:#137cbd}.jupyter-wrapper .bp3-spinner.bp3-intent-success .bp3-spinner-head{stroke:#0f9960}.jupyter-wrapper .bp3-spinner.bp3-intent-warning .bp3-spinner-head{stroke:#d9822b}.jupyter-wrapper .bp3-spinner.bp3-intent-danger .bp3-spinner-head{stroke:#db3737}.jupyter-wrapper .bp3-tabs.bp3-vertical{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab{border-radius:3px;width:100%;padding:0 10px}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab[aria-selected=true]{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.2)}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{top:0;right:0;bottom:0;left:0;border-radius:3px;background-color:rgba(19,124,189,.2);height:auto}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-panel{margin-top:0;padding-left:20px}.jupyter-wrapper .bp3-tab-list{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:relative;margin:0;border:none;padding:0;list-style:none}.jupyter-wrapper .bp3-tab-list>*:not(:last-child){margin-right:20px}.jupyter-wrapper .bp3-tab{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;cursor:pointer;max-width:100%;vertical-align:top;line-height:30px;color:#182026;font-size:14px}.jupyter-wrapper .bp3-tab a{display:block;text-decoration:none;color:inherit}.jupyter-wrapper .bp3-tab-indicator-wrapper~.bp3-tab{-webkit-box-shadow:none !important;box-shadow:none !important;background-color:rgba(0,0,0,0) !important}.jupyter-wrapper .bp3-tab[aria-disabled=true]{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tab[aria-selected=true]{border-radius:0;-webkit-box-shadow:inset 0 -3px 0 #106ba3;box-shadow:inset 0 -3px 0 #106ba3}.jupyter-wrapper .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-tab:not([aria-disabled=true]):hover{color:#106ba3}.jupyter-wrapper .bp3-tab:focus{-moz-outline-radius:0}.jupyter-wrapper .bp3-large>.bp3-tab{line-height:40px;font-size:16px}.jupyter-wrapper .bp3-tab-panel{margin-top:20px}.jupyter-wrapper .bp3-tab-panel[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-tab-indicator-wrapper{position:absolute;top:0;left:0;-webkit-transform:translateX(0),translateY(0);transform:translateX(0),translateY(0);-webkit-transition:height,width,-webkit-transform;transition:height,width,-webkit-transform;transition:height,transform,width;transition:height,transform,width,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);pointer-events:none}.jupyter-wrapper .bp3-tab-indicator-wrapper .bp3-tab-indicator{position:absolute;right:0;bottom:0;left:0;background-color:#106ba3;height:3px}.jupyter-wrapper .bp3-tab-indicator-wrapper.bp3-no-animation{-webkit-transition:none;transition:none}.jupyter-wrapper .bp3-dark .bp3-tab{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tab[aria-disabled=true]{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true]{-webkit-box-shadow:inset 0 -3px 0 #48aff0;box-shadow:inset 0 -3px 0 #48aff0}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-dark .bp3-tab:not([aria-disabled=true]):hover{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tab-indicator{background-color:#48aff0}.jupyter-wrapper .bp3-flex-expander{-webkit-box-flex:1;-ms-flex:1 1;flex:1 1}.jupyter-wrapper .bp3-tag{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border:none;border-radius:3px;-webkit-box-shadow:none;box-shadow:none;background-color:#5c7080;min-width:20px;max-width:100%;min-height:20px;padding:2px 6px;line-height:16px;color:#f5f8fa;font-size:12px}.jupyter-wrapper .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-interactive:hover{background-color:rgba(92,112,128,.85)}.jupyter-wrapper .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-interactive:active{background-color:rgba(92,112,128,.7)}.jupyter-wrapper .bp3-tag>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag::before,.jupyter-wrapper .bp3-tag>*{margin-right:4px}.jupyter-wrapper .bp3-tag:empty::before,.jupyter-wrapper .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag:focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag.bp3-round{border-radius:30px;padding-right:8px;padding-left:8px}.jupyter-wrapper .bp3-dark .bp3-tag{background-color:#bfccd6;color:#182026}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:hover{background-color:rgba(191,204,214,.85)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:active{background-color:rgba(191,204,214,.7)}.jupyter-wrapper .bp3-dark .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-large{fill:currentColor}.jupyter-wrapper .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-tag .bp3-icon-large{fill:#fff}.jupyter-wrapper .bp3-tag.bp3-large,.jupyter-wrapper .bp3-large .bp3-tag{min-width:30px;min-height:30px;padding:0 10px;line-height:20px;font-size:14px}.jupyter-wrapper .bp3-tag.bp3-large::before,.jupyter-wrapper .bp3-tag.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-tag::before,.jupyter-wrapper .bp3-large .bp3-tag>*{margin-right:7px}.jupyter-wrapper .bp3-tag.bp3-large:empty::before,.jupyter-wrapper .bp3-tag.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-tag:empty::before,.jupyter-wrapper .bp3-large .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag.bp3-large.bp3-round,.jupyter-wrapper .bp3-large .bp3-tag.bp3-round{padding-right:12px;padding-left:12px}.jupyter-wrapper .bp3-tag.bp3-intent-primary{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-success{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-warning{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-danger{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.7)}.jupyter-wrapper .bp3-tag.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-tag.bp3-minimal>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-large{fill:#5c7080}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){background-color:rgba(138,155,168,.2);color:#182026}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(191,204,214,.3)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-])>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-large{fill:#a7b6c2}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{fill:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.25);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{fill:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.25);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{fill:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.25);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{fill:#db3737}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.25);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.45)}.jupyter-wrapper .bp3-tag-remove{display:-webkit-box;display:-ms-flexbox;display:flex;opacity:.5;margin-top:-2px;margin-right:-6px !important;margin-bottom:-2px;border:none;background:none;cursor:pointer;padding:2px;padding-left:0;color:inherit}.jupyter-wrapper .bp3-tag-remove:hover{opacity:.8;background:none;text-decoration:none}.jupyter-wrapper .bp3-tag-remove:active{opacity:1}.jupyter-wrapper .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6d7\"}.jupyter-wrapper .bp3-large .bp3-tag-remove{margin-right:-10px !important;padding:5px;padding-left:0}.jupyter-wrapper .bp3-large .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal}.jupyter-wrapper .bp3-tag-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;cursor:text;height:auto;min-height:30px;padding-right:0;padding-left:5px;line-height:inherit}.jupyter-wrapper .bp3-tag-input>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input>.bp3-tag-input-values{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-icon{margin-top:7px;margin-right:7px;margin-left:2px;color:#5c7080}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-item-align:stretch;align-self:stretch;margin-top:5px;margin-right:7px;min-width:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-right:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:empty::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{padding-left:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-bottom:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag{overflow-wrap:break-word}.jupyter-wrapper .bp3-tag-input .bp3-tag.bp3-active{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:80px;line-height:20px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost:disabled,.jupyter-wrapper .bp3-tag-input .bp3-input-ghost.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-tag-input .bp3-button,.jupyter-wrapper .bp3-tag-input .bp3-spinner{margin:3px;margin-left:0}.jupyter-wrapper .bp3-tag-input .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-tag-input.bp3-large{height:auto;min-height:40px}.jupyter-wrapper .bp3-tag-input.bp3-large::before,.jupyter-wrapper .bp3-tag-input.bp3-large>*{margin-right:10px}.jupyter-wrapper .bp3-tag-input.bp3-large:empty::before,.jupyter-wrapper .bp3-tag-input.bp3-large>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-tag-input-icon{margin-top:10px;margin-left:5px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-input-ghost{line-height:30px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-button{min-width:30px;min-height:30px;padding:5px 10px;margin:5px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-spinner{margin:8px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-tag-input-icon,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-tag-input-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-input-ghost{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;padding:0}.jupyter-wrapper .bp3-input-ghost::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:focus{outline:none !important}.jupyter-wrapper .bp3-toast{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;position:relative !important;margin:20px 0 0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background-color:#fff;min-width:300px;max-width:500px;pointer-events:all}.jupyter-wrapper .bp3-toast.bp3-toast-enter,.jupyter-wrapper .bp3-toast.bp3-toast-appear{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-enter~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit{opacity:1;-webkit-filter:blur(0);filter:blur(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active{opacity:0;-webkit-filter:blur(10px);filter:blur(10px);-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:opacity,filter;transition-property:opacity,filter,-webkit-filter;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:50ms;transition-delay:50ms}.jupyter-wrapper .bp3-toast .bp3-button-group{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;padding-left:0}.jupyter-wrapper .bp3-toast>.bp3-icon{margin:12px;margin-right:0;color:#5c7080}.jupyter-wrapper .bp3-toast.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-toast{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background-color:#394b59}.jupyter-wrapper .bp3-toast.bp3-dark>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-toast>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a:hover{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-]>.bp3-icon{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::before,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button .bp3-icon,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{color:rgba(255,255,255,.7) !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:focus{outline-color:rgba(255,255,255,.5)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:hover{background-color:rgba(255,255,255,.15) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{background-color:rgba(255,255,255,.3) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::after{background:rgba(255,255,255,.3) !important}.jupyter-wrapper .bp3-toast.bp3-intent-primary{background-color:#137cbd;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-success{background-color:#0f9960;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-warning{background-color:#d9822b;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-danger{background-color:#db3737;color:#fff}.jupyter-wrapper .bp3-toast-message{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;padding:11px;word-break:break-word}.jupyter-wrapper .bp3-toast-container{display:-webkit-box !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;right:0;left:0;z-index:40;overflow:hidden;padding:0 20px 20px;pointer-events:none}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-top{top:0;bottom:auto}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-bottom{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;top:auto;bottom:0}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-left{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-right{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active~.bp3-toast{-webkit-transform:translateY(60px);transform:translateY(60px)}.jupyter-wrapper .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow{position:absolute;width:22px;height:22px}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{margin:4px;width:14px;height:14px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip{margin-top:-11px;margin-bottom:11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{bottom:-8px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip{margin-left:11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{left:-8px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip{margin-top:11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{top:-8px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip{margin-right:11px;margin-left:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{right:-8px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-tooltip>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-tooltip>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{top:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{right:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{left:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{bottom:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-fill{fill:#394b59}.jupyter-wrapper .bp3-popover-enter>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear-active>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{padding:10px 12px}.jupyter-wrapper .bp3-tooltip.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-content{background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{fill:#e1e8ed}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-content{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{fill:#137cbd}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-content{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{fill:#0f9960}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-content{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{fill:#d9822b}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-content{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{fill:#db3737}.jupyter-wrapper .bp3-tooltip-indicator{border-bottom:dotted 1px;cursor:help}.jupyter-wrapper .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-tree .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-tree-node-list{margin:0;padding-left:0;list-style:none}.jupyter-wrapper .bp3-tree-root{position:relative;background-color:rgba(0,0,0,0);cursor:default;padding-left:0}.jupyter-wrapper .bp3-tree-node-content-0{padding-left:0px}.jupyter-wrapper .bp3-tree-node-content-1{padding-left:23px}.jupyter-wrapper .bp3-tree-node-content-2{padding-left:46px}.jupyter-wrapper .bp3-tree-node-content-3{padding-left:69px}.jupyter-wrapper .bp3-tree-node-content-4{padding-left:92px}.jupyter-wrapper .bp3-tree-node-content-5{padding-left:115px}.jupyter-wrapper .bp3-tree-node-content-6{padding-left:138px}.jupyter-wrapper .bp3-tree-node-content-7{padding-left:161px}.jupyter-wrapper .bp3-tree-node-content-8{padding-left:184px}.jupyter-wrapper .bp3-tree-node-content-9{padding-left:207px}.jupyter-wrapper .bp3-tree-node-content-10{padding-left:230px}.jupyter-wrapper .bp3-tree-node-content-11{padding-left:253px}.jupyter-wrapper .bp3-tree-node-content-12{padding-left:276px}.jupyter-wrapper .bp3-tree-node-content-13{padding-left:299px}.jupyter-wrapper .bp3-tree-node-content-14{padding-left:322px}.jupyter-wrapper .bp3-tree-node-content-15{padding-left:345px}.jupyter-wrapper .bp3-tree-node-content-16{padding-left:368px}.jupyter-wrapper .bp3-tree-node-content-17{padding-left:391px}.jupyter-wrapper .bp3-tree-node-content-18{padding-left:414px}.jupyter-wrapper .bp3-tree-node-content-19{padding-left:437px}.jupyter-wrapper .bp3-tree-node-content-20{padding-left:460px}.jupyter-wrapper .bp3-tree-node-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:30px;padding-right:5px}.jupyter-wrapper .bp3-tree-node-content:hover{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node-caret-none{min-width:30px}.jupyter-wrapper .bp3-tree-node-caret{color:#5c7080;-webkit-transform:rotate(0deg);transform:rotate(0deg);cursor:pointer;padding:7px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-tree-node-caret:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret:hover{color:#f5f8fa}.jupyter-wrapper .bp3-tree-node-caret.bp3-tree-node-caret-open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tree-node-caret.bp3-icon-standard::before{content:\"\ue695\"}.jupyter-wrapper .bp3-tree-node-icon{position:relative;margin-right:7px}.jupyter-wrapper .bp3-tree-node-label{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-label span{display:inline}.jupyter-wrapper .bp3-tree-node-secondary-label{padding:0 5px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-wrapper,.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-content{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-standard,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-large{color:#fff}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret::before{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret:hover::before{color:#fff}.jupyter-wrapper .bp3-dark .bp3-tree-node-content:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-dark .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-omnibar{-webkit-filter:blur(0);filter:blur(0);opacity:1;top:20vh;left:calc(50% - 250px);z-index:21;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background-color:#fff;width:500px}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter-active,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear-active{-webkit-filter:blur(0);filter:blur(0);opacity:1;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit{-webkit-filter:blur(0);filter:blur(0);opacity:1}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit-active{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar .bp3-input{border-radius:0;background-color:rgba(0,0,0,0)}.jupyter-wrapper .bp3-omnibar .bp3-input,.jupyter-wrapper .bp3-omnibar .bp3-input:focus{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-omnibar .bp3-menu{border-radius:0;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);background-color:rgba(0,0,0,0);max-height:calc(60vh - 40px);overflow:auto}.jupyter-wrapper .bp3-omnibar .bp3-menu:empty{display:none}.jupyter-wrapper .bp3-dark .bp3-omnibar,.jupyter-wrapper .bp3-omnibar.bp3-dark{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-omnibar-overlay .bp3-overlay-backdrop{background-color:rgba(16,22,26,.2)}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper .bp3-multi-select{min-width:150px}.jupyter-wrapper .bp3-multi-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper :root{--jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-build: url(data:image/svg+xml;base64,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);--jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);--jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);--jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);--jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);--jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);--jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter: url(data:image/svg+xml;base64,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);--jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);--jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);--jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);--jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);--jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);--jp-icon-listings-info: url(data:image/svg+xml;base64,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);--jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);--jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);--jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);--jp-icon-react: url(data:image/svg+xml;base64,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);--jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);--jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);--jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-terminal: url(data:image/svg+xml;base64,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);--jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);--jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);--jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K)}.jupyter-wrapper .jp-AddIcon{background-image:var(--jp-icon-add)}.jupyter-wrapper .jp-BugIcon{background-image:var(--jp-icon-bug)}.jupyter-wrapper .jp-BuildIcon{background-image:var(--jp-icon-build)}.jupyter-wrapper .jp-CaretDownEmptyIcon{background-image:var(--jp-icon-caret-down-empty)}.jupyter-wrapper .jp-CaretDownEmptyThinIcon{background-image:var(--jp-icon-caret-down-empty-thin)}.jupyter-wrapper .jp-CaretDownIcon{background-image:var(--jp-icon-caret-down)}.jupyter-wrapper .jp-CaretLeftIcon{background-image:var(--jp-icon-caret-left)}.jupyter-wrapper .jp-CaretRightIcon{background-image:var(--jp-icon-caret-right)}.jupyter-wrapper .jp-CaretUpEmptyThinIcon{background-image:var(--jp-icon-caret-up-empty-thin)}.jupyter-wrapper .jp-CaretUpIcon{background-image:var(--jp-icon-caret-up)}.jupyter-wrapper .jp-CaseSensitiveIcon{background-image:var(--jp-icon-case-sensitive)}.jupyter-wrapper .jp-CheckIcon{background-image:var(--jp-icon-check)}.jupyter-wrapper .jp-CircleEmptyIcon{background-image:var(--jp-icon-circle-empty)}.jupyter-wrapper .jp-CircleIcon{background-image:var(--jp-icon-circle)}.jupyter-wrapper .jp-ClearIcon{background-image:var(--jp-icon-clear)}.jupyter-wrapper .jp-CloseIcon{background-image:var(--jp-icon-close)}.jupyter-wrapper .jp-ConsoleIcon{background-image:var(--jp-icon-console)}.jupyter-wrapper .jp-CopyIcon{background-image:var(--jp-icon-copy)}.jupyter-wrapper .jp-CutIcon{background-image:var(--jp-icon-cut)}.jupyter-wrapper .jp-DownloadIcon{background-image:var(--jp-icon-download)}.jupyter-wrapper .jp-EditIcon{background-image:var(--jp-icon-edit)}.jupyter-wrapper .jp-EllipsesIcon{background-image:var(--jp-icon-ellipses)}.jupyter-wrapper .jp-ExtensionIcon{background-image:var(--jp-icon-extension)}.jupyter-wrapper .jp-FastForwardIcon{background-image:var(--jp-icon-fast-forward)}.jupyter-wrapper .jp-FileIcon{background-image:var(--jp-icon-file)}.jupyter-wrapper .jp-FileUploadIcon{background-image:var(--jp-icon-file-upload)}.jupyter-wrapper .jp-FilterListIcon{background-image:var(--jp-icon-filter-list)}.jupyter-wrapper .jp-FolderIcon{background-image:var(--jp-icon-folder)}.jupyter-wrapper .jp-Html5Icon{background-image:var(--jp-icon-html5)}.jupyter-wrapper .jp-ImageIcon{background-image:var(--jp-icon-image)}.jupyter-wrapper .jp-InspectorIcon{background-image:var(--jp-icon-inspector)}.jupyter-wrapper .jp-JsonIcon{background-image:var(--jp-icon-json)}.jupyter-wrapper .jp-JupyterFaviconIcon{background-image:var(--jp-icon-jupyter-favicon)}.jupyter-wrapper .jp-JupyterIcon{background-image:var(--jp-icon-jupyter)}.jupyter-wrapper .jp-JupyterlabWordmarkIcon{background-image:var(--jp-icon-jupyterlab-wordmark)}.jupyter-wrapper .jp-KernelIcon{background-image:var(--jp-icon-kernel)}.jupyter-wrapper .jp-KeyboardIcon{background-image:var(--jp-icon-keyboard)}.jupyter-wrapper .jp-LauncherIcon{background-image:var(--jp-icon-launcher)}.jupyter-wrapper .jp-LineFormIcon{background-image:var(--jp-icon-line-form)}.jupyter-wrapper .jp-LinkIcon{background-image:var(--jp-icon-link)}.jupyter-wrapper .jp-ListIcon{background-image:var(--jp-icon-list)}.jupyter-wrapper .jp-ListingsInfoIcon{background-image:var(--jp-icon-listings-info)}.jupyter-wrapper .jp-MarkdownIcon{background-image:var(--jp-icon-markdown)}.jupyter-wrapper .jp-NewFolderIcon{background-image:var(--jp-icon-new-folder)}.jupyter-wrapper .jp-NotTrustedIcon{background-image:var(--jp-icon-not-trusted)}.jupyter-wrapper .jp-NotebookIcon{background-image:var(--jp-icon-notebook)}.jupyter-wrapper .jp-PaletteIcon{background-image:var(--jp-icon-palette)}.jupyter-wrapper .jp-PasteIcon{background-image:var(--jp-icon-paste)}.jupyter-wrapper .jp-PythonIcon{background-image:var(--jp-icon-python)}.jupyter-wrapper .jp-RKernelIcon{background-image:var(--jp-icon-r-kernel)}.jupyter-wrapper .jp-ReactIcon{background-image:var(--jp-icon-react)}.jupyter-wrapper .jp-RefreshIcon{background-image:var(--jp-icon-refresh)}.jupyter-wrapper .jp-RegexIcon{background-image:var(--jp-icon-regex)}.jupyter-wrapper .jp-RunIcon{background-image:var(--jp-icon-run)}.jupyter-wrapper .jp-RunningIcon{background-image:var(--jp-icon-running)}.jupyter-wrapper .jp-SaveIcon{background-image:var(--jp-icon-save)}.jupyter-wrapper .jp-SearchIcon{background-image:var(--jp-icon-search)}.jupyter-wrapper .jp-SettingsIcon{background-image:var(--jp-icon-settings)}.jupyter-wrapper .jp-SpreadsheetIcon{background-image:var(--jp-icon-spreadsheet)}.jupyter-wrapper .jp-StopIcon{background-image:var(--jp-icon-stop)}.jupyter-wrapper .jp-TabIcon{background-image:var(--jp-icon-tab)}.jupyter-wrapper .jp-TerminalIcon{background-image:var(--jp-icon-terminal)}.jupyter-wrapper .jp-TextEditorIcon{background-image:var(--jp-icon-text-editor)}.jupyter-wrapper .jp-TrustedIcon{background-image:var(--jp-icon-trusted)}.jupyter-wrapper .jp-UndoIcon{background-image:var(--jp-icon-undo)}.jupyter-wrapper .jp-VegaIcon{background-image:var(--jp-icon-vega)}.jupyter-wrapper .jp-YamlIcon{background-image:var(--jp-icon-yaml)}.jupyter-wrapper :root{--jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==)}.jupyter-wrapper .jp-Icon,.jupyter-wrapper .jp-MaterialIcon{background-position:center;background-repeat:no-repeat;background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-cover{background-position:center;background-repeat:no-repeat;background-size:cover}.jupyter-wrapper .jp-Icon-16{background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-18{background-size:18px;min-width:18px;min-height:18px}.jupyter-wrapper .jp-Icon-20{background-size:20px;min-width:20px;min-height:20px}.jupyter-wrapper .jp-icon0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-accent0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-accent0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-none[fill]{fill:none}.jupyter-wrapper .jp-icon-none[stroke]{stroke:none}.jupyter-wrapper .jp-icon-brand0[fill]{fill:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[fill]{fill:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[fill]{fill:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[fill]{fill:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-brand0[stroke]{stroke:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[stroke]{stroke:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[stroke]{stroke:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[stroke]{stroke:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[stroke]{stroke:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-warn0[fill]{fill:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[fill]{fill:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[fill]{fill:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[fill]{fill:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-warn0[stroke]{stroke:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[stroke]{stroke:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[stroke]{stroke:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[stroke]{stroke:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-contrast0[fill]{fill:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[fill]{fill:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[fill]{fill:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[fill]{fill:var(--jp-icon-contrast-color3)}.jupyter-wrapper .jp-icon-contrast0[stroke]{stroke:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[stroke]{stroke:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[stroke]{stroke:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[stroke]{stroke:var(--jp-icon-contrast-color3)}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:#fff}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper :root{--jp-warn-color0: var(--md-orange-700)}.jupyter-wrapper .jp-DragIcon{margin-right:4px}.jupyter-wrapper .jp-icon-alt .jp-icon0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hoverShow:not(:hover) svg{display:none !important}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[fill]{fill:none}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[stroke]{stroke:none}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper :focus{outline:unset;outline-offset:unset;-moz-outline-radius:unset}.jupyter-wrapper .jp-Button{border-radius:var(--jp-border-radius);padding:0px 12px;font-size:var(--jp-ui-font-size1)}.jupyter-wrapper button.jp-Button.bp3-button.bp3-minimal:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Button.minimal{color:unset !important}.jupyter-wrapper .jp-Button.jp-ToolbarButtonComponent{text-transform:none}.jupyter-wrapper .jp-InputGroup input{box-sizing:border-box;border-radius:0;background-color:rgba(0,0,0,0);color:var(--jp-ui-font-color0);box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .jp-InputGroup input:focus{box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .jp-InputGroup input::placeholder,.jupyter-wrapper input::placeholder{color:var(--jp-ui-font-color3)}.jupyter-wrapper .jp-BPIcon{display:inline-block;vertical-align:middle;margin:auto}.jupyter-wrapper .bp3-icon.jp-BPIcon>svg:not([fill]){fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-InputGroupAction{padding:6px}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select{background-color:initial;border:none;border-radius:0;box-shadow:none;color:var(--jp-ui-font-color0);display:block;font-size:var(--jp-ui-font-size1);height:24px;line-height:14px;padding:0 25px 0 10px;text-align:left;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select:hover,.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select>option{background-color:var(--jp-layout-color2);color:var(--jp-ui-font-color0)}.jupyter-wrapper select{box-sizing:border-box}.jupyter-wrapper .jp-Collapse{display:flex;flex-direction:column;align-items:stretch;border-top:1px solid var(--jp-border-color2);border-bottom:1px solid var(--jp-border-color2)}.jupyter-wrapper .jp-Collapse-header{padding:1px 12px;color:var(--jp-ui-font-color1);background-color:var(--jp-layout-color1);font-size:var(--jp-ui-font-size2)}.jupyter-wrapper .jp-Collapse-header:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Collapse-contents{padding:0px 12px 0px 12px;background-color:var(--jp-layout-color1);color:var(--jp-ui-font-color1);overflow:auto}.jupyter-wrapper :root{--jp-private-commandpalette-search-height: 28px}.jupyter-wrapper .lm-CommandPalette{padding-bottom:0px;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-search{padding:4px;background-color:var(--jp-layout-color1);z-index:2}.jupyter-wrapper .lm-CommandPalette-wrapper{overflow:overlay;padding:0px 9px;background-color:var(--jp-input-active-background);height:30px;box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper{box-shadow:inset 0 0 0 1px var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .lm-CommandPalette-wrapper::after{content:\" \";color:#fff;background-color:var(--jp-brand-color1);position:absolute;top:4px;right:4px;height:30px;width:10px;padding:0px 10px;background-image:var(--jp-icon-search-white);background-size:20px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .lm-CommandPalette-input{background:rgba(0,0,0,0);width:calc(100% - 18px);float:left;border:none;outline:none;font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);line-height:var(--jp-private-commandpalette-search-height)}.jupyter-wrapper .lm-CommandPalette-input::-webkit-input-placeholder,.jupyter-wrapper .lm-CommandPalette-input::-moz-placeholder,.jupyter-wrapper .lm-CommandPalette-input:-ms-input-placeholder{color:var(--jp-ui-font-color3);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-header:first-child{margin-top:0px}.jupyter-wrapper .lm-CommandPalette-header{border-bottom:solid var(--jp-border-width) var(--jp-border-color2);color:var(--jp-ui-font-color1);cursor:pointer;display:flex;font-size:var(--jp-ui-font-size0);font-weight:600;letter-spacing:1px;margin-top:8px;padding:8px 0 8px 12px;text-transform:uppercase}.jupyter-wrapper .lm-CommandPalette-header.lm-mod-active{background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-header>mark{background-color:rgba(0,0,0,0);font-weight:bold;color:var(--jp-ui-font-color1)}.jupyter-wrapper .lm-CommandPalette-item{padding:4px 12px 4px 4px;color:var(--jp-ui-font-color1);font-size:var(--jp-ui-font-size1);font-weight:400;display:flex}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active{background:var(--jp-layout-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled){background:var(--jp-layout-color4)}.jupyter-wrapper .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled){background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-itemContent{overflow:hidden}.jupyter-wrapper .lm-CommandPalette-itemLabel>mark{color:var(--jp-ui-font-color0);background-color:rgba(0,0,0,0);font-weight:bold}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled mark{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemIcon{margin:0 4px 0 0;position:relative;width:16px;top:2px;flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon{opacity:.4}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-itemCaption{display:none}.jupyter-wrapper .lm-CommandPalette-content{background-color:var(--jp-layout-color1)}.jupyter-wrapper .lm-CommandPalette-content:empty:after{content:\"No results\";margin:auto;margin-top:20px;width:100px;display:block;font-size:var(--jp-ui-font-size2);font-family:var(--jp-ui-font-family);font-weight:lighter}.jupyter-wrapper .lm-CommandPalette-emptyMessage{text-align:center;margin-top:24px;line-height:1.32;padding:0px 8px;color:var(--jp-content-font-color3)}.jupyter-wrapper .jp-Dialog{position:absolute;z-index:10000;display:flex;flex-direction:column;align-items:center;justify-content:center;top:0px;left:0px;margin:0;padding:0;width:100%;height:100%;background:var(--jp-dialog-background)}.jupyter-wrapper .jp-Dialog-content{display:flex;flex-direction:column;margin-left:auto;margin-right:auto;background:var(--jp-layout-color1);padding:24px;padding-bottom:12px;min-width:300px;min-height:150px;max-width:1000px;max-height:500px;box-sizing:border-box;box-shadow:var(--jp-elevation-z20);word-wrap:break-word;border-radius:var(--jp-border-radius);font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color1)}.jupyter-wrapper .jp-Dialog-button{overflow:visible}.jupyter-wrapper button.jp-Dialog-button:focus{outline:1px solid var(--jp-brand-color1);outline-offset:4px;-moz-outline-radius:0px}.jupyter-wrapper button.jp-Dialog-button:focus::-moz-focus-inner{border:0}.jupyter-wrapper .jp-Dialog-header{flex:0 0 auto;padding-bottom:12px;font-size:var(--jp-ui-font-size3);font-weight:400;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-body{display:flex;flex-direction:column;flex:1 1 auto;font-size:var(--jp-ui-font-size1);background:var(--jp-layout-color1);overflow:auto}.jupyter-wrapper .jp-Dialog-footer{display:flex;flex-direction:row;justify-content:flex-end;flex:0 0 auto;margin-left:-12px;margin-right:-12px;padding:12px}.jupyter-wrapper .jp-Dialog-title{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .jp-Dialog-body>.jp-select-wrapper{width:100%}.jupyter-wrapper .jp-Dialog-body>button{padding:0px 16px}.jupyter-wrapper .jp-Dialog-body>label{line-height:1.4;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-button.jp-mod-styled:not(:last-child){margin-right:12px}.jupyter-wrapper .jp-HoverBox{position:fixed}.jupyter-wrapper .jp-HoverBox.jp-mod-outofview{display:none}.jupyter-wrapper .jp-IFrame{width:100%;height:100%}.jupyter-wrapper .jp-IFrame>iframe{border:none}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MainAreaWidget>:focus{outline:none}.jupyter-wrapper :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper .jp-Spinner{position:absolute;display:flex;justify-content:center;align-items:center;z-index:10;left:0;top:0;width:100%;height:100%;background:var(--jp-layout-color0);outline:none}.jupyter-wrapper .jp-SpinnerContent{font-size:10px;margin:50px auto;text-indent:-9999em;width:3em;height:3em;border-radius:50%;background:var(--jp-brand-color3);background:linear-gradient(to right, #f37626 10%, rgba(255, 255, 255, 0) 42%);position:relative;animation:load3 1s infinite linear,fadeIn 1s}.jupyter-wrapper .jp-SpinnerContent:before{width:50%;height:50%;background:#f37626;border-radius:100% 0 0 0;position:absolute;top:0;left:0;content:\"\"}.jupyter-wrapper .jp-SpinnerContent:after{background:var(--jp-layout-color0);width:75%;height:75%;border-radius:50%;content:\"\";margin:auto;position:absolute;top:0;left:0;bottom:0;right:0}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}@keyframes load3{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.jupyter-wrapper button.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:none;box-sizing:border-box;text-align:center;line-height:32px;height:32px;padding:0px 12px;letter-spacing:.8px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled{background:var(--jp-input-background);height:28px;box-sizing:border-box;border:var(--jp-border-width) solid var(--jp-border-color1);padding-left:7px;padding-right:7px;font-size:var(--jp-ui-font-size2);color:var(--jp-ui-font-color0);outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled:focus{border:var(--jp-border-width) solid var(--md-blue-500);box-shadow:inset 0 0 4px var(--md-blue-300)}.jupyter-wrapper .jp-select-wrapper{display:flex;position:relative;flex-direction:column;padding:1px;background-color:var(--jp-layout-color1);height:28px;box-sizing:border-box;margin-bottom:12px}.jupyter-wrapper .jp-select-wrapper.jp-mod-focused select.jp-mod-styled{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-input-active-background)}.jupyter-wrapper select.jp-mod-styled:hover{background-color:var(--jp-layout-color1);cursor:pointer;color:var(--jp-ui-font-color0);background-color:var(--jp-input-hover-background);box-shadow:inset 0 0px 1px rgba(0,0,0,.5)}.jupyter-wrapper select.jp-mod-styled{flex:1 1 auto;height:32px;width:100%;font-size:var(--jp-ui-font-size2);background:var(--jp-input-background);color:var(--jp-ui-font-color0);padding:0 25px 0 8px;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper :root{--jp-private-toolbar-height: calc( 28px + var(--jp-border-width) )}.jupyter-wrapper .jp-Toolbar{color:var(--jp-ui-font-color1);flex:0 0 auto;display:flex;flex-direction:row;border-bottom:var(--jp-border-width) solid var(--jp-toolbar-border-color);box-shadow:var(--jp-toolbar-box-shadow);background:var(--jp-toolbar-background);min-height:var(--jp-toolbar-micro-height);padding:2px;z-index:1}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item.jp-Toolbar-spacer{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-Toolbar-item.jp-Toolbar-kernelStatus{display:inline-block;width:32px;background-repeat:no-repeat;background-position:center;background-size:16px}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item{flex:0 0 auto;display:flex;padding-left:1px;padding-right:1px;font-size:var(--jp-ui-font-size1);line-height:var(--jp-private-toolbar-height);height:100%}.jupyter-wrapper div.jp-ToolbarButton{color:rgba(0,0,0,0);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px;margin:0px}.jupyter-wrapper button.jp-ToolbarButtonComponent{background:var(--jp-layout-color1);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px 6px;margin:0px;height:24px;border-radius:var(--jp-border-radius);display:flex;align-items:center;text-align:center;font-size:14px;min-width:unset;min-height:unset}.jupyter-wrapper button.jp-ToolbarButtonComponent:disabled{opacity:.4}.jupyter-wrapper button.jp-ToolbarButtonComponent span{padding:0px;flex:0 0 auto}.jupyter-wrapper button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label{font-size:var(--jp-ui-font-size1);line-height:100%;padding-left:2px;color:var(--jp-ui-font-color1)}.jupyter-wrapper body.p-mod-override-cursor *,.jupyter-wrapper body.lm-mod-override-cursor *{cursor:inherit !important}.jupyter-wrapper .jp-JSONEditor{display:flex;flex-direction:column;width:100%}.jupyter-wrapper .jp-JSONEditor-host{flex:1 1 auto;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;background:var(--jp-layout-color0);min-height:50px;padding:1px}.jupyter-wrapper .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host{border-color:red;outline-color:red}.jupyter-wrapper .jp-JSONEditor-header{display:flex;flex:1 0 auto;padding:0 0 0 12px}.jupyter-wrapper .jp-JSONEditor-header label{flex:0 0 auto}.jupyter-wrapper .jp-JSONEditor-commitButton{height:16px;width:16px;background-size:18px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .jp-JSONEditor-host.jp-mod-focused{background-color:var(--jp-input-active-background);border:1px solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .jp-Editor.jp-mod-dropTarget{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.jupyter-wrapper .CodeMirror-lines{padding:4px 0}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{padding:0 4px}.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{background-color:#fff}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.jupyter-wrapper .CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.jupyter-wrapper .CodeMirror-guttermarker{color:#000}.jupyter-wrapper .CodeMirror-guttermarker-subtle{color:#999}.jupyter-wrapper .CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.jupyter-wrapper .CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.jupyter-wrapper .cm-fat-cursor .CodeMirror-cursor{width:auto;border:0 !important;background:#7e7}.jupyter-wrapper .cm-fat-cursor div.CodeMirror-cursors{z-index:1}.jupyter-wrapper .cm-fat-cursor-mark{background-color:rgba(20,255,20,.5);-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite}.jupyter-wrapper .cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@-webkit-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@keyframes blink{50%{background-color:rgba(0,0,0,0)}}.jupyter-wrapper .cm-tab{display:inline-block;text-decoration:inherit}.jupyter-wrapper .CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:0;overflow:hidden}.jupyter-wrapper .CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.jupyter-wrapper .cm-s-default .cm-header{color:blue}.jupyter-wrapper .cm-s-default .cm-quote{color:#090}.jupyter-wrapper .cm-negative{color:#d44}.jupyter-wrapper .cm-positive{color:#292}.jupyter-wrapper .cm-header,.jupyter-wrapper .cm-strong{font-weight:bold}.jupyter-wrapper .cm-em{font-style:italic}.jupyter-wrapper .cm-link{text-decoration:underline}.jupyter-wrapper .cm-strikethrough{text-decoration:line-through}.jupyter-wrapper .cm-s-default .cm-keyword{color:#708}.jupyter-wrapper .cm-s-default .cm-atom{color:#219}.jupyter-wrapper .cm-s-default .cm-number{color:#164}.jupyter-wrapper .cm-s-default .cm-def{color:blue}.jupyter-wrapper .cm-s-default .cm-variable-2{color:#05a}.jupyter-wrapper .cm-s-default .cm-variable-3,.jupyter-wrapper .cm-s-default .cm-type{color:#085}.jupyter-wrapper .cm-s-default .cm-comment{color:#a50}.jupyter-wrapper .cm-s-default .cm-string{color:#a11}.jupyter-wrapper .cm-s-default .cm-string-2{color:#f50}.jupyter-wrapper .cm-s-default .cm-meta{color:#555}.jupyter-wrapper .cm-s-default .cm-qualifier{color:#555}.jupyter-wrapper .cm-s-default .cm-builtin{color:#30a}.jupyter-wrapper .cm-s-default .cm-bracket{color:#997}.jupyter-wrapper .cm-s-default .cm-tag{color:#170}.jupyter-wrapper .cm-s-default .cm-attribute{color:#00c}.jupyter-wrapper .cm-s-default .cm-hr{color:#999}.jupyter-wrapper .cm-s-default .cm-link{color:#00c}.jupyter-wrapper .cm-s-default .cm-error{color:red}.jupyter-wrapper .cm-invalidchar{color:red}.jupyter-wrapper .CodeMirror-composing{border-bottom:2px solid}.jupyter-wrapper div.CodeMirror span.CodeMirror-matchingbracket{color:#0b0}.jupyter-wrapper div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#a22}.jupyter-wrapper .CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.jupyter-wrapper .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .CodeMirror{position:relative;overflow:hidden;background:#fff}.jupyter-wrapper .CodeMirror-scroll{overflow:scroll !important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:none;position:relative}.jupyter-wrapper .CodeMirror-sizer{position:relative;border-right:30px solid rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-vscrollbar,.jupyter-wrapper .CodeMirror-hscrollbar,.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{position:absolute;z-index:6;display:none}.jupyter-wrapper .CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.jupyter-wrapper .CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.jupyter-wrapper .CodeMirror-scrollbar-filler{right:0;bottom:0}.jupyter-wrapper .CodeMirror-gutter-filler{left:0;bottom:0}.jupyter-wrapper .CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.jupyter-wrapper .CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.jupyter-wrapper .CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:none !important;border:none !important}.jupyter-wrapper .CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.jupyter-wrapper .CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.jupyter-wrapper .CodeMirror-gutter-wrapper ::selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-gutter-wrapper ::-moz-selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-lines{cursor:text;min-height:1px}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:rgba(0,0,0,0);font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line,.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line-like{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.jupyter-wrapper .CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.jupyter-wrapper .CodeMirror-linewidget{position:relative;z-index:2;padding:.1px}.jupyter-wrapper .CodeMirror-rtl pre{direction:rtl}.jupyter-wrapper .CodeMirror-code{outline:none}.jupyter-wrapper .CodeMirror-scroll,.jupyter-wrapper .CodeMirror-sizer,.jupyter-wrapper .CodeMirror-gutter,.jupyter-wrapper .CodeMirror-gutters,.jupyter-wrapper .CodeMirror-linenumber{-moz-box-sizing:content-box;box-sizing:content-box}.jupyter-wrapper .CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.jupyter-wrapper .CodeMirror-cursor{position:absolute;pointer-events:none}.jupyter-wrapper .CodeMirror-measure pre{position:static}.jupyter-wrapper div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}.jupyter-wrapper div.CodeMirror-dragcursors{visibility:visible}.jupyter-wrapper .CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.jupyter-wrapper .CodeMirror-selected{background:#d9d9d9}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.jupyter-wrapper .CodeMirror-crosshair{cursor:crosshair}.jupyter-wrapper .CodeMirror-line::selection,.jupyter-wrapper .CodeMirror-line>span::selection,.jupyter-wrapper .CodeMirror-line>span>span::selection{background:#d7d4f0}.jupyter-wrapper .CodeMirror-line::-moz-selection,.jupyter-wrapper .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.jupyter-wrapper .cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.jupyter-wrapper .cm-force-border{padding-right:.1px}@media print{.jupyter-wrapper .CodeMirror div.CodeMirror-cursors{visibility:hidden}}.jupyter-wrapper .cm-tab-wrap-hack:after{content:\"\"}.jupyter-wrapper span.CodeMirror-selectedtext{background:none}.jupyter-wrapper .CodeMirror-dialog{position:absolute;left:0;right:0;background:inherit;z-index:15;padding:.1em .8em;overflow:hidden;color:inherit}.jupyter-wrapper .CodeMirror-dialog-top{border-bottom:1px solid #eee;top:0}.jupyter-wrapper .CodeMirror-dialog-bottom{border-top:1px solid #eee;bottom:0}.jupyter-wrapper .CodeMirror-dialog input{border:none;outline:none;background:rgba(0,0,0,0);width:20em;color:inherit;font-family:monospace}.jupyter-wrapper .CodeMirror-dialog button{font-size:70%}.jupyter-wrapper .CodeMirror-foldmarker{color:blue;text-shadow:#b9f 1px 1px 2px,#b9f -1px -1px 2px,#b9f 1px -1px 2px,#b9f -1px 1px 2px;font-family:arial;line-height:.3;cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter{width:.7em}.jupyter-wrapper .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter-open:after{content:\"\u25be\"}.jupyter-wrapper .CodeMirror-foldgutter-folded:after{content:\"\u25b8\"}.jupyter-wrapper .cm-s-material.CodeMirror{background-color:#263238;color:#eff}.jupyter-wrapper .cm-s-material .CodeMirror-gutters{background:#263238;color:#546e7a;border:none}.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker,.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker-subtle,.jupyter-wrapper .cm-s-material .CodeMirror-linenumber{color:#546e7a}.jupyter-wrapper .cm-s-material .CodeMirror-cursor{border-left:1px solid #fc0}.jupyter-wrapper .cm-s-material div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material.CodeMirror-focused div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::-moz-selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-activeline-background{background:rgba(0,0,0,.5)}.jupyter-wrapper .cm-s-material .cm-keyword{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-operator{color:#89ddff}.jupyter-wrapper .cm-s-material .cm-variable-2{color:#eff}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#f07178}.jupyter-wrapper .cm-s-material .cm-builtin{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-atom{color:#f78c6c}.jupyter-wrapper .cm-s-material .cm-number{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-def{color:#82aaff}.jupyter-wrapper .cm-s-material .cm-string{color:#c3e88d}.jupyter-wrapper .cm-s-material .cm-string-2{color:#f07178}.jupyter-wrapper .cm-s-material .cm-comment{color:#546e7a}.jupyter-wrapper .cm-s-material .cm-variable{color:#f07178}.jupyter-wrapper .cm-s-material .cm-tag{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-meta{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-attribute{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-property{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-qualifier{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-error{color:#fff;background-color:#ff5370}.jupyter-wrapper .cm-s-material .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-gutters{background:#3f3f3f !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{color:#999}.jupyter-wrapper .cm-s-zenburn .CodeMirror-cursor{border-left:1px solid #fff}.jupyter-wrapper .cm-s-zenburn{background-color:#3f3f3f;color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-builtin{color:#dcdccc;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-comment{color:#7f9f7f}.jupyter-wrapper .cm-s-zenburn span.cm-keyword{color:#f0dfaf;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-atom{color:#bfebbf}.jupyter-wrapper .cm-s-zenburn span.cm-def{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-variable{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-variable-2{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-string{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-string-2{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-number{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-tag{color:#93e0e3}.jupyter-wrapper .cm-s-zenburn span.cm-property{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-attribute{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-qualifier{color:#7cb8bb}.jupyter-wrapper .cm-s-zenburn span.cm-meta{color:#f0dfaf}.jupyter-wrapper .cm-s-zenburn span.cm-header{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.cm-operator{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-matchingbracket{box-sizing:border-box;background:rgba(0,0,0,0);border-bottom:1px solid}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-nonmatchingbracket{border-bottom:1px solid;background:none}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline{background:#000}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline-background{background:#000}.jupyter-wrapper .cm-s-zenburn div.CodeMirror-selected{background:#545454}.jupyter-wrapper .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected{background:#4f4f4f}.jupyter-wrapper .cm-s-abcdef.CodeMirror{background:#0f0f0f;color:#defdef}.jupyter-wrapper .cm-s-abcdef div.CodeMirror-selected{background:#515151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::-moz-selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-gutters{background:#555;border-right:2px solid #314151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker{color:#222}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker-subtle{color:azure}.jupyter-wrapper .cm-s-abcdef .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-abcdef .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-abcdef span.cm-keyword{color:#b8860b;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-atom{color:#77f}.jupyter-wrapper .cm-s-abcdef span.cm-number{color:violet}.jupyter-wrapper .cm-s-abcdef span.cm-def{color:#fffabc}.jupyter-wrapper .cm-s-abcdef span.cm-variable{color:#abcdef}.jupyter-wrapper .cm-s-abcdef span.cm-variable-2{color:#cacbcc}.jupyter-wrapper .cm-s-abcdef span.cm-variable-3,.jupyter-wrapper .cm-s-abcdef span.cm-type{color:#def}.jupyter-wrapper .cm-s-abcdef span.cm-property{color:#fedcba}.jupyter-wrapper .cm-s-abcdef span.cm-operator{color:#ff0}.jupyter-wrapper .cm-s-abcdef span.cm-comment{color:#7a7b7c;font-style:italic}.jupyter-wrapper .cm-s-abcdef span.cm-string{color:#2b4}.jupyter-wrapper .cm-s-abcdef span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-abcdef span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-abcdef span.cm-builtin{color:#30aabc}.jupyter-wrapper .cm-s-abcdef span.cm-bracket{color:#8a8a8a}.jupyter-wrapper .cm-s-abcdef span.cm-tag{color:#fd4}.jupyter-wrapper .cm-s-abcdef span.cm-attribute{color:#df0}.jupyter-wrapper .cm-s-abcdef span.cm-error{color:red}.jupyter-wrapper .cm-s-abcdef span.cm-header{color:#7fffd4;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-link{color:#8a2be2}.jupyter-wrapper .cm-s-abcdef .CodeMirror-activeline-background{background:#314151}.jupyter-wrapper .cm-s-base16-light.CodeMirror{background:#f5f5f5;color:#202020}.jupyter-wrapper .cm-s-base16-light div.CodeMirror-selected{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::-moz-selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-gutters{background:#f5f5f5;border-right:0px}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker-subtle{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-linenumber{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-cursor{border-left:1px solid #505050}.jupyter-wrapper .cm-s-base16-light span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-light span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-property,.jupyter-wrapper .cm-s-base16-light span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-light span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-light span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-light span.cm-bracket{color:#202020}.jupyter-wrapper .cm-s-base16-light span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-error{background:#ac4142;color:#505050}.jupyter-wrapper .cm-s-base16-light .CodeMirror-activeline-background{background:#dddcdc}.jupyter-wrapper .cm-s-base16-light .CodeMirror-matchingbracket{color:#f5f5f5 !important;background-color:#6a9fb5 !important}.jupyter-wrapper .cm-s-base16-dark.CodeMirror{background:#151515;color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark div.CodeMirror-selected{background:#303030}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-gutters{background:#151515;border-right:0px}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker-subtle{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-linenumber{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-cursor{border-left:1px solid #b0b0b0}.jupyter-wrapper .cm-s-base16-dark span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-dark span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-property,.jupyter-wrapper .cm-s-base16-dark span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-dark span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-dark span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-dark span.cm-bracket{color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-error{background:#ac4142;color:#b0b0b0}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-activeline-background{background:#202020}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-dracula.CodeMirror,.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{background-color:#282a36 !important;color:#f8f8f2 !important;border:none}.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{color:#282a36}.jupyter-wrapper .cm-s-dracula .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-dracula .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-dracula .CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula span.cm-comment{color:#6272a4}.jupyter-wrapper .cm-s-dracula span.cm-string,.jupyter-wrapper .cm-s-dracula span.cm-string-2{color:#f1fa8c}.jupyter-wrapper .cm-s-dracula span.cm-number{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-variable{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-2{color:#fff}.jupyter-wrapper .cm-s-dracula span.cm-def{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-operator{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-atom{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-meta{color:#f8f8f2}.jupyter-wrapper .cm-s-dracula span.cm-tag{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-attribute{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-qualifier{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-property{color:#66d9ef}.jupyter-wrapper .cm-s-dracula span.cm-builtin{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-3,.jupyter-wrapper .cm-s-dracula span.cm-type{color:#ffb86c}.jupyter-wrapper .cm-s-dracula .CodeMirror-activeline-background{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch.CodeMirror{background:#322931;color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch div.CodeMirror-selected{background:#433b42 !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-gutters{background:#322931;border-right:0px}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-linenumber{color:#797379}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-cursor{border-left:1px solid #989498 !important}.jupyter-wrapper .cm-s-hopscotch span.cm-comment{color:#b33508}.jupyter-wrapper .cm-s-hopscotch span.cm-atom{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-number{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-property,.jupyter-wrapper .cm-s-hopscotch span.cm-attribute{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-keyword{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-string{color:#fdcc59}.jupyter-wrapper .cm-s-hopscotch span.cm-variable{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-variable-2{color:#1290bf}.jupyter-wrapper .cm-s-hopscotch span.cm-def{color:#fd8b19}.jupyter-wrapper .cm-s-hopscotch span.cm-error{background:#dd464c;color:#989498}.jupyter-wrapper .cm-s-hopscotch span.cm-bracket{color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch span.cm-tag{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-link{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-activeline-background{background:#302020}.jupyter-wrapper .cm-s-mbo.CodeMirror{background:#2c2c2c;color:#ffffec}.jupyter-wrapper .cm-s-mbo div.CodeMirror-selected{background:#716c62}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::-moz-selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-gutters{background:#4e4e4e;border-right:0px}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker{color:#fff}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker-subtle{color:gray}.jupyter-wrapper .cm-s-mbo .CodeMirror-linenumber{color:#dadada}.jupyter-wrapper .cm-s-mbo .CodeMirror-cursor{border-left:1px solid #ffffec}.jupyter-wrapper .cm-s-mbo span.cm-comment{color:#95958a}.jupyter-wrapper .cm-s-mbo span.cm-atom{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-number{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-property,.jupyter-wrapper .cm-s-mbo span.cm-attribute{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-keyword{color:#ffb928}.jupyter-wrapper .cm-s-mbo span.cm-string{color:#ffcf6c}.jupyter-wrapper .cm-s-mbo span.cm-string.cm-property{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable-2{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-def{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-bracket{color:#fffffc;font-weight:bold}.jupyter-wrapper .cm-s-mbo span.cm-tag{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-link{color:#f54b07}.jupyter-wrapper .cm-s-mbo span.cm-error{border-bottom:#636363;color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-qualifier{color:#ffffec}.jupyter-wrapper .cm-s-mbo .CodeMirror-activeline-background{background:#494b41}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingbracket{color:#ffb928 !important}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingtag{background:rgba(255,255,255,.37)}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{color:#999;background-color:#fff}.jupyter-wrapper .cm-s-mdn-like div.CodeMirror-selected{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::-moz-selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-gutters{background:#f8f8f8;border-left:6px solid rgba(0,83,159,.65);color:#333}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-linenumber{color:#aaa;padding-left:8px}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-cursor{border-left:2px solid #222}.jupyter-wrapper .cm-s-mdn-like .cm-keyword{color:#6262ff}.jupyter-wrapper .cm-s-mdn-like .cm-atom{color:#f90}.jupyter-wrapper .cm-s-mdn-like .cm-number{color:#ca7841}.jupyter-wrapper .cm-s-mdn-like .cm-def{color:#8da6ce}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-2,.jupyter-wrapper .cm-s-mdn-like span.cm-tag{color:#690}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-3,.jupyter-wrapper .cm-s-mdn-like span.cm-def,.jupyter-wrapper .cm-s-mdn-like span.cm-type{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-variable{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-property{color:#905}.jupyter-wrapper .cm-s-mdn-like .cm-qualifier{color:#690}.jupyter-wrapper .cm-s-mdn-like .cm-operator{color:#cda869}.jupyter-wrapper .cm-s-mdn-like .cm-comment{color:#777;font-weight:normal}.jupyter-wrapper .cm-s-mdn-like .cm-string{color:#07a;font-style:italic}.jupyter-wrapper .cm-s-mdn-like .cm-string-2{color:#bd6b18}.jupyter-wrapper .cm-s-mdn-like .cm-meta{color:#000}.jupyter-wrapper .cm-s-mdn-like .cm-builtin{color:#9b7536}.jupyter-wrapper .cm-s-mdn-like .cm-tag{color:#997643}.jupyter-wrapper .cm-s-mdn-like .cm-attribute{color:#d6bb6d}.jupyter-wrapper .cm-s-mdn-like .cm-header{color:#ff6400}.jupyter-wrapper .cm-s-mdn-like .cm-hr{color:#aeaeae}.jupyter-wrapper .cm-s-mdn-like .cm-link{color:#ad9361;font-style:italic;text-decoration:none}.jupyter-wrapper .cm-s-mdn-like .cm-error{border-bottom:1px solid red}.jupyter-wrapper div.cm-s-mdn-like .CodeMirror-activeline-background{background:#efefff}.jupyter-wrapper div.cm-s-mdn-like span.CodeMirror-matchingbracket{outline:1px solid gray;color:inherit}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{background-image:url(data:image/png;base64,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)}.jupyter-wrapper .cm-s-seti.CodeMirror{background-color:#151718 !important;color:#cfd2d1 !important;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-gutters{color:#404b53;background-color:#0e1112;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-seti .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-seti.CodeMirror-focused div.CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti span.cm-comment{color:#41535b}.jupyter-wrapper .cm-s-seti span.cm-string,.jupyter-wrapper .cm-s-seti span.cm-string-2{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-number{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-variable{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-variable-2{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-def{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-seti span.cm-operator{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#e6cd69}.jupyter-wrapper .cm-s-seti span.cm-atom{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-meta{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-tag{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-attribute{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-qualifier{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-property{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-variable-3,.jupyter-wrapper .cm-s-seti span.cm-type{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-builtin{color:#9fca56}.jupyter-wrapper .cm-s-seti .CodeMirror-activeline-background{background:#101213}.jupyter-wrapper .cm-s-seti .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .solarized.base03{color:#002b36}.jupyter-wrapper .solarized.base02{color:#073642}.jupyter-wrapper .solarized.base01{color:#586e75}.jupyter-wrapper .solarized.base00{color:#657b83}.jupyter-wrapper .solarized.base0{color:#839496}.jupyter-wrapper .solarized.base1{color:#93a1a1}.jupyter-wrapper .solarized.base2{color:#eee8d5}.jupyter-wrapper .solarized.base3{color:#fdf6e3}.jupyter-wrapper .solarized.solar-yellow{color:#b58900}.jupyter-wrapper .solarized.solar-orange{color:#cb4b16}.jupyter-wrapper .solarized.solar-red{color:#dc322f}.jupyter-wrapper .solarized.solar-magenta{color:#d33682}.jupyter-wrapper .solarized.solar-violet{color:#6c71c4}.jupyter-wrapper .solarized.solar-blue{color:#268bd2}.jupyter-wrapper .solarized.solar-cyan{color:#2aa198}.jupyter-wrapper .solarized.solar-green{color:#859900}.jupyter-wrapper .cm-s-solarized{line-height:1.45em;color-profile:sRGB;rendering-intent:auto}.jupyter-wrapper .cm-s-solarized.cm-s-dark{color:#839496;background-color:#002b36;text-shadow:#002b36 0 1px}.jupyter-wrapper .cm-s-solarized.cm-s-light{background-color:#fdf6e3;color:#657b83;text-shadow:#eee8d5 0 1px}.jupyter-wrapper .cm-s-solarized .CodeMirror-widget{text-shadow:none}.jupyter-wrapper .cm-s-solarized .cm-header{color:#586e75}.jupyter-wrapper .cm-s-solarized .cm-quote{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-keyword{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .cm-atom{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-number{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-def{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-variable{color:#839496}.jupyter-wrapper .cm-s-solarized .cm-variable-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-variable-3,.jupyter-wrapper .cm-s-solarized .cm-type{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-property{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-operator{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-comment{color:#586e75;font-style:italic}.jupyter-wrapper .cm-s-solarized .cm-string{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-string-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-meta{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-qualifier{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-builtin{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-bracket{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-matchingbracket{color:#859900}.jupyter-wrapper .cm-s-solarized .CodeMirror-nonmatchingbracket{color:#dc322f}.jupyter-wrapper .cm-s-solarized .cm-tag{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-attribute{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-hr{color:rgba(0,0,0,0);border-top:1px solid #586e75;display:block}.jupyter-wrapper .cm-s-solarized .cm-link{color:#93a1a1;cursor:pointer}.jupyter-wrapper .cm-s-solarized .cm-special{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-em{color:#999;text-decoration:underline;text-decoration-style:dotted}.jupyter-wrapper .cm-s-solarized .cm-error,.jupyter-wrapper .cm-s-solarized .cm-invalidchar{color:#586e75;border-bottom:1px dotted #dc322f}.jupyter-wrapper .cm-s-solarized.cm-s-dark div.CodeMirror-selected{background:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark.CodeMirror ::selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-light div.CodeMirror-selected{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span>span::selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span>span::-moz-selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.CodeMirror{-moz-box-shadow:inset 7px 0 12px -6px #000;-webkit-box-shadow:inset 7px 0 12px -6px #000;box-shadow:inset 7px 0 12px -6px #000}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutters{border-right:0}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-gutters{background-color:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-linenumber{color:#586e75;text-shadow:#021014 0 -1px}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-gutters{background-color:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-linenumber{color:#839496}.jupyter-wrapper .cm-s-solarized .CodeMirror-linenumber{padding:0 5px}.jupyter-wrapper .cm-s-solarized .CodeMirror-guttermarker-subtle{color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker{color:#ddd}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-guttermarker{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text{color:#586e75}.jupyter-wrapper .cm-s-solarized .CodeMirror-cursor{border-left:1px solid #819090}.jupyter-wrapper .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor{background:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-light .cm-animate-fat-cursor{background-color:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor{background:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor{background-color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background{background:rgba(255,255,255,.06)}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-activeline-background{background:rgba(0,0,0,.06)}.jupyter-wrapper .cm-s-the-matrix.CodeMirror{background:#000;color:lime}.jupyter-wrapper .cm-s-the-matrix div.CodeMirror-selected{background:#2d2d2d}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::-moz-selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-gutters{background:#060;border-right:2px solid lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker{color:lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker-subtle{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-the-matrix span.cm-keyword{color:#008803;font-weight:bold}.jupyter-wrapper .cm-s-the-matrix span.cm-atom{color:#3ff}.jupyter-wrapper .cm-s-the-matrix span.cm-number{color:#ffb94f}.jupyter-wrapper .cm-s-the-matrix span.cm-def{color:#99c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable{color:#f6c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-2{color:#c6f}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-3,.jupyter-wrapper .cm-s-the-matrix span.cm-type{color:#96f}.jupyter-wrapper .cm-s-the-matrix span.cm-property{color:#62ffa0}.jupyter-wrapper .cm-s-the-matrix span.cm-operator{color:#999}.jupyter-wrapper .cm-s-the-matrix span.cm-comment{color:#ccc}.jupyter-wrapper .cm-s-the-matrix span.cm-string{color:#39c}.jupyter-wrapper .cm-s-the-matrix span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-the-matrix span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-builtin{color:#30a}.jupyter-wrapper .cm-s-the-matrix span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-the-matrix span.cm-tag{color:#ffbd40}.jupyter-wrapper .cm-s-the-matrix span.cm-attribute{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-error{color:red}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-activeline-background{background:#040}.jupyter-wrapper .cm-s-xq-light span.cm-keyword{line-height:1em;font-weight:bold;color:#5a5cad}.jupyter-wrapper .cm-s-xq-light span.cm-atom{color:#6c8cd5}.jupyter-wrapper .cm-s-xq-light span.cm-number{color:#164}.jupyter-wrapper .cm-s-xq-light span.cm-def{text-decoration:underline}.jupyter-wrapper .cm-s-xq-light span.cm-variable{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-2{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-3,.jupyter-wrapper .cm-s-xq-light span.cm-type{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-comment{color:#0080ff;font-style:italic}.jupyter-wrapper .cm-s-xq-light span.cm-string{color:red}.jupyter-wrapper .cm-s-xq-light span.cm-meta{color:#ff0}.jupyter-wrapper .cm-s-xq-light span.cm-qualifier{color:gray}.jupyter-wrapper .cm-s-xq-light span.cm-builtin{color:#7ea656}.jupyter-wrapper .cm-s-xq-light span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-xq-light span.cm-tag{color:#3f7f7f}.jupyter-wrapper .cm-s-xq-light span.cm-attribute{color:#7f007f}.jupyter-wrapper .cm-s-xq-light span.cm-error{color:red}.jupyter-wrapper .cm-s-xq-light .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .cm-s-xq-light .CodeMirror-matchingbracket{outline:1px solid gray;color:#000 !important;background:#ff0}.jupyter-wrapper .CodeMirror{line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);border:0;border-radius:0;height:auto}.jupyter-wrapper .CodeMirror pre{padding:0 var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-dialog{background-color:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .CodeMirror-lines{padding:var(--jp-code-padding) 0}.jupyter-wrapper .CodeMirror-linenumber{padding:0 8px}.jupyter-wrapper .jp-CodeMirrorEditor-static{margin:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor,.jupyter-wrapper .jp-CodeMirrorEditor-static{cursor:text}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}@media screen and (min-width: 2138px)and (max-width: 4319px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color)}}@media screen and (min-width: 4320px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color)}}.jupyter-wrapper .CodeMirror.jp-mod-readOnly .CodeMirror-cursor{display:none}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid var(--jp-border-color2);background-color:var(--jp-layout-color0)}.jupyter-wrapper .jp-CollaboratorCursor{border-left:5px solid rgba(0,0,0,0);border-right:5px solid rgba(0,0,0,0);border-top:none;border-bottom:3px solid;background-clip:content-box;margin-left:-5px;margin-right:-5px}.jupyter-wrapper .CodeMirror-selectedtext.cm-searching{background-color:var(--jp-search-selected-match-background-color) !important;color:var(--jp-search-selected-match-color) !important}.jupyter-wrapper .cm-searching{background-color:var(--jp-search-unselected-match-background-color) !important;color:var(--jp-search-unselected-match-color) !important}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background-color:var(--jp-editor-selected-focused-background)}.jupyter-wrapper .CodeMirror-selected{background-color:var(--jp-editor-selected-background)}.jupyter-wrapper .jp-CollaboratorCursor-hover{position:absolute;z-index:1;transform:translateX(-50%);color:#fff;border-radius:3px;padding-left:4px;padding-right:4px;padding-top:1px;padding-bottom:1px;text-align:center;font-size:var(--jp-ui-font-size1);white-space:nowrap}.jupyter-wrapper .jp-CodeMirror-ruler{border-left:1px dashed var(--jp-border-color2)}.jupyter-wrapper .CodeMirror.cm-s-jupyter{background:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .jp-CodeConsole .CodeMirror.cm-s-jupyter,.jupyter-wrapper .jp-Notebook .CodeMirror.cm-s-jupyter{background:rgba(0,0,0,0)}.jupyter-wrapper .cm-s-jupyter .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}.jupyter-wrapper .cm-s-jupyter span.cm-keyword{color:var(--jp-mirror-editor-keyword-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-atom{color:var(--jp-mirror-editor-atom-color)}.jupyter-wrapper .cm-s-jupyter span.cm-number{color:var(--jp-mirror-editor-number-color)}.jupyter-wrapper .cm-s-jupyter span.cm-def{color:var(--jp-mirror-editor-def-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable{color:var(--jp-mirror-editor-variable-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-2{color:var(--jp-mirror-editor-variable-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-3{color:var(--jp-mirror-editor-variable-3-color)}.jupyter-wrapper .cm-s-jupyter span.cm-punctuation{color:var(--jp-mirror-editor-punctuation-color)}.jupyter-wrapper .cm-s-jupyter span.cm-property{color:var(--jp-mirror-editor-property-color)}.jupyter-wrapper .cm-s-jupyter span.cm-operator{color:var(--jp-mirror-editor-operator-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-comment{color:var(--jp-mirror-editor-comment-color);font-style:italic}.jupyter-wrapper .cm-s-jupyter span.cm-string{color:var(--jp-mirror-editor-string-color)}.jupyter-wrapper .cm-s-jupyter span.cm-string-2{color:var(--jp-mirror-editor-string-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-meta{color:var(--jp-mirror-editor-meta-color)}.jupyter-wrapper .cm-s-jupyter span.cm-qualifier{color:var(--jp-mirror-editor-qualifier-color)}.jupyter-wrapper .cm-s-jupyter span.cm-builtin{color:var(--jp-mirror-editor-builtin-color)}.jupyter-wrapper .cm-s-jupyter span.cm-bracket{color:var(--jp-mirror-editor-bracket-color)}.jupyter-wrapper .cm-s-jupyter span.cm-tag{color:var(--jp-mirror-editor-tag-color)}.jupyter-wrapper .cm-s-jupyter span.cm-attribute{color:var(--jp-mirror-editor-attribute-color)}.jupyter-wrapper .cm-s-jupyter span.cm-header{color:var(--jp-mirror-editor-header-color)}.jupyter-wrapper .cm-s-jupyter span.cm-quote{color:var(--jp-mirror-editor-quote-color)}.jupyter-wrapper .cm-s-jupyter span.cm-link{color:var(--jp-mirror-editor-link-color)}.jupyter-wrapper .cm-s-jupyter span.cm-error{color:var(--jp-mirror-editor-error-color)}.jupyter-wrapper .cm-s-jupyter span.cm-hr{color:#999}.jupyter-wrapper .cm-s-jupyter span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}.jupyter-wrapper .cm-s-jupyter .CodeMirror-activeline-background,.jupyter-wrapper .cm-s-jupyter .CodeMirror-gutter{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-RenderedLatex{color:var(--jp-content-font-color1);font-size:var(--jp-content-font-size1);line-height:var(--jp-content-line-height)}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedLatex{padding:var(--jp-code-padding);text-align:left}.jupyter-wrapper .jp-MimeDocument{outline:none}.jupyter-wrapper :root{--jp-private-filebrowser-button-height: 28px;--jp-private-filebrowser-button-width: 48px}.jupyter-wrapper .jp-FileBrowser{display:flex;flex-direction:column;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{border-bottom:none;height:auto;margin:var(--jp-toolbar-header-margin);box-shadow:none}.jupyter-wrapper .jp-BreadCrumbs{flex:0 0 auto;margin:4px 12px}.jupyter-wrapper .jp-BreadCrumbs-item{margin:0px 2px;padding:0px 2px;border-radius:var(--jp-border-radius);cursor:pointer}.jupyter-wrapper .jp-BreadCrumbs-item:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-BreadCrumbs-item:first-child{margin-left:0px}.jupyter-wrapper .jp-BreadCrumbs-item.jp-mod-dropTarget{background-color:var(--jp-brand-color2);opacity:.7}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{padding:0px}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{justify-content:space-evenly}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item{flex:1}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent{width:100%}.jupyter-wrapper .jp-DirListing{flex:1 1 auto;display:flex;flex-direction:column;outline:0}.jupyter-wrapper .jp-DirListing-header{flex:0 0 auto;display:flex;flex-direction:row;overflow:hidden;border-top:var(--jp-border-width) solid var(--jp-border-color2);border-bottom:var(--jp-border-width) solid var(--jp-border-color1);box-shadow:var(--jp-toolbar-box-shadow);z-index:2}.jupyter-wrapper .jp-DirListing-headerItem{padding:4px 12px 2px 12px;font-weight:500}.jupyter-wrapper .jp-DirListing-headerItem:hover{background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-name{flex:1 0 84px}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-modified{flex:0 0 112px;border-left:var(--jp-border-width) solid var(--jp-border-color2);text-align:right}.jupyter-wrapper .jp-DirListing-narrow .jp-id-modified,.jupyter-wrapper .jp-DirListing-narrow .jp-DirListing-itemModified{display:none}.jupyter-wrapper .jp-DirListing-headerItem.jp-mod-selected{font-weight:600}.jupyter-wrapper .jp-DirListing-content{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-DirListing.jp-mod-native-drop .jp-DirListing-content{outline:5px dashed rgba(128,128,128,.5);outline-offset:-10px;cursor:copy}.jupyter-wrapper .jp-DirListing-item{display:flex;flex-direction:row;padding:4px 12px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected{color:#fff;background:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-dropTarget{background:var(--jp-brand-color3)}.jupyter-wrapper .jp-DirListing-item:hover:not(.jp-mod-selected){background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-itemIcon{flex:0 0 20px;margin-right:4px}.jupyter-wrapper .jp-DirListing-itemText{flex:1 0 64px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;user-select:none}.jupyter-wrapper .jp-DirListing-itemModified{flex:0 0 125px;text-align:right}.jupyter-wrapper .jp-DirListing-editor{flex:1 0 64px;outline:none;border:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before{color:#32cd32;content:\"\u25cf\";font-size:8px;position:absolute;left:-8px}.jupyter-wrapper .jp-DirListing-item.lm-mod-drag-image,.jupyter-wrapper .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image{font-size:var(--jp-ui-font-size1);padding-left:4px;margin-left:4px;width:160px;background-color:var(--jp-ui-inverse-font-color2);box-shadow:var(--jp-elevation-z2);border-radius:0px;color:var(--jp-ui-font-color1);transform:translateX(-40%) translateY(-58%)}.jupyter-wrapper .jp-DirListing-deadSpace{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-Document{min-width:120px;min-height:120px;outline:none}.jupyter-wrapper .jp-FileDialog.jp-mod-conflict input{color:red}.jupyter-wrapper .jp-FileDialog .jp-new-name-title{margin-top:12px}.jupyter-wrapper .jp-OutputArea{overflow-y:auto}.jupyter-wrapper .jp-OutputArea-child{display:flex;flex-direction:row}.jupyter-wrapper .jp-OutputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-outprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-OutputArea-output{height:auto;overflow:auto;user-select:text;-moz-user-select:text;-webkit-user-select:text;-ms-user-select:text}.jupyter-wrapper .jp-OutputArea-child .jp-OutputArea-output{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-OutputArea-output.jp-mod-isolated{width:100%;display:block}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-OutputArea-output pre{border:none;margin:0px;padding:0px;overflow-x:auto;overflow-y:auto;word-break:break-all;word-wrap:break-word;white-space:pre-wrap}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedHTMLCommon table{margin-left:0;margin-right:0}.jupyter-wrapper .jp-OutputArea-output dl,.jupyter-wrapper .jp-OutputArea-output dt,.jupyter-wrapper .jp-OutputArea-output dd{display:block}.jupyter-wrapper .jp-OutputArea-output dl{width:100%;overflow:hidden;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dt{font-weight:bold;float:left;width:20%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dd{float:left;width:80%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt{display:none}.jupyter-wrapper .jp-OutputArea-output.jp-OutputArea-executeResult{margin-left:0px;flex:1 1 auto}.jupyter-wrapper .jp-OutputArea-executeResult.jp-RenderedText{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-OutputArea-stdin{line-height:var(--jp-code-line-height);padding-top:var(--jp-code-padding);display:flex}.jupyter-wrapper .jp-Stdin-prompt{color:var(--jp-content-font-color0);padding-right:var(--jp-code-padding);vertical-align:baseline;flex:0 0 auto}.jupyter-wrapper .jp-Stdin-input{font-family:var(--jp-code-font-family);font-size:inherit;color:inherit;background-color:inherit;width:42%;min-width:200px;vertical-align:baseline;padding:0em .25em;margin:0em .25em;flex:0 0 70%}.jupyter-wrapper .jp-Stdin-input:focus{box-shadow:none}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea{height:100%;display:block}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea-output:only-child{height:100%}.jupyter-wrapper .jp-Collapser{flex:0 0 var(--jp-cell-collapser-width);padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0);border-radius:var(--jp-border-radius);opacity:1}.jupyter-wrapper .jp-Collapser-child{display:block;width:100%;box-sizing:border-box;position:absolute;top:0px;bottom:0px}.jupyter-wrapper .jp-CellHeader,.jupyter-wrapper .jp-CellFooter{height:0px;width:100%;padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-InputArea{display:flex;flex-direction:row}.jupyter-wrapper .jp-InputArea-editor{flex:1 1 auto}.jupyter-wrapper .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);border-radius:0px;background:var(--jp-cell-editor-background)}.jupyter-wrapper .jp-InputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-inprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);opacity:var(--jp-cell-prompt-opacity);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-Placeholder{display:flex;flex-direction:row;flex:1 1 auto}.jupyter-wrapper .jp-Placeholder-prompt{box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content{flex:1 1 auto;border:none;background:rgba(0,0,0,0);height:20px;box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon{width:32px;height:16px;border:1px solid rgba(0,0,0,0);border-radius:var(--jp-border-radius)}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon:hover{border:1px solid var(--jp-border-color1);box-shadow:0px 0px 2px 0px rgba(0,0,0,.25);background-color:var(--jp-layout-color0)}.jupyter-wrapper :root{--jp-private-cell-scrolling-output-offset: 5px}.jupyter-wrapper .jp-Cell{padding:var(--jp-cell-padding);margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Cell-inputWrapper,.jupyter-wrapper .jp-Cell-outputWrapper{display:flex;flex-direction:row;padding:0px;margin:0px;overflow:visible}.jupyter-wrapper .jp-Cell-inputArea,.jupyter-wrapper .jp-Cell-outputArea{flex:1 1 auto}.jupyter-wrapper .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser{border:none !important;background:rgba(0,0,0,0) !important}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser{min-height:var(--jp-cell-collapser-min-height)}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper{margin-top:5px}.jupyter-wrapper .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea{overflow-y:auto;max-height:200px;box-shadow:inset 0 0 6px 2px rgba(0,0,0,.3);margin-left:var(--jp-private-cell-scrolling-output-offset)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt{flex:0 0 calc(var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset))}.jupyter-wrapper .jp-MarkdownOutput{flex:1 1 auto;margin-top:0;margin-bottom:0;padding-left:var(--jp-code-padding)}.jupyter-wrapper .jp-MarkdownOutput.jp-RenderedHTMLCommon{overflow:auto}.jupyter-wrapper .jp-NotebookPanel-toolbar{padding:2px}.jupyter-wrapper .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused{border:none;box-shadow:none}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown select{height:24px;font-size:var(--jp-ui-font-size1);line-height:14px;border-radius:0;display:block}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown span{top:5px !important}.jupyter-wrapper :root{--jp-private-notebook-dragImage-width: 304px;--jp-private-notebook-dragImage-height: 36px;--jp-private-notebook-selected-color: var(--md-blue-400);--jp-private-notebook-active-color: var(--md-green-400)}.jupyter-wrapper .jp-NotebookPanel{display:block;height:100%}.jupyter-wrapper .jp-NotebookPanel.jp-Document{min-width:240px;min-height:120px}.jupyter-wrapper .jp-Notebook{padding:var(--jp-notebook-padding);outline:none;overflow:auto;background:var(--jp-layout-color0)}.jupyter-wrapper .jp-Notebook.jp-mod-scrollPastEnd::after{display:block;content:\"\";min-height:var(--jp-notebook-scroll-padding)}.jupyter-wrapper .jp-Notebook .jp-Cell{overflow:visible}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-InputPrompt{cursor:move}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser{background:var(--jp-brand-color1)}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-Collapser:hover{box-shadow:var(--jp-elevation-z2);background:var(--jp-brand-color1);opacity:var(--jp-cell-collapser-not-active-hover-opacity)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover{background:var(--jp-brand-color0);opacity:1}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected{background:var(--jp-notebook-multiselected-color)}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected){background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-cell-editor-active-background)}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropSource{opacity:.5}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropTarget,.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget{border-top-color:var(--jp-private-notebook-selected-color);border-top-style:solid;border-top-width:2px}.jupyter-wrapper .jp-dragImage{display:flex;flex-direction:row;width:var(--jp-private-notebook-dragImage-width);height:var(--jp-private-notebook-dragImage-height);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background);overflow:visible}.jupyter-wrapper .jp-dragImage-singlePrompt{box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-dragImage .jp-dragImage-content{flex:1 1 auto;z-index:2;font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);line-height:var(--jp-code-line-height);padding:var(--jp-code-padding);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background-color);color:var(--jp-content-font-color3);text-align:left;margin:4px 4px 4px 0px}.jupyter-wrapper .jp-dragImage .jp-dragImage-prompt{flex:0 0 auto;min-width:36px;color:var(--jp-cell-inprompt-font-color);padding:var(--jp-code-padding);padding-left:12px;font-family:var(--jp-cell-prompt-font-family);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:1.9;font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0)}.jupyter-wrapper .jp-dragImage-multipleBack{z-index:-1;position:absolute;height:32px;width:300px;top:8px;left:8px;background:var(--jp-layout-color2);border:var(--jp-border-width) solid var(--jp-input-border-color);box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-NotebookTools{display:block;min-width:var(--jp-sidebar-min-width);color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1);overflow:auto}.jupyter-wrapper .jp-NotebookTools-tool{padding:0px 12px 0 12px}.jupyter-wrapper .jp-ActiveCellTool{padding:12px;background-color:var(--jp-layout-color1);border-top:none !important}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-prompt{flex:0 0 auto;padding-left:0px}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor{flex:1 1 auto;background:var(--jp-cell-editor-background);border-color:var(--jp-cell-editor-border-color)}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror{background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MetadataEditorTool{flex-direction:column;padding:12px 0px 12px 0px}.jupyter-wrapper .jp-RankedPanel>:not(:first-child){margin-top:12px}.jupyter-wrapper .jp-KeySelector select.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:var(--jp-border-width) solid var(--jp-border-color1)}.jupyter-wrapper .jp-KeySelector label,.jupyter-wrapper .jp-MetadataEditorTool label{line-height:1.4}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook{--jp-content-font-size1: var(--jp-content-presentation-font-size1);--jp-code-font-size: var(--jp-code-presentation-font-size)}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt{flex:0 0 110px}.jupyter-wrapper .md-typeset__scrollwrap{margin:0}.jupyter-wrapper .jp-MarkdownOutput{padding:0}.jupyter-wrapper h1 .anchor-link,.jupyter-wrapper h2 .anchor-link,.jupyter-wrapper h3 .anchor-link,.jupyter-wrapper h4 .anchor-link,.jupyter-wrapper h5 .anchor-link,.jupyter-wrapper h6 .anchor-link{display:none;margin-left:.5rem;color:var(--md-default-fg-color--lighter)}.jupyter-wrapper h1 .anchor-link:hover,.jupyter-wrapper h2 .anchor-link:hover,.jupyter-wrapper h3 .anchor-link:hover,.jupyter-wrapper h4 .anchor-link:hover,.jupyter-wrapper h5 .anchor-link:hover,.jupyter-wrapper h6 .anchor-link:hover{text-decoration:none;color:var(--md-accent-fg-color)}.jupyter-wrapper h1:hover .anchor-link,.jupyter-wrapper h2:hover .anchor-link,.jupyter-wrapper h3:hover .anchor-link,.jupyter-wrapper h4:hover .anchor-link,.jupyter-wrapper h5:hover .anchor-link,.jupyter-wrapper h6:hover .anchor-link{display:inline-block}.jupyter-wrapper .jp-InputArea{width:100%}.jupyter-wrapper .jp-Cell-inputArea{width:100%}.jupyter-wrapper .jp-RenderedHTMLCommon{width:100%}.jupyter-wrapper .jp-Cell-inputWrapper .jp-InputPrompt{display:none}.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt{display:block}.jupyter-wrapper .highlight pre{overflow:auto}.jupyter-wrapper .celltoolbar{border:none;background:#eee;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;box-pack:end;justify-content:flex-start;display:-webkit-flex}.jupyter-wrapper .celltoolbar .tags_button_container{display:flex}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container{display:flex;flex-direction:row;flex-grow:1;overflow:hidden;position:relative}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;box-shadow:none;width:inherit;font-size:11px;font-family:\"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace;height:22px;display:inline-block}.jupyter-wrapper .jp-InputArea-editor{width:1px}.jupyter-wrapper .jp-InputPrompt{overflow:unset}.jupyter-wrapper .jp-OutputPrompt{overflow:unset}.jupyter-wrapper .jp-RenderedText{font-size:var(--jp-code-font-size)}.jupyter-wrapper .highlight-ipynb{overflow:auto}.jupyter-wrapper .highlight-ipynb pre{margin:0;padding:5px 10px}.jupyter-wrapper table{width:max-content}.jupyter-wrapper table.dataframe{margin-left:auto;margin-right:auto;border:none;border-collapse:collapse;border-spacing:0;color:#000;font-size:12px;table-layout:fixed}.jupyter-wrapper table.dataframe thead{border-bottom:1px solid #000;vertical-align:bottom}.jupyter-wrapper table.dataframe tr,.jupyter-wrapper table.dataframe th,.jupyter-wrapper table.dataframe td{text-align:right;vertical-align:middle;padding:.5em .5em;line-height:normal;white-space:normal;max-width:none;border:none}.jupyter-wrapper table.dataframe th{font-weight:bold}.jupyter-wrapper table.dataframe tbody tr:nth-child(odd){background:#f5f5f5}.jupyter-wrapper table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}.jupyter-wrapper *+table{margin-top:1em}.jupyter-wrapper .jp-InputArea-editor{position:relative}.jupyter-wrapper .zeroclipboard-container{position:absolute;top:-3px;right:0;z-index:1000}.jupyter-wrapper .zeroclipboard-container clipboard-copy{-webkit-appearance:button;-moz-appearance:button;padding:7px 5px;font:11px system-ui,sans-serif;display:inline-block;cursor:default}.jupyter-wrapper .zeroclipboard-container .clipboard-copy-icon{padding:4px 4px 2px;color:#57606a;vertical-align:text-bottom}.jupyter-wrapper .clipboard-copy-txt{display:none}[data-md-color-scheme=slate] .clipboard-copy-icon{color:#fff !important}[data-md-color-scheme=slate] table.dataframe{color:#e9ebfc}[data-md-color-scheme=slate] table.dataframe thead{border-bottom:1px solid rgba(233,235,252,.12)}[data-md-color-scheme=slate] table.dataframe tbody tr:nth-child(odd){background:#222}[data-md-color-scheme=slate] table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}table{width:max-content} /*# sourceMappingURL=mkdocs-jupyter.css.map*/ init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Inference acceleration of T5 for large batch size / long sequence length / > large models \u00b6 Every week or so, a new impressive few shots learning work taking advantage of autoregressive models is released by some team around the world. Still LLM inference is rarely discussed and few projects are focusing on this aspect. In this notebook, we describe our take to significantly improve autoregressive model latency. We plan to intensively test large autoregressive models, so we want something: which scales : the improvement exists on small and large models, for short and long sequences, in greedy and beam search; This is very important in a few shots learning where sequences are most of the time hundreds or thousands tokens long and beam search is used to improve text quality. that has no hidden cost : no big increase in memory usage, no degradation in quality of generated text, support state-of-the-art decoding algorithms; that is generic : works for any transformer based architecture, and not specific to an inference engine; that is easy to maintain : no hard-coded behaviors or other technical debt if it doesn't bring a clear advantage. To be clear, we are not targeting the best performance ever but the right trade off (for us at least) between simplicity to use/maintain and acceptable latency. The challenge \u00b6 In most situations, performing inference with Onnx Runtime or TensorRT usually bring large improvement over Pytorch implementations. It's very true with transformer based models. The main reason is that these tools will perform kernel fusions (merging several operations into a single one) and therefore reduce the number of memory bounded operations. Sometimes they also replace some operations by a much faster approximation. In the very specific case of autoregressive languages, things are a bit more complicated. On most Pytorch implementations of these models, there is a cache of K and V values. Let's remind us that in attention blocks, each token is projected on 3 matrices called Query , Key , and Value . Then, those projections will be used to compute a representation of each token which takes into account the information from the related other tokens of the sequence. As autoregressive models generate the sequence one token at a time, they should recompute final representation of all past tokens for each new token to generate. Because each token can only attend to the past, the result of these computations never changes; therefore one simple trick to reduce latency is to just memorize them and reuse them later, avoiding lots of computation. Out of the box, the cache mechanism can't be exported to Onnx from Hugging Face models (and all other Pytorch implementations we are aware of). The reason is that those models are not torchscript scripting compliant (it requires Pytorch code to follow some restrictive rules ). Because of that, Onnx export is done through tracing which erases any control flow instructions (including the If instruction to enable or not a cache). Existing solutions \u00b6 Some interesting solutions targeting inference latency that we have considered and/or tested: TensorRT , which targets GPU , heavily optimizes the computation graph, making T5 inference very fast (they report X10 speedup on small-T5 ). The trick is that it doesn't use any cache (see below for more details), so it's very fast on short sequence and small models, as it avoids many memory bounded operations by redoing full computation again and again... but as several users have already found ( 1 , 2 , 3 , 4 , ...), this approach doesn't scale when the computation intensity increases, i.e., when base or large models are used instead of a small one, when generation is done on moderately long sequence of few hundred of tokens or if beam search is used instead of a greedy search; FastT5 , which targets CPU , exports 2 versions of the decoder, one with cache and one without. You need the no cache version to compute the first token and the first past state tensors (aka the cached tensors), and for all the other tokens you use the cache version of the computation graph. Basically, it makes the memory foot print 2 times bigger as all weights are duplicated. As generative models tend to be huge, they work around the memory issue by using dynamic int-8 quantization, the final memory foot print of the decoders is now the same as Hugging Face in FP16 ... but 1/ dynamic quantization only works on CPU , and 2/ according to several reports dynamic quantization degrades significantly generative model output, to a point where it may make them useless ( 1 , 2 , and here you can find a report in the GPT-2 context from a Microsoft engineer: \" int8 quantization are not recommended due to accuracy loss \"). Onnx Runtime T5 export tool targets both GPU and CPU . It works in a similar way than FastT5 : decoder module is exported 2 times. Like FastT5 , the memory footprint of the decoder part is doubled (this time there is no int-8 quantization). FasterTransformer targets GPU and is a mix of Pytorch and CUDA / C++ dedicated code. The performance boost is huge on T5 , they report a 10X speedup like TensorRT . However, it may significantly decrease the accuracy of the model ( here when sampling is enabled, it reduces BLEU score of translation task by 8 points, the cause may be a bug in the decoding algorithm or an approximation a bit too aggressive) plus the speedup is computed on a translation task where sequences are 25 tokens long on average. In our experience, improvement on very short sequences tends to decrease by large margins on longer sequences. It seems to us that their objectives are different from ours. With the existing solutions, you need to choose one or two items of the following: double decoder memory footprint; be slower than Hugging Face for moderately long sequence length / beam search; degrade output quality. Our approach \u00b6 Our approach to make autoregressive transformer based models 2X faster than Hugging Face Pytorch implementation (the base line) is based on 3 key ingredients: storing 2 computation graphs in a single Onnx file: this let us have both cache and no cache support without having any duplicated weights, zero copy to retrieve output from Onnx Runtime : we built over our past work to connect in the most efficient way Pytorch tensors (used in the decoding part) and Onnx Runtime . Our previous work was to avoid host <-> GPU tensor copy, but it still required a GPU <-> GPU . It is now part of the official Onnx Runtime documentation (apparently thanks of our project !). This time we found out a way to directly expose the internal state of Onnx Runtime through a Pytorch tensor in zero copy way. Combined with cache mechanism, this is responsible for most of the speedup we have obtained. a generic tool to convert any model (whatever the architecture) to FP16 without any risk of having out of range values or rounding to zero: FP16 is still the way to reduce memory footprint of a model. The main issue is that some nodes may output values outside of FP16 range or round others to zero, resulting in NaN output; moreover, very small values may be rounded to zero which is an issue for log and div operations. We have built a tool which detect those nodes so we can keep their precision in FP32 . It's quite important to reduce memory footprint of these models, not just because they tend to be huge, but also because past states (that we cache) and internal buffers can be even bigger than the weights of the model itself. Results \u00b6 As demonstrated at the end of this notebook, we are able to provide a X2 speedup whatever the batch size, the sequence length or the model size. For TensorRT we have our own implementation of our approach described above which helps to provide similar latency to Onnx Runtime . It's in a Python script in the same folder as this notebook. We had to work around a documented limitation. Because of that the code is slightly more complex and we wanted to keep this notebook easy to follow. In [1]: Copied! ! nvidia - smi ! nvidia-smi Mon May 23 21:22:55 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.43.04 Driver Version: 515.43.04 CUDA Version: 11.7 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 39% 47C P8 41W / 350W | 263MiB / 24576MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 2366 G /usr/lib/xorg/Xorg 160MiB | | 0 N/A N/A 6173 G ...ome-remote-desktop-daemon 4MiB | | 0 N/A N/A 8174 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 8918 G ...on/Bin/AgentConnectix.bin 4MiB | | 0 N/A N/A 393472 G ...176700596390776551,131072 47MiB | +-----------------------------------------------------------------------------+ Onnx Runtime compilation \u00b6 Version 1.11.1 of Onnx Runtime and older have a bug which makes them much slower when most inputs are used by subgraphs of an If node. Unfortunately, it's exactly what will do below, so we need to compile our own version of Onnx Runtime until the version 1.12 is released (in June 2022). Code below has been tested on Ubuntu 22.04 and supposes that your machine has CUDA 11.4 installed. If not, use the Docker image of this library. We use a specific commit of Onnx Runtime with a better management of If / Else / Then Onnx nodes: git clone --recursive https://github.com/Microsoft/onnxruntime cd onnxruntime git checkout -b fix_if e1c04eed29d48f295de1cfbd48713158537cdaa7 CUDACXX = /usr/local/cuda-11.4/bin/nvcc ./build.sh \\ --config Release \\ --build_wheel \\ --parallel \\ --use_cuda \\ --cuda_home /usr/local/cuda-11.4 \\ --cudnn_home /usr/lib/x86_ -linux-gnu/ \\ --skip_test # pip install ... # other required dependencies # pip install nvtx seaborn On our machine, it takes around 20 minutes. to clear previous compilation, delete content of ./build folder In [2]: Copied! import json import random from transformer_deploy.backends.ort_utils import get_keep_fp32_nodes from transformer_deploy.backends.ort_utils import convert_fp16 import time from typing import Callable , Dict , Optional , List import matplotlib.pylab as plt from onnxruntime import IOBinding import numpy as np import onnx import torch from pathlib import Path from typing import Tuple from onnx import GraphProto , ModelProto , helper from torch.nn import Linear from transformers import AutoModelForSeq2SeqLM , AutoTokenizer , PretrainedConfig , T5ForConditionalGeneration , TensorType from transformers.generation_utils import GenerationMixin from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions , Seq2SeqLMOutput from transformers.models.t5.modeling_t5 import T5Stack from nvtx import nvtx from copy import copy from transformer_deploy.backends.ort_utils import create_model_for_provider , inference_onnx_binding from transformer_deploy.backends.pytorch_utils import convert_to_onnx import seaborn as sns import operator from collections import defaultdict import gc import json import random from transformer_deploy.backends.ort_utils import get_keep_fp32_nodes from transformer_deploy.backends.ort_utils import convert_fp16 import time from typing import Callable, Dict, Optional, List import matplotlib.pylab as plt from onnxruntime import IOBinding import numpy as np import onnx import torch from pathlib import Path from typing import Tuple from onnx import GraphProto, ModelProto, helper from torch.nn import Linear from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PretrainedConfig, T5ForConditionalGeneration, TensorType from transformers.generation_utils import GenerationMixin from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput from transformers.models.t5.modeling_t5 import T5Stack from nvtx import nvtx from copy import copy from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding from transformer_deploy.backends.pytorch_utils import convert_to_onnx import seaborn as sns import operator from collections import defaultdict import gc Loading Hugging Face model / tokenizer \u00b6 Below we load the model and set global variables of this notebook. In [3]: Copied! np . random . seed ( 123 ) torch . random . manual_seed ( 123 ) # other possible values: t5-small, t5-base, t5-large. t5-3b should work when ORT library is fixed model_name = \"t5-large\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) input_ids : torch . Tensor = tokenizer ( \"translate English to French: This model is now very fast!\" , return_tensors = TensorType . PYTORCH ) . input_ids input_ids = input_ids . type ( torch . int32 ) . to ( \"cuda\" ) pytorch_model : T5ForConditionalGeneration = AutoModelForSeq2SeqLM . from_pretrained ( model_name ) pytorch_model = pytorch_model . eval () pytorch_model = pytorch_model . cuda () pytorch_model . config . use_cache = True # not really needed, just to make things obvious num_layers = pytorch_model . config . num_layers # tolerance between Onnx FP16 and Pytorch FP32. # Rounding errors increase with number of layers: 1e-1 for t5-small, 5e-1 for large, 3 for 3b. 11b not tested. # Do not impact final quality fp16_default_tolerance = 5e-1 def are_equal ( a : torch . Tensor , b : torch . Tensor , atol : float = fp16_default_tolerance ) -> None : assert np . allclose ( a = a . detach () . cpu () . numpy (), b = b . detach () . cpu () . numpy (), atol = atol ), f \" { a } \\n\\n VS \\n\\n { b } \" def save_onnx ( proto : onnx . ModelProto , model_path : str ) -> None : # protobuff doesn't support files > 2Gb, in this case, weights are stored in another binary file save_external_data : bool = proto . ByteSize () > 2 * 1024 ** 3 filename = Path ( model_path ) . name onnx . save_model ( proto = proto , f = model_path , save_as_external_data = save_external_data , all_tensors_to_one_file = True , location = filename + \".data\" , ) def prepare_folder ( path : str ) -> Tuple [ str , str ]: p = Path ( path ) p . mkdir ( parents = True , exist_ok = True ) [ item . unlink () for item in Path ( path ) . glob ( \"*\" ) if item . is_file ()] return path + \"/model.onnx\" , path + \"/model_fp16.onnx\" # create/clean folders where each model will be stored. # as multiple files will be saved for T5-3B and 11B, we use different folders for the encoder and the decoders. encoder_model_path , encoder_fp16_model_path = prepare_folder ( path = \"./test-enc\" ) dec_cache_model_path , dec_cache_fp16_model_path = prepare_folder ( path = \"./test-dec-cache\" ) dec_no_cache_model_path , dec_no_cache_fp16_model_path = prepare_folder ( path = \"./test-dec-no-cache\" ) _ , dec_if_fp16_model_path = prepare_folder ( path = \"./test-dec-if\" ) # some outputs to compare with out_enc : BaseModelOutputWithPastAndCrossAttentions = pytorch_model . encoder ( input_ids = input_ids ) out_full : Seq2SeqLMOutput = pytorch_model ( input_ids = input_ids , decoder_input_ids = input_ids ) np.random.seed(123) torch.random.manual_seed(123) # other possible values: t5-small, t5-base, t5-large. t5-3b should work when ORT library is fixed model_name = \"t5-large\" tokenizer = AutoTokenizer.from_pretrained(model_name) input_ids: torch.Tensor = tokenizer( \"translate English to French: This model is now very fast!\", return_tensors=TensorType.PYTORCH ).input_ids input_ids = input_ids.type(torch.int32).to(\"cuda\") pytorch_model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model_name) pytorch_model = pytorch_model.eval() pytorch_model = pytorch_model.cuda() pytorch_model.config.use_cache = True # not really needed, just to make things obvious num_layers = pytorch_model.config.num_layers # tolerance between Onnx FP16 and Pytorch FP32. # Rounding errors increase with number of layers: 1e-1 for t5-small, 5e-1 for large, 3 for 3b. 11b not tested. # Do not impact final quality fp16_default_tolerance = 5e-1 def are_equal(a: torch.Tensor, b: torch.Tensor, atol: float = fp16_default_tolerance) -> None: assert np.allclose(a=a.detach().cpu().numpy(), b=b.detach().cpu().numpy(), atol=atol), f\"{a}\\n\\nVS\\n\\n{b}\" def save_onnx(proto: onnx.ModelProto, model_path: str) -> None: # protobuff doesn't support files > 2Gb, in this case, weights are stored in another binary file save_external_data: bool = proto.ByteSize() > 2 * 1024**3 filename = Path(model_path).name onnx.save_model( proto=proto, f=model_path, save_as_external_data=save_external_data, all_tensors_to_one_file=True, location=filename + \".data\", ) def prepare_folder(path: str) -> Tuple[str, str]: p = Path(path) p.mkdir(parents=True, exist_ok=True) [item.unlink() for item in Path(path).glob(\"*\") if item.is_file()] return path + \"/model.onnx\", path + \"/model_fp16.onnx\" # create/clean folders where each model will be stored. # as multiple files will be saved for T5-3B and 11B, we use different folders for the encoder and the decoders. encoder_model_path, encoder_fp16_model_path = prepare_folder(path=\"./test-enc\") dec_cache_model_path, dec_cache_fp16_model_path = prepare_folder(path=\"./test-dec-cache\") dec_no_cache_model_path, dec_no_cache_fp16_model_path = prepare_folder(path=\"./test-dec-no-cache\") _, dec_if_fp16_model_path = prepare_folder(path=\"./test-dec-if\") # some outputs to compare with out_enc: BaseModelOutputWithPastAndCrossAttentions = pytorch_model.encoder(input_ids=input_ids) out_full: Seq2SeqLMOutput = pytorch_model(input_ids=input_ids, decoder_input_ids=input_ids) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5. For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`. - Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding. - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding. - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value. warnings.warn( Export to Onnx \u00b6 First step is to export the model to Onnx graph. T5 is made of 2 parts, an encoder and a decoder . Export encoder part \u00b6 The encoder part export doesn't imply any specific challenge. We use export function built for Bert like model, exported model is in FP16 . In [4]: Copied! pytorch_model = pytorch_model . to ( \"cuda\" ) convert_to_onnx ( model_pytorch = pytorch_model . encoder , output_path = encoder_model_path , inputs_pytorch = { \"input_ids\" : input_ids }, var_output_seq = True , quantization = False , ) pytorch_model = pytorch_model.to(\"cuda\") convert_to_onnx( model_pytorch=pytorch_model.encoder, output_path=encoder_model_path, inputs_pytorch={\"input_ids\": input_ids}, var_output_seq=True, quantization=False, ) Conversion to mixed precision \u00b6 Why mixed precision? \u00b6 As T5 can have up to 11 billion parameters, it requires lots of computation, and even more important, it takes lots of space in device memory. We convert the encoder to half precision. If we blindly convert the whole graph to FP16 , we will have 2 issues: overflow : some nodes, like exponential nodes, will try to output values out of the FP16 range, at the end you get some NaN . underflow : values very close to 0 will be rounded to 0, which may be an issue for some operations like Div and Log . The challenge \u00b6 Mixed precision is done out of the box by Pytorch and follow some strict rules described in https://pytorch.org/docs/stable/amp.html Those rules are generic and quite conservative. Many nodes will be kept in FP32 even if their output is always in the FP16 range. Other approaches we have found: Onnx Runtime T5 demo : provide a list of operations to keep in FP32 (Pow, ReduceMean, Add, Sqrt, Div, Mul, Softmax, Relu). We have found this approach to need more an more tweaking on larger networks and encoder part (decoder part seems simpler to manage, https://github.com/microsoft/onnxruntime/issues/11119 ); TensorRT T5 demo : provide the exact pattern of nodes to keep in FP32 . This approach is much more effective, but imply lots of code to describe the patterns and may not generalize well, basically what works for a base model may not work for 11 billion parameters model. And it does not scale to other architectures without adaptations, for a library like transformer-deploy , it would lead to unmaintainable technical debt. Our approach \u00b6 We have chosen an architecture agnostic approach: we inject random input sequences and audit the output of each computation graph node; finally, we make a list of all nodes that have output values out of the FP16 range /close to zero values and perform some cleaning (to avoid unnecessary casting). We have chosen to use random values only for the input_ids field as the search space is limited: positive integers lower than the vocabulary size. You can also decide to send real data from a dataset you want to work on. To finish, we provide the list of nodes to keep in FP32 to the conversion function. In [5]: Copied! def get_random_input_encoder () -> Dict [ str , torch . Tensor ]: max_seq = 512 seq_len = random . randint ( a = 1 , b = max_seq ) batch = max_seq // seq_len random_input_ids = torch . randint ( low = 0 , high = tokenizer . vocab_size , size = ( batch , seq_len ), dtype = torch . int32 , device = \"cuda\" ) inputs = { \"input_ids\" : random_input_ids } return inputs keep_fp32_encoder = get_keep_fp32_nodes ( onnx_model_path = encoder_model_path , get_input = get_random_input_encoder ) assert len ( keep_fp32_encoder ) > 0 enc_model_onnx = convert_fp16 ( onnx_model = encoder_model_path , nodes_to_exclude = keep_fp32_encoder ) save_onnx ( proto = enc_model_onnx , model_path = encoder_fp16_model_path ) del enc_model_onnx torch . cuda . empty_cache () gc . collect () def get_random_input_encoder() -> Dict[str, torch.Tensor]: max_seq = 512 seq_len = random.randint(a=1, b=max_seq) batch = max_seq // seq_len random_input_ids = torch.randint( low=0, high=tokenizer.vocab_size, size=(batch, seq_len), dtype=torch.int32, device=\"cuda\" ) inputs = {\"input_ids\": random_input_ids} return inputs keep_fp32_encoder = get_keep_fp32_nodes(onnx_model_path=encoder_model_path, get_input=get_random_input_encoder) assert len(keep_fp32_encoder) > 0 enc_model_onnx = convert_fp16(onnx_model=encoder_model_path, nodes_to_exclude=keep_fp32_encoder) save_onnx(proto=enc_model_onnx, model_path=encoder_fp16_model_path) del enc_model_onnx torch.cuda.empty_cache() gc.collect() Out[5]: 3772 In [6]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_encoder ) } ):\" ) keep_fp32_encoder [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_encoder)}):\") keep_fp32_encoder[:20] 20 first nodes to keep in FP32 (total 1247): Out[6]: ['MatMul_62', 'Log_89', 'Div_91', 'Mul_93', 'Softmax_109', 'Pow_120', 'MatMul_129', 'Relu_130', 'Softmax_168', 'Pow_194', 'Mul_202', 'Softmax_227', 'Pow_238', 'Pow_253', 'Softmax_286', 'Pow_297', 'Pow_312', 'Softmax_345', 'Pow_356', 'Pow_371'] Compare the output of the Onnx FP16 model with Pytorch one In [7]: Copied! enc_fp16_onnx = create_model_for_provider ( encoder_fp16_model_path , \"CUDAExecutionProvider\" ) enc_fp16_onnx_binding : IOBinding = enc_fp16_onnx . io_binding () enc_onnx_out = inference_onnx_binding ( model_onnx = enc_fp16_onnx , binding = enc_fp16_onnx_binding , inputs = { \"input_ids\" : input_ids }, device = input_ids . device . type , )[ \"output\" ] are_equal ( a = enc_onnx_out , b = out_enc . last_hidden_state ) enc_fp16_onnx = create_model_for_provider(encoder_fp16_model_path, \"CUDAExecutionProvider\") enc_fp16_onnx_binding: IOBinding = enc_fp16_onnx.io_binding() enc_onnx_out = inference_onnx_binding( model_onnx=enc_fp16_onnx, binding=enc_fp16_onnx_binding, inputs={\"input_ids\": input_ids}, device=input_ids.device.type, )[\"output\"] are_equal(a=enc_onnx_out, b=out_enc.last_hidden_state) Export decoder \u00b6 The decoder export part is more challenging: we first need to wrap it in a Pytorch model to add the final layer so it's output provide scores for each vocabulary token and can be directly used by the Hugging Face decoding algorithm then, we need to manipulate the Onnx graph to add support of Key / Value cache The second point is the key ingredient of the observed acceleration of Onnx vs Hugging Face inference. Wrapper to include some post-processing on the decoder output \u00b6 The post-processing is mainly a projection of the decoder output on a matrix with one of its dimensions equal to model vocabulary size, so we have scores for each possible token. In [8]: Copied! class ExportT5 ( torch . nn . Module ): def __init__ ( self , decoder : T5Stack , lm_head : Linear ): super ( ExportT5 , self ) . __init__ () self . decoder = decoder self . lm_head = lm_head def forward ( self , input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , past_key_values : Tuple = None ): out_dec = self . decoder . forward ( input_ids = input_ids , encoder_hidden_states = encoder_hidden_states , past_key_values = past_key_values ) # Rescale output before projecting on vocab out_dec [ \"last_hidden_state\" ] = out_dec [ \"last_hidden_state\" ] * ( pytorch_model . model_dim **- 0.5 ) out_dec [ \"last_hidden_state\" ] = self . lm_head ( out_dec [ \"last_hidden_state\" ]) return out_dec pytorch_model . cuda () model_decoder = ExportT5 ( decoder = pytorch_model . decoder , lm_head = pytorch_model . lm_head ) . eval () out_model_export : torch . Tensor = model_decoder ( input_ids = input_ids , encoder_hidden_states = out_enc . last_hidden_state ) are_equal ( a = out_model_export [ \"last_hidden_state\" ], b = out_full . logits ) class ExportT5(torch.nn.Module): def __init__(self, decoder: T5Stack, lm_head: Linear): super(ExportT5, self).__init__() self.decoder = decoder self.lm_head = lm_head def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, past_key_values: Tuple = None): out_dec = self.decoder.forward( input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values ) # Rescale output before projecting on vocab out_dec[\"last_hidden_state\"] = out_dec[\"last_hidden_state\"] * (pytorch_model.model_dim**-0.5) out_dec[\"last_hidden_state\"] = self.lm_head(out_dec[\"last_hidden_state\"]) return out_dec pytorch_model.cuda() model_decoder = ExportT5(decoder=pytorch_model.decoder, lm_head=pytorch_model.lm_head).eval() out_model_export: torch.Tensor = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state) are_equal(a=out_model_export[\"last_hidden_state\"], b=out_full.logits) Export decoder part to Onnx \u00b6 Below we export 2 versions of the decoder, one without cache support and one with it. Model inputs with past states (cache support): In [9]: Copied! model_decoder . cuda () # decoder output one step before out_dec_pytorch = model_decoder ( input_ids = input_ids [:, : - 1 ], encoder_hidden_states = out_enc . last_hidden_state ) model_inputs = { \"input_ids\" : input_ids [:, - 1 :] . type ( torch . int32 ), \"encoder_hidden_states\" : out_enc . last_hidden_state , \"past_key_values\" : out_dec_pytorch . past_key_values , } input_names = [ \"input_ids\" , \"encoder_hidden_states\" ] for i in range ( num_layers ): input_names . append ( f \"past_key_values. { i } .decoder.key\" ) input_names . append ( f \"past_key_values. { i } .decoder.value\" ) input_names . append ( f \"past_key_values. { i } .encoder.key\" ) input_names . append ( f \"past_key_values. { i } .encoder.value\" ) output_names = [ \"logits\" ] for i in range ( num_layers ): output_names . append ( f \"present. { i } .decoder.key\" ) output_names . append ( f \"present. { i } .decoder.value\" ) output_names . append ( f \"present. { i } .encoder.key\" ) output_names . append ( f \"present. { i } .encoder.value\" ) dynamic_axis = { \"input_ids\" : { 0 : \"batch\" , 1 : \"encoder_sequence\" }, \"encoder_hidden_states\" : { 0 : \"batch\" , 1 : \"encoder_sequence\" }, \"logits\" : { 0 : \"batch\" , 1 : \"decoder_sequence\" }, } for i in range ( num_layers ): dynamic_axis [ f \"past_key_values. { i } .decoder.key\" ] = { 0 : \"batch\" , 2 : \"past_decoder_sequence\" } dynamic_axis [ f \"past_key_values. { i } .decoder.value\" ] = { 0 : \"batch\" , 2 : \"past_decoder_sequence\" } dynamic_axis [ f \"past_key_values. { i } .encoder.key\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"past_key_values. { i } .encoder.value\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"present. { i } .decoder.key\" ] = { 0 : \"batch\" , 2 : \"decoder_sequence\" } dynamic_axis [ f \"present. { i } .decoder.value\" ] = { 0 : \"batch\" , 2 : \"decoder_sequence\" } dynamic_axis [ f \"present. { i } .encoder.key\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"present. { i } .encoder.value\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } model_decoder.cuda() # decoder output one step before out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state) model_inputs = { \"input_ids\": input_ids[:, -1:].type(torch.int32), \"encoder_hidden_states\": out_enc.last_hidden_state, \"past_key_values\": out_dec_pytorch.past_key_values, } input_names = [\"input_ids\", \"encoder_hidden_states\"] for i in range(num_layers): input_names.append(f\"past_key_values.{i}.decoder.key\") input_names.append(f\"past_key_values.{i}.decoder.value\") input_names.append(f\"past_key_values.{i}.encoder.key\") input_names.append(f\"past_key_values.{i}.encoder.value\") output_names = [\"logits\"] for i in range(num_layers): output_names.append(f\"present.{i}.decoder.key\") output_names.append(f\"present.{i}.decoder.value\") output_names.append(f\"present.{i}.encoder.key\") output_names.append(f\"present.{i}.encoder.value\") dynamic_axis = { \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"}, \"encoder_hidden_states\": {0: \"batch\", 1: \"encoder_sequence\"}, \"logits\": {0: \"batch\", 1: \"decoder_sequence\"}, } for i in range(num_layers): dynamic_axis[f\"past_key_values.{i}.decoder.key\"] = {0: \"batch\", 2: \"past_decoder_sequence\"} dynamic_axis[f\"past_key_values.{i}.decoder.value\"] = {0: \"batch\", 2: \"past_decoder_sequence\"} dynamic_axis[f\"past_key_values.{i}.encoder.key\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"past_key_values.{i}.encoder.value\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"present.{i}.decoder.key\"] = {0: \"batch\", 2: \"decoder_sequence\"} dynamic_axis[f\"present.{i}.decoder.value\"] = {0: \"batch\", 2: \"decoder_sequence\"} dynamic_axis[f\"present.{i}.encoder.key\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"present.{i}.encoder.value\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} Export of the model with cache support: In [10]: Copied! with torch . no_grad (): pytorch_model . config . return_dict = True pytorch_model . eval () # export can works with named args but the dict containing named args as to be last element of the args tuple torch . onnx . export ( model_decoder , ( model_inputs ,), f = dec_cache_model_path , input_names = input_names , output_names = output_names , dynamic_axes = dynamic_axis , do_constant_folding = True , opset_version = 13 , ) with torch.no_grad(): pytorch_model.config.return_dict = True pytorch_model.eval() # export can works with named args but the dict containing named args as to be last element of the args tuple torch.onnx.export( model_decoder, (model_inputs,), f=dec_cache_model_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axis, do_constant_folding=True, opset_version=13, ) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:668: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! if causal_mask.shape[1] < attention_mask.shape[1]: In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode Export of the model computing Key/Values for the whole sequence (we basically just remove past states from the input, the Pytorch code will recompute them): In [11]: Copied! model_inputs_no_cache = { \"input_ids\" : input_ids , \"encoder_hidden_states\" : out_enc . last_hidden_state , } with torch . no_grad (): pytorch_model . config . return_dict = True pytorch_model . eval () # export can works with named args but the dict containing named args as to be last element of the args tuple torch . onnx . export ( model_decoder , ( model_inputs_no_cache ,), f = dec_no_cache_model_path , input_names = list ( model_inputs_no_cache . keys ()), output_names = output_names , dynamic_axes = { k : v for k , v in dynamic_axis . items () if \"past_key_values\" not in k }, do_constant_folding = True , opset_version = 13 , ) _ = pytorch_model . cpu () # free cuda memory torch . cuda . empty_cache () model_inputs_no_cache = { \"input_ids\": input_ids, \"encoder_hidden_states\": out_enc.last_hidden_state, } with torch.no_grad(): pytorch_model.config.return_dict = True pytorch_model.eval() # export can works with named args but the dict containing named args as to be last element of the args tuple torch.onnx.export( model_decoder, (model_inputs_no_cache,), f=dec_no_cache_model_path, input_names=list(model_inputs_no_cache.keys()), output_names=output_names, dynamic_axes={k: v for k, v in dynamic_axis.items() if \"past_key_values\" not in k}, do_constant_folding=True, opset_version=13, ) _ = pytorch_model.cpu() # free cuda memory torch.cuda.empty_cache() Conversion to mixed precision \u00b6 Decoder module has different kinds of inputs, input_ids but also some float tensors. It would a bit more complicated to generate random values for those tensors: in theory it can be of any value in the FP32 range, but because of how models are initialized and trained, most of them are close to 0. To avoid too much guessing, we have decided to just take the output of the real model being fed with random input_ids . In [12]: Copied! def get_random_input_no_cache () -> Dict [ str , torch . Tensor ]: inputs = get_random_input_encoder () encoder_hidden_states = inference_onnx_binding ( model_onnx = enc_fp16_onnx , binding = enc_fp16_onnx_binding , inputs = inputs , device = \"cuda\" , clone_tensor = False , )[ \"output\" ] # it will serve as input of a FP32 model inputs [ \"encoder_hidden_states\" ] = encoder_hidden_states . type ( torch . float32 ) return inputs keep_fp32_no_cache = get_keep_fp32_nodes ( onnx_model_path = dec_no_cache_model_path , get_input = get_random_input_no_cache ) onnx_model_no_cache_fp16 = convert_fp16 ( onnx_model = dec_no_cache_model_path , nodes_to_exclude = keep_fp32_no_cache ) save_onnx ( proto = onnx_model_no_cache_fp16 , model_path = dec_no_cache_fp16_model_path ) def get_random_input_no_cache() -> Dict[str, torch.Tensor]: inputs = get_random_input_encoder() encoder_hidden_states = inference_onnx_binding( model_onnx=enc_fp16_onnx, binding=enc_fp16_onnx_binding, inputs=inputs, device=\"cuda\", clone_tensor=False, )[\"output\"] # it will serve as input of a FP32 model inputs[\"encoder_hidden_states\"] = encoder_hidden_states.type(torch.float32) return inputs keep_fp32_no_cache = get_keep_fp32_nodes(onnx_model_path=dec_no_cache_model_path, get_input=get_random_input_no_cache) onnx_model_no_cache_fp16 = convert_fp16(onnx_model=dec_no_cache_model_path, nodes_to_exclude=keep_fp32_no_cache) save_onnx(proto=onnx_model_no_cache_fp16, model_path=dec_no_cache_fp16_model_path) In [13]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_no_cache ) } ):\" ) keep_fp32_no_cache [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_no_cache)}):\") keep_fp32_no_cache[:20] 20 first nodes to keep in FP32 (total 2340): Out[13]: ['Constant_74', 'Pow_78', 'Log_135', 'Div_137', 'Mul_139', 'Softmax_154', 'Pow_165', 'MatMul_183', 'Reshape_187', 'Transpose_188', 'Softmax_212', 'Pow_223', 'Pow_238', 'Softmax_272', 'Pow_283', 'Softmax_317', 'Pow_328', 'Pow_343', 'Softmax_377', 'Pow_388'] In [14]: Copied! dec_no_cache_ort_model = create_model_for_provider ( dec_no_cache_model_path , \"CUDAExecutionProvider\" ) # use info from tokenizer size and max shape provided through the command line def get_random_input_cache () -> Dict [ str , torch . Tensor ]: inputs = get_random_input_no_cache () dec_past_states = inference_onnx_binding ( model_onnx = dec_no_cache_ort_model , inputs = inputs , device = \"cuda\" , clone_tensor = False , ) for k , v in dec_past_states . items (): if k == \"logits\" : continue new_k = k . replace ( \"present\" , \"past_key_values\" ) inputs [ new_k ] = v batch , _ = inputs [ \"input_ids\" ] . shape complement = torch . randint ( low = 0 , high = tokenizer . vocab_size , size = ( batch , 1 ), dtype = torch . int32 , device = \"cuda\" ) inputs [ \"input_ids\" ] = torch . concat ( tensors = [ inputs [ \"input_ids\" ], complement ], dim = 1 ) return inputs keep_fp32_cache = get_keep_fp32_nodes ( onnx_model_path = dec_cache_model_path , get_input = get_random_input_cache ) del dec_no_cache_ort_model # free cuda memory torch . cuda . empty_cache () gc . collect () onnx_model_cache_fp16 = convert_fp16 ( onnx_model = dec_cache_model_path , nodes_to_exclude = keep_fp32_cache ) save_onnx ( proto = onnx_model_cache_fp16 , model_path = dec_cache_fp16_model_path ) dec_no_cache_ort_model = create_model_for_provider(dec_no_cache_model_path, \"CUDAExecutionProvider\") # use info from tokenizer size and max shape provided through the command line def get_random_input_cache() -> Dict[str, torch.Tensor]: inputs = get_random_input_no_cache() dec_past_states = inference_onnx_binding( model_onnx=dec_no_cache_ort_model, inputs=inputs, device=\"cuda\", clone_tensor=False, ) for k, v in dec_past_states.items(): if k == \"logits\": continue new_k = k.replace(\"present\", \"past_key_values\") inputs[new_k] = v batch, _ = inputs[\"input_ids\"].shape complement = torch.randint(low=0, high=tokenizer.vocab_size, size=(batch, 1), dtype=torch.int32, device=\"cuda\") inputs[\"input_ids\"] = torch.concat(tensors=[inputs[\"input_ids\"], complement], dim=1) return inputs keep_fp32_cache = get_keep_fp32_nodes(onnx_model_path=dec_cache_model_path, get_input=get_random_input_cache) del dec_no_cache_ort_model # free cuda memory torch.cuda.empty_cache() gc.collect() onnx_model_cache_fp16 = convert_fp16(onnx_model=dec_cache_model_path, nodes_to_exclude=keep_fp32_cache) save_onnx(proto=onnx_model_cache_fp16, model_path=dec_cache_fp16_model_path) In [15]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_cache ) } ):\" ) keep_fp32_cache [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_cache)}):\") keep_fp32_cache[:20] 20 first nodes to keep in FP32 (total 2240): Out[15]: ['Constant_94', 'Pow_98', 'Log_161', 'Div_163', 'Mul_165', 'Softmax_188', 'MatMul_189', 'Transpose_190', 'Reshape_194', 'Pow_202', 'MatMul_221', 'Reshape_225', 'Transpose_226', 'Softmax_246', 'MatMul_247', 'Transpose_248', 'Reshape_252', 'Pow_257', 'Pow_272', 'Softmax_308'] Merge Onnx computation graph to deduplicate weights \u00b6 Finally, we will merge the 2 decoders together. The idea is simple: we prefix the node / edge names of one of them to avoid naming collision we deduplicate the weights (the same weight matrix will have different names in the 2 models) we join the 2 computation graphs through an If node we generate the Onnx file The new model will take a new input, enable_cache . When it contains a True value, computation graph with cache support is used. code below is written to be easy to read, but could be made much faster to run In [16]: Copied! prefix = \"cache_node_\" mapping_initializer_cache_to_no_cache = dict () # search for not-duplicated weights, called initializer in Onnx to_add = list () for node_cache in onnx_model_cache_fp16 . graph . initializer : found = False for node_no_cache in onnx_model_no_cache_fp16 . graph . initializer : if node_cache . raw_data == node_no_cache . raw_data : found = True mapping_initializer_cache_to_no_cache [ node_cache . name ] = node_no_cache . name break if not found : node_cache . name = prefix + node_cache . name to_add . append ( node_cache ) mapping_initializer_cache_to_no_cache [ node_cache . name ] = node_cache . name onnx_model_no_cache_fp16 . graph . initializer . extend ( to_add ) # I/O model names should not be prefixed model_io_names = [ n . name for n in list ( onnx_model_cache_fp16 . graph . input ) + list ( onnx_model_cache_fp16 . graph . output )] # replace pointers to duplicated weights to their deduplicated version for node in onnx_model_cache_fp16 . graph . node : for index , input_name in enumerate ( node . input ): if input_name in model_io_names : continue node . input [ index ] = mapping_initializer_cache_to_no_cache . get ( input_name , prefix + input_name ) for index , output_name in enumerate ( node . output ): if output_name in model_io_names : continue node . output [ index ] = prefix + output_name node . name = prefix + node . name model_io_names = [ n . name for n in list ( onnx_model_cache_fp16 . graph . input ) + list ( onnx_model_cache_fp16 . graph . output )] # prefix nodes to avoid naming collision prefix = \"init_\" cache = dict () for node in onnx_model_no_cache_fp16 . graph . initializer : if node . name in model_io_names : new_name = prefix + node . name cache [ node . name ] = new_name node . name = new_name for node in onnx_model_no_cache_fp16 . graph . node : for input_index , n in enumerate ( node . input ): node . input [ input_index ] = cache . get ( n , n ) # mandatory for subgraph in if/else node assert len ( onnx_model_cache_fp16 . graph . output ) == len ( onnx_model_no_cache_fp16 . graph . output ), f \" { len ( onnx_model_cache_fp16 . graph . output ) } vs { len ( onnx_model_no_cache_fp16 . graph . output ) } \" # build a computation graph with cache support graph_cache : onnx . GraphProto = onnx . helper . make_graph ( nodes = list ( onnx_model_cache_fp16 . graph . node ), name = \"graph-cache\" , inputs = [], outputs = list ( onnx_model_cache_fp16 . graph . output ), initializer = [], ) # build a computation which doesn't need past states to run graph_no_cache : onnx . GraphProto = onnx . helper . make_graph ( nodes = list ( onnx_model_no_cache_fp16 . graph . node ), name = \"graph-no-cache\" , inputs = [], outputs = list ( onnx_model_no_cache_fp16 . graph . output ), initializer = [], ) # a new input to decide if we use past state or not enable_cache_input = onnx . helper . make_tensor_value_info ( name = \"enable_cache\" , elem_type = onnx . TensorProto . BOOL , shape = [ 1 ]) if_node = onnx . helper . make_node ( op_type = \"If\" , inputs = [ \"enable_cache\" ], outputs = [ o . name for o in list ( onnx_model_no_cache_fp16 . graph . output )], then_branch = graph_cache , else_branch = graph_no_cache , ) # final model which can disable its cache if_graph_def : GraphProto = helper . make_graph ( nodes = [ if_node ], name = \"if-model\" , inputs = list ( onnx_model_cache_fp16 . graph . input ) + [ enable_cache_input ], outputs = list ( onnx_model_no_cache_fp16 . graph . output ), initializer = list ( onnx_model_no_cache_fp16 . graph . initializer ), ) # serialization and cleaning model_if : ModelProto = helper . make_model ( if_graph_def , producer_name = \"onnx-example\" , opset_imports = [ helper . make_opsetid ( onnx . defs . ONNX_DOMAIN , 13 )] ) save_onnx ( proto = model_if , model_path = dec_if_fp16_model_path ) del model_if torch . cuda . empty_cache () gc . collect () prefix = \"cache_node_\" mapping_initializer_cache_to_no_cache = dict() # search for not-duplicated weights, called initializer in Onnx to_add = list() for node_cache in onnx_model_cache_fp16.graph.initializer: found = False for node_no_cache in onnx_model_no_cache_fp16.graph.initializer: if node_cache.raw_data == node_no_cache.raw_data: found = True mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name break if not found: node_cache.name = prefix + node_cache.name to_add.append(node_cache) mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name onnx_model_no_cache_fp16.graph.initializer.extend(to_add) # I/O model names should not be prefixed model_io_names = [n.name for n in list(onnx_model_cache_fp16.graph.input) + list(onnx_model_cache_fp16.graph.output)] # replace pointers to duplicated weights to their deduplicated version for node in onnx_model_cache_fp16.graph.node: for index, input_name in enumerate(node.input): if input_name in model_io_names: continue node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name) for index, output_name in enumerate(node.output): if output_name in model_io_names: continue node.output[index] = prefix + output_name node.name = prefix + node.name model_io_names = [n.name for n in list(onnx_model_cache_fp16.graph.input) + list(onnx_model_cache_fp16.graph.output)] # prefix nodes to avoid naming collision prefix = \"init_\" cache = dict() for node in onnx_model_no_cache_fp16.graph.initializer: if node.name in model_io_names: new_name = prefix + node.name cache[node.name] = new_name node.name = new_name for node in onnx_model_no_cache_fp16.graph.node: for input_index, n in enumerate(node.input): node.input[input_index] = cache.get(n, n) # mandatory for subgraph in if/else node assert len(onnx_model_cache_fp16.graph.output) == len( onnx_model_no_cache_fp16.graph.output ), f\"{len(onnx_model_cache_fp16.graph.output)} vs {len(onnx_model_no_cache_fp16.graph.output)}\" # build a computation graph with cache support graph_cache: onnx.GraphProto = onnx.helper.make_graph( nodes=list(onnx_model_cache_fp16.graph.node), name=\"graph-cache\", inputs=[], outputs=list(onnx_model_cache_fp16.graph.output), initializer=[], ) # build a computation which doesn't need past states to run graph_no_cache: onnx.GraphProto = onnx.helper.make_graph( nodes=list(onnx_model_no_cache_fp16.graph.node), name=\"graph-no-cache\", inputs=[], outputs=list(onnx_model_no_cache_fp16.graph.output), initializer=[], ) # a new input to decide if we use past state or not enable_cache_input = onnx.helper.make_tensor_value_info(name=\"enable_cache\", elem_type=onnx.TensorProto.BOOL, shape=[1]) if_node = onnx.helper.make_node( op_type=\"If\", inputs=[\"enable_cache\"], outputs=[o.name for o in list(onnx_model_no_cache_fp16.graph.output)], then_branch=graph_cache, else_branch=graph_no_cache, ) # final model which can disable its cache if_graph_def: GraphProto = helper.make_graph( nodes=[if_node], name=\"if-model\", inputs=list(onnx_model_cache_fp16.graph.input) + [enable_cache_input], outputs=list(onnx_model_no_cache_fp16.graph.output), initializer=list(onnx_model_no_cache_fp16.graph.initializer), ) # serialization and cleaning model_if: ModelProto = helper.make_model( if_graph_def, producer_name=\"onnx-example\", opset_imports=[helper.make_opsetid(onnx.defs.ONNX_DOMAIN, 13)] ) save_onnx(proto=model_if, model_path=dec_if_fp16_model_path) del model_if torch.cuda.empty_cache() gc.collect() Out[16]: 6366 Check Onnx decoder output \u00b6 Compare Onnx output with and without cache, plus compare with Pytorch output. In [17]: Copied! pytorch_model = pytorch_model . cuda () model_decoder = model_decoder . cuda () input_ids = input_ids . cuda () pytorch_model = pytorch_model . eval () model_decoder = model_decoder . eval () dec_onnx = create_model_for_provider ( dec_if_fp16_model_path , \"CUDAExecutionProvider\" , log_severity = 3 ) dec_onnx_binding : IOBinding = dec_onnx . io_binding () pytorch_model = pytorch_model.cuda() model_decoder = model_decoder.cuda() input_ids = input_ids.cuda() pytorch_model = pytorch_model.eval() model_decoder = model_decoder.eval() dec_onnx = create_model_for_provider(dec_if_fp16_model_path, \"CUDAExecutionProvider\", log_severity=3) dec_onnx_binding: IOBinding = dec_onnx.io_binding() Zero copy output \u00b6 Below, we check that the new model output is similar to the ones from Pytorch . We use our new implementation of inference call. The idea is the following: we ask Onnx Runtime to output a pointer to the CUDA array containing the result of the inference; we use Cupy API to wrap the array and provide information regarding tensor shape and type. Cupy doesn't own the data; we use Dlpack support to convert the Cupy tensor to Pytorch , another zero copy process. This pipeline is unsafe, as the content of the tensor may change or disappear silently: only Onnx Runtime has the control of the array containing the data. It will happen at the next inference call. Because we know that during the text generation we discard each output before recalling Onnx Runtime , it works well in our case. A second benefit of this approach is that we do not have anymore to guess the output shape. Before using this approach, to avoid the output to be stored on host memory (RAM) which made inference slower, we had to provide Onnx Runtime with a pointer to Pytorch tensor with the right size. As the size change with the sequence length (so it changes for each generated token), we had to store the logic to guess the size somewhere in the code. The new approach frees us from this burden. In [18]: Copied! pytorch_model = pytorch_model . half () with torch . inference_mode (): out_enc_pytorch : BaseModelOutputWithPastAndCrossAttentions = pytorch_model . encoder ( input_ids = input_ids ) previous_step_pytorch : BaseModelOutputWithPastAndCrossAttentions = model_decoder ( input_ids = input_ids [:, : - 1 ], encoder_hidden_states = out_enc_pytorch . last_hidden_state ) out_dec_pytorch : BaseModelOutputWithPastAndCrossAttentions = model_decoder ( input_ids = input_ids , encoder_hidden_states = out_enc_pytorch . last_hidden_state ) pytorch_model = pytorch_model.half() with torch.inference_mode(): out_enc_pytorch: BaseModelOutputWithPastAndCrossAttentions = pytorch_model.encoder(input_ids=input_ids) previous_step_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder( input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc_pytorch.last_hidden_state ) out_dec_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder( input_ids=input_ids, encoder_hidden_states=out_enc_pytorch.last_hidden_state ) In [19]: Copied! def decoder_pytorch_inference ( decoder_input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , ** _ ): with torch . inference_mode (): return model_decoder ( input_ids = decoder_input_ids , encoder_hidden_states = encoder_hidden_states ) def decoder_onnx_inference ( decoder_input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , enable_cache : torch . Tensor , past_key_values : Optional [ torch . Tensor ], ): inputs_onnx_dict = { \"input_ids\" : decoder_input_ids , \"encoder_hidden_states\" : encoder_hidden_states , \"enable_cache\" : enable_cache , } if past_key_values is not None : for index , ( k_dec , v_dec , k_enc , v_enc ) in enumerate ( past_key_values ): inputs_onnx_dict [ f \"past_key_values. { index } .decoder.key\" ] = k_dec inputs_onnx_dict [ f \"past_key_values. { index } .decoder.value\" ] = v_dec inputs_onnx_dict [ f \"past_key_values. { index } .encoder.key\" ] = k_enc inputs_onnx_dict [ f \"past_key_values. { index } .encoder.value\" ] = v_enc result_dict = inference_onnx_binding ( model_onnx = dec_onnx , inputs = inputs_onnx_dict , binding = dec_onnx_binding , # recycle the binding device = decoder_input_ids . device . type , clone_tensor = False , # no memory copy -> best perf and lowest memory footprint! ) past_states = list () for index in range ( pytorch_model . config . num_layers ): kv = ( result_dict [ f \"present. { index } .decoder.key\" ], result_dict [ f \"present. { index } .decoder.value\" ], result_dict [ f \"present. { index } .encoder.key\" ], result_dict [ f \"present. { index } .encoder.value\" ], ) past_states . append ( kv ) return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = result_dict [ \"logits\" ], past_key_values = past_states , ) out_dec_onnx_no_cache = decoder_onnx_inference ( decoder_input_ids = input_ids , encoder_hidden_states = out_enc_pytorch . last_hidden_state , enable_cache = torch . tensor ([ False ], device = \"cuda\" , dtype = torch . bool ), past_key_values = None , ) are_equal ( a = out_dec_onnx_no_cache . last_hidden_state [:, - 1 :, :], b = out_dec_pytorch . last_hidden_state [:, - 1 :, :]) # check that past states are identical between Onnx and Pytorch assert len ( out_dec_onnx_no_cache . past_key_values ) == len ( out_dec_pytorch . past_key_values ) for ( o_dec_k , o_dev_v , o_enc_k , o_enc_v ), ( p_dec_k , p_dev_v , p_enc_k , p_enc_v ) in zip ( out_dec_onnx_no_cache . past_key_values , out_dec_pytorch . past_key_values ): are_equal ( a = o_dec_k , b = p_dec_k ) are_equal ( a = o_dev_v , b = p_dev_v ) are_equal ( a = o_enc_k , b = p_enc_k ) are_equal ( a = o_enc_v , b = p_enc_v ) def decoder_pytorch_inference(decoder_input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, **_): with torch.inference_mode(): return model_decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_hidden_states) def decoder_onnx_inference( decoder_input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, past_key_values: Optional[torch.Tensor], ): inputs_onnx_dict = { \"input_ids\": decoder_input_ids, \"encoder_hidden_states\": encoder_hidden_states, \"enable_cache\": enable_cache, } if past_key_values is not None: for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(past_key_values): inputs_onnx_dict[f\"past_key_values.{index}.decoder.key\"] = k_dec inputs_onnx_dict[f\"past_key_values.{index}.decoder.value\"] = v_dec inputs_onnx_dict[f\"past_key_values.{index}.encoder.key\"] = k_enc inputs_onnx_dict[f\"past_key_values.{index}.encoder.value\"] = v_enc result_dict = inference_onnx_binding( model_onnx=dec_onnx, inputs=inputs_onnx_dict, binding=dec_onnx_binding, # recycle the binding device=decoder_input_ids.device.type, clone_tensor=False, # no memory copy -> best perf and lowest memory footprint! ) past_states = list() for index in range(pytorch_model.config.num_layers): kv = ( result_dict[f\"present.{index}.decoder.key\"], result_dict[f\"present.{index}.decoder.value\"], result_dict[f\"present.{index}.encoder.key\"], result_dict[f\"present.{index}.encoder.value\"], ) past_states.append(kv) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=result_dict[\"logits\"], past_key_values=past_states, ) out_dec_onnx_no_cache = decoder_onnx_inference( decoder_input_ids=input_ids, encoder_hidden_states=out_enc_pytorch.last_hidden_state, enable_cache=torch.tensor([False], device=\"cuda\", dtype=torch.bool), past_key_values=None, ) are_equal(a=out_dec_onnx_no_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :]) # check that past states are identical between Onnx and Pytorch assert len(out_dec_onnx_no_cache.past_key_values) == len(out_dec_pytorch.past_key_values) for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip( out_dec_onnx_no_cache.past_key_values, out_dec_pytorch.past_key_values ): are_equal(a=o_dec_k, b=p_dec_k) are_equal(a=o_dev_v, b=p_dev_v) are_equal(a=o_enc_k, b=p_enc_k) are_equal(a=o_enc_v, b=p_enc_v) In [20]: Copied! out_dec_onnx_cache = decoder_onnx_inference ( decoder_input_ids = input_ids [:, - 1 :], encoder_hidden_states = out_enc_pytorch . last_hidden_state , enable_cache = torch . tensor ([ True ], device = \"cuda\" , dtype = torch . bool ), past_key_values = previous_step_pytorch . past_key_values , ) are_equal ( a = out_dec_onnx_cache . last_hidden_state [:, - 1 :, :], b = out_dec_pytorch . last_hidden_state [:, - 1 :, :]) # check that past states are identical between Onnx and Pytorch assert len ( out_dec_onnx_cache . past_key_values ) == len ( out_dec_pytorch . past_key_values ) for ( o_dec_k , o_dev_v , o_enc_k , o_enc_v ), ( p_dec_k , p_dev_v , p_enc_k , p_enc_v ) in zip ( out_dec_onnx_cache . past_key_values , out_dec_pytorch . past_key_values ): are_equal ( a = o_dec_k , b = p_dec_k ) are_equal ( a = o_dev_v , b = p_dev_v ) are_equal ( a = o_enc_k , b = p_enc_k ) are_equal ( a = o_enc_v , b = p_enc_v ) out_dec_onnx_cache = decoder_onnx_inference( decoder_input_ids=input_ids[:, -1:], encoder_hidden_states=out_enc_pytorch.last_hidden_state, enable_cache=torch.tensor([True], device=\"cuda\", dtype=torch.bool), past_key_values=previous_step_pytorch.past_key_values, ) are_equal(a=out_dec_onnx_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :]) # check that past states are identical between Onnx and Pytorch assert len(out_dec_onnx_cache.past_key_values) == len(out_dec_pytorch.past_key_values) for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip( out_dec_onnx_cache.past_key_values, out_dec_pytorch.past_key_values ): are_equal(a=o_dec_k, b=p_dec_k) are_equal(a=o_dev_v, b=p_dev_v) are_equal(a=o_enc_k, b=p_enc_k) are_equal(a=o_enc_v, b=p_enc_v) Benchmarks! \u00b6 Finally, we will compare the performances of 4 setup in end-to-end scenarii: Pytorch Pytorch + cache Onnx Onnx + cache For the comparison, we first do a sanity check by just generating a short sequence (we already have checked that output tensors are OK). Then we force each model to generate: 256 tokens + batch size 1 (similar to TensorRT demo) 1000 tokens + batch size 4 In [21]: Copied! def encoder_onnx_inference ( input_ids : torch . Tensor , ** _ ) -> BaseModelOutputWithPastAndCrossAttentions : last_hidden_state = inference_onnx_binding ( model_onnx = enc_fp16_onnx , # noqa: F821 inputs = { \"input_ids\" : input_ids }, device = input_ids . device . type , binding = enc_fp16_onnx_binding , )[ \"output\" ] return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = last_hidden_state . type ( torch . float16 )) def encoder_pytorch_inference ( input_ids , ** _ ) -> BaseModelOutputWithPastAndCrossAttentions : with torch . inference_mode (): res = pytorch_model . encoder ( input_ids = input_ids ) . type ( torch . float16 ) return res # https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py class ExtT5 ( torch . nn . Module , GenerationMixin ): def __init__ ( self , config : PretrainedConfig , device : torch . device , encoder_func : Callable , decoder_func : Callable ): super ( ExtT5 , self ) . __init__ () self . main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 self . config : PretrainedConfig = config self . device : torch . device = device self . encoder_func = encoder_func self . decoder_func = decoder_func self . use_cache = True self . timings = list () def get_encoder ( self ): return self . encoder_func def get_decoder ( self ): return self . decoder_func def set_cache ( self , enable : bool ) -> None : self . use_cache = enable # from transformers library (modeling_t5.py) def _reorder_cache ( self , past , beam_idx ): reordered_decoder_past = () for layer_past_states in past : # get the correct batch idx from layer past batch dim # batch dim of `past` is at 2nd position reordered_layer_past_states = () for layer_past_state in layer_past_states : # need to set correct `past` for each of the four key / value states reordered_layer_past_states = reordered_layer_past_states + ( layer_past_state . index_select ( 0 , beam_idx ), ) assert reordered_layer_past_states [ 0 ] . shape == layer_past_states [ 0 ] . shape assert len ( reordered_layer_past_states ) == len ( layer_past_states ) reordered_decoder_past = reordered_decoder_past + ( reordered_layer_past_states ,) return reordered_decoder_past def prepare_inputs_for_generation ( self , input_ids , past = None , use_cache = None , ** kwargs ) -> Dict [ str , torch . Tensor ]: params = { \"encoder_hidden_states\" : kwargs [ \"encoder_outputs\" ][ \"last_hidden_state\" ], } if past is None : # this is the 1st inferred token self . timings = list () if not self . use_cache : past = None if past is None : params [ self . main_input_name ] = input_ids params [ \"enable_cache\" ] = torch . tensor ([ False ], device = \"cuda\" , dtype = torch . bool ) else : params [ self . main_input_name ] = input_ids [:, - 1 :] params [ \"enable_cache\" ] = torch . tensor ([ True ], device = \"cuda\" , dtype = torch . bool ) params [ \"past_key_values\" ] = past return params def forward ( self , input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , enable_cache : torch . Tensor , past_key_values : Optional [ torch . Tensor ] = None , ** _ , ): start_timer = time . monotonic () dec_output = self . get_decoder ()( decoder_input_ids = input_ids , encoder_hidden_states = encoder_hidden_states , enable_cache = enable_cache , past_key_values = past_key_values , ) self . timings . append ( time . monotonic () - start_timer ) return Seq2SeqLMOutput ( logits = dec_output . last_hidden_state , past_key_values = dec_output . past_key_values ) model_gen = ( ExtT5 ( config = pytorch_model . config , device = pytorch_model . device , encoder_func = encoder_onnx_inference , # encoder_pytorch_inference decoder_func = decoder_onnx_inference , # decoder_pytorch_inference ) . cuda () . eval () ) torch . cuda . synchronize () with torch . inference_mode (): print ( \"Onnx:\" ) print ( tokenizer . decode ( model_gen . generate ( inputs = input_ids , min_length = 3 , max_length = 60 , num_beams = 4 , no_repeat_ngram_size = 2 , )[ 0 ], skip_special_tokens = True , ) ) print ( \"Pytorch:\" ) print ( tokenizer . decode ( pytorch_model . generate ( input_ids = input_ids , min_length = 3 , max_length = 60 , num_beams = 4 , no_repeat_ngram_size = 2 , )[ 0 ], skip_special_tokens = True , ) ) def encoder_onnx_inference(input_ids: torch.Tensor, **_) -> BaseModelOutputWithPastAndCrossAttentions: last_hidden_state = inference_onnx_binding( model_onnx=enc_fp16_onnx, # noqa: F821 inputs={\"input_ids\": input_ids}, device=input_ids.device.type, binding=enc_fp16_onnx_binding, )[\"output\"] return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state.type(torch.float16)) def encoder_pytorch_inference(input_ids, **_) -> BaseModelOutputWithPastAndCrossAttentions: with torch.inference_mode(): res = pytorch_model.encoder(input_ids=input_ids).type(torch.float16) return res # https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py class ExtT5(torch.nn.Module, GenerationMixin): def __init__(self, config: PretrainedConfig, device: torch.device, encoder_func: Callable, decoder_func: Callable): super(ExtT5, self).__init__() self.main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 self.config: PretrainedConfig = config self.device: torch.device = device self.encoder_func = encoder_func self.decoder_func = decoder_func self.use_cache = True self.timings = list() def get_encoder(self): return self.encoder_func def get_decoder(self): return self.decoder_func def set_cache(self, enable: bool) -> None: self.use_cache = enable # from transformers library (modeling_t5.py) def _reorder_cache(self, past, beam_idx): reordered_decoder_past = () for layer_past_states in past: # get the correct batch idx from layer past batch dim # batch dim of `past` is at 2nd position reordered_layer_past_states = () for layer_past_state in layer_past_states: # need to set correct `past` for each of the four key / value states reordered_layer_past_states = reordered_layer_past_states + ( layer_past_state.index_select(0, beam_idx), ) assert reordered_layer_past_states[0].shape == layer_past_states[0].shape assert len(reordered_layer_past_states) == len(layer_past_states) reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,) return reordered_decoder_past def prepare_inputs_for_generation(self, input_ids, past=None, use_cache=None, **kwargs) -> Dict[str, torch.Tensor]: params = { \"encoder_hidden_states\": kwargs[\"encoder_outputs\"][\"last_hidden_state\"], } if past is None: # this is the 1st inferred token self.timings = list() if not self.use_cache: past = None if past is None: params[self.main_input_name] = input_ids params[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool) else: params[self.main_input_name] = input_ids[:, -1:] params[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool) params[\"past_key_values\"] = past return params def forward( self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, past_key_values: Optional[torch.Tensor] = None, **_, ): start_timer = time.monotonic() dec_output = self.get_decoder()( decoder_input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, enable_cache=enable_cache, past_key_values=past_key_values, ) self.timings.append(time.monotonic() - start_timer) return Seq2SeqLMOutput(logits=dec_output.last_hidden_state, past_key_values=dec_output.past_key_values) model_gen = ( ExtT5( config=pytorch_model.config, device=pytorch_model.device, encoder_func=encoder_onnx_inference, # encoder_pytorch_inference decoder_func=decoder_onnx_inference, # decoder_pytorch_inference ) .cuda() .eval() ) torch.cuda.synchronize() with torch.inference_mode(): print(\"Onnx:\") print( tokenizer.decode( model_gen.generate( inputs=input_ids, min_length=3, max_length=60, num_beams=4, no_repeat_ngram_size=2, )[0], skip_special_tokens=True, ) ) print(\"Pytorch:\") print( tokenizer.decode( pytorch_model.generate( input_ids=input_ids, min_length=3, max_length=60, num_beams=4, no_repeat_ngram_size=2, )[0], skip_special_tokens=True, ) ) Onnx: Ce mod\u00e8le est maintenant tr\u00e8s rapide! Pytorch: Ce mod\u00e8le est maintenant tr\u00e8s rapide! In [22]: Copied! def print_timings ( name : str , total : float , inference : float ): percent_inference = 100 * inference / total print ( f \" { name } : { total : .1f } , including inference: { inference : .1f } ( { percent_inference : .1f } %)\" ) all_timings : Dict [ str , Dict [ str , List [ float ]]] = dict () for seq_len , num_beam in [( 256 , 1 ), ( 1000 , 4 )]: timings = dict () print ( f \"seq len: { seq_len } / # beam (batch size): { num_beam } \" ) task = \"Onnx\" with nvtx . annotate ( task , color = \"red\" ): # nvtx is for Nvidia nsight profiler, you can remove the line or install the library model_gen . set_cache ( enable = False ) # warmup model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) start = time . monotonic () model_gen . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start print_timings ( name = task , total = total_time , inference = sum ( model_gen . timings )) timings [ f \" { task } \" ] = model_gen . timings task = \"Onnx + cache\" with nvtx . annotate ( task , color = \"red\" ): model_gen . set_cache ( enable = True ) # warmup model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) start = time . monotonic () model_gen . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start print_timings ( name = task , total = total_time , inference = sum ( model_gen . timings )) timings [ f \" { task } \" ] = model_gen . timings # monckey patching of forward function to add a timer per generated token old_fw = pytorch_model . forward timing_pytorch = list () def new_fw ( self , * args , ** kwargs ): timer_start = time . monotonic () res = old_fw ( self , * args , ** kwargs ) torch . cuda . synchronize () # makes timings correct without having significant impact on e2e latency total_time = time . monotonic () - timer_start timing_pytorch . append ( total_time ) return res task = \"Pytorch\" with nvtx . annotate ( task , color = \"orange\" ): pytorch_model . config . use_cache = False with torch . inference_mode (): with torch . cuda . amp . autocast (): # warmup pytorch_model . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) pytorch_model . forward = new_fw . __get__ ( pytorch_model ) start = time . monotonic () pytorch_model . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start pytorch_model . forward = old_fw inference_time = np . sum ( timing_pytorch ) print_timings ( name = \"Pytorch\" , total = total_time , inference = inference_time ) timing_pytorch_no_cache = copy ( timing_pytorch ) timings [ f \" { task } \" ] = copy ( timing_pytorch ) timing_pytorch . clear () torch . cuda . empty_cache () task = \"Pytorch + cache\" with nvtx . annotate ( \"Pytorch + cache\" , color = \"green\" ): pytorch_model . config . use_cache = True with torch . inference_mode (): with torch . cuda . amp . autocast (): # warmup pytorch_model . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) pytorch_model . forward = new_fw . __get__ ( pytorch_model ) start = time . monotonic () pytorch_model . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start pytorch_model . forward = old_fw print_timings ( name = \"Pytorch + cache\" , total = total_time , inference = sum ( timing_pytorch )) timings [ f \" { task } \" ] = copy ( timing_pytorch ) timing_pytorch . clear () all_timings [ f \" { seq_len } / { num_beam } \" ] = timings torch . cuda . empty_cache () def print_timings(name: str, total: float, inference: float): percent_inference = 100 * inference / total print(f\"{name}: {total:.1f}, including inference: {inference:.1f} ({percent_inference:.1f}%)\") all_timings: Dict[str, Dict[str, List[float]]] = dict() for seq_len, num_beam in [(256, 1), (1000, 4)]: timings = dict() print(f\"seq len: {seq_len} / # beam (batch size): {num_beam}\") task = \"Onnx\" with nvtx.annotate( task, color=\"red\" ): # nvtx is for Nvidia nsight profiler, you can remove the line or install the library model_gen.set_cache(enable=False) # warmup model_gen.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) start = time.monotonic() model_gen.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start print_timings(name=task, total=total_time, inference=sum(model_gen.timings)) timings[f\"{task}\"] = model_gen.timings task = \"Onnx + cache\" with nvtx.annotate(task, color=\"red\"): model_gen.set_cache(enable=True) # warmup model_gen.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) start = time.monotonic() model_gen.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start print_timings(name=task, total=total_time, inference=sum(model_gen.timings)) timings[f\"{task}\"] = model_gen.timings # monckey patching of forward function to add a timer per generated token old_fw = pytorch_model.forward timing_pytorch = list() def new_fw(self, *args, **kwargs): timer_start = time.monotonic() res = old_fw(self, *args, **kwargs) torch.cuda.synchronize() # makes timings correct without having significant impact on e2e latency total_time = time.monotonic() - timer_start timing_pytorch.append(total_time) return res task = \"Pytorch\" with nvtx.annotate(task, color=\"orange\"): pytorch_model.config.use_cache = False with torch.inference_mode(): with torch.cuda.amp.autocast(): # warmup pytorch_model.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) pytorch_model.forward = new_fw.__get__(pytorch_model) start = time.monotonic() pytorch_model.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start pytorch_model.forward = old_fw inference_time = np.sum(timing_pytorch) print_timings(name=\"Pytorch\", total=total_time, inference=inference_time) timing_pytorch_no_cache = copy(timing_pytorch) timings[f\"{task}\"] = copy(timing_pytorch) timing_pytorch.clear() torch.cuda.empty_cache() task = \"Pytorch + cache\" with nvtx.annotate(\"Pytorch + cache\", color=\"green\"): pytorch_model.config.use_cache = True with torch.inference_mode(): with torch.cuda.amp.autocast(): # warmup pytorch_model.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) pytorch_model.forward = new_fw.__get__(pytorch_model) start = time.monotonic() pytorch_model.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start pytorch_model.forward = old_fw print_timings(name=\"Pytorch + cache\", total=total_time, inference=sum(timing_pytorch)) timings[f\"{task}\"] = copy(timing_pytorch) timing_pytorch.clear() all_timings[f\"{seq_len} / {num_beam}\"] = timings torch.cuda.empty_cache() seq len: 256 / # beam (batch size): 1 Onnx: 3.9, including inference: 3.8 (96.9%) Onnx + cache: 3.3, including inference: 3.1 (96.3%) Pytorch: 9.8, including inference: 9.7 (99.0%) Pytorch + cache: 8.2, including inference: 8.1 (98.7%) seq len: 1000 / # beam (batch size): 4 Onnx: 99.4, including inference: 97.1 (97.7%) Onnx + cache: 17.7, including inference: 15.6 (87.8%) Pytorch: 96.3, including inference: 95.5 (99.1%) Pytorch + cache: 37.0, including inference: 35.0 (94.6%) Benchmark analysis \u00b6 Below, we plot for each setup (short and long sequence): the time spent on each token generation the full time to generate the sequence (for each length) We can see that for short sequence and batch size of 1, cache or not, latency appears to be stable. However, for longer sequences, we can see that the no cache approach (being Pytorch or Onnx based) doesn't scale well, and at some point, Onnx is even slower than Hugging Face code with cache support. On the other side, Onnx timings are mostly stable whatever the sequence length which is quite remarkable. It's because we are working one token at a time and converted a quadratic complexity in the attention layer into a linear one. In [23]: Copied! sns . set_style ( \"darkgrid\" ) # darkgrid, whitegrid, dark, white and ticks plt . rc ( \"axes\" , titlesize = 15 ) # fontsize of the axes title plt . rc ( \"axes\" , labelsize = 14 ) # fontsize of the x and y labels plt . rc ( \"xtick\" , labelsize = 13 ) # fontsize of the tick labels plt . rc ( \"ytick\" , labelsize = 13 ) # fontsize of the tick labels plt . rc ( \"legend\" , fontsize = 15 ) # legend fontsize plt . rc ( \"font\" , size = 13 ) # controls default text sizes colors = sns . color_palette ( \"deep\" ) fig = plt . figure ( constrained_layout = True , figsize = ( 12 , 8 )) subfigs = fig . subfigures ( nrows = 2 , ncols = 1 ) fig . supxlabel ( \"seq len (# tokens)\" ) fig . supylabel ( \"latency (s)\" ) fig . suptitle ( f \"Small seq len and greedy search on { model_name } don't tell the whole (inference) story...\" ) for row , ( plot_name , timings ) in enumerate ( all_timings . items ()): subfigs [ row ] . suptitle ( f \"setup # { 1 + row } : { plot_name } (seq len / beam search)\" ) axs = subfigs [ row ] . subplots ( nrows = 1 , ncols = 2 ) for col , accumulated in enumerate ([ False , True ]): plot_axis = axs [ col ] for index , ( k , v ) in enumerate ( timings . items ()): axis = range ( len ( v )) color = colors [ index ] v = np . array ( v ) # remove extreme values p99 = np . percentile ( v , 99 ) v [ v > p99 ] = p99 v = np . cumsum ( v ) if accumulated else v plot_axis . scatter ( axis , v , label = k , s = 2 ) title = f \"latency for the full sequence\" if accumulated else f \"latency for each token\" plot_axis . title . set_text ( title ) # legend deduplication handles , labels = plt . gca () . get_legend_handles_labels () by_label = dict ( zip ( labels , handles )) fig . legend ( by_label . values (), by_label . keys (), bbox_to_anchor = ( 1 , 1 ), loc = \"upper left\" , markerscale = 5 ) plt . show () sns.set_style(\"darkgrid\") # darkgrid, whitegrid, dark, white and ticks plt.rc(\"axes\", titlesize=15) # fontsize of the axes title plt.rc(\"axes\", labelsize=14) # fontsize of the x and y labels plt.rc(\"xtick\", labelsize=13) # fontsize of the tick labels plt.rc(\"ytick\", labelsize=13) # fontsize of the tick labels plt.rc(\"legend\", fontsize=15) # legend fontsize plt.rc(\"font\", size=13) # controls default text sizes colors = sns.color_palette(\"deep\") fig = plt.figure(constrained_layout=True, figsize=(12, 8)) subfigs = fig.subfigures(nrows=2, ncols=1) fig.supxlabel(\"seq len (# tokens)\") fig.supylabel(\"latency (s)\") fig.suptitle(f\"Small seq len and greedy search on {model_name} don't tell the whole (inference) story...\") for row, (plot_name, timings) in enumerate(all_timings.items()): subfigs[row].suptitle(f\"setup #{1+row}: {plot_name} (seq len / beam search)\") axs = subfigs[row].subplots(nrows=1, ncols=2) for col, accumulated in enumerate([False, True]): plot_axis = axs[col] for index, (k, v) in enumerate(timings.items()): axis = range(len(v)) color = colors[index] v = np.array(v) # remove extreme values p99 = np.percentile(v, 99) v[v > p99] = p99 v = np.cumsum(v) if accumulated else v plot_axis.scatter(axis, v, label=k, s=2) title = f\"latency for the full sequence\" if accumulated else f\"latency for each token\" plot_axis.title.set_text(title) # legend deduplication handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) fig.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1, 1), loc=\"upper left\", markerscale=5) plt.show() Profiling model at the kernel level \u00b6 Below we reload the decoder model with Onnx Runtime kernel profiling enabled. It will help us to understand on which part of the computation graph the GPU spends its time. The number of events that Onnx Runtime can save is limited to 1 million . It is not an issue as we have seen that timings per token are mostly stable, so having only n first token information don't change anything. The main information it gives us is that 30% of the time is spent on matrix multiplication when caching is used. The rest of the time is spent on mostly memory bound operations: element-wise operations which require little computation ( add , mul , div , etc.) copy pasting tensors GPU <-> GPU with little transformation in between ( transpose , concat , cast , etc.) It matches the information provided by both nvidia-smi and Nvidia Nsight (the GPU profiler from Nvidia): the GPU is under utilized. That's why we think that a tool like TensorRT which will perform aggressive kernel fusion, reducing time spent on memory bounded operations, should be a good fit for autoregressive models. there is a nice opportunity to increase the speedup by reducing the number of casting operations. We keep this work for the future. In [24]: Copied! dec_onnx = create_model_for_provider ( dec_if_fp16_model_path , \"CUDAExecutionProvider\" , enable_profiling = True , log_severity = 3 ) dec_onnx_binding : IOBinding = dec_onnx . io_binding () _ = model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = 4 , min_length = 10 ) profile_name = dec_onnx . end_profiling () with open ( profile_name ) as f : content = json . load ( f ) op_timings = defaultdict ( lambda : 0 ) for c in content : if \"op_name\" not in c [ \"args\" ]: continue op_name = c [ \"args\" ][ \"op_name\" ] if op_name == \"If\" : continue # subgraph time_taken = c [ \"dur\" ] op_timings [ op_name ] += time_taken op_timings_filter = dict ( sorted ( op_timings . items (), key = operator . itemgetter ( 1 ), reverse = True )[: 10 ]) total_kernel_timing = sum ( op_timings . values ()) op_timings_percent = { k : 100 * v / total_kernel_timing for k , v in op_timings_filter . items ()} plt . barh ( list ( op_timings_percent . keys ()), list ( op_timings_percent . values ())) plt . title ( \"Time spent per kernel \\n (top 10 kernels)\" ) plt . xlabel ( \"% total inference time\" ) plt . show () dec_onnx = create_model_for_provider( dec_if_fp16_model_path, \"CUDAExecutionProvider\", enable_profiling=True, log_severity=3 ) dec_onnx_binding: IOBinding = dec_onnx.io_binding() _ = model_gen.generate(inputs=input_ids, max_length=10, num_beams=4, min_length=10) profile_name = dec_onnx.end_profiling() with open(profile_name) as f: content = json.load(f) op_timings = defaultdict(lambda: 0) for c in content: if \"op_name\" not in c[\"args\"]: continue op_name = c[\"args\"][\"op_name\"] if op_name == \"If\": continue # subgraph time_taken = c[\"dur\"] op_timings[op_name] += time_taken op_timings_filter = dict(sorted(op_timings.items(), key=operator.itemgetter(1), reverse=True)[:10]) total_kernel_timing = sum(op_timings.values()) op_timings_percent = {k: 100 * v / total_kernel_timing for k, v in op_timings_filter.items()} plt.barh(list(op_timings_percent.keys()), list(op_timings_percent.values())) plt.title(\"Time spent per kernel\\n(top 10 kernels)\") plt.xlabel(\"% total inference time\") plt.show()","title":"Accelerate text generation with T5"},{"location":"t5/#inference-acceleration-of-t5-for-large-batch-size-long-sequence-length-large-models","text":"Every week or so, a new impressive few shots learning work taking advantage of autoregressive models is released by some team around the world. Still LLM inference is rarely discussed and few projects are focusing on this aspect. In this notebook, we describe our take to significantly improve autoregressive model latency. We plan to intensively test large autoregressive models, so we want something: which scales : the improvement exists on small and large models, for short and long sequences, in greedy and beam search; This is very important in a few shots learning where sequences are most of the time hundreds or thousands tokens long and beam search is used to improve text quality. that has no hidden cost : no big increase in memory usage, no degradation in quality of generated text, support state-of-the-art decoding algorithms; that is generic : works for any transformer based architecture, and not specific to an inference engine; that is easy to maintain : no hard-coded behaviors or other technical debt if it doesn't bring a clear advantage. To be clear, we are not targeting the best performance ever but the right trade off (for us at least) between simplicity to use/maintain and acceptable latency.","title":"Inference acceleration of T5 for large batch size / long sequence length / &gt; large models"},{"location":"t5/#the-challenge","text":"In most situations, performing inference with Onnx Runtime or TensorRT usually bring large improvement over Pytorch implementations. It's very true with transformer based models. The main reason is that these tools will perform kernel fusions (merging several operations into a single one) and therefore reduce the number of memory bounded operations. Sometimes they also replace some operations by a much faster approximation. In the very specific case of autoregressive languages, things are a bit more complicated. On most Pytorch implementations of these models, there is a cache of K and V values. Let's remind us that in attention blocks, each token is projected on 3 matrices called Query , Key , and Value . Then, those projections will be used to compute a representation of each token which takes into account the information from the related other tokens of the sequence. As autoregressive models generate the sequence one token at a time, they should recompute final representation of all past tokens for each new token to generate. Because each token can only attend to the past, the result of these computations never changes; therefore one simple trick to reduce latency is to just memorize them and reuse them later, avoiding lots of computation. Out of the box, the cache mechanism can't be exported to Onnx from Hugging Face models (and all other Pytorch implementations we are aware of). The reason is that those models are not torchscript scripting compliant (it requires Pytorch code to follow some restrictive rules ). Because of that, Onnx export is done through tracing which erases any control flow instructions (including the If instruction to enable or not a cache).","title":"The challenge"},{"location":"t5/#existing-solutions","text":"Some interesting solutions targeting inference latency that we have considered and/or tested: TensorRT , which targets GPU , heavily optimizes the computation graph, making T5 inference very fast (they report X10 speedup on small-T5 ). The trick is that it doesn't use any cache (see below for more details), so it's very fast on short sequence and small models, as it avoids many memory bounded operations by redoing full computation again and again... but as several users have already found ( 1 , 2 , 3 , 4 , ...), this approach doesn't scale when the computation intensity increases, i.e., when base or large models are used instead of a small one, when generation is done on moderately long sequence of few hundred of tokens or if beam search is used instead of a greedy search; FastT5 , which targets CPU , exports 2 versions of the decoder, one with cache and one without. You need the no cache version to compute the first token and the first past state tensors (aka the cached tensors), and for all the other tokens you use the cache version of the computation graph. Basically, it makes the memory foot print 2 times bigger as all weights are duplicated. As generative models tend to be huge, they work around the memory issue by using dynamic int-8 quantization, the final memory foot print of the decoders is now the same as Hugging Face in FP16 ... but 1/ dynamic quantization only works on CPU , and 2/ according to several reports dynamic quantization degrades significantly generative model output, to a point where it may make them useless ( 1 , 2 , and here you can find a report in the GPT-2 context from a Microsoft engineer: \" int8 quantization are not recommended due to accuracy loss \"). Onnx Runtime T5 export tool targets both GPU and CPU . It works in a similar way than FastT5 : decoder module is exported 2 times. Like FastT5 , the memory footprint of the decoder part is doubled (this time there is no int-8 quantization). FasterTransformer targets GPU and is a mix of Pytorch and CUDA / C++ dedicated code. The performance boost is huge on T5 , they report a 10X speedup like TensorRT . However, it may significantly decrease the accuracy of the model ( here when sampling is enabled, it reduces BLEU score of translation task by 8 points, the cause may be a bug in the decoding algorithm or an approximation a bit too aggressive) plus the speedup is computed on a translation task where sequences are 25 tokens long on average. In our experience, improvement on very short sequences tends to decrease by large margins on longer sequences. It seems to us that their objectives are different from ours. With the existing solutions, you need to choose one or two items of the following: double decoder memory footprint; be slower than Hugging Face for moderately long sequence length / beam search; degrade output quality.","title":"Existing solutions"},{"location":"t5/#our-approach","text":"Our approach to make autoregressive transformer based models 2X faster than Hugging Face Pytorch implementation (the base line) is based on 3 key ingredients: storing 2 computation graphs in a single Onnx file: this let us have both cache and no cache support without having any duplicated weights, zero copy to retrieve output from Onnx Runtime : we built over our past work to connect in the most efficient way Pytorch tensors (used in the decoding part) and Onnx Runtime . Our previous work was to avoid host <-> GPU tensor copy, but it still required a GPU <-> GPU . It is now part of the official Onnx Runtime documentation (apparently thanks of our project !). This time we found out a way to directly expose the internal state of Onnx Runtime through a Pytorch tensor in zero copy way. Combined with cache mechanism, this is responsible for most of the speedup we have obtained. a generic tool to convert any model (whatever the architecture) to FP16 without any risk of having out of range values or rounding to zero: FP16 is still the way to reduce memory footprint of a model. The main issue is that some nodes may output values outside of FP16 range or round others to zero, resulting in NaN output; moreover, very small values may be rounded to zero which is an issue for log and div operations. We have built a tool which detect those nodes so we can keep their precision in FP32 . It's quite important to reduce memory footprint of these models, not just because they tend to be huge, but also because past states (that we cache) and internal buffers can be even bigger than the weights of the model itself.","title":"Our approach"},{"location":"t5/#results","text":"As demonstrated at the end of this notebook, we are able to provide a X2 speedup whatever the batch size, the sequence length or the model size. For TensorRT we have our own implementation of our approach described above which helps to provide similar latency to Onnx Runtime . It's in a Python script in the same folder as this notebook. We had to work around a documented limitation. Because of that the code is slightly more complex and we wanted to keep this notebook easy to follow. In [1]: Copied! ! nvidia - smi ! nvidia-smi Mon May 23 21:22:55 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.43.04 Driver Version: 515.43.04 CUDA Version: 11.7 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 39% 47C P8 41W / 350W | 263MiB / 24576MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 2366 G /usr/lib/xorg/Xorg 160MiB | | 0 N/A N/A 6173 G ...ome-remote-desktop-daemon 4MiB | | 0 N/A N/A 8174 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 8918 G ...on/Bin/AgentConnectix.bin 4MiB | | 0 N/A N/A 393472 G ...176700596390776551,131072 47MiB | +-----------------------------------------------------------------------------+","title":"Results"},{"location":"t5/#onnx-runtime-compilation","text":"Version 1.11.1 of Onnx Runtime and older have a bug which makes them much slower when most inputs are used by subgraphs of an If node. Unfortunately, it's exactly what will do below, so we need to compile our own version of Onnx Runtime until the version 1.12 is released (in June 2022). Code below has been tested on Ubuntu 22.04 and supposes that your machine has CUDA 11.4 installed. If not, use the Docker image of this library. We use a specific commit of Onnx Runtime with a better management of If / Else / Then Onnx nodes: git clone --recursive https://github.com/Microsoft/onnxruntime cd onnxruntime git checkout -b fix_if e1c04eed29d48f295de1cfbd48713158537cdaa7 CUDACXX = /usr/local/cuda-11.4/bin/nvcc ./build.sh \\ --config Release \\ --build_wheel \\ --parallel \\ --use_cuda \\ --cuda_home /usr/local/cuda-11.4 \\ --cudnn_home /usr/lib/x86_ -linux-gnu/ \\ --skip_test # pip install ... # other required dependencies # pip install nvtx seaborn On our machine, it takes around 20 minutes. to clear previous compilation, delete content of ./build folder In [2]: Copied! import json import random from transformer_deploy.backends.ort_utils import get_keep_fp32_nodes from transformer_deploy.backends.ort_utils import convert_fp16 import time from typing import Callable , Dict , Optional , List import matplotlib.pylab as plt from onnxruntime import IOBinding import numpy as np import onnx import torch from pathlib import Path from typing import Tuple from onnx import GraphProto , ModelProto , helper from torch.nn import Linear from transformers import AutoModelForSeq2SeqLM , AutoTokenizer , PretrainedConfig , T5ForConditionalGeneration , TensorType from transformers.generation_utils import GenerationMixin from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions , Seq2SeqLMOutput from transformers.models.t5.modeling_t5 import T5Stack from nvtx import nvtx from copy import copy from transformer_deploy.backends.ort_utils import create_model_for_provider , inference_onnx_binding from transformer_deploy.backends.pytorch_utils import convert_to_onnx import seaborn as sns import operator from collections import defaultdict import gc import json import random from transformer_deploy.backends.ort_utils import get_keep_fp32_nodes from transformer_deploy.backends.ort_utils import convert_fp16 import time from typing import Callable, Dict, Optional, List import matplotlib.pylab as plt from onnxruntime import IOBinding import numpy as np import onnx import torch from pathlib import Path from typing import Tuple from onnx import GraphProto, ModelProto, helper from torch.nn import Linear from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PretrainedConfig, T5ForConditionalGeneration, TensorType from transformers.generation_utils import GenerationMixin from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput from transformers.models.t5.modeling_t5 import T5Stack from nvtx import nvtx from copy import copy from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding from transformer_deploy.backends.pytorch_utils import convert_to_onnx import seaborn as sns import operator from collections import defaultdict import gc","title":"Onnx Runtime compilation"},{"location":"t5/#loading-hugging-face-model-tokenizer","text":"Below we load the model and set global variables of this notebook. In [3]: Copied! np . random . seed ( 123 ) torch . random . manual_seed ( 123 ) # other possible values: t5-small, t5-base, t5-large. t5-3b should work when ORT library is fixed model_name = \"t5-large\" tokenizer = AutoTokenizer . from_pretrained ( model_name ) input_ids : torch . Tensor = tokenizer ( \"translate English to French: This model is now very fast!\" , return_tensors = TensorType . PYTORCH ) . input_ids input_ids = input_ids . type ( torch . int32 ) . to ( \"cuda\" ) pytorch_model : T5ForConditionalGeneration = AutoModelForSeq2SeqLM . from_pretrained ( model_name ) pytorch_model = pytorch_model . eval () pytorch_model = pytorch_model . cuda () pytorch_model . config . use_cache = True # not really needed, just to make things obvious num_layers = pytorch_model . config . num_layers # tolerance between Onnx FP16 and Pytorch FP32. # Rounding errors increase with number of layers: 1e-1 for t5-small, 5e-1 for large, 3 for 3b. 11b not tested. # Do not impact final quality fp16_default_tolerance = 5e-1 def are_equal ( a : torch . Tensor , b : torch . Tensor , atol : float = fp16_default_tolerance ) -> None : assert np . allclose ( a = a . detach () . cpu () . numpy (), b = b . detach () . cpu () . numpy (), atol = atol ), f \" { a } \\n\\n VS \\n\\n { b } \" def save_onnx ( proto : onnx . ModelProto , model_path : str ) -> None : # protobuff doesn't support files > 2Gb, in this case, weights are stored in another binary file save_external_data : bool = proto . ByteSize () > 2 * 1024 ** 3 filename = Path ( model_path ) . name onnx . save_model ( proto = proto , f = model_path , save_as_external_data = save_external_data , all_tensors_to_one_file = True , location = filename + \".data\" , ) def prepare_folder ( path : str ) -> Tuple [ str , str ]: p = Path ( path ) p . mkdir ( parents = True , exist_ok = True ) [ item . unlink () for item in Path ( path ) . glob ( \"*\" ) if item . is_file ()] return path + \"/model.onnx\" , path + \"/model_fp16.onnx\" # create/clean folders where each model will be stored. # as multiple files will be saved for T5-3B and 11B, we use different folders for the encoder and the decoders. encoder_model_path , encoder_fp16_model_path = prepare_folder ( path = \"./test-enc\" ) dec_cache_model_path , dec_cache_fp16_model_path = prepare_folder ( path = \"./test-dec-cache\" ) dec_no_cache_model_path , dec_no_cache_fp16_model_path = prepare_folder ( path = \"./test-dec-no-cache\" ) _ , dec_if_fp16_model_path = prepare_folder ( path = \"./test-dec-if\" ) # some outputs to compare with out_enc : BaseModelOutputWithPastAndCrossAttentions = pytorch_model . encoder ( input_ids = input_ids ) out_full : Seq2SeqLMOutput = pytorch_model ( input_ids = input_ids , decoder_input_ids = input_ids ) np.random.seed(123) torch.random.manual_seed(123) # other possible values: t5-small, t5-base, t5-large. t5-3b should work when ORT library is fixed model_name = \"t5-large\" tokenizer = AutoTokenizer.from_pretrained(model_name) input_ids: torch.Tensor = tokenizer( \"translate English to French: This model is now very fast!\", return_tensors=TensorType.PYTORCH ).input_ids input_ids = input_ids.type(torch.int32).to(\"cuda\") pytorch_model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model_name) pytorch_model = pytorch_model.eval() pytorch_model = pytorch_model.cuda() pytorch_model.config.use_cache = True # not really needed, just to make things obvious num_layers = pytorch_model.config.num_layers # tolerance between Onnx FP16 and Pytorch FP32. # Rounding errors increase with number of layers: 1e-1 for t5-small, 5e-1 for large, 3 for 3b. 11b not tested. # Do not impact final quality fp16_default_tolerance = 5e-1 def are_equal(a: torch.Tensor, b: torch.Tensor, atol: float = fp16_default_tolerance) -> None: assert np.allclose(a=a.detach().cpu().numpy(), b=b.detach().cpu().numpy(), atol=atol), f\"{a}\\n\\nVS\\n\\n{b}\" def save_onnx(proto: onnx.ModelProto, model_path: str) -> None: # protobuff doesn't support files > 2Gb, in this case, weights are stored in another binary file save_external_data: bool = proto.ByteSize() > 2 * 1024**3 filename = Path(model_path).name onnx.save_model( proto=proto, f=model_path, save_as_external_data=save_external_data, all_tensors_to_one_file=True, location=filename + \".data\", ) def prepare_folder(path: str) -> Tuple[str, str]: p = Path(path) p.mkdir(parents=True, exist_ok=True) [item.unlink() for item in Path(path).glob(\"*\") if item.is_file()] return path + \"/model.onnx\", path + \"/model_fp16.onnx\" # create/clean folders where each model will be stored. # as multiple files will be saved for T5-3B and 11B, we use different folders for the encoder and the decoders. encoder_model_path, encoder_fp16_model_path = prepare_folder(path=\"./test-enc\") dec_cache_model_path, dec_cache_fp16_model_path = prepare_folder(path=\"./test-dec-cache\") dec_no_cache_model_path, dec_no_cache_fp16_model_path = prepare_folder(path=\"./test-dec-no-cache\") _, dec_if_fp16_model_path = prepare_folder(path=\"./test-dec-if\") # some outputs to compare with out_enc: BaseModelOutputWithPastAndCrossAttentions = pytorch_model.encoder(input_ids=input_ids) out_full: Seq2SeqLMOutput = pytorch_model(input_ids=input_ids, decoder_input_ids=input_ids) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5. For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`. - Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding. - If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding. - To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value. warnings.warn(","title":"Loading Hugging Face model / tokenizer"},{"location":"t5/#export-to-onnx","text":"First step is to export the model to Onnx graph. T5 is made of 2 parts, an encoder and a decoder .","title":"Export to Onnx"},{"location":"t5/#export-encoder-part","text":"The encoder part export doesn't imply any specific challenge. We use export function built for Bert like model, exported model is in FP16 . In [4]: Copied! pytorch_model = pytorch_model . to ( \"cuda\" ) convert_to_onnx ( model_pytorch = pytorch_model . encoder , output_path = encoder_model_path , inputs_pytorch = { \"input_ids\" : input_ids }, var_output_seq = True , quantization = False , ) pytorch_model = pytorch_model.to(\"cuda\") convert_to_onnx( model_pytorch=pytorch_model.encoder, output_path=encoder_model_path, inputs_pytorch={\"input_ids\": input_ids}, var_output_seq=True, quantization=False, )","title":"Export encoder part"},{"location":"t5/#conversion-to-mixed-precision","text":"","title":"Conversion to mixed precision"},{"location":"t5/#why-mixed-precision","text":"As T5 can have up to 11 billion parameters, it requires lots of computation, and even more important, it takes lots of space in device memory. We convert the encoder to half precision. If we blindly convert the whole graph to FP16 , we will have 2 issues: overflow : some nodes, like exponential nodes, will try to output values out of the FP16 range, at the end you get some NaN . underflow : values very close to 0 will be rounded to 0, which may be an issue for some operations like Div and Log .","title":"Why mixed precision?"},{"location":"t5/#the-challenge","text":"Mixed precision is done out of the box by Pytorch and follow some strict rules described in https://pytorch.org/docs/stable/amp.html Those rules are generic and quite conservative. Many nodes will be kept in FP32 even if their output is always in the FP16 range. Other approaches we have found: Onnx Runtime T5 demo : provide a list of operations to keep in FP32 (Pow, ReduceMean, Add, Sqrt, Div, Mul, Softmax, Relu). We have found this approach to need more an more tweaking on larger networks and encoder part (decoder part seems simpler to manage, https://github.com/microsoft/onnxruntime/issues/11119 ); TensorRT T5 demo : provide the exact pattern of nodes to keep in FP32 . This approach is much more effective, but imply lots of code to describe the patterns and may not generalize well, basically what works for a base model may not work for 11 billion parameters model. And it does not scale to other architectures without adaptations, for a library like transformer-deploy , it would lead to unmaintainable technical debt.","title":"The challenge"},{"location":"t5/#our-approach","text":"We have chosen an architecture agnostic approach: we inject random input sequences and audit the output of each computation graph node; finally, we make a list of all nodes that have output values out of the FP16 range /close to zero values and perform some cleaning (to avoid unnecessary casting). We have chosen to use random values only for the input_ids field as the search space is limited: positive integers lower than the vocabulary size. You can also decide to send real data from a dataset you want to work on. To finish, we provide the list of nodes to keep in FP32 to the conversion function. In [5]: Copied! def get_random_input_encoder () -> Dict [ str , torch . Tensor ]: max_seq = 512 seq_len = random . randint ( a = 1 , b = max_seq ) batch = max_seq // seq_len random_input_ids = torch . randint ( low = 0 , high = tokenizer . vocab_size , size = ( batch , seq_len ), dtype = torch . int32 , device = \"cuda\" ) inputs = { \"input_ids\" : random_input_ids } return inputs keep_fp32_encoder = get_keep_fp32_nodes ( onnx_model_path = encoder_model_path , get_input = get_random_input_encoder ) assert len ( keep_fp32_encoder ) > 0 enc_model_onnx = convert_fp16 ( onnx_model = encoder_model_path , nodes_to_exclude = keep_fp32_encoder ) save_onnx ( proto = enc_model_onnx , model_path = encoder_fp16_model_path ) del enc_model_onnx torch . cuda . empty_cache () gc . collect () def get_random_input_encoder() -> Dict[str, torch.Tensor]: max_seq = 512 seq_len = random.randint(a=1, b=max_seq) batch = max_seq // seq_len random_input_ids = torch.randint( low=0, high=tokenizer.vocab_size, size=(batch, seq_len), dtype=torch.int32, device=\"cuda\" ) inputs = {\"input_ids\": random_input_ids} return inputs keep_fp32_encoder = get_keep_fp32_nodes(onnx_model_path=encoder_model_path, get_input=get_random_input_encoder) assert len(keep_fp32_encoder) > 0 enc_model_onnx = convert_fp16(onnx_model=encoder_model_path, nodes_to_exclude=keep_fp32_encoder) save_onnx(proto=enc_model_onnx, model_path=encoder_fp16_model_path) del enc_model_onnx torch.cuda.empty_cache() gc.collect() Out[5]: 3772 In [6]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_encoder ) } ):\" ) keep_fp32_encoder [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_encoder)}):\") keep_fp32_encoder[:20] 20 first nodes to keep in FP32 (total 1247): Out[6]: ['MatMul_62', 'Log_89', 'Div_91', 'Mul_93', 'Softmax_109', 'Pow_120', 'MatMul_129', 'Relu_130', 'Softmax_168', 'Pow_194', 'Mul_202', 'Softmax_227', 'Pow_238', 'Pow_253', 'Softmax_286', 'Pow_297', 'Pow_312', 'Softmax_345', 'Pow_356', 'Pow_371'] Compare the output of the Onnx FP16 model with Pytorch one In [7]: Copied! enc_fp16_onnx = create_model_for_provider ( encoder_fp16_model_path , \"CUDAExecutionProvider\" ) enc_fp16_onnx_binding : IOBinding = enc_fp16_onnx . io_binding () enc_onnx_out = inference_onnx_binding ( model_onnx = enc_fp16_onnx , binding = enc_fp16_onnx_binding , inputs = { \"input_ids\" : input_ids }, device = input_ids . device . type , )[ \"output\" ] are_equal ( a = enc_onnx_out , b = out_enc . last_hidden_state ) enc_fp16_onnx = create_model_for_provider(encoder_fp16_model_path, \"CUDAExecutionProvider\") enc_fp16_onnx_binding: IOBinding = enc_fp16_onnx.io_binding() enc_onnx_out = inference_onnx_binding( model_onnx=enc_fp16_onnx, binding=enc_fp16_onnx_binding, inputs={\"input_ids\": input_ids}, device=input_ids.device.type, )[\"output\"] are_equal(a=enc_onnx_out, b=out_enc.last_hidden_state)","title":"Our approach"},{"location":"t5/#export-decoder","text":"The decoder export part is more challenging: we first need to wrap it in a Pytorch model to add the final layer so it's output provide scores for each vocabulary token and can be directly used by the Hugging Face decoding algorithm then, we need to manipulate the Onnx graph to add support of Key / Value cache The second point is the key ingredient of the observed acceleration of Onnx vs Hugging Face inference.","title":"Export decoder"},{"location":"t5/#wrapper-to-include-some-post-processing-on-the-decoder-output","text":"The post-processing is mainly a projection of the decoder output on a matrix with one of its dimensions equal to model vocabulary size, so we have scores for each possible token. In [8]: Copied! class ExportT5 ( torch . nn . Module ): def __init__ ( self , decoder : T5Stack , lm_head : Linear ): super ( ExportT5 , self ) . __init__ () self . decoder = decoder self . lm_head = lm_head def forward ( self , input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , past_key_values : Tuple = None ): out_dec = self . decoder . forward ( input_ids = input_ids , encoder_hidden_states = encoder_hidden_states , past_key_values = past_key_values ) # Rescale output before projecting on vocab out_dec [ \"last_hidden_state\" ] = out_dec [ \"last_hidden_state\" ] * ( pytorch_model . model_dim **- 0.5 ) out_dec [ \"last_hidden_state\" ] = self . lm_head ( out_dec [ \"last_hidden_state\" ]) return out_dec pytorch_model . cuda () model_decoder = ExportT5 ( decoder = pytorch_model . decoder , lm_head = pytorch_model . lm_head ) . eval () out_model_export : torch . Tensor = model_decoder ( input_ids = input_ids , encoder_hidden_states = out_enc . last_hidden_state ) are_equal ( a = out_model_export [ \"last_hidden_state\" ], b = out_full . logits ) class ExportT5(torch.nn.Module): def __init__(self, decoder: T5Stack, lm_head: Linear): super(ExportT5, self).__init__() self.decoder = decoder self.lm_head = lm_head def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, past_key_values: Tuple = None): out_dec = self.decoder.forward( input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values ) # Rescale output before projecting on vocab out_dec[\"last_hidden_state\"] = out_dec[\"last_hidden_state\"] * (pytorch_model.model_dim**-0.5) out_dec[\"last_hidden_state\"] = self.lm_head(out_dec[\"last_hidden_state\"]) return out_dec pytorch_model.cuda() model_decoder = ExportT5(decoder=pytorch_model.decoder, lm_head=pytorch_model.lm_head).eval() out_model_export: torch.Tensor = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state) are_equal(a=out_model_export[\"last_hidden_state\"], b=out_full.logits)","title":"Wrapper to include some post-processing on the decoder output"},{"location":"t5/#export-decoder-part-to-onnx","text":"Below we export 2 versions of the decoder, one without cache support and one with it. Model inputs with past states (cache support): In [9]: Copied! model_decoder . cuda () # decoder output one step before out_dec_pytorch = model_decoder ( input_ids = input_ids [:, : - 1 ], encoder_hidden_states = out_enc . last_hidden_state ) model_inputs = { \"input_ids\" : input_ids [:, - 1 :] . type ( torch . int32 ), \"encoder_hidden_states\" : out_enc . last_hidden_state , \"past_key_values\" : out_dec_pytorch . past_key_values , } input_names = [ \"input_ids\" , \"encoder_hidden_states\" ] for i in range ( num_layers ): input_names . append ( f \"past_key_values. { i } .decoder.key\" ) input_names . append ( f \"past_key_values. { i } .decoder.value\" ) input_names . append ( f \"past_key_values. { i } .encoder.key\" ) input_names . append ( f \"past_key_values. { i } .encoder.value\" ) output_names = [ \"logits\" ] for i in range ( num_layers ): output_names . append ( f \"present. { i } .decoder.key\" ) output_names . append ( f \"present. { i } .decoder.value\" ) output_names . append ( f \"present. { i } .encoder.key\" ) output_names . append ( f \"present. { i } .encoder.value\" ) dynamic_axis = { \"input_ids\" : { 0 : \"batch\" , 1 : \"encoder_sequence\" }, \"encoder_hidden_states\" : { 0 : \"batch\" , 1 : \"encoder_sequence\" }, \"logits\" : { 0 : \"batch\" , 1 : \"decoder_sequence\" }, } for i in range ( num_layers ): dynamic_axis [ f \"past_key_values. { i } .decoder.key\" ] = { 0 : \"batch\" , 2 : \"past_decoder_sequence\" } dynamic_axis [ f \"past_key_values. { i } .decoder.value\" ] = { 0 : \"batch\" , 2 : \"past_decoder_sequence\" } dynamic_axis [ f \"past_key_values. { i } .encoder.key\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"past_key_values. { i } .encoder.value\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"present. { i } .decoder.key\" ] = { 0 : \"batch\" , 2 : \"decoder_sequence\" } dynamic_axis [ f \"present. { i } .decoder.value\" ] = { 0 : \"batch\" , 2 : \"decoder_sequence\" } dynamic_axis [ f \"present. { i } .encoder.key\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } dynamic_axis [ f \"present. { i } .encoder.value\" ] = { 0 : \"batch\" , 2 : \"encoder_sequence_length\" } model_decoder.cuda() # decoder output one step before out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state) model_inputs = { \"input_ids\": input_ids[:, -1:].type(torch.int32), \"encoder_hidden_states\": out_enc.last_hidden_state, \"past_key_values\": out_dec_pytorch.past_key_values, } input_names = [\"input_ids\", \"encoder_hidden_states\"] for i in range(num_layers): input_names.append(f\"past_key_values.{i}.decoder.key\") input_names.append(f\"past_key_values.{i}.decoder.value\") input_names.append(f\"past_key_values.{i}.encoder.key\") input_names.append(f\"past_key_values.{i}.encoder.value\") output_names = [\"logits\"] for i in range(num_layers): output_names.append(f\"present.{i}.decoder.key\") output_names.append(f\"present.{i}.decoder.value\") output_names.append(f\"present.{i}.encoder.key\") output_names.append(f\"present.{i}.encoder.value\") dynamic_axis = { \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"}, \"encoder_hidden_states\": {0: \"batch\", 1: \"encoder_sequence\"}, \"logits\": {0: \"batch\", 1: \"decoder_sequence\"}, } for i in range(num_layers): dynamic_axis[f\"past_key_values.{i}.decoder.key\"] = {0: \"batch\", 2: \"past_decoder_sequence\"} dynamic_axis[f\"past_key_values.{i}.decoder.value\"] = {0: \"batch\", 2: \"past_decoder_sequence\"} dynamic_axis[f\"past_key_values.{i}.encoder.key\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"past_key_values.{i}.encoder.value\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"present.{i}.decoder.key\"] = {0: \"batch\", 2: \"decoder_sequence\"} dynamic_axis[f\"present.{i}.decoder.value\"] = {0: \"batch\", 2: \"decoder_sequence\"} dynamic_axis[f\"present.{i}.encoder.key\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} dynamic_axis[f\"present.{i}.encoder.value\"] = {0: \"batch\", 2: \"encoder_sequence_length\"} Export of the model with cache support: In [10]: Copied! with torch . no_grad (): pytorch_model . config . return_dict = True pytorch_model . eval () # export can works with named args but the dict containing named args as to be last element of the args tuple torch . onnx . export ( model_decoder , ( model_inputs ,), f = dec_cache_model_path , input_names = input_names , output_names = output_names , dynamic_axes = dynamic_axis , do_constant_folding = True , opset_version = 13 , ) with torch.no_grad(): pytorch_model.config.return_dict = True pytorch_model.eval() # export can works with named args but the dict containing named args as to be last element of the args tuple torch.onnx.export( model_decoder, (model_inputs,), f=dec_cache_model_path, input_names=input_names, output_names=output_names, dynamic_axes=dynamic_axis, do_constant_folding=True, opset_version=13, ) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:668: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! if causal_mask.shape[1] < attention_mask.shape[1]: In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode Export of the model computing Key/Values for the whole sequence (we basically just remove past states from the input, the Pytorch code will recompute them): In [11]: Copied! model_inputs_no_cache = { \"input_ids\" : input_ids , \"encoder_hidden_states\" : out_enc . last_hidden_state , } with torch . no_grad (): pytorch_model . config . return_dict = True pytorch_model . eval () # export can works with named args but the dict containing named args as to be last element of the args tuple torch . onnx . export ( model_decoder , ( model_inputs_no_cache ,), f = dec_no_cache_model_path , input_names = list ( model_inputs_no_cache . keys ()), output_names = output_names , dynamic_axes = { k : v for k , v in dynamic_axis . items () if \"past_key_values\" not in k }, do_constant_folding = True , opset_version = 13 , ) _ = pytorch_model . cpu () # free cuda memory torch . cuda . empty_cache () model_inputs_no_cache = { \"input_ids\": input_ids, \"encoder_hidden_states\": out_enc.last_hidden_state, } with torch.no_grad(): pytorch_model.config.return_dict = True pytorch_model.eval() # export can works with named args but the dict containing named args as to be last element of the args tuple torch.onnx.export( model_decoder, (model_inputs_no_cache,), f=dec_no_cache_model_path, input_names=list(model_inputs_no_cache.keys()), output_names=output_names, dynamic_axes={k: v for k, v in dynamic_axis.items() if \"past_key_values\" not in k}, do_constant_folding=True, opset_version=13, ) _ = pytorch_model.cpu() # free cuda memory torch.cuda.empty_cache()","title":"Export decoder part to Onnx"},{"location":"t5/#conversion-to-mixed-precision","text":"Decoder module has different kinds of inputs, input_ids but also some float tensors. It would a bit more complicated to generate random values for those tensors: in theory it can be of any value in the FP32 range, but because of how models are initialized and trained, most of them are close to 0. To avoid too much guessing, we have decided to just take the output of the real model being fed with random input_ids . In [12]: Copied! def get_random_input_no_cache () -> Dict [ str , torch . Tensor ]: inputs = get_random_input_encoder () encoder_hidden_states = inference_onnx_binding ( model_onnx = enc_fp16_onnx , binding = enc_fp16_onnx_binding , inputs = inputs , device = \"cuda\" , clone_tensor = False , )[ \"output\" ] # it will serve as input of a FP32 model inputs [ \"encoder_hidden_states\" ] = encoder_hidden_states . type ( torch . float32 ) return inputs keep_fp32_no_cache = get_keep_fp32_nodes ( onnx_model_path = dec_no_cache_model_path , get_input = get_random_input_no_cache ) onnx_model_no_cache_fp16 = convert_fp16 ( onnx_model = dec_no_cache_model_path , nodes_to_exclude = keep_fp32_no_cache ) save_onnx ( proto = onnx_model_no_cache_fp16 , model_path = dec_no_cache_fp16_model_path ) def get_random_input_no_cache() -> Dict[str, torch.Tensor]: inputs = get_random_input_encoder() encoder_hidden_states = inference_onnx_binding( model_onnx=enc_fp16_onnx, binding=enc_fp16_onnx_binding, inputs=inputs, device=\"cuda\", clone_tensor=False, )[\"output\"] # it will serve as input of a FP32 model inputs[\"encoder_hidden_states\"] = encoder_hidden_states.type(torch.float32) return inputs keep_fp32_no_cache = get_keep_fp32_nodes(onnx_model_path=dec_no_cache_model_path, get_input=get_random_input_no_cache) onnx_model_no_cache_fp16 = convert_fp16(onnx_model=dec_no_cache_model_path, nodes_to_exclude=keep_fp32_no_cache) save_onnx(proto=onnx_model_no_cache_fp16, model_path=dec_no_cache_fp16_model_path) In [13]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_no_cache ) } ):\" ) keep_fp32_no_cache [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_no_cache)}):\") keep_fp32_no_cache[:20] 20 first nodes to keep in FP32 (total 2340): Out[13]: ['Constant_74', 'Pow_78', 'Log_135', 'Div_137', 'Mul_139', 'Softmax_154', 'Pow_165', 'MatMul_183', 'Reshape_187', 'Transpose_188', 'Softmax_212', 'Pow_223', 'Pow_238', 'Softmax_272', 'Pow_283', 'Softmax_317', 'Pow_328', 'Pow_343', 'Softmax_377', 'Pow_388'] In [14]: Copied! dec_no_cache_ort_model = create_model_for_provider ( dec_no_cache_model_path , \"CUDAExecutionProvider\" ) # use info from tokenizer size and max shape provided through the command line def get_random_input_cache () -> Dict [ str , torch . Tensor ]: inputs = get_random_input_no_cache () dec_past_states = inference_onnx_binding ( model_onnx = dec_no_cache_ort_model , inputs = inputs , device = \"cuda\" , clone_tensor = False , ) for k , v in dec_past_states . items (): if k == \"logits\" : continue new_k = k . replace ( \"present\" , \"past_key_values\" ) inputs [ new_k ] = v batch , _ = inputs [ \"input_ids\" ] . shape complement = torch . randint ( low = 0 , high = tokenizer . vocab_size , size = ( batch , 1 ), dtype = torch . int32 , device = \"cuda\" ) inputs [ \"input_ids\" ] = torch . concat ( tensors = [ inputs [ \"input_ids\" ], complement ], dim = 1 ) return inputs keep_fp32_cache = get_keep_fp32_nodes ( onnx_model_path = dec_cache_model_path , get_input = get_random_input_cache ) del dec_no_cache_ort_model # free cuda memory torch . cuda . empty_cache () gc . collect () onnx_model_cache_fp16 = convert_fp16 ( onnx_model = dec_cache_model_path , nodes_to_exclude = keep_fp32_cache ) save_onnx ( proto = onnx_model_cache_fp16 , model_path = dec_cache_fp16_model_path ) dec_no_cache_ort_model = create_model_for_provider(dec_no_cache_model_path, \"CUDAExecutionProvider\") # use info from tokenizer size and max shape provided through the command line def get_random_input_cache() -> Dict[str, torch.Tensor]: inputs = get_random_input_no_cache() dec_past_states = inference_onnx_binding( model_onnx=dec_no_cache_ort_model, inputs=inputs, device=\"cuda\", clone_tensor=False, ) for k, v in dec_past_states.items(): if k == \"logits\": continue new_k = k.replace(\"present\", \"past_key_values\") inputs[new_k] = v batch, _ = inputs[\"input_ids\"].shape complement = torch.randint(low=0, high=tokenizer.vocab_size, size=(batch, 1), dtype=torch.int32, device=\"cuda\") inputs[\"input_ids\"] = torch.concat(tensors=[inputs[\"input_ids\"], complement], dim=1) return inputs keep_fp32_cache = get_keep_fp32_nodes(onnx_model_path=dec_cache_model_path, get_input=get_random_input_cache) del dec_no_cache_ort_model # free cuda memory torch.cuda.empty_cache() gc.collect() onnx_model_cache_fp16 = convert_fp16(onnx_model=dec_cache_model_path, nodes_to_exclude=keep_fp32_cache) save_onnx(proto=onnx_model_cache_fp16, model_path=dec_cache_fp16_model_path) In [15]: Copied! print ( f \"20 first nodes to keep in FP32 (total { len ( keep_fp32_cache ) } ):\" ) keep_fp32_cache [: 20 ] print(f\"20 first nodes to keep in FP32 (total {len(keep_fp32_cache)}):\") keep_fp32_cache[:20] 20 first nodes to keep in FP32 (total 2240): Out[15]: ['Constant_94', 'Pow_98', 'Log_161', 'Div_163', 'Mul_165', 'Softmax_188', 'MatMul_189', 'Transpose_190', 'Reshape_194', 'Pow_202', 'MatMul_221', 'Reshape_225', 'Transpose_226', 'Softmax_246', 'MatMul_247', 'Transpose_248', 'Reshape_252', 'Pow_257', 'Pow_272', 'Softmax_308']","title":"Conversion to mixed precision"},{"location":"t5/#merge-onnx-computation-graph-to-deduplicate-weights","text":"Finally, we will merge the 2 decoders together. The idea is simple: we prefix the node / edge names of one of them to avoid naming collision we deduplicate the weights (the same weight matrix will have different names in the 2 models) we join the 2 computation graphs through an If node we generate the Onnx file The new model will take a new input, enable_cache . When it contains a True value, computation graph with cache support is used. code below is written to be easy to read, but could be made much faster to run In [16]: Copied! prefix = \"cache_node_\" mapping_initializer_cache_to_no_cache = dict () # search for not-duplicated weights, called initializer in Onnx to_add = list () for node_cache in onnx_model_cache_fp16 . graph . initializer : found = False for node_no_cache in onnx_model_no_cache_fp16 . graph . initializer : if node_cache . raw_data == node_no_cache . raw_data : found = True mapping_initializer_cache_to_no_cache [ node_cache . name ] = node_no_cache . name break if not found : node_cache . name = prefix + node_cache . name to_add . append ( node_cache ) mapping_initializer_cache_to_no_cache [ node_cache . name ] = node_cache . name onnx_model_no_cache_fp16 . graph . initializer . extend ( to_add ) # I/O model names should not be prefixed model_io_names = [ n . name for n in list ( onnx_model_cache_fp16 . graph . input ) + list ( onnx_model_cache_fp16 . graph . output )] # replace pointers to duplicated weights to their deduplicated version for node in onnx_model_cache_fp16 . graph . node : for index , input_name in enumerate ( node . input ): if input_name in model_io_names : continue node . input [ index ] = mapping_initializer_cache_to_no_cache . get ( input_name , prefix + input_name ) for index , output_name in enumerate ( node . output ): if output_name in model_io_names : continue node . output [ index ] = prefix + output_name node . name = prefix + node . name model_io_names = [ n . name for n in list ( onnx_model_cache_fp16 . graph . input ) + list ( onnx_model_cache_fp16 . graph . output )] # prefix nodes to avoid naming collision prefix = \"init_\" cache = dict () for node in onnx_model_no_cache_fp16 . graph . initializer : if node . name in model_io_names : new_name = prefix + node . name cache [ node . name ] = new_name node . name = new_name for node in onnx_model_no_cache_fp16 . graph . node : for input_index , n in enumerate ( node . input ): node . input [ input_index ] = cache . get ( n , n ) # mandatory for subgraph in if/else node assert len ( onnx_model_cache_fp16 . graph . output ) == len ( onnx_model_no_cache_fp16 . graph . output ), f \" { len ( onnx_model_cache_fp16 . graph . output ) } vs { len ( onnx_model_no_cache_fp16 . graph . output ) } \" # build a computation graph with cache support graph_cache : onnx . GraphProto = onnx . helper . make_graph ( nodes = list ( onnx_model_cache_fp16 . graph . node ), name = \"graph-cache\" , inputs = [], outputs = list ( onnx_model_cache_fp16 . graph . output ), initializer = [], ) # build a computation which doesn't need past states to run graph_no_cache : onnx . GraphProto = onnx . helper . make_graph ( nodes = list ( onnx_model_no_cache_fp16 . graph . node ), name = \"graph-no-cache\" , inputs = [], outputs = list ( onnx_model_no_cache_fp16 . graph . output ), initializer = [], ) # a new input to decide if we use past state or not enable_cache_input = onnx . helper . make_tensor_value_info ( name = \"enable_cache\" , elem_type = onnx . TensorProto . BOOL , shape = [ 1 ]) if_node = onnx . helper . make_node ( op_type = \"If\" , inputs = [ \"enable_cache\" ], outputs = [ o . name for o in list ( onnx_model_no_cache_fp16 . graph . output )], then_branch = graph_cache , else_branch = graph_no_cache , ) # final model which can disable its cache if_graph_def : GraphProto = helper . make_graph ( nodes = [ if_node ], name = \"if-model\" , inputs = list ( onnx_model_cache_fp16 . graph . input ) + [ enable_cache_input ], outputs = list ( onnx_model_no_cache_fp16 . graph . output ), initializer = list ( onnx_model_no_cache_fp16 . graph . initializer ), ) # serialization and cleaning model_if : ModelProto = helper . make_model ( if_graph_def , producer_name = \"onnx-example\" , opset_imports = [ helper . make_opsetid ( onnx . defs . ONNX_DOMAIN , 13 )] ) save_onnx ( proto = model_if , model_path = dec_if_fp16_model_path ) del model_if torch . cuda . empty_cache () gc . collect () prefix = \"cache_node_\" mapping_initializer_cache_to_no_cache = dict() # search for not-duplicated weights, called initializer in Onnx to_add = list() for node_cache in onnx_model_cache_fp16.graph.initializer: found = False for node_no_cache in onnx_model_no_cache_fp16.graph.initializer: if node_cache.raw_data == node_no_cache.raw_data: found = True mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name break if not found: node_cache.name = prefix + node_cache.name to_add.append(node_cache) mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name onnx_model_no_cache_fp16.graph.initializer.extend(to_add) # I/O model names should not be prefixed model_io_names = [n.name for n in list(onnx_model_cache_fp16.graph.input) + list(onnx_model_cache_fp16.graph.output)] # replace pointers to duplicated weights to their deduplicated version for node in onnx_model_cache_fp16.graph.node: for index, input_name in enumerate(node.input): if input_name in model_io_names: continue node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name) for index, output_name in enumerate(node.output): if output_name in model_io_names: continue node.output[index] = prefix + output_name node.name = prefix + node.name model_io_names = [n.name for n in list(onnx_model_cache_fp16.graph.input) + list(onnx_model_cache_fp16.graph.output)] # prefix nodes to avoid naming collision prefix = \"init_\" cache = dict() for node in onnx_model_no_cache_fp16.graph.initializer: if node.name in model_io_names: new_name = prefix + node.name cache[node.name] = new_name node.name = new_name for node in onnx_model_no_cache_fp16.graph.node: for input_index, n in enumerate(node.input): node.input[input_index] = cache.get(n, n) # mandatory for subgraph in if/else node assert len(onnx_model_cache_fp16.graph.output) == len( onnx_model_no_cache_fp16.graph.output ), f\"{len(onnx_model_cache_fp16.graph.output)} vs {len(onnx_model_no_cache_fp16.graph.output)}\" # build a computation graph with cache support graph_cache: onnx.GraphProto = onnx.helper.make_graph( nodes=list(onnx_model_cache_fp16.graph.node), name=\"graph-cache\", inputs=[], outputs=list(onnx_model_cache_fp16.graph.output), initializer=[], ) # build a computation which doesn't need past states to run graph_no_cache: onnx.GraphProto = onnx.helper.make_graph( nodes=list(onnx_model_no_cache_fp16.graph.node), name=\"graph-no-cache\", inputs=[], outputs=list(onnx_model_no_cache_fp16.graph.output), initializer=[], ) # a new input to decide if we use past state or not enable_cache_input = onnx.helper.make_tensor_value_info(name=\"enable_cache\", elem_type=onnx.TensorProto.BOOL, shape=[1]) if_node = onnx.helper.make_node( op_type=\"If\", inputs=[\"enable_cache\"], outputs=[o.name for o in list(onnx_model_no_cache_fp16.graph.output)], then_branch=graph_cache, else_branch=graph_no_cache, ) # final model which can disable its cache if_graph_def: GraphProto = helper.make_graph( nodes=[if_node], name=\"if-model\", inputs=list(onnx_model_cache_fp16.graph.input) + [enable_cache_input], outputs=list(onnx_model_no_cache_fp16.graph.output), initializer=list(onnx_model_no_cache_fp16.graph.initializer), ) # serialization and cleaning model_if: ModelProto = helper.make_model( if_graph_def, producer_name=\"onnx-example\", opset_imports=[helper.make_opsetid(onnx.defs.ONNX_DOMAIN, 13)] ) save_onnx(proto=model_if, model_path=dec_if_fp16_model_path) del model_if torch.cuda.empty_cache() gc.collect() Out[16]: 6366","title":"Merge Onnx computation graph to deduplicate weights"},{"location":"t5/#check-onnx-decoder-output","text":"Compare Onnx output with and without cache, plus compare with Pytorch output. In [17]: Copied! pytorch_model = pytorch_model . cuda () model_decoder = model_decoder . cuda () input_ids = input_ids . cuda () pytorch_model = pytorch_model . eval () model_decoder = model_decoder . eval () dec_onnx = create_model_for_provider ( dec_if_fp16_model_path , \"CUDAExecutionProvider\" , log_severity = 3 ) dec_onnx_binding : IOBinding = dec_onnx . io_binding () pytorch_model = pytorch_model.cuda() model_decoder = model_decoder.cuda() input_ids = input_ids.cuda() pytorch_model = pytorch_model.eval() model_decoder = model_decoder.eval() dec_onnx = create_model_for_provider(dec_if_fp16_model_path, \"CUDAExecutionProvider\", log_severity=3) dec_onnx_binding: IOBinding = dec_onnx.io_binding()","title":"Check Onnx decoder output"},{"location":"t5/#zero-copy-output","text":"Below, we check that the new model output is similar to the ones from Pytorch . We use our new implementation of inference call. The idea is the following: we ask Onnx Runtime to output a pointer to the CUDA array containing the result of the inference; we use Cupy API to wrap the array and provide information regarding tensor shape and type. Cupy doesn't own the data; we use Dlpack support to convert the Cupy tensor to Pytorch , another zero copy process. This pipeline is unsafe, as the content of the tensor may change or disappear silently: only Onnx Runtime has the control of the array containing the data. It will happen at the next inference call. Because we know that during the text generation we discard each output before recalling Onnx Runtime , it works well in our case. A second benefit of this approach is that we do not have anymore to guess the output shape. Before using this approach, to avoid the output to be stored on host memory (RAM) which made inference slower, we had to provide Onnx Runtime with a pointer to Pytorch tensor with the right size. As the size change with the sequence length (so it changes for each generated token), we had to store the logic to guess the size somewhere in the code. The new approach frees us from this burden. In [18]: Copied! pytorch_model = pytorch_model . half () with torch . inference_mode (): out_enc_pytorch : BaseModelOutputWithPastAndCrossAttentions = pytorch_model . encoder ( input_ids = input_ids ) previous_step_pytorch : BaseModelOutputWithPastAndCrossAttentions = model_decoder ( input_ids = input_ids [:, : - 1 ], encoder_hidden_states = out_enc_pytorch . last_hidden_state ) out_dec_pytorch : BaseModelOutputWithPastAndCrossAttentions = model_decoder ( input_ids = input_ids , encoder_hidden_states = out_enc_pytorch . last_hidden_state ) pytorch_model = pytorch_model.half() with torch.inference_mode(): out_enc_pytorch: BaseModelOutputWithPastAndCrossAttentions = pytorch_model.encoder(input_ids=input_ids) previous_step_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder( input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc_pytorch.last_hidden_state ) out_dec_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder( input_ids=input_ids, encoder_hidden_states=out_enc_pytorch.last_hidden_state ) In [19]: Copied! def decoder_pytorch_inference ( decoder_input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , ** _ ): with torch . inference_mode (): return model_decoder ( input_ids = decoder_input_ids , encoder_hidden_states = encoder_hidden_states ) def decoder_onnx_inference ( decoder_input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , enable_cache : torch . Tensor , past_key_values : Optional [ torch . Tensor ], ): inputs_onnx_dict = { \"input_ids\" : decoder_input_ids , \"encoder_hidden_states\" : encoder_hidden_states , \"enable_cache\" : enable_cache , } if past_key_values is not None : for index , ( k_dec , v_dec , k_enc , v_enc ) in enumerate ( past_key_values ): inputs_onnx_dict [ f \"past_key_values. { index } .decoder.key\" ] = k_dec inputs_onnx_dict [ f \"past_key_values. { index } .decoder.value\" ] = v_dec inputs_onnx_dict [ f \"past_key_values. { index } .encoder.key\" ] = k_enc inputs_onnx_dict [ f \"past_key_values. { index } .encoder.value\" ] = v_enc result_dict = inference_onnx_binding ( model_onnx = dec_onnx , inputs = inputs_onnx_dict , binding = dec_onnx_binding , # recycle the binding device = decoder_input_ids . device . type , clone_tensor = False , # no memory copy -> best perf and lowest memory footprint! ) past_states = list () for index in range ( pytorch_model . config . num_layers ): kv = ( result_dict [ f \"present. { index } .decoder.key\" ], result_dict [ f \"present. { index } .decoder.value\" ], result_dict [ f \"present. { index } .encoder.key\" ], result_dict [ f \"present. { index } .encoder.value\" ], ) past_states . append ( kv ) return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = result_dict [ \"logits\" ], past_key_values = past_states , ) out_dec_onnx_no_cache = decoder_onnx_inference ( decoder_input_ids = input_ids , encoder_hidden_states = out_enc_pytorch . last_hidden_state , enable_cache = torch . tensor ([ False ], device = \"cuda\" , dtype = torch . bool ), past_key_values = None , ) are_equal ( a = out_dec_onnx_no_cache . last_hidden_state [:, - 1 :, :], b = out_dec_pytorch . last_hidden_state [:, - 1 :, :]) # check that past states are identical between Onnx and Pytorch assert len ( out_dec_onnx_no_cache . past_key_values ) == len ( out_dec_pytorch . past_key_values ) for ( o_dec_k , o_dev_v , o_enc_k , o_enc_v ), ( p_dec_k , p_dev_v , p_enc_k , p_enc_v ) in zip ( out_dec_onnx_no_cache . past_key_values , out_dec_pytorch . past_key_values ): are_equal ( a = o_dec_k , b = p_dec_k ) are_equal ( a = o_dev_v , b = p_dev_v ) are_equal ( a = o_enc_k , b = p_enc_k ) are_equal ( a = o_enc_v , b = p_enc_v ) def decoder_pytorch_inference(decoder_input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, **_): with torch.inference_mode(): return model_decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_hidden_states) def decoder_onnx_inference( decoder_input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, past_key_values: Optional[torch.Tensor], ): inputs_onnx_dict = { \"input_ids\": decoder_input_ids, \"encoder_hidden_states\": encoder_hidden_states, \"enable_cache\": enable_cache, } if past_key_values is not None: for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(past_key_values): inputs_onnx_dict[f\"past_key_values.{index}.decoder.key\"] = k_dec inputs_onnx_dict[f\"past_key_values.{index}.decoder.value\"] = v_dec inputs_onnx_dict[f\"past_key_values.{index}.encoder.key\"] = k_enc inputs_onnx_dict[f\"past_key_values.{index}.encoder.value\"] = v_enc result_dict = inference_onnx_binding( model_onnx=dec_onnx, inputs=inputs_onnx_dict, binding=dec_onnx_binding, # recycle the binding device=decoder_input_ids.device.type, clone_tensor=False, # no memory copy -> best perf and lowest memory footprint! ) past_states = list() for index in range(pytorch_model.config.num_layers): kv = ( result_dict[f\"present.{index}.decoder.key\"], result_dict[f\"present.{index}.decoder.value\"], result_dict[f\"present.{index}.encoder.key\"], result_dict[f\"present.{index}.encoder.value\"], ) past_states.append(kv) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=result_dict[\"logits\"], past_key_values=past_states, ) out_dec_onnx_no_cache = decoder_onnx_inference( decoder_input_ids=input_ids, encoder_hidden_states=out_enc_pytorch.last_hidden_state, enable_cache=torch.tensor([False], device=\"cuda\", dtype=torch.bool), past_key_values=None, ) are_equal(a=out_dec_onnx_no_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :]) # check that past states are identical between Onnx and Pytorch assert len(out_dec_onnx_no_cache.past_key_values) == len(out_dec_pytorch.past_key_values) for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip( out_dec_onnx_no_cache.past_key_values, out_dec_pytorch.past_key_values ): are_equal(a=o_dec_k, b=p_dec_k) are_equal(a=o_dev_v, b=p_dev_v) are_equal(a=o_enc_k, b=p_enc_k) are_equal(a=o_enc_v, b=p_enc_v) In [20]: Copied! out_dec_onnx_cache = decoder_onnx_inference ( decoder_input_ids = input_ids [:, - 1 :], encoder_hidden_states = out_enc_pytorch . last_hidden_state , enable_cache = torch . tensor ([ True ], device = \"cuda\" , dtype = torch . bool ), past_key_values = previous_step_pytorch . past_key_values , ) are_equal ( a = out_dec_onnx_cache . last_hidden_state [:, - 1 :, :], b = out_dec_pytorch . last_hidden_state [:, - 1 :, :]) # check that past states are identical between Onnx and Pytorch assert len ( out_dec_onnx_cache . past_key_values ) == len ( out_dec_pytorch . past_key_values ) for ( o_dec_k , o_dev_v , o_enc_k , o_enc_v ), ( p_dec_k , p_dev_v , p_enc_k , p_enc_v ) in zip ( out_dec_onnx_cache . past_key_values , out_dec_pytorch . past_key_values ): are_equal ( a = o_dec_k , b = p_dec_k ) are_equal ( a = o_dev_v , b = p_dev_v ) are_equal ( a = o_enc_k , b = p_enc_k ) are_equal ( a = o_enc_v , b = p_enc_v ) out_dec_onnx_cache = decoder_onnx_inference( decoder_input_ids=input_ids[:, -1:], encoder_hidden_states=out_enc_pytorch.last_hidden_state, enable_cache=torch.tensor([True], device=\"cuda\", dtype=torch.bool), past_key_values=previous_step_pytorch.past_key_values, ) are_equal(a=out_dec_onnx_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :]) # check that past states are identical between Onnx and Pytorch assert len(out_dec_onnx_cache.past_key_values) == len(out_dec_pytorch.past_key_values) for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip( out_dec_onnx_cache.past_key_values, out_dec_pytorch.past_key_values ): are_equal(a=o_dec_k, b=p_dec_k) are_equal(a=o_dev_v, b=p_dev_v) are_equal(a=o_enc_k, b=p_enc_k) are_equal(a=o_enc_v, b=p_enc_v)","title":"Zero copy output"},{"location":"t5/#benchmarks","text":"Finally, we will compare the performances of 4 setup in end-to-end scenarii: Pytorch Pytorch + cache Onnx Onnx + cache For the comparison, we first do a sanity check by just generating a short sequence (we already have checked that output tensors are OK). Then we force each model to generate: 256 tokens + batch size 1 (similar to TensorRT demo) 1000 tokens + batch size 4 In [21]: Copied! def encoder_onnx_inference ( input_ids : torch . Tensor , ** _ ) -> BaseModelOutputWithPastAndCrossAttentions : last_hidden_state = inference_onnx_binding ( model_onnx = enc_fp16_onnx , # noqa: F821 inputs = { \"input_ids\" : input_ids }, device = input_ids . device . type , binding = enc_fp16_onnx_binding , )[ \"output\" ] return BaseModelOutputWithPastAndCrossAttentions ( last_hidden_state = last_hidden_state . type ( torch . float16 )) def encoder_pytorch_inference ( input_ids , ** _ ) -> BaseModelOutputWithPastAndCrossAttentions : with torch . inference_mode (): res = pytorch_model . encoder ( input_ids = input_ids ) . type ( torch . float16 ) return res # https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py class ExtT5 ( torch . nn . Module , GenerationMixin ): def __init__ ( self , config : PretrainedConfig , device : torch . device , encoder_func : Callable , decoder_func : Callable ): super ( ExtT5 , self ) . __init__ () self . main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 self . config : PretrainedConfig = config self . device : torch . device = device self . encoder_func = encoder_func self . decoder_func = decoder_func self . use_cache = True self . timings = list () def get_encoder ( self ): return self . encoder_func def get_decoder ( self ): return self . decoder_func def set_cache ( self , enable : bool ) -> None : self . use_cache = enable # from transformers library (modeling_t5.py) def _reorder_cache ( self , past , beam_idx ): reordered_decoder_past = () for layer_past_states in past : # get the correct batch idx from layer past batch dim # batch dim of `past` is at 2nd position reordered_layer_past_states = () for layer_past_state in layer_past_states : # need to set correct `past` for each of the four key / value states reordered_layer_past_states = reordered_layer_past_states + ( layer_past_state . index_select ( 0 , beam_idx ), ) assert reordered_layer_past_states [ 0 ] . shape == layer_past_states [ 0 ] . shape assert len ( reordered_layer_past_states ) == len ( layer_past_states ) reordered_decoder_past = reordered_decoder_past + ( reordered_layer_past_states ,) return reordered_decoder_past def prepare_inputs_for_generation ( self , input_ids , past = None , use_cache = None , ** kwargs ) -> Dict [ str , torch . Tensor ]: params = { \"encoder_hidden_states\" : kwargs [ \"encoder_outputs\" ][ \"last_hidden_state\" ], } if past is None : # this is the 1st inferred token self . timings = list () if not self . use_cache : past = None if past is None : params [ self . main_input_name ] = input_ids params [ \"enable_cache\" ] = torch . tensor ([ False ], device = \"cuda\" , dtype = torch . bool ) else : params [ self . main_input_name ] = input_ids [:, - 1 :] params [ \"enable_cache\" ] = torch . tensor ([ True ], device = \"cuda\" , dtype = torch . bool ) params [ \"past_key_values\" ] = past return params def forward ( self , input_ids : torch . Tensor , encoder_hidden_states : torch . Tensor , enable_cache : torch . Tensor , past_key_values : Optional [ torch . Tensor ] = None , ** _ , ): start_timer = time . monotonic () dec_output = self . get_decoder ()( decoder_input_ids = input_ids , encoder_hidden_states = encoder_hidden_states , enable_cache = enable_cache , past_key_values = past_key_values , ) self . timings . append ( time . monotonic () - start_timer ) return Seq2SeqLMOutput ( logits = dec_output . last_hidden_state , past_key_values = dec_output . past_key_values ) model_gen = ( ExtT5 ( config = pytorch_model . config , device = pytorch_model . device , encoder_func = encoder_onnx_inference , # encoder_pytorch_inference decoder_func = decoder_onnx_inference , # decoder_pytorch_inference ) . cuda () . eval () ) torch . cuda . synchronize () with torch . inference_mode (): print ( \"Onnx:\" ) print ( tokenizer . decode ( model_gen . generate ( inputs = input_ids , min_length = 3 , max_length = 60 , num_beams = 4 , no_repeat_ngram_size = 2 , )[ 0 ], skip_special_tokens = True , ) ) print ( \"Pytorch:\" ) print ( tokenizer . decode ( pytorch_model . generate ( input_ids = input_ids , min_length = 3 , max_length = 60 , num_beams = 4 , no_repeat_ngram_size = 2 , )[ 0 ], skip_special_tokens = True , ) ) def encoder_onnx_inference(input_ids: torch.Tensor, **_) -> BaseModelOutputWithPastAndCrossAttentions: last_hidden_state = inference_onnx_binding( model_onnx=enc_fp16_onnx, # noqa: F821 inputs={\"input_ids\": input_ids}, device=input_ids.device.type, binding=enc_fp16_onnx_binding, )[\"output\"] return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state.type(torch.float16)) def encoder_pytorch_inference(input_ids, **_) -> BaseModelOutputWithPastAndCrossAttentions: with torch.inference_mode(): res = pytorch_model.encoder(input_ids=input_ids).type(torch.float16) return res # https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py class ExtT5(torch.nn.Module, GenerationMixin): def __init__(self, config: PretrainedConfig, device: torch.device, encoder_func: Callable, decoder_func: Callable): super(ExtT5, self).__init__() self.main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 self.config: PretrainedConfig = config self.device: torch.device = device self.encoder_func = encoder_func self.decoder_func = decoder_func self.use_cache = True self.timings = list() def get_encoder(self): return self.encoder_func def get_decoder(self): return self.decoder_func def set_cache(self, enable: bool) -> None: self.use_cache = enable # from transformers library (modeling_t5.py) def _reorder_cache(self, past, beam_idx): reordered_decoder_past = () for layer_past_states in past: # get the correct batch idx from layer past batch dim # batch dim of `past` is at 2nd position reordered_layer_past_states = () for layer_past_state in layer_past_states: # need to set correct `past` for each of the four key / value states reordered_layer_past_states = reordered_layer_past_states + ( layer_past_state.index_select(0, beam_idx), ) assert reordered_layer_past_states[0].shape == layer_past_states[0].shape assert len(reordered_layer_past_states) == len(layer_past_states) reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,) return reordered_decoder_past def prepare_inputs_for_generation(self, input_ids, past=None, use_cache=None, **kwargs) -> Dict[str, torch.Tensor]: params = { \"encoder_hidden_states\": kwargs[\"encoder_outputs\"][\"last_hidden_state\"], } if past is None: # this is the 1st inferred token self.timings = list() if not self.use_cache: past = None if past is None: params[self.main_input_name] = input_ids params[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool) else: params[self.main_input_name] = input_ids[:, -1:] params[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool) params[\"past_key_values\"] = past return params def forward( self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, past_key_values: Optional[torch.Tensor] = None, **_, ): start_timer = time.monotonic() dec_output = self.get_decoder()( decoder_input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, enable_cache=enable_cache, past_key_values=past_key_values, ) self.timings.append(time.monotonic() - start_timer) return Seq2SeqLMOutput(logits=dec_output.last_hidden_state, past_key_values=dec_output.past_key_values) model_gen = ( ExtT5( config=pytorch_model.config, device=pytorch_model.device, encoder_func=encoder_onnx_inference, # encoder_pytorch_inference decoder_func=decoder_onnx_inference, # decoder_pytorch_inference ) .cuda() .eval() ) torch.cuda.synchronize() with torch.inference_mode(): print(\"Onnx:\") print( tokenizer.decode( model_gen.generate( inputs=input_ids, min_length=3, max_length=60, num_beams=4, no_repeat_ngram_size=2, )[0], skip_special_tokens=True, ) ) print(\"Pytorch:\") print( tokenizer.decode( pytorch_model.generate( input_ids=input_ids, min_length=3, max_length=60, num_beams=4, no_repeat_ngram_size=2, )[0], skip_special_tokens=True, ) ) Onnx: Ce mod\u00e8le est maintenant tr\u00e8s rapide! Pytorch: Ce mod\u00e8le est maintenant tr\u00e8s rapide! In [22]: Copied! def print_timings ( name : str , total : float , inference : float ): percent_inference = 100 * inference / total print ( f \" { name } : { total : .1f } , including inference: { inference : .1f } ( { percent_inference : .1f } %)\" ) all_timings : Dict [ str , Dict [ str , List [ float ]]] = dict () for seq_len , num_beam in [( 256 , 1 ), ( 1000 , 4 )]: timings = dict () print ( f \"seq len: { seq_len } / # beam (batch size): { num_beam } \" ) task = \"Onnx\" with nvtx . annotate ( task , color = \"red\" ): # nvtx is for Nvidia nsight profiler, you can remove the line or install the library model_gen . set_cache ( enable = False ) # warmup model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) start = time . monotonic () model_gen . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start print_timings ( name = task , total = total_time , inference = sum ( model_gen . timings )) timings [ f \" { task } \" ] = model_gen . timings task = \"Onnx + cache\" with nvtx . annotate ( task , color = \"red\" ): model_gen . set_cache ( enable = True ) # warmup model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) start = time . monotonic () model_gen . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start print_timings ( name = task , total = total_time , inference = sum ( model_gen . timings )) timings [ f \" { task } \" ] = model_gen . timings # monckey patching of forward function to add a timer per generated token old_fw = pytorch_model . forward timing_pytorch = list () def new_fw ( self , * args , ** kwargs ): timer_start = time . monotonic () res = old_fw ( self , * args , ** kwargs ) torch . cuda . synchronize () # makes timings correct without having significant impact on e2e latency total_time = time . monotonic () - timer_start timing_pytorch . append ( total_time ) return res task = \"Pytorch\" with nvtx . annotate ( task , color = \"orange\" ): pytorch_model . config . use_cache = False with torch . inference_mode (): with torch . cuda . amp . autocast (): # warmup pytorch_model . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) pytorch_model . forward = new_fw . __get__ ( pytorch_model ) start = time . monotonic () pytorch_model . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start pytorch_model . forward = old_fw inference_time = np . sum ( timing_pytorch ) print_timings ( name = \"Pytorch\" , total = total_time , inference = inference_time ) timing_pytorch_no_cache = copy ( timing_pytorch ) timings [ f \" { task } \" ] = copy ( timing_pytorch ) timing_pytorch . clear () torch . cuda . empty_cache () task = \"Pytorch + cache\" with nvtx . annotate ( \"Pytorch + cache\" , color = \"green\" ): pytorch_model . config . use_cache = True with torch . inference_mode (): with torch . cuda . amp . autocast (): # warmup pytorch_model . generate ( inputs = input_ids , max_length = 10 , num_beams = num_beam , min_length = 10 ) pytorch_model . forward = new_fw . __get__ ( pytorch_model ) start = time . monotonic () pytorch_model . generate ( inputs = input_ids , max_length = seq_len , num_beams = num_beam , min_length = seq_len ) total_time = time . monotonic () - start pytorch_model . forward = old_fw print_timings ( name = \"Pytorch + cache\" , total = total_time , inference = sum ( timing_pytorch )) timings [ f \" { task } \" ] = copy ( timing_pytorch ) timing_pytorch . clear () all_timings [ f \" { seq_len } / { num_beam } \" ] = timings torch . cuda . empty_cache () def print_timings(name: str, total: float, inference: float): percent_inference = 100 * inference / total print(f\"{name}: {total:.1f}, including inference: {inference:.1f} ({percent_inference:.1f}%)\") all_timings: Dict[str, Dict[str, List[float]]] = dict() for seq_len, num_beam in [(256, 1), (1000, 4)]: timings = dict() print(f\"seq len: {seq_len} / # beam (batch size): {num_beam}\") task = \"Onnx\" with nvtx.annotate( task, color=\"red\" ): # nvtx is for Nvidia nsight profiler, you can remove the line or install the library model_gen.set_cache(enable=False) # warmup model_gen.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) start = time.monotonic() model_gen.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start print_timings(name=task, total=total_time, inference=sum(model_gen.timings)) timings[f\"{task}\"] = model_gen.timings task = \"Onnx + cache\" with nvtx.annotate(task, color=\"red\"): model_gen.set_cache(enable=True) # warmup model_gen.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) start = time.monotonic() model_gen.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start print_timings(name=task, total=total_time, inference=sum(model_gen.timings)) timings[f\"{task}\"] = model_gen.timings # monckey patching of forward function to add a timer per generated token old_fw = pytorch_model.forward timing_pytorch = list() def new_fw(self, *args, **kwargs): timer_start = time.monotonic() res = old_fw(self, *args, **kwargs) torch.cuda.synchronize() # makes timings correct without having significant impact on e2e latency total_time = time.monotonic() - timer_start timing_pytorch.append(total_time) return res task = \"Pytorch\" with nvtx.annotate(task, color=\"orange\"): pytorch_model.config.use_cache = False with torch.inference_mode(): with torch.cuda.amp.autocast(): # warmup pytorch_model.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) pytorch_model.forward = new_fw.__get__(pytorch_model) start = time.monotonic() pytorch_model.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start pytorch_model.forward = old_fw inference_time = np.sum(timing_pytorch) print_timings(name=\"Pytorch\", total=total_time, inference=inference_time) timing_pytorch_no_cache = copy(timing_pytorch) timings[f\"{task}\"] = copy(timing_pytorch) timing_pytorch.clear() torch.cuda.empty_cache() task = \"Pytorch + cache\" with nvtx.annotate(\"Pytorch + cache\", color=\"green\"): pytorch_model.config.use_cache = True with torch.inference_mode(): with torch.cuda.amp.autocast(): # warmup pytorch_model.generate(inputs=input_ids, max_length=10, num_beams=num_beam, min_length=10) pytorch_model.forward = new_fw.__get__(pytorch_model) start = time.monotonic() pytorch_model.generate(inputs=input_ids, max_length=seq_len, num_beams=num_beam, min_length=seq_len) total_time = time.monotonic() - start pytorch_model.forward = old_fw print_timings(name=\"Pytorch + cache\", total=total_time, inference=sum(timing_pytorch)) timings[f\"{task}\"] = copy(timing_pytorch) timing_pytorch.clear() all_timings[f\"{seq_len} / {num_beam}\"] = timings torch.cuda.empty_cache() seq len: 256 / # beam (batch size): 1 Onnx: 3.9, including inference: 3.8 (96.9%) Onnx + cache: 3.3, including inference: 3.1 (96.3%) Pytorch: 9.8, including inference: 9.7 (99.0%) Pytorch + cache: 8.2, including inference: 8.1 (98.7%) seq len: 1000 / # beam (batch size): 4 Onnx: 99.4, including inference: 97.1 (97.7%) Onnx + cache: 17.7, including inference: 15.6 (87.8%) Pytorch: 96.3, including inference: 95.5 (99.1%) Pytorch + cache: 37.0, including inference: 35.0 (94.6%)","title":"Benchmarks!"},{"location":"t5/#benchmark-analysis","text":"Below, we plot for each setup (short and long sequence): the time spent on each token generation the full time to generate the sequence (for each length) We can see that for short sequence and batch size of 1, cache or not, latency appears to be stable. However, for longer sequences, we can see that the no cache approach (being Pytorch or Onnx based) doesn't scale well, and at some point, Onnx is even slower than Hugging Face code with cache support. On the other side, Onnx timings are mostly stable whatever the sequence length which is quite remarkable. It's because we are working one token at a time and converted a quadratic complexity in the attention layer into a linear one. In [23]: Copied! sns . set_style ( \"darkgrid\" ) # darkgrid, whitegrid, dark, white and ticks plt . rc ( \"axes\" , titlesize = 15 ) # fontsize of the axes title plt . rc ( \"axes\" , labelsize = 14 ) # fontsize of the x and y labels plt . rc ( \"xtick\" , labelsize = 13 ) # fontsize of the tick labels plt . rc ( \"ytick\" , labelsize = 13 ) # fontsize of the tick labels plt . rc ( \"legend\" , fontsize = 15 ) # legend fontsize plt . rc ( \"font\" , size = 13 ) # controls default text sizes colors = sns . color_palette ( \"deep\" ) fig = plt . figure ( constrained_layout = True , figsize = ( 12 , 8 )) subfigs = fig . subfigures ( nrows = 2 , ncols = 1 ) fig . supxlabel ( \"seq len (# tokens)\" ) fig . supylabel ( \"latency (s)\" ) fig . suptitle ( f \"Small seq len and greedy search on { model_name } don't tell the whole (inference) story...\" ) for row , ( plot_name , timings ) in enumerate ( all_timings . items ()): subfigs [ row ] . suptitle ( f \"setup # { 1 + row } : { plot_name } (seq len / beam search)\" ) axs = subfigs [ row ] . subplots ( nrows = 1 , ncols = 2 ) for col , accumulated in enumerate ([ False , True ]): plot_axis = axs [ col ] for index , ( k , v ) in enumerate ( timings . items ()): axis = range ( len ( v )) color = colors [ index ] v = np . array ( v ) # remove extreme values p99 = np . percentile ( v , 99 ) v [ v > p99 ] = p99 v = np . cumsum ( v ) if accumulated else v plot_axis . scatter ( axis , v , label = k , s = 2 ) title = f \"latency for the full sequence\" if accumulated else f \"latency for each token\" plot_axis . title . set_text ( title ) # legend deduplication handles , labels = plt . gca () . get_legend_handles_labels () by_label = dict ( zip ( labels , handles )) fig . legend ( by_label . values (), by_label . keys (), bbox_to_anchor = ( 1 , 1 ), loc = \"upper left\" , markerscale = 5 ) plt . show () sns.set_style(\"darkgrid\") # darkgrid, whitegrid, dark, white and ticks plt.rc(\"axes\", titlesize=15) # fontsize of the axes title plt.rc(\"axes\", labelsize=14) # fontsize of the x and y labels plt.rc(\"xtick\", labelsize=13) # fontsize of the tick labels plt.rc(\"ytick\", labelsize=13) # fontsize of the tick labels plt.rc(\"legend\", fontsize=15) # legend fontsize plt.rc(\"font\", size=13) # controls default text sizes colors = sns.color_palette(\"deep\") fig = plt.figure(constrained_layout=True, figsize=(12, 8)) subfigs = fig.subfigures(nrows=2, ncols=1) fig.supxlabel(\"seq len (# tokens)\") fig.supylabel(\"latency (s)\") fig.suptitle(f\"Small seq len and greedy search on {model_name} don't tell the whole (inference) story...\") for row, (plot_name, timings) in enumerate(all_timings.items()): subfigs[row].suptitle(f\"setup #{1+row}: {plot_name} (seq len / beam search)\") axs = subfigs[row].subplots(nrows=1, ncols=2) for col, accumulated in enumerate([False, True]): plot_axis = axs[col] for index, (k, v) in enumerate(timings.items()): axis = range(len(v)) color = colors[index] v = np.array(v) # remove extreme values p99 = np.percentile(v, 99) v[v > p99] = p99 v = np.cumsum(v) if accumulated else v plot_axis.scatter(axis, v, label=k, s=2) title = f\"latency for the full sequence\" if accumulated else f\"latency for each token\" plot_axis.title.set_text(title) # legend deduplication handles, labels = plt.gca().get_legend_handles_labels() by_label = dict(zip(labels, handles)) fig.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(1, 1), loc=\"upper left\", markerscale=5) plt.show()","title":"Benchmark analysis"},{"location":"t5/#profiling-model-at-the-kernel-level","text":"Below we reload the decoder model with Onnx Runtime kernel profiling enabled. It will help us to understand on which part of the computation graph the GPU spends its time. The number of events that Onnx Runtime can save is limited to 1 million . It is not an issue as we have seen that timings per token are mostly stable, so having only n first token information don't change anything. The main information it gives us is that 30% of the time is spent on matrix multiplication when caching is used. The rest of the time is spent on mostly memory bound operations: element-wise operations which require little computation ( add , mul , div , etc.) copy pasting tensors GPU <-> GPU with little transformation in between ( transpose , concat , cast , etc.) It matches the information provided by both nvidia-smi and Nvidia Nsight (the GPU profiler from Nvidia): the GPU is under utilized. That's why we think that a tool like TensorRT which will perform aggressive kernel fusion, reducing time spent on memory bounded operations, should be a good fit for autoregressive models. there is a nice opportunity to increase the speedup by reducing the number of casting operations. We keep this work for the future. In [24]: Copied! dec_onnx = create_model_for_provider ( dec_if_fp16_model_path , \"CUDAExecutionProvider\" , enable_profiling = True , log_severity = 3 ) dec_onnx_binding : IOBinding = dec_onnx . io_binding () _ = model_gen . generate ( inputs = input_ids , max_length = 10 , num_beams = 4 , min_length = 10 ) profile_name = dec_onnx . end_profiling () with open ( profile_name ) as f : content = json . load ( f ) op_timings = defaultdict ( lambda : 0 ) for c in content : if \"op_name\" not in c [ \"args\" ]: continue op_name = c [ \"args\" ][ \"op_name\" ] if op_name == \"If\" : continue # subgraph time_taken = c [ \"dur\" ] op_timings [ op_name ] += time_taken op_timings_filter = dict ( sorted ( op_timings . items (), key = operator . itemgetter ( 1 ), reverse = True )[: 10 ]) total_kernel_timing = sum ( op_timings . values ()) op_timings_percent = { k : 100 * v / total_kernel_timing for k , v in op_timings_filter . items ()} plt . barh ( list ( op_timings_percent . keys ()), list ( op_timings_percent . values ())) plt . title ( \"Time spent per kernel \\n (top 10 kernels)\" ) plt . xlabel ( \"% total inference time\" ) plt . show () dec_onnx = create_model_for_provider( dec_if_fp16_model_path, \"CUDAExecutionProvider\", enable_profiling=True, log_severity=3 ) dec_onnx_binding: IOBinding = dec_onnx.io_binding() _ = model_gen.generate(inputs=input_ids, max_length=10, num_beams=4, min_length=10) profile_name = dec_onnx.end_profiling() with open(profile_name) as f: content = json.load(f) op_timings = defaultdict(lambda: 0) for c in content: if \"op_name\" not in c[\"args\"]: continue op_name = c[\"args\"][\"op_name\"] if op_name == \"If\": continue # subgraph time_taken = c[\"dur\"] op_timings[op_name] += time_taken op_timings_filter = dict(sorted(op_timings.items(), key=operator.itemgetter(1), reverse=True)[:10]) total_kernel_timing = sum(op_timings.values()) op_timings_percent = {k: 100 * v / total_kernel_timing for k, v in op_timings_filter.items()} plt.barh(list(op_timings_percent.keys()), list(op_timings_percent.values())) plt.title(\"Time spent per kernel\\n(top 10 kernels)\") plt.xlabel(\"% total inference time\") plt.show()","title":"Profiling model at the kernel level"},{"location":"quantization/quantization/","text":"(function (global, factory) { typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() : typeof define === 'function' && define.amd ? define(factory) : (global = global || self, global.ClipboardCopyElement = factory()); }(this, function () { 'use strict'; function createNode(text) { const node = document.createElement('pre'); node.style.width = '1px'; node.style.height = '1px'; node.style.position = 'fixed'; node.style.top = '5px'; node.textContent = text; return node; } function copyNode(node) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(node.textContent); } const selection = getSelection(); if (selection == null) { return Promise.reject(new Error()); } selection.removeAllRanges(); const range = document.createRange(); range.selectNodeContents(node); selection.addRange(range); document.execCommand('copy'); selection.removeAllRanges(); return Promise.resolve(); } function copyText(text) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(text); } const body = document.body; if (!body) { return Promise.reject(new Error()); } const node = createNode(text); body.appendChild(node); copyNode(node); body.removeChild(node); return Promise.resolve(); } function copy(button) { const id = button.getAttribute('for'); const text = button.getAttribute('value'); function trigger() { button.dispatchEvent(new CustomEvent('clipboard-copy', { bubbles: true })); } if (text) { copyText(text).then(trigger); } else if (id) { const root = 'getRootNode' in Element.prototype ? button.getRootNode() : button.ownerDocument; if (!(root instanceof Document || 'ShadowRoot' in window && root instanceof ShadowRoot)) return; const node = root.getElementById(id); if (node) copyTarget(node).then(trigger); } } function copyTarget(content) { if (content instanceof HTMLInputElement || content instanceof HTMLTextAreaElement) { return copyText(content.value); } else if (content instanceof HTMLAnchorElement && content.hasAttribute('href')) { return copyText(content.href); } else { return copyNode(content); } } function clicked(event) { const button = event.currentTarget; if (button instanceof HTMLElement) { copy(button); } } function keydown(event) { if (event.key === ' ' || event.key === 'Enter') { const button = event.currentTarget; if (button instanceof HTMLElement) { event.preventDefault(); copy(button); } } } function focused(event) { event.currentTarget.addEventListener('keydown', keydown); } function blurred(event) { event.currentTarget.removeEventListener('keydown', keydown); } class ClipboardCopyElement extends HTMLElement { constructor() { super(); this.addEventListener('click', clicked); this.addEventListener('focus', focused); this.addEventListener('blur', blurred); } connectedCallback() { if (!this.hasAttribute('tabindex')) { this.setAttribute('tabindex', '0'); } if (!this.hasAttribute('role')) { this.setAttribute('role', 'button'); } } get value() { return this.getAttribute('value') || ''; } set value(text) { this.setAttribute('value', text); } } if (!window.customElements.get('clipboard-copy')) { window.ClipboardCopyElement = ClipboardCopyElement; window.customElements.define('clipboard-copy', ClipboardCopyElement); } return ClipboardCopyElement; })); document.addEventListener('clipboard-copy', function(event) { const notice = event.target.querySelector('.notice') notice.hidden = false setTimeout(function() { notice.hidden = true }, 1000) }) (function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { var widgetRendererSrc = 'https://unpkg.com/@jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight-ipynb .hll { background-color: var(--jp-cell-editor-active-background) } .highlight-ipynb { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight-ipynb .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight-ipynb .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight-ipynb .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight-ipynb .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight-ipynb .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight-ipynb .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight-ipynb .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight-ipynb .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight-ipynb .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight-ipynb .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight-ipynb .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight-ipynb .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight-ipynb .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight-ipynb .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight-ipynb .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight-ipynb .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight-ipynb .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight-ipynb .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight-ipynb .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ /* This file is taken from the built JupyterLab theme.css Found on share/nbconvert/templates/lab/static Some changes have been made and marked with CHANGE */ .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ --jp-shadow-base-lightness: 0; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-400); --jp-border-color1: var(--md-grey-400); --jp-border-color2: var(--md-grey-300); --jp-border-color3: var(--md-grey-200); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(0, 0, 0, 1); --jp-ui-font-color1: rgba(0, 0, 0, 0.87); --jp-ui-font-color2: rgba(0, 0, 0, 0.54); --jp-ui-font-color3: rgba(0, 0, 0, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color1: rgba(255, 255, 255, 1); --jp-ui-inverse-font-color2: rgba(255, 255, 255, 0.7); --jp-ui-inverse-font-color3: rgba(255, 255, 255, 0.5); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(0, 0, 0, 1); --jp-content-font-color1: rgba(0, 0, 0, 0.87); --jp-content-font-color2: rgba(0, 0, 0, 0.54); --jp-content-font-color3: rgba(0, 0, 0, 0.38); --jp-content-link-color: var(--md-blue-700); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: white; --jp-layout-color1: white; --jp-layout-color2: var(--md-grey-200); --jp-layout-color3: var(--md-grey-400); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: #111111; --jp-inverse-layout-color1: var(--md-grey-900); --jp-inverse-layout-color2: var(--md-grey-800); --jp-inverse-layout-color3: var(--md-grey-700); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-900); --jp-brand-color1: var(--md-blue-700); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-900); --jp-accent-color1: var(--md-green-700); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-900); --jp-warn-color1: var(--md-orange-700); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-900); --jp-error-color1: var(--md-red-700); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-900); --jp-success-color1: var(--md-green-700); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-900); --jp-info-color1: var(--md-cyan-700); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--md-grey-100); --jp-cell-editor-border-color: var(--md-grey-300); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 0.5; --jp-cell-prompt-not-active-font-color: var(--md-grey-700); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: var(--md-blue-50); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: #fdd; --jp-rendermime-table-row-background: var(--md-grey-100); --jp-rendermime-table-row-hover-background: var(--md-light-blue-50); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.25); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color1); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.24); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--md-grey-300); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color1); --jp-input-hover-background: var(--jp-layout-color1); --jp-input-background: var(--md-grey-100); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: #d9d9d9; --jp-editor-selected-focused-background: #d7d4f0; --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: #008000; --jp-mirror-editor-atom-color: #88f; --jp-mirror-editor-number-color: #080; --jp-mirror-editor-def-color: #00f; --jp-mirror-editor-variable-color: var(--md-grey-900); --jp-mirror-editor-variable-2-color: #05a; --jp-mirror-editor-variable-3-color: #085; --jp-mirror-editor-punctuation-color: #05a; --jp-mirror-editor-property-color: #05a; --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ba2121; --jp-mirror-editor-string-2-color: #708; --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: #008000; --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: #170; --jp-mirror-editor-attribute-color: #00c; --jp-mirror-editor-header-color: blue; --jp-mirror-editor-quote-color: #090; --jp-mirror-editor-link-color: #00c; --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: white; /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.5; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(245, 200, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } [data-md-color-scheme=\"slate\"] .jupyter-wrapper { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ /* The dark theme shadows need a bit of work, but this will probably also require work on the core layout * colors used in the theme as well. */ --jp-shadow-base-lightness: 32; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-700); --jp-border-color1: var(--md-grey-700); --jp-border-color2: var(--md-grey-800); --jp-border-color3: var(--md-grey-900); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(255, 255, 255, 1); --jp-ui-font-color1: rgba(255, 255, 255, 0.87); --jp-ui-font-color2: rgba(255, 255, 255, 0.54); --jp-ui-font-color3: rgba(255, 255, 255, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(0, 0, 0, 1); --jp-ui-inverse-font-color1: rgba(0, 0, 0, 0.8); --jp-ui-inverse-font-color2: rgba(0, 0, 0, 0.5); --jp-ui-inverse-font-color3: rgba(0, 0, 0, 0.3); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(255, 255, 255, 1); --jp-content-font-color1: rgba(255, 255, 255, 1); --jp-content-font-color2: rgba(255, 255, 255, 0.7); --jp-content-font-color3: rgba(255, 255, 255, 0.5); --jp-content-link-color: var(--md-blue-300); --jp-content-font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Helvetica, Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\"; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, \"DejaVu Sans Mono\", monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: #111111; --jp-layout-color1: var(--md-grey-900); --jp-layout-color2: var(--md-grey-800); --jp-layout-color3: var(--md-grey-700); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: white; --jp-inverse-layout-color1: white; --jp-inverse-layout-color2: var(--md-grey-200); --jp-inverse-layout-color3: var(--md-grey-400); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--jp-layout-color1); --jp-cell-editor-border-color: var(--md-grey-700); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 1; --jp-cell-prompt-not-active-font-color: var(--md-grey-300); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: rgba(33, 150, 243, 0.24); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: rgba(244, 67, 54, 0.28); --jp-rendermime-table-row-background: var(--md-grey-900); --jp-rendermime-table-row-hover-background: rgba(3, 169, 244, 0.2); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.6); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color2); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.8); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--jp-layout-color0); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color0); --jp-input-hover-background: var(--jp-layout-color2); --jp-input-background: var(--md-grey-800); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: var(--jp-layout-color2); --jp-editor-selected-focused-background: rgba(33, 150, 243, 0.24); --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: var(--md-green-500); --jp-mirror-editor-atom-color: var(--md-blue-300); --jp-mirror-editor-number-color: var(--md-green-400); --jp-mirror-editor-def-color: var(--md-blue-600); --jp-mirror-editor-variable-color: var(--md-grey-300); --jp-mirror-editor-variable-2-color: var(--md-blue-400); --jp-mirror-editor-variable-3-color: var(--md-green-600); --jp-mirror-editor-punctuation-color: var(--md-blue-400); --jp-mirror-editor-property-color: var(--md-blue-400); --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ff7070; --jp-mirror-editor-string-2-color: var(--md-purple-300); --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: var(--md-green-600); --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: var(--md-green-700); --jp-mirror-editor-attribute-color: var(--md-blue-700); --jp-mirror-editor-header-color: var(--md-blue-500); --jp-mirror-editor-quote-color: var(--md-green-300); --jp-mirror-editor-link-color: var(--md-blue-700); --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: var(--md-grey-400); /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.6; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(255, 225, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* scrollbar related styles. Supports every browser except Edge. */ /* colors based on JetBrain's Darcula theme */ --jp-scrollbar-background-color: #3f4244; --jp-scrollbar-thumb-color: 88, 96, 97; /* need to specify thumb color as an RGB triplet */ --jp-scrollbar-endpad: 3px; /* the minimum gap between the thumb and the ends of a scrollbar */ /* hacks for setting the thumb shape. These do nothing in Firefox */ --jp-scrollbar-thumb-margin: 3.5px; /* the space in between the sides of the thumb and the track */ --jp-scrollbar-thumb-radius: 9px; /* set to a large-ish value for rounded endcaps on the thumb */ /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper{/*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. *//*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */}.jupyter-wrapper [data-jp-theme-scrollbars=true]{scrollbar-color:rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar{scrollbar-color:rgba(var(--jp-scrollbar-thumb-color), 0.5) rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-corner{background:var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-thumb{background:rgb(var(--jp-scrollbar-thumb-color));border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-right:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-bottom:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-corner,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-corner{background-color:rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb{background:rgba(var(--jp-scrollbar-thumb-color), 0.5);border:var(--jp-scrollbar-thumb-margin) solid rgba(0,0,0,0);background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-right:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0);border-bottom:var(--jp-scrollbar-endpad) solid rgba(0,0,0,0)}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{min-height:16px;max-height:16px;min-width:45px;border-top:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{min-width:16px;max-width:16px;min-height:45px;border-left:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar-button{background-color:#f0f0f0;background-position:center center;min-height:15px;max-height:15px;min-width:15px;max-width:15px}.jupyter-wrapper .lm-ScrollBar-button:hover{background-color:#dadada}.jupyter-wrapper .lm-ScrollBar-button.lm-mod-active{background-color:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-track{background:#f0f0f0}.jupyter-wrapper .lm-ScrollBar-thumb{background:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-thumb:hover{background:#bababa}.jupyter-wrapper .lm-ScrollBar-thumb.lm-mod-active{background:#a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-thumb{height:100%;min-width:15px;border-left:1px solid #a0a0a0;border-right:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-thumb{width:100%;min-height:15px;border-top:1px solid #a0a0a0;border-bottom:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-left);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-right);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-up);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-down);background-size:17px}.jupyter-wrapper .p-Widget,.jupyter-wrapper .lm-Widget{box-sizing:border-box;position:relative;overflow:hidden;cursor:default}.jupyter-wrapper .p-Widget.p-mod-hidden,.jupyter-wrapper .lm-Widget.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-CommandPalette,.jupyter-wrapper .lm-CommandPalette{display:flex;flex-direction:column;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-CommandPalette-search,.jupyter-wrapper .lm-CommandPalette-search{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-content,.jupyter-wrapper .lm-CommandPalette-content{flex:1 1 auto;margin:0;padding:0;min-height:0;overflow:auto;list-style-type:none}.jupyter-wrapper .p-CommandPalette-header,.jupyter-wrapper .lm-CommandPalette-header{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-CommandPalette-item,.jupyter-wrapper .lm-CommandPalette-item{display:flex;flex-direction:row}.jupyter-wrapper .p-CommandPalette-itemIcon,.jupyter-wrapper .lm-CommandPalette-itemIcon{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemContent,.jupyter-wrapper .lm-CommandPalette-itemContent{flex:1 1 auto;overflow:hidden}.jupyter-wrapper .p-CommandPalette-itemShortcut,.jupyter-wrapper .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemLabel,.jupyter-wrapper .lm-CommandPalette-itemLabel{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-DockPanel,.jupyter-wrapper .lm-DockPanel{z-index:0}.jupyter-wrapper .p-DockPanel-widget,.jupyter-wrapper .lm-DockPanel-widget{z-index:0}.jupyter-wrapper .p-DockPanel-tabBar,.jupyter-wrapper .lm-DockPanel-tabBar{z-index:1}.jupyter-wrapper .p-DockPanel-handle,.jupyter-wrapper .lm-DockPanel-handle{z-index:2}.jupyter-wrapper .p-DockPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-DockPanel-handle:after,.jupyter-wrapper .lm-DockPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]{cursor:ew-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]{cursor:ns-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-DockPanel-overlay,.jupyter-wrapper .lm-DockPanel-overlay{z-index:3;box-sizing:border-box;pointer-events:none}.jupyter-wrapper .p-DockPanel-overlay.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-overlay.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-Menu,.jupyter-wrapper .lm-Menu{z-index:10000;position:absolute;white-space:nowrap;overflow-x:hidden;overflow-y:auto;outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-Menu-content,.jupyter-wrapper .lm-Menu-content{margin:0;padding:0;display:table;list-style-type:none}.jupyter-wrapper .p-Menu-item,.jupyter-wrapper .lm-Menu-item{display:table-row}.jupyter-wrapper .p-Menu-item.p-mod-hidden,.jupyter-wrapper .p-Menu-item.p-mod-collapsed,.jupyter-wrapper .lm-Menu-item.lm-mod-hidden,.jupyter-wrapper .lm-Menu-item.lm-mod-collapsed{display:none !important}.jupyter-wrapper .p-Menu-itemIcon,.jupyter-wrapper .p-Menu-itemSubmenuIcon,.jupyter-wrapper .lm-Menu-itemIcon,.jupyter-wrapper .lm-Menu-itemSubmenuIcon{display:table-cell;text-align:center}.jupyter-wrapper .p-Menu-itemLabel,.jupyter-wrapper .lm-Menu-itemLabel{display:table-cell;text-align:left}.jupyter-wrapper .p-Menu-itemShortcut,.jupyter-wrapper .lm-Menu-itemShortcut{display:table-cell;text-align:right}.jupyter-wrapper .p-MenuBar,.jupyter-wrapper .lm-MenuBar{outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-MenuBar-content,.jupyter-wrapper .lm-MenuBar-content{margin:0;padding:0;display:flex;flex-direction:row;list-style-type:none}.jupyter-wrapper .p--MenuBar-item,.jupyter-wrapper .lm-MenuBar-item{box-sizing:border-box}.jupyter-wrapper .p-MenuBar-itemIcon,.jupyter-wrapper .p-MenuBar-itemLabel,.jupyter-wrapper .lm-MenuBar-itemIcon,.jupyter-wrapper .lm-MenuBar-itemLabel{display:inline-block}.jupyter-wrapper .p-ScrollBar,.jupyter-wrapper .lm-ScrollBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-ScrollBar[data-orientation=horizontal],.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-ScrollBar[data-orientation=vertical],.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-ScrollBar-button,.jupyter-wrapper .lm-ScrollBar-button{box-sizing:border-box;flex:0 0 auto}.jupyter-wrapper .p-ScrollBar-track,.jupyter-wrapper .lm-ScrollBar-track{box-sizing:border-box;position:relative;overflow:hidden;flex:1 1 auto}.jupyter-wrapper .p-ScrollBar-thumb,.jupyter-wrapper .lm-ScrollBar-thumb{box-sizing:border-box;position:absolute}.jupyter-wrapper .p-SplitPanel-child,.jupyter-wrapper .lm-SplitPanel-child{z-index:0}.jupyter-wrapper .p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel-handle{z-index:1}.jupyter-wrapper .p-SplitPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-SplitPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle{cursor:ew-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle{cursor:ns-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-TabBar,.jupyter-wrapper .lm-TabBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal],.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical],.jupyter-wrapper .lm-TabBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-TabBar-content,.jupyter-wrapper .lm-TabBar-content{margin:0;padding:0;display:flex;flex:1 1 auto;list-style-type:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]>.lm-TabBar-content{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=vertical]>.lm-TabBar-content{flex-direction:column}.jupyter-wrapper .p-TabBar-tab,.jupyter-wrapper .lm-TabBar-tab{display:flex;flex-direction:row;box-sizing:border-box;overflow:hidden}.jupyter-wrapper .p-TabBar-tabIcon,.jupyter-wrapper .p-TabBar-tabCloseIcon,.jupyter-wrapper .lm-TabBar-tabIcon,.jupyter-wrapper .lm-TabBar-tabCloseIcon{flex:0 0 auto}.jupyter-wrapper .p-TabBar-tabLabel,.jupyter-wrapper .lm-TabBar-tabLabel{flex:1 1 auto;overflow:hidden;white-space:nowrap}.jupyter-wrapper .p-TabBar-tab.p-mod-hidden,.jupyter-wrapper .lm-TabBar-tab.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging .lm-TabBar-tab{position:relative}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=horizontal] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=horizontal] .lm-TabBar-tab{left:0;transition:left 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=vertical] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=vertical] .lm-TabBar-tab{top:0;transition:top 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging{transition:none}.jupyter-wrapper .p-TabPanel-tabBar,.jupyter-wrapper .lm-TabPanel-tabBar{z-index:1}.jupyter-wrapper .p-TabPanel-stackedPanel,.jupyter-wrapper .lm-TabPanel-stackedPanel{z-index:0}.jupyter-wrapper ::-moz-selection{background:rgba(125,188,255,.6)}.jupyter-wrapper ::selection{background:rgba(125,188,255,.6)}.jupyter-wrapper .bp3-heading{color:#182026;font-weight:600;margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-dark .bp3-heading{color:#f5f8fa}.jupyter-wrapper h1.bp3-heading,.jupyter-wrapper .bp3-running-text h1{line-height:40px;font-size:36px}.jupyter-wrapper h2.bp3-heading,.jupyter-wrapper .bp3-running-text h2{line-height:32px;font-size:28px}.jupyter-wrapper h3.bp3-heading,.jupyter-wrapper .bp3-running-text h3{line-height:25px;font-size:22px}.jupyter-wrapper h4.bp3-heading,.jupyter-wrapper .bp3-running-text h4{line-height:21px;font-size:18px}.jupyter-wrapper h5.bp3-heading,.jupyter-wrapper .bp3-running-text h5{line-height:19px;font-size:16px}.jupyter-wrapper h6.bp3-heading,.jupyter-wrapper .bp3-running-text h6{line-height:16px;font-size:14px}.jupyter-wrapper .bp3-ui-text{text-transform:none;line-height:1.28581;letter-spacing:0;font-size:14px;font-weight:400}.jupyter-wrapper .bp3-monospace-text{text-transform:none;font-family:monospace}.jupyter-wrapper .bp3-text-muted{color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-text-muted{color:#a7b6c2}.jupyter-wrapper .bp3-text-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-text-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal}.jupyter-wrapper .bp3-running-text{line-height:1.5;font-size:14px}.jupyter-wrapper .bp3-running-text h1{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h1{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h2{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h2{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h3{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h3{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h4{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h4{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h5{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h5{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h6{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h6{color:#f5f8fa}.jupyter-wrapper .bp3-running-text hr{margin:20px 0;border:none;border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text hr{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-running-text p{margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-text-large{font-size:16px}.jupyter-wrapper .bp3-text-small{font-size:12px}.jupyter-wrapper a{text-decoration:none;color:#106ba3}.jupyter-wrapper a:hover{cursor:pointer;text-decoration:underline;color:#106ba3}.jupyter-wrapper a .bp3-icon,.jupyter-wrapper a .bp3-icon-standard,.jupyter-wrapper a .bp3-icon-large{color:inherit}.jupyter-wrapper a code,.jupyter-wrapper .bp3-dark a code{color:inherit}.jupyter-wrapper .bp3-dark a,.jupyter-wrapper .bp3-dark a:hover{color:#48aff0}.jupyter-wrapper .bp3-dark a .bp3-icon,.jupyter-wrapper .bp3-dark a .bp3-icon-standard,.jupyter-wrapper .bp3-dark a .bp3-icon-large,.jupyter-wrapper .bp3-dark a:hover .bp3-icon,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-standard,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-large{color:inherit}.jupyter-wrapper .bp3-running-text code,.jupyter-wrapper .bp3-code{text-transform:none;font-family:monospace;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);background:rgba(255,255,255,.7);padding:2px 5px;color:#5c7080;font-size:smaller}.jupyter-wrapper .bp3-dark .bp3-running-text code,.jupyter-wrapper .bp3-running-text .bp3-dark code,.jupyter-wrapper .bp3-dark .bp3-code{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#a7b6c2}.jupyter-wrapper .bp3-running-text a>code,.jupyter-wrapper a>.bp3-code{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-running-text a>code,.jupyter-wrapper .bp3-running-text .bp3-dark a>code,.jupyter-wrapper .bp3-dark a>.bp3-code{color:inherit}.jupyter-wrapper .bp3-running-text pre,.jupyter-wrapper .bp3-code-block{text-transform:none;font-family:monospace;display:block;margin:10px 0;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);background:rgba(255,255,255,.7);padding:13px 15px 12px;line-height:1.4;color:#182026;font-size:13px;word-break:break-all;word-wrap:break-word}.jupyter-wrapper .bp3-dark .bp3-running-text pre,.jupyter-wrapper .bp3-running-text .bp3-dark pre,.jupyter-wrapper .bp3-dark .bp3-code-block{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-running-text pre>code,.jupyter-wrapper .bp3-code-block>code{-webkit-box-shadow:none;box-shadow:none;background:none;padding:0;color:inherit;font-size:inherit}.jupyter-wrapper .bp3-running-text kbd,.jupyter-wrapper .bp3-key{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background:#fff;min-width:24px;height:24px;padding:3px 6px;vertical-align:middle;line-height:24px;color:#5c7080;font-family:inherit;font-size:12px}.jupyter-wrapper .bp3-running-text kbd .bp3-icon,.jupyter-wrapper .bp3-key .bp3-icon,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-standard,.jupyter-wrapper .bp3-key .bp3-icon-standard,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-large,.jupyter-wrapper .bp3-key .bp3-icon-large{margin-right:5px}.jupyter-wrapper .bp3-dark .bp3-running-text kbd,.jupyter-wrapper .bp3-running-text .bp3-dark kbd,.jupyter-wrapper .bp3-dark .bp3-key{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);background:#394b59;color:#a7b6c2}.jupyter-wrapper .bp3-running-text blockquote,.jupyter-wrapper .bp3-blockquote{margin:0 0 10px;border-left:solid 4px rgba(167,182,194,.5);padding:0 20px}.jupyter-wrapper .bp3-dark .bp3-running-text blockquote,.jupyter-wrapper .bp3-running-text .bp3-dark blockquote,.jupyter-wrapper .bp3-dark .bp3-blockquote{border-color:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-running-text ul,.jupyter-wrapper .bp3-running-text ol,.jupyter-wrapper .bp3-list{margin:10px 0;padding-left:30px}.jupyter-wrapper .bp3-running-text ul li:not(:last-child),.jupyter-wrapper .bp3-running-text ol li:not(:last-child),.jupyter-wrapper .bp3-list li:not(:last-child){margin-bottom:5px}.jupyter-wrapper .bp3-running-text ul ol,.jupyter-wrapper .bp3-running-text ol ol,.jupyter-wrapper .bp3-list ol,.jupyter-wrapper .bp3-running-text ul ul,.jupyter-wrapper .bp3-running-text ol ul,.jupyter-wrapper .bp3-list ul{margin-top:5px}.jupyter-wrapper .bp3-list-unstyled{margin:0;padding:0;list-style:none}.jupyter-wrapper .bp3-list-unstyled li{padding:0}.jupyter-wrapper .bp3-rtl{text-align:right}.jupyter-wrapper .bp3-dark{color:#f5f8fa}.jupyter-wrapper :focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-focus-disabled :focus{outline:none !important}.jupyter-wrapper .bp3-focus-disabled :focus~.bp3-control-indicator{outline:none !important}.jupyter-wrapper .bp3-alert{max-width:400px;padding:20px}.jupyter-wrapper .bp3-alert-body{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-alert-body .bp3-icon{margin-top:0;margin-right:20px;font-size:40px}.jupyter-wrapper .bp3-alert-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;margin-top:10px}.jupyter-wrapper .bp3-alert-footer .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-breadcrumbs{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;cursor:default;height:30px;padding:0;list-style:none}.jupyter-wrapper .bp3-breadcrumbs>li{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-breadcrumbs>li::after{display:block;margin:0 5px;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\");width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs>li:last-of-type::after{display:none}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumb-current,.jupyter-wrapper .bp3-breadcrumbs-collapsed{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumbs-collapsed{color:#5c7080}.jupyter-wrapper .bp3-breadcrumb:hover{text-decoration:none}.jupyter-wrapper .bp3-breadcrumb.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-breadcrumb .bp3-icon{margin-right:5px}.jupyter-wrapper .bp3-breadcrumb-current{color:inherit;font-weight:600}.jupyter-wrapper .bp3-breadcrumb-current .bp3-input{vertical-align:baseline;font-size:inherit;font-weight:inherit}.jupyter-wrapper .bp3-breadcrumbs-collapsed{margin-right:2px;border:none;border-radius:3px;background:#ced9e0;cursor:pointer;padding:1px 5px;vertical-align:text-bottom}.jupyter-wrapper .bp3-breadcrumbs-collapsed::before{display:block;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat;width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs-collapsed:hover{background:#bfccd6;text-decoration:none;color:#182026}.jupyter-wrapper .bp3-dark .bp3-breadcrumb,.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs>li::after{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumb.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-breadcrumb-current{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed:hover{background:rgba(16,22,26,.6);color:#f5f8fa}.jupyter-wrapper .bp3-button{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;min-width:30px;min-height:30px}.jupyter-wrapper .bp3-button>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-button>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-button::before,.jupyter-wrapper .bp3-button>*{margin-right:7px}.jupyter-wrapper .bp3-button:empty::before,.jupyter-wrapper .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button:empty{padding:0 !important}.jupyter-wrapper .bp3-button:disabled,.jupyter-wrapper .bp3-button.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-button.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button.bp3-align-right,.jupyter-wrapper .bp3-align-right .bp3-button{text-align:right}.jupyter-wrapper .bp3-button.bp3-align-left,.jupyter-wrapper .bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active:hover,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-button.bp3-intent-primary{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-success{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0f9960;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0d8050}.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0a6640;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(15,153,96,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-warning{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#d9822b;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#bf7326}.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a66321;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(217,130,43,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-danger{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#db3737;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#c23030}.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a82a2a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-disabled{border-color:rgba(0,0,0,0);-webkit-box-shadow:none;box-shadow:none;background-color:rgba(219,55,55,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#fff}.jupyter-wrapper .bp3-button.bp3-large,.jupyter-wrapper .bp3-large .bp3-button{min-width:40px;min-height:40px;padding:5px 15px;font-size:16px}.jupyter-wrapper .bp3-button.bp3-large::before,.jupyter-wrapper .bp3-button.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-button::before,.jupyter-wrapper .bp3-large .bp3-button>*{margin-right:10px}.jupyter-wrapper .bp3-button.bp3-large:empty::before,.jupyter-wrapper .bp3-button.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-button:empty::before,.jupyter-wrapper .bp3-large .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button.bp3-small,.jupyter-wrapper .bp3-small .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-button.bp3-loading{position:relative}.jupyter-wrapper .bp3-button.bp3-loading[class*=bp3-icon-]::before{visibility:hidden}.jupyter-wrapper .bp3-button.bp3-loading .bp3-button-spinner{position:absolute;margin:0}.jupyter-wrapper .bp3-button.bp3-loading>:not(.bp3-button-spinner){visibility:hidden}.jupyter-wrapper .bp3-button[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon,.jupyter-wrapper .bp3-button .bp3-icon-standard,.jupyter-wrapper .bp3-button .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-standard.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-large.bp3-align-right{margin-left:7px}.jupyter-wrapper .bp3-button .bp3-icon:first-child:last-child,.jupyter-wrapper .bp3-button .bp3-spinner+.bp3-icon:last-child{margin:0 -7px}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-])[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-image:none;color:rgba(255,255,255,.3)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-button:disabled::before,.jupyter-wrapper .bp3-button:disabled .bp3-icon,.jupyter-wrapper .bp3-button:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button:disabled .bp3-icon-large,.jupyter-wrapper .bp3-button.bp3-disabled::before,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-large,.jupyter-wrapper .bp3-button[class*=bp3-intent-]::before,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-standard,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-large{color:inherit !important}.jupyter-wrapper .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button.bp3-minimal:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper a.bp3-button{text-align:center;text-decoration:none;-webkit-transition:none;transition:none}.jupyter-wrapper a.bp3-button,.jupyter-wrapper a.bp3-button:hover,.jupyter-wrapper a.bp3-button:active{color:#182026}.jupyter-wrapper a.bp3-button.bp3-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-text{-webkit-box-flex:0;-ms-flex:0 1 auto;flex:0 1 auto}.jupyter-wrapper .bp3-button.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button.bp3-align-right .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-right .bp3-button-text{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.jupyter-wrapper .bp3-button-group .bp3-button{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;z-index:4}.jupyter-wrapper .bp3-button-group .bp3-button:focus{z-index:5}.jupyter-wrapper .bp3-button-group .bp3-button:hover{z-index:6}.jupyter-wrapper .bp3-button-group .bp3-button:active,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-active{z-index:7}.jupyter-wrapper .bp3-button-group .bp3-button:disabled,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]{z-index:9}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:focus{z-index:10}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:hover{z-index:11}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-active{z-index:12}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:first-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:-1px;border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-button-group .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button-group .bp3-button.bp3-fill,.jupyter-wrapper .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;vertical-align:top}.jupyter-wrapper .bp3-button-group.bp3-vertical.bp3-fill{width:unset;height:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical .bp3-button{margin-right:0 !important;width:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:first-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:first-child{border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:last-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-bottom:-1px}.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:1px}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-button:not(:last-child){margin-bottom:1px}.jupyter-wrapper .bp3-callout{line-height:1.5;font-size:14px;position:relative;border-radius:3px;background-color:rgba(138,155,168,.15);width:100%;padding:10px 12px 9px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]{padding-left:40px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout.bp3-callout-icon{padding-left:40px}.jupyter-wrapper .bp3-callout.bp3-callout-icon>.bp3-icon:first-child{position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout .bp3-heading{margin-top:0;margin-bottom:5px;line-height:20px}.jupyter-wrapper .bp3-callout .bp3-heading:last-child{margin-bottom:0}.jupyter-wrapper .bp3-dark .bp3-callout{background-color:rgba(138,155,168,.2)}.jupyter-wrapper .bp3-dark .bp3-callout[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-primary .bp3-heading{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{color:#48aff0}.jupyter-wrapper .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-success .bp3-heading{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{color:#3dcc91}.jupyter-wrapper .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-warning .bp3-heading{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{color:#ffb366}.jupyter-wrapper .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-danger .bp3-heading{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{color:#ff7373}.jupyter-wrapper .bp3-running-text .bp3-callout{margin:20px 0}.jupyter-wrapper .bp3-card{border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#fff;padding:20px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-card.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#30404d}.jupyter-wrapper .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-0.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-1.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-2.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-3.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-4.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);cursor:pointer}.jupyter-wrapper .bp3-card.bp3-interactive:hover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:active{opacity:.9;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);-webkit-transition-duration:0;transition-duration:0}.jupyter-wrapper .bp3-card.bp3-interactive:active.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-collapse{height:0;overflow-y:hidden;-webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body{-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-context-menu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-context-menu-popover-target{position:fixed}.jupyter-wrapper .bp3-divider{margin:5px;border-right:1px solid rgba(16,22,26,.15);border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-divider{border-color:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dialog-container{opacity:1;-webkit-transform:scale(1);transform:scale(1);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;min-height:100%;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter-active>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear-active>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit-active>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:30px 0;border-radius:6px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#ebf1f5;width:500px;padding-bottom:20px;pointer-events:all;-webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text}.jupyter-wrapper .bp3-dialog:focus{outline:0}.jupyter-wrapper .bp3-dialog.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-dialog{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#293742;color:#f5f8fa}.jupyter-wrapper .bp3-dialog-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:6px 6px 0 0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);background:#fff;min-height:40px;padding-right:5px;padding-left:20px}.jupyter-wrapper .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dialog-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-dialog-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-dialog-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-dialog-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4);background:#30404d}.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dialog-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:20px;line-height:18px}.jupyter-wrapper .bp3-dialog-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin:0 20px}.jupyter-wrapper .bp3-dialog-footer-actions{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end}.jupyter-wrapper .bp3-dialog-footer-actions .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-drawer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#fff;padding:0}.jupyter-wrapper .bp3-drawer:focus{outline:0}.jupyter-wrapper .bp3-drawer.bp3-position-top{top:0;right:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{-webkit-transform:translateY(-100%);transform:translateY(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left{top:0;bottom:0;left:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear{-webkit-transform:translateX(-100%);transform:translateX(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{-webkit-transform:translateX(-100%);transform:translateX(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right{top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical){top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-drawer{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-drawer-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border-radius:0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);min-height:40px;padding:5px;padding-left:20px}.jupyter-wrapper .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-drawer-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-drawer-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-drawer-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-drawer-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-drawer-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;overflow:auto;line-height:18px}.jupyter-wrapper .bp3-drawer-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);padding:10px 20px}.jupyter-wrapper .bp3-dark .bp3-drawer-footer{-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.4);box-shadow:inset 0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text{display:inline-block;position:relative;cursor:text;max-width:100%;vertical-align:top;white-space:nowrap}.jupyter-wrapper .bp3-editable-text::before{position:absolute;top:-3px;right:-3px;bottom:-3px;left:-3px;border-radius:3px;content:\"\";-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#137cbd}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#0f9960}.jupyter-wrapper .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#d9822b}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#db3737}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4);box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4);box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4);box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4);box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text-content{display:inherit;position:relative;min-width:inherit;max-width:inherit;vertical-align:top;text-transform:inherit;letter-spacing:inherit;color:inherit;font:inherit;resize:none}.jupyter-wrapper .bp3-editable-text-input{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;width:100%;padding:0;white-space:pre-wrap}.jupyter-wrapper .bp3-editable-text-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:focus{outline:none}.jupyter-wrapper .bp3-editable-text-input::-ms-clear{display:none}.jupyter-wrapper .bp3-editable-text-content{overflow:hidden;padding-right:2px;text-overflow:ellipsis;white-space:pre}.jupyter-wrapper .bp3-editable-text-editing>.bp3-editable-text-content{position:absolute;left:0;visibility:hidden}.jupyter-wrapper .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-editable-text.bp3-multiline{display:block}.jupyter-wrapper .bp3-editable-text.bp3-multiline .bp3-editable-text-content{overflow:auto;white-space:pre-wrap;word-wrap:break-word}.jupyter-wrapper .bp3-control-group{-webkit-transform:translateZ(0);transform:translateZ(0);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-control-group>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select,.jupyter-wrapper .bp3-control-group .bp3-input,.jupyter-wrapper .bp3-control-group .bp3-select{position:relative}.jupyter-wrapper .bp3-control-group .bp3-input{z-index:2;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-input:focus{z-index:14;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-input[readonly],.jupyter-wrapper .bp3-control-group .bp3-input:disabled,.jupyter-wrapper .bp3-control-group .bp3-input.bp3-disabled{z-index:1}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select select,.jupyter-wrapper .bp3-control-group .bp3-select select{-webkit-transform:translateZ(0);transform:translateZ(0);z-index:4;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-button:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select:focus,.jupyter-wrapper .bp3-control-group .bp3-select select:focus{z-index:5}.jupyter-wrapper .bp3-control-group .bp3-button:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select:hover,.jupyter-wrapper .bp3-control-group .bp3-select select:hover{z-index:6}.jupyter-wrapper .bp3-control-group .bp3-button:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select:active,.jupyter-wrapper .bp3-control-group .bp3-select select:active{z-index:7}.jupyter-wrapper .bp3-control-group .bp3-button[readonly],.jupyter-wrapper .bp3-control-group .bp3-button:disabled,.jupyter-wrapper .bp3-control-group .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]{z-index:9}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:focus{z-index:10}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:hover{z-index:11}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:active{z-index:12}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-input-action{z-index:16}.jupyter-wrapper .bp3-control-group .bp3-select::after,.jupyter-wrapper .bp3-control-group .bp3-html-select::after,.jupyter-wrapper .bp3-control-group .bp3-select>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-html-select>.bp3-icon{z-index:17}.jupyter-wrapper .bp3-control-group:not(.bp3-vertical)>*{margin-right:-1px}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>*{margin-right:0}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>.bp3-button+.bp3-button{margin-left:1px}.jupyter-wrapper .bp3-control-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-control-group .bp3-popover-target{border-radius:inherit}.jupyter-wrapper .bp3-control-group>:first-child{border-radius:3px 0 0 3px}.jupyter-wrapper .bp3-control-group>:last-child{margin-right:0;border-radius:0 3px 3px 0}.jupyter-wrapper .bp3-control-group>:only-child{margin-right:0;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input-group .bp3-button{border-radius:3px}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-fill>*:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.jupyter-wrapper .bp3-control-group.bp3-vertical>*{margin-top:-1px}.jupyter-wrapper .bp3-control-group.bp3-vertical>:first-child{margin-top:0;border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-control-group.bp3-vertical>:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-control{display:block;position:relative;margin-bottom:10px;cursor:pointer;text-transform:none}.jupyter-wrapper .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control:not(.bp3-align-right){padding-left:26px}.jupyter-wrapper .bp3-control:not(.bp3-align-right) .bp3-control-indicator{margin-left:-26px}.jupyter-wrapper .bp3-control.bp3-align-right{padding-right:26px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{margin-right:-26px}.jupyter-wrapper .bp3-control.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-control.bp3-inline{display:inline-block;margin-right:20px}.jupyter-wrapper .bp3-control input{position:absolute;top:0;left:0;opacity:0;z-index:-1}.jupyter-wrapper .bp3-control .bp3-control-indicator{display:inline-block;position:relative;margin-top:-3px;margin-right:10px;border:none;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));cursor:pointer;width:1em;height:1em;vertical-align:middle;font-size:16px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-control .bp3-control-indicator::before{display:block;width:1em;height:1em;content:\"\"}.jupyter-wrapper .bp3-control:hover .bp3-control-indicator{background-color:#ebf1f5}.jupyter-wrapper .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background:#d8e1e8}.jupyter-wrapper .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed}.jupyter-wrapper .bp3-control input:focus~.bp3-control-indicator{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{float:right;margin-top:1px;margin-left:10px}.jupyter-wrapper .bp3-control.bp3-large{font-size:16px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right){padding-left:30px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right{padding-right:30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-30px}.jupyter-wrapper .bp3-control.bp3-large .bp3-control-indicator{font-size:20px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-top:0}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control.bp3-checkbox .bp3-control-indicator{border-radius:3px}.jupyter-wrapper .bp3-control.bp3-checkbox input:checked~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-radio .bp3-control-indicator{border-radius:50%}.jupyter-wrapper .bp3-control.bp3-radio input:checked~.bp3-control-indicator::before{background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%)}.jupyter-wrapper .bp3-control.bp3-radio input:checked:disabled~.bp3-control-indicator::before{opacity:.5}.jupyter-wrapper .bp3-control.bp3-radio input:focus~.bp3-control-indicator{-moz-outline-radius:16px}.jupyter-wrapper .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(167,182,194,.5)}.jupyter-wrapper .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(92,112,128,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(206,217,224,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right){padding-left:38px}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{margin-left:-38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right{padding-right:38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{margin-right:-38px}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator{border:none;border-radius:1.75em;-webkit-box-shadow:none !important;box-shadow:none !important;width:auto;min-width:1.75em;-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator::before{position:absolute;left:0;margin:2px;border-radius:50%;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);background:#fff;width:calc(1em - 4px);height:calc(1em - 4px);-webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{left:calc(100% - 1em)}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){padding-left:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right{padding-right:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-45px}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(16,22,26,.7)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(16,22,26,.9)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(57,75,89,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background:#394b59}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-switch-inner-text{text-align:center;font-size:.7em}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{visibility:hidden;margin-right:1.2em;margin-left:.5em;line-height:0}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{visibility:visible;margin-right:.5em;margin-left:1.2em;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:first-child{visibility:visible;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:last-child{visibility:hidden;line-height:0}.jupyter-wrapper .bp3-dark .bp3-control{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-control.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-control .bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0))}.jupyter-wrapper .bp3-dark .bp3-control:hover .bp3-control-indicator{background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background:#202b33}.jupyter-wrapper .bp3-dark .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);cursor:not-allowed}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked~.bp3-control-indicator,.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-file-input{display:inline-block;position:relative;cursor:pointer;height:30px}.jupyter-wrapper .bp3-file-input input{opacity:0;margin:0;min-width:200px}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active:hover,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#182026}.jupyter-wrapper .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#f5f8fa}.jupyter-wrapper .bp3-file-input.bp3-fill{width:100%}.jupyter-wrapper .bp3-file-input.bp3-large,.jupyter-wrapper .bp3-large .bp3-file-input{height:40px}.jupyter-wrapper .bp3-file-input .bp3-file-upload-input-custom-text::after{content:attr(bp3-button-text)}.jupyter-wrapper .bp3-file-upload-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;left:0;padding-right:80px;color:rgba(92,112,128,.6);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-file-upload-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:focus,.jupyter-wrapper .bp3-file-upload-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-file-upload-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;min-width:24px;min-height:24px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;margin:3px;border-radius:3px;width:70px;text-align:center;line-height:24px;content:\"Browse\"}.jupyter-wrapper .bp3-file-upload-input::after:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active:hover,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-file-upload-input:hover::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input:active::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-large .bp3-file-upload-input{height:40px;line-height:40px;font-size:16px;padding-right:95px}.jupyter-wrapper .bp3-large .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-large .bp3-file-upload-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-large .bp3-file-upload-input::after{min-width:30px;min-height:30px;margin:5px;width:85px;line-height:30px}.jupyter-wrapper .bp3-dark .bp3-file-upload-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:hover::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:active::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1)}.jupyter-wrapper .bp3-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0 0 15px}.jupyter-wrapper .bp3-form-group label.bp3-label{margin-bottom:5px}.jupyter-wrapper .bp3-form-group .bp3-control{margin-top:7px}.jupyter-wrapper .bp3-form-group .bp3-form-helper-text{margin-top:5px;color:#5c7080;font-size:12px}.jupyter-wrapper .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#106ba3}.jupyter-wrapper .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#0d8050}.jupyter-wrapper .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#bf7326}.jupyter-wrapper .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#c23030}.jupyter-wrapper .bp3-form-group.bp3-inline{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-form-group.bp3-inline.bp3-large label.bp3-label{margin:0 10px 0 0;line-height:40px}.jupyter-wrapper .bp3-form-group.bp3-inline label.bp3-label{margin:0 10px 0 0;line-height:30px}.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-form-group .bp3-form-helper-text{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-input-group{display:block;position:relative}.jupyter-wrapper .bp3-input-group .bp3-input{position:relative;width:100%}.jupyter-wrapper .bp3-input-group .bp3-input:not(:first-child){padding-left:30px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:last-child){padding-right:30px}.jupyter-wrapper .bp3-input-group .bp3-input-action,.jupyter-wrapper .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-input-group>.bp3-icon{position:absolute;top:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:first-child,.jupyter-wrapper .bp3-input-group>.bp3-button:first-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:first-child{left:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:last-child,.jupyter-wrapper .bp3-input-group>.bp3-button:last-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:last-child{right:0}.jupyter-wrapper .bp3-input-group .bp3-button{min-width:24px;min-height:24px;margin:3px;padding:0 7px}.jupyter-wrapper .bp3-input-group .bp3-button:empty{padding:0}.jupyter-wrapper .bp3-input-group>.bp3-icon{z-index:1;color:#5c7080}.jupyter-wrapper .bp3-input-group>.bp3-icon:empty{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input-action>.bp3-spinner{margin:7px}.jupyter-wrapper .bp3-input-group .bp3-tag{margin:5px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#a7b6c2}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-button{min-width:30px;min-height:30px;margin:5px}.jupyter-wrapper .bp3-input-group.bp3-large>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input-action>.bp3-spinner{margin:12px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:first-child){padding-left:40px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:last-child){padding-right:40px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-button{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-tag{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input-action>.bp3-spinner{margin:4px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:first-child){padding-left:24px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:last-child){padding-right:24px}.jupyter-wrapper .bp3-input-group.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-input-group.bp3-round .bp3-button,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-input,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-tag{border-radius:30px}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#48aff0}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-success>.bp3-icon{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-success>.bp3-icon{color:#3dcc91}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#ffb366}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#ff7373}.jupyter-wrapper .bp3-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none}.jupyter-wrapper .bp3-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:focus,.jupyter-wrapper .bp3-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input[type=search],.jupyter-wrapper .bp3-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-input:disabled,.jupyter-wrapper .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-input.bp3-large{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input.bp3-large[type=search],.jupyter-wrapper .bp3-input.bp3-large.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input.bp3-small{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input.bp3-small[type=search],.jupyter-wrapper .bp3-input.bp3-small.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-dark .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input::-ms-clear{display:none}.jupyter-wrapper textarea.bp3-input{max-width:100%;padding:10px}.jupyter-wrapper textarea.bp3-input,.jupyter-wrapper textarea.bp3-input.bp3-large,.jupyter-wrapper textarea.bp3-input.bp3-small{height:auto;line-height:inherit}.jupyter-wrapper textarea.bp3-input.bp3-small{padding:8px}.jupyter-wrapper .bp3-dark textarea.bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark textarea.bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input:disabled,.jupyter-wrapper .bp3-dark textarea.bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper label.bp3-label{display:block;margin-top:0;margin-bottom:15px}.jupyter-wrapper label.bp3-label .bp3-html-select,.jupyter-wrapper label.bp3-label .bp3-input,.jupyter-wrapper label.bp3-label .bp3-select,.jupyter-wrapper label.bp3-label .bp3-slider,.jupyter-wrapper label.bp3-label .bp3-popover-wrapper{display:block;margin-top:5px;text-transform:none}.jupyter-wrapper label.bp3-label .bp3-button-group{margin-top:5px}.jupyter-wrapper label.bp3-label .bp3-select select,.jupyter-wrapper label.bp3-label .bp3-html-select select{width:100%;vertical-align:top;font-weight:400}.jupyter-wrapper label.bp3-label.bp3-disabled,.jupyter-wrapper label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(92,112,128,.6)}.jupyter-wrapper label.bp3-label.bp3-inline{line-height:30px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-html-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-popover-wrapper{display:inline-block;margin:0 0 0 5px;vertical-align:top}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-button-group{margin:0 0 0 5px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group .bp3-input{margin-left:0}.jupyter-wrapper label.bp3-label.bp3-inline.bp3-large{line-height:40px}.jupyter-wrapper label.bp3-label:not(.bp3-inline) .bp3-popover-target{display:block}.jupyter-wrapper .bp3-dark label.bp3-label{color:#f5f8fa}.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled,.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button{-webkit-box-flex:1;-ms-flex:1 1 14px;flex:1 1 14px;width:30px;min-height:0;padding:0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:first-child{border-radius:0 3px 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:last-child{border-radius:0 0 3px 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:first-child{border-radius:3px 0 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:last-child{border-radius:0 0 0 3px}.jupyter-wrapper .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical>.bp3-button{width:40px}.jupyter-wrapper form{display:block}.jupyter-wrapper .bp3-html-select select,.jupyter-wrapper .bp3-select select{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;border-radius:3px;width:100%;height:30px;padding:0 25px 0 10px;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-html-select select>.bp3-fill,.jupyter-wrapper .bp3-select select>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-html-select select::before,.jupyter-wrapper .bp3-select select::before,.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{margin-right:7px}.jupyter-wrapper .bp3-html-select select:empty::before,.jupyter-wrapper .bp3-select select:empty::before,.jupyter-wrapper .bp3-html-select select>:last-child,.jupyter-wrapper .bp3-select select>:last-child{margin-right:0}.jupyter-wrapper .bp3-html-select select:hover,.jupyter-wrapper .bp3-select select:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-html-select select:active,.jupyter-wrapper .bp3-select select:active,.jupyter-wrapper .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-select select.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled,.jupyter-wrapper .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-select select.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal select{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-large select,.jupyter-wrapper .bp3-select.bp3-large select{height:40px;padding-right:35px;font-size:16px}.jupyter-wrapper .bp3-dark .bp3-html-select select,.jupyter-wrapper .bp3-dark .bp3-select select{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon,.jupyter-wrapper .bp3-select::after{position:absolute;top:7px;right:7px;color:#5c7080;pointer-events:none}.jupyter-wrapper .bp3-html-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-disabled.bp3-select::after{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select,.jupyter-wrapper .bp3-select{display:inline-block;position:relative;vertical-align:middle;letter-spacing:normal}.jupyter-wrapper .bp3-html-select select::-ms-expand,.jupyter-wrapper .bp3-select select::-ms-expand{display:none}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon{color:#5c7080}.jupyter-wrapper .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-select .bp3-icon:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon:hover{color:#f5f8fa}.jupyter-wrapper .bp3-html-select.bp3-large::after,.jupyter-wrapper .bp3-html-select.bp3-large .bp3-icon,.jupyter-wrapper .bp3-select.bp3-large::after,.jupyter-wrapper .bp3-select.bp3-large .bp3-icon{top:12px;right:12px}.jupyter-wrapper .bp3-html-select.bp3-fill,.jupyter-wrapper .bp3-html-select.bp3-fill select,.jupyter-wrapper .bp3-select.bp3-fill,.jupyter-wrapper .bp3-select.bp3-fill select{width:100%}.jupyter-wrapper .bp3-dark .bp3-html-select option,.jupyter-wrapper .bp3-dark .bp3-select option{background-color:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select::after,.jupyter-wrapper .bp3-dark .bp3-select::after{color:#a7b6c2}.jupyter-wrapper .bp3-select::after{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6c6\"}.jupyter-wrapper .bp3-running-text table,.jupyter-wrapper table.bp3-html-table{border-spacing:0;font-size:14px}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th,.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{padding:11px;vertical-align:top;text-align:left}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th{color:#182026;font-weight:600}.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{color:#182026}.jupyter-wrapper .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text table th,.jupyter-wrapper .bp3-running-text .bp3-dark table th,.jupyter-wrapper .bp3-dark table.bp3-html-table th{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table td,.jupyter-wrapper .bp3-running-text .bp3-dark table td,.jupyter-wrapper .bp3-dark table.bp3-html-table td{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child th,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child td,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed th,.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed td,.jupyter-wrapper table.bp3-html-table.bp3-small th,.jupyter-wrapper table.bp3-html-table.bp3-small td{padding-top:6px;padding-bottom:6px}.jupyter-wrapper table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(191,204,214,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(191,204,214,.3);cursor:pointer}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(92,112,128,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(92,112,128,.3);cursor:pointer}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-key-combo{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-key-combo>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-key-combo>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-key-combo::before,.jupyter-wrapper .bp3-key-combo>*{margin-right:5px}.jupyter-wrapper .bp3-key-combo:empty::before,.jupyter-wrapper .bp3-key-combo>:last-child{margin-right:0}.jupyter-wrapper .bp3-hotkey-dialog{top:40px;padding-bottom:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-dialog-body{margin:0;padding:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-hotkey-label{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.jupyter-wrapper .bp3-hotkey-column{margin:auto;max-height:80vh;overflow-y:auto;padding:30px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading{margin-bottom:20px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading:not(:first-child){margin-top:40px}.jupyter-wrapper .bp3-hotkey{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;margin-right:0;margin-left:0}.jupyter-wrapper .bp3-hotkey:not(:last-child){margin-bottom:10px}.jupyter-wrapper .bp3-icon{display:inline-block;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;vertical-align:text-bottom}.jupyter-wrapper .bp3-icon:not(:empty)::before{content:\"\" !important;content:unset !important}.jupyter-wrapper .bp3-icon>svg{display:block}.jupyter-wrapper .bp3-icon>svg:not([fill]){fill:currentColor}.jupyter-wrapper .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-icon-large.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-icon-large.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-icon-large.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-icon-large.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-danger{color:#ff7373}.jupyter-wrapper span.bp3-icon-standard{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon-large{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon:empty{line-height:1;font-family:\"Icons20\";font-size:inherit;font-weight:400;font-style:normal}.jupyter-wrapper span.bp3-icon:empty::before{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-icon-add::before{content:\"\ue63e\"}.jupyter-wrapper .bp3-icon-add-column-left::before{content:\"\ue6f9\"}.jupyter-wrapper .bp3-icon-add-column-right::before{content:\"\ue6fa\"}.jupyter-wrapper .bp3-icon-add-row-bottom::before{content:\"\ue6f8\"}.jupyter-wrapper .bp3-icon-add-row-top::before{content:\"\ue6f7\"}.jupyter-wrapper .bp3-icon-add-to-artifact::before{content:\"\ue67c\"}.jupyter-wrapper .bp3-icon-add-to-folder::before{content:\"\ue6d2\"}.jupyter-wrapper .bp3-icon-airplane::before{content:\"\ue74b\"}.jupyter-wrapper .bp3-icon-align-center::before{content:\"\ue603\"}.jupyter-wrapper .bp3-icon-align-justify::before{content:\"\ue605\"}.jupyter-wrapper .bp3-icon-align-left::before{content:\"\ue602\"}.jupyter-wrapper .bp3-icon-align-right::before{content:\"\ue604\"}.jupyter-wrapper .bp3-icon-alignment-bottom::before{content:\"\ue727\"}.jupyter-wrapper .bp3-icon-alignment-horizontal-center::before{content:\"\ue726\"}.jupyter-wrapper .bp3-icon-alignment-left::before{content:\"\ue722\"}.jupyter-wrapper .bp3-icon-alignment-right::before{content:\"\ue724\"}.jupyter-wrapper .bp3-icon-alignment-top::before{content:\"\ue725\"}.jupyter-wrapper .bp3-icon-alignment-vertical-center::before{content:\"\ue723\"}.jupyter-wrapper .bp3-icon-annotation::before{content:\"\ue6f0\"}.jupyter-wrapper .bp3-icon-application::before{content:\"\ue735\"}.jupyter-wrapper .bp3-icon-applications::before{content:\"\ue621\"}.jupyter-wrapper .bp3-icon-archive::before{content:\"\ue907\"}.jupyter-wrapper .bp3-icon-arrow-bottom-left::before{content:\"\u2199\"}.jupyter-wrapper .bp3-icon-arrow-bottom-right::before{content:\"\u2198\"}.jupyter-wrapper .bp3-icon-arrow-down::before{content:\"\u2193\"}.jupyter-wrapper .bp3-icon-arrow-left::before{content:\"\u2190\"}.jupyter-wrapper .bp3-icon-arrow-right::before{content:\"\u2192\"}.jupyter-wrapper .bp3-icon-arrow-top-left::before{content:\"\u2196\"}.jupyter-wrapper .bp3-icon-arrow-top-right::before{content:\"\u2197\"}.jupyter-wrapper .bp3-icon-arrow-up::before{content:\"\u2191\"}.jupyter-wrapper .bp3-icon-arrows-horizontal::before{content:\"\u2194\"}.jupyter-wrapper .bp3-icon-arrows-vertical::before{content:\"\u2195\"}.jupyter-wrapper .bp3-icon-asterisk::before{content:\"*\"}.jupyter-wrapper .bp3-icon-automatic-updates::before{content:\"\ue65f\"}.jupyter-wrapper .bp3-icon-badge::before{content:\"\ue6e3\"}.jupyter-wrapper .bp3-icon-ban-circle::before{content:\"\ue69d\"}.jupyter-wrapper .bp3-icon-bank-account::before{content:\"\ue76f\"}.jupyter-wrapper .bp3-icon-barcode::before{content:\"\ue676\"}.jupyter-wrapper .bp3-icon-blank::before{content:\"\ue900\"}.jupyter-wrapper .bp3-icon-blocked-person::before{content:\"\ue768\"}.jupyter-wrapper .bp3-icon-bold::before{content:\"\ue606\"}.jupyter-wrapper .bp3-icon-book::before{content:\"\ue6b8\"}.jupyter-wrapper .bp3-icon-bookmark::before{content:\"\ue61a\"}.jupyter-wrapper .bp3-icon-box::before{content:\"\ue6bf\"}.jupyter-wrapper .bp3-icon-briefcase::before{content:\"\ue674\"}.jupyter-wrapper .bp3-icon-bring-data::before{content:\"\ue90a\"}.jupyter-wrapper .bp3-icon-build::before{content:\"\ue72d\"}.jupyter-wrapper .bp3-icon-calculator::before{content:\"\ue70b\"}.jupyter-wrapper .bp3-icon-calendar::before{content:\"\ue62b\"}.jupyter-wrapper .bp3-icon-camera::before{content:\"\ue69e\"}.jupyter-wrapper .bp3-icon-caret-down::before{content:\"\u2304\"}.jupyter-wrapper .bp3-icon-caret-left::before{content:\"\u2329\"}.jupyter-wrapper .bp3-icon-caret-right::before{content:\"\u232a\"}.jupyter-wrapper .bp3-icon-caret-up::before{content:\"\u2303\"}.jupyter-wrapper .bp3-icon-cell-tower::before{content:\"\ue770\"}.jupyter-wrapper .bp3-icon-changes::before{content:\"\ue623\"}.jupyter-wrapper .bp3-icon-chart::before{content:\"\ue67e\"}.jupyter-wrapper .bp3-icon-chat::before{content:\"\ue689\"}.jupyter-wrapper .bp3-icon-chevron-backward::before{content:\"\ue6df\"}.jupyter-wrapper .bp3-icon-chevron-down::before{content:\"\ue697\"}.jupyter-wrapper .bp3-icon-chevron-forward::before{content:\"\ue6e0\"}.jupyter-wrapper .bp3-icon-chevron-left::before{content:\"\ue694\"}.jupyter-wrapper .bp3-icon-chevron-right::before{content:\"\ue695\"}.jupyter-wrapper .bp3-icon-chevron-up::before{content:\"\ue696\"}.jupyter-wrapper .bp3-icon-circle::before{content:\"\ue66a\"}.jupyter-wrapper .bp3-icon-circle-arrow-down::before{content:\"\ue68e\"}.jupyter-wrapper .bp3-icon-circle-arrow-left::before{content:\"\ue68c\"}.jupyter-wrapper .bp3-icon-circle-arrow-right::before{content:\"\ue68b\"}.jupyter-wrapper .bp3-icon-circle-arrow-up::before{content:\"\ue68d\"}.jupyter-wrapper .bp3-icon-citation::before{content:\"\ue61b\"}.jupyter-wrapper .bp3-icon-clean::before{content:\"\ue7c5\"}.jupyter-wrapper .bp3-icon-clipboard::before{content:\"\ue61d\"}.jupyter-wrapper .bp3-icon-cloud::before{content:\"\u2601\"}.jupyter-wrapper .bp3-icon-cloud-download::before{content:\"\ue690\"}.jupyter-wrapper .bp3-icon-cloud-upload::before{content:\"\ue691\"}.jupyter-wrapper .bp3-icon-code::before{content:\"\ue661\"}.jupyter-wrapper .bp3-icon-code-block::before{content:\"\ue6c5\"}.jupyter-wrapper .bp3-icon-cog::before{content:\"\ue645\"}.jupyter-wrapper .bp3-icon-collapse-all::before{content:\"\ue763\"}.jupyter-wrapper .bp3-icon-column-layout::before{content:\"\ue6da\"}.jupyter-wrapper .bp3-icon-comment::before{content:\"\ue68a\"}.jupyter-wrapper .bp3-icon-comparison::before{content:\"\ue637\"}.jupyter-wrapper .bp3-icon-compass::before{content:\"\ue79c\"}.jupyter-wrapper .bp3-icon-compressed::before{content:\"\ue6c0\"}.jupyter-wrapper .bp3-icon-confirm::before{content:\"\ue639\"}.jupyter-wrapper .bp3-icon-console::before{content:\"\ue79b\"}.jupyter-wrapper .bp3-icon-contrast::before{content:\"\ue6cb\"}.jupyter-wrapper .bp3-icon-control::before{content:\"\ue67f\"}.jupyter-wrapper .bp3-icon-credit-card::before{content:\"\ue649\"}.jupyter-wrapper .bp3-icon-cross::before{content:\"\u2717\"}.jupyter-wrapper .bp3-icon-crown::before{content:\"\ue7b4\"}.jupyter-wrapper .bp3-icon-cube::before{content:\"\ue7c8\"}.jupyter-wrapper .bp3-icon-cube-add::before{content:\"\ue7c9\"}.jupyter-wrapper .bp3-icon-cube-remove::before{content:\"\ue7d0\"}.jupyter-wrapper .bp3-icon-curved-range-chart::before{content:\"\ue71b\"}.jupyter-wrapper .bp3-icon-cut::before{content:\"\ue6ef\"}.jupyter-wrapper .bp3-icon-dashboard::before{content:\"\ue751\"}.jupyter-wrapper .bp3-icon-data-lineage::before{content:\"\ue908\"}.jupyter-wrapper .bp3-icon-database::before{content:\"\ue683\"}.jupyter-wrapper .bp3-icon-delete::before{content:\"\ue644\"}.jupyter-wrapper .bp3-icon-delta::before{content:\"\u0394\"}.jupyter-wrapper .bp3-icon-derive-column::before{content:\"\ue739\"}.jupyter-wrapper .bp3-icon-desktop::before{content:\"\ue6af\"}.jupyter-wrapper .bp3-icon-diagram-tree::before{content:\"\ue7b3\"}.jupyter-wrapper .bp3-icon-direction-left::before{content:\"\ue681\"}.jupyter-wrapper .bp3-icon-direction-right::before{content:\"\ue682\"}.jupyter-wrapper .bp3-icon-disable::before{content:\"\ue600\"}.jupyter-wrapper .bp3-icon-document::before{content:\"\ue630\"}.jupyter-wrapper .bp3-icon-document-open::before{content:\"\ue71e\"}.jupyter-wrapper .bp3-icon-document-share::before{content:\"\ue71f\"}.jupyter-wrapper .bp3-icon-dollar::before{content:\"$\"}.jupyter-wrapper .bp3-icon-dot::before{content:\"\u2022\"}.jupyter-wrapper .bp3-icon-double-caret-horizontal::before{content:\"\ue6c7\"}.jupyter-wrapper .bp3-icon-double-caret-vertical::before{content:\"\ue6c6\"}.jupyter-wrapper .bp3-icon-double-chevron-down::before{content:\"\ue703\"}.jupyter-wrapper .bp3-icon-double-chevron-left::before{content:\"\ue6ff\"}.jupyter-wrapper .bp3-icon-double-chevron-right::before{content:\"\ue701\"}.jupyter-wrapper .bp3-icon-double-chevron-up::before{content:\"\ue702\"}.jupyter-wrapper .bp3-icon-doughnut-chart::before{content:\"\ue6ce\"}.jupyter-wrapper .bp3-icon-download::before{content:\"\ue62f\"}.jupyter-wrapper .bp3-icon-drag-handle-horizontal::before{content:\"\ue716\"}.jupyter-wrapper .bp3-icon-drag-handle-vertical::before{content:\"\ue715\"}.jupyter-wrapper .bp3-icon-draw::before{content:\"\ue66b\"}.jupyter-wrapper .bp3-icon-drive-time::before{content:\"\ue615\"}.jupyter-wrapper .bp3-icon-duplicate::before{content:\"\ue69c\"}.jupyter-wrapper .bp3-icon-edit::before{content:\"\u270e\"}.jupyter-wrapper .bp3-icon-eject::before{content:\"\u23cf\"}.jupyter-wrapper .bp3-icon-endorsed::before{content:\"\ue75f\"}.jupyter-wrapper .bp3-icon-envelope::before{content:\"\u2709\"}.jupyter-wrapper .bp3-icon-equals::before{content:\"\ue7d9\"}.jupyter-wrapper .bp3-icon-eraser::before{content:\"\ue773\"}.jupyter-wrapper .bp3-icon-error::before{content:\"\ue648\"}.jupyter-wrapper .bp3-icon-euro::before{content:\"\u20ac\"}.jupyter-wrapper .bp3-icon-exchange::before{content:\"\ue636\"}.jupyter-wrapper .bp3-icon-exclude-row::before{content:\"\ue6ea\"}.jupyter-wrapper .bp3-icon-expand-all::before{content:\"\ue764\"}.jupyter-wrapper .bp3-icon-export::before{content:\"\ue633\"}.jupyter-wrapper .bp3-icon-eye-off::before{content:\"\ue6cc\"}.jupyter-wrapper .bp3-icon-eye-on::before{content:\"\ue75a\"}.jupyter-wrapper .bp3-icon-eye-open::before{content:\"\ue66f\"}.jupyter-wrapper .bp3-icon-fast-backward::before{content:\"\ue6a8\"}.jupyter-wrapper .bp3-icon-fast-forward::before{content:\"\ue6ac\"}.jupyter-wrapper .bp3-icon-feed::before{content:\"\ue656\"}.jupyter-wrapper .bp3-icon-feed-subscribed::before{content:\"\ue78f\"}.jupyter-wrapper .bp3-icon-film::before{content:\"\ue6a1\"}.jupyter-wrapper .bp3-icon-filter::before{content:\"\ue638\"}.jupyter-wrapper .bp3-icon-filter-keep::before{content:\"\ue78c\"}.jupyter-wrapper .bp3-icon-filter-list::before{content:\"\ue6ee\"}.jupyter-wrapper .bp3-icon-filter-open::before{content:\"\ue7d7\"}.jupyter-wrapper .bp3-icon-filter-remove::before{content:\"\ue78d\"}.jupyter-wrapper .bp3-icon-flag::before{content:\"\u2691\"}.jupyter-wrapper .bp3-icon-flame::before{content:\"\ue7a9\"}.jupyter-wrapper .bp3-icon-flash::before{content:\"\ue6b3\"}.jupyter-wrapper .bp3-icon-floppy-disk::before{content:\"\ue6b7\"}.jupyter-wrapper .bp3-icon-flow-branch::before{content:\"\ue7c1\"}.jupyter-wrapper .bp3-icon-flow-end::before{content:\"\ue7c4\"}.jupyter-wrapper .bp3-icon-flow-linear::before{content:\"\ue7c0\"}.jupyter-wrapper .bp3-icon-flow-review::before{content:\"\ue7c2\"}.jupyter-wrapper .bp3-icon-flow-review-branch::before{content:\"\ue7c3\"}.jupyter-wrapper .bp3-icon-flows::before{content:\"\ue659\"}.jupyter-wrapper .bp3-icon-folder-close::before{content:\"\ue652\"}.jupyter-wrapper .bp3-icon-folder-new::before{content:\"\ue7b0\"}.jupyter-wrapper .bp3-icon-folder-open::before{content:\"\ue651\"}.jupyter-wrapper .bp3-icon-folder-shared::before{content:\"\ue653\"}.jupyter-wrapper .bp3-icon-folder-shared-open::before{content:\"\ue670\"}.jupyter-wrapper .bp3-icon-follower::before{content:\"\ue760\"}.jupyter-wrapper .bp3-icon-following::before{content:\"\ue761\"}.jupyter-wrapper .bp3-icon-font::before{content:\"\ue6b4\"}.jupyter-wrapper .bp3-icon-fork::before{content:\"\ue63a\"}.jupyter-wrapper .bp3-icon-form::before{content:\"\ue795\"}.jupyter-wrapper .bp3-icon-full-circle::before{content:\"\ue685\"}.jupyter-wrapper .bp3-icon-full-stacked-chart::before{content:\"\ue75e\"}.jupyter-wrapper .bp3-icon-fullscreen::before{content:\"\ue699\"}.jupyter-wrapper .bp3-icon-function::before{content:\"\ue6e5\"}.jupyter-wrapper .bp3-icon-gantt-chart::before{content:\"\ue6f4\"}.jupyter-wrapper .bp3-icon-geolocation::before{content:\"\ue640\"}.jupyter-wrapper .bp3-icon-geosearch::before{content:\"\ue613\"}.jupyter-wrapper .bp3-icon-git-branch::before{content:\"\ue72a\"}.jupyter-wrapper .bp3-icon-git-commit::before{content:\"\ue72b\"}.jupyter-wrapper .bp3-icon-git-merge::before{content:\"\ue729\"}.jupyter-wrapper .bp3-icon-git-new-branch::before{content:\"\ue749\"}.jupyter-wrapper .bp3-icon-git-pull::before{content:\"\ue728\"}.jupyter-wrapper .bp3-icon-git-push::before{content:\"\ue72c\"}.jupyter-wrapper .bp3-icon-git-repo::before{content:\"\ue748\"}.jupyter-wrapper .bp3-icon-glass::before{content:\"\ue6b1\"}.jupyter-wrapper .bp3-icon-globe::before{content:\"\ue666\"}.jupyter-wrapper .bp3-icon-globe-network::before{content:\"\ue7b5\"}.jupyter-wrapper .bp3-icon-graph::before{content:\"\ue673\"}.jupyter-wrapper .bp3-icon-graph-remove::before{content:\"\ue609\"}.jupyter-wrapper .bp3-icon-greater-than::before{content:\"\ue7e1\"}.jupyter-wrapper .bp3-icon-greater-than-or-equal-to::before{content:\"\ue7e2\"}.jupyter-wrapper .bp3-icon-grid::before{content:\"\ue6d0\"}.jupyter-wrapper .bp3-icon-grid-view::before{content:\"\ue6e4\"}.jupyter-wrapper .bp3-icon-group-objects::before{content:\"\ue60a\"}.jupyter-wrapper .bp3-icon-grouped-bar-chart::before{content:\"\ue75d\"}.jupyter-wrapper .bp3-icon-hand::before{content:\"\ue6de\"}.jupyter-wrapper .bp3-icon-hand-down::before{content:\"\ue6bb\"}.jupyter-wrapper .bp3-icon-hand-left::before{content:\"\ue6bc\"}.jupyter-wrapper .bp3-icon-hand-right::before{content:\"\ue6b9\"}.jupyter-wrapper .bp3-icon-hand-up::before{content:\"\ue6ba\"}.jupyter-wrapper .bp3-icon-header::before{content:\"\ue6b5\"}.jupyter-wrapper .bp3-icon-header-one::before{content:\"\ue793\"}.jupyter-wrapper .bp3-icon-header-two::before{content:\"\ue794\"}.jupyter-wrapper .bp3-icon-headset::before{content:\"\ue6dc\"}.jupyter-wrapper .bp3-icon-heart::before{content:\"\u2665\"}.jupyter-wrapper .bp3-icon-heart-broken::before{content:\"\ue7a2\"}.jupyter-wrapper .bp3-icon-heat-grid::before{content:\"\ue6f3\"}.jupyter-wrapper .bp3-icon-heatmap::before{content:\"\ue614\"}.jupyter-wrapper .bp3-icon-help::before{content:\"?\"}.jupyter-wrapper .bp3-icon-helper-management::before{content:\"\ue66d\"}.jupyter-wrapper .bp3-icon-highlight::before{content:\"\ue6ed\"}.jupyter-wrapper .bp3-icon-history::before{content:\"\ue64a\"}.jupyter-wrapper .bp3-icon-home::before{content:\"\u2302\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart::before{content:\"\ue70c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-asc::before{content:\"\ue75c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-desc::before{content:\"\ue71d\"}.jupyter-wrapper .bp3-icon-horizontal-distribution::before{content:\"\ue720\"}.jupyter-wrapper .bp3-icon-id-number::before{content:\"\ue771\"}.jupyter-wrapper .bp3-icon-image-rotate-left::before{content:\"\ue73a\"}.jupyter-wrapper .bp3-icon-image-rotate-right::before{content:\"\ue73b\"}.jupyter-wrapper .bp3-icon-import::before{content:\"\ue632\"}.jupyter-wrapper .bp3-icon-inbox::before{content:\"\ue629\"}.jupyter-wrapper .bp3-icon-inbox-filtered::before{content:\"\ue7d1\"}.jupyter-wrapper .bp3-icon-inbox-geo::before{content:\"\ue7d2\"}.jupyter-wrapper .bp3-icon-inbox-search::before{content:\"\ue7d3\"}.jupyter-wrapper .bp3-icon-inbox-update::before{content:\"\ue7d4\"}.jupyter-wrapper .bp3-icon-info-sign::before{content:\"\u2139\"}.jupyter-wrapper .bp3-icon-inheritance::before{content:\"\ue7d5\"}.jupyter-wrapper .bp3-icon-inner-join::before{content:\"\ue7a3\"}.jupyter-wrapper .bp3-icon-insert::before{content:\"\ue66c\"}.jupyter-wrapper .bp3-icon-intersection::before{content:\"\ue765\"}.jupyter-wrapper .bp3-icon-ip-address::before{content:\"\ue772\"}.jupyter-wrapper .bp3-icon-issue::before{content:\"\ue774\"}.jupyter-wrapper .bp3-icon-issue-closed::before{content:\"\ue776\"}.jupyter-wrapper .bp3-icon-issue-new::before{content:\"\ue775\"}.jupyter-wrapper .bp3-icon-italic::before{content:\"\ue607\"}.jupyter-wrapper .bp3-icon-join-table::before{content:\"\ue738\"}.jupyter-wrapper .bp3-icon-key::before{content:\"\ue78e\"}.jupyter-wrapper .bp3-icon-key-backspace::before{content:\"\ue707\"}.jupyter-wrapper .bp3-icon-key-command::before{content:\"\ue705\"}.jupyter-wrapper .bp3-icon-key-control::before{content:\"\ue704\"}.jupyter-wrapper .bp3-icon-key-delete::before{content:\"\ue708\"}.jupyter-wrapper .bp3-icon-key-enter::before{content:\"\ue70a\"}.jupyter-wrapper .bp3-icon-key-escape::before{content:\"\ue709\"}.jupyter-wrapper .bp3-icon-key-option::before{content:\"\ue742\"}.jupyter-wrapper .bp3-icon-key-shift::before{content:\"\ue706\"}.jupyter-wrapper .bp3-icon-key-tab::before{content:\"\ue757\"}.jupyter-wrapper .bp3-icon-known-vehicle::before{content:\"\ue73c\"}.jupyter-wrapper .bp3-icon-label::before{content:\"\ue665\"}.jupyter-wrapper .bp3-icon-layer::before{content:\"\ue6cf\"}.jupyter-wrapper .bp3-icon-layers::before{content:\"\ue618\"}.jupyter-wrapper .bp3-icon-layout::before{content:\"\ue60c\"}.jupyter-wrapper .bp3-icon-layout-auto::before{content:\"\ue60d\"}.jupyter-wrapper .bp3-icon-layout-balloon::before{content:\"\ue6d3\"}.jupyter-wrapper .bp3-icon-layout-circle::before{content:\"\ue60e\"}.jupyter-wrapper .bp3-icon-layout-grid::before{content:\"\ue610\"}.jupyter-wrapper .bp3-icon-layout-group-by::before{content:\"\ue611\"}.jupyter-wrapper .bp3-icon-layout-hierarchy::before{content:\"\ue60f\"}.jupyter-wrapper .bp3-icon-layout-linear::before{content:\"\ue6c3\"}.jupyter-wrapper .bp3-icon-layout-skew-grid::before{content:\"\ue612\"}.jupyter-wrapper .bp3-icon-layout-sorted-clusters::before{content:\"\ue6d4\"}.jupyter-wrapper .bp3-icon-learning::before{content:\"\ue904\"}.jupyter-wrapper .bp3-icon-left-join::before{content:\"\ue7a4\"}.jupyter-wrapper .bp3-icon-less-than::before{content:\"\ue7e3\"}.jupyter-wrapper .bp3-icon-less-than-or-equal-to::before{content:\"\ue7e4\"}.jupyter-wrapper .bp3-icon-lifesaver::before{content:\"\ue7c7\"}.jupyter-wrapper .bp3-icon-lightbulb::before{content:\"\ue6b0\"}.jupyter-wrapper .bp3-icon-link::before{content:\"\ue62d\"}.jupyter-wrapper .bp3-icon-list::before{content:\"\u2630\"}.jupyter-wrapper .bp3-icon-list-columns::before{content:\"\ue7b9\"}.jupyter-wrapper .bp3-icon-list-detail-view::before{content:\"\ue743\"}.jupyter-wrapper .bp3-icon-locate::before{content:\"\ue619\"}.jupyter-wrapper .bp3-icon-lock::before{content:\"\ue625\"}.jupyter-wrapper .bp3-icon-log-in::before{content:\"\ue69a\"}.jupyter-wrapper .bp3-icon-log-out::before{content:\"\ue64c\"}.jupyter-wrapper .bp3-icon-manual::before{content:\"\ue6f6\"}.jupyter-wrapper .bp3-icon-manually-entered-data::before{content:\"\ue74a\"}.jupyter-wrapper .bp3-icon-map::before{content:\"\ue662\"}.jupyter-wrapper .bp3-icon-map-create::before{content:\"\ue741\"}.jupyter-wrapper .bp3-icon-map-marker::before{content:\"\ue67d\"}.jupyter-wrapper .bp3-icon-maximize::before{content:\"\ue635\"}.jupyter-wrapper .bp3-icon-media::before{content:\"\ue62c\"}.jupyter-wrapper .bp3-icon-menu::before{content:\"\ue762\"}.jupyter-wrapper .bp3-icon-menu-closed::before{content:\"\ue655\"}.jupyter-wrapper .bp3-icon-menu-open::before{content:\"\ue654\"}.jupyter-wrapper .bp3-icon-merge-columns::before{content:\"\ue74f\"}.jupyter-wrapper .bp3-icon-merge-links::before{content:\"\ue60b\"}.jupyter-wrapper .bp3-icon-minimize::before{content:\"\ue634\"}.jupyter-wrapper .bp3-icon-minus::before{content:\"\u2212\"}.jupyter-wrapper .bp3-icon-mobile-phone::before{content:\"\ue717\"}.jupyter-wrapper .bp3-icon-mobile-video::before{content:\"\ue69f\"}.jupyter-wrapper .bp3-icon-moon::before{content:\"\ue754\"}.jupyter-wrapper .bp3-icon-more::before{content:\"\ue62a\"}.jupyter-wrapper .bp3-icon-mountain::before{content:\"\ue7b1\"}.jupyter-wrapper .bp3-icon-move::before{content:\"\ue693\"}.jupyter-wrapper .bp3-icon-mugshot::before{content:\"\ue6db\"}.jupyter-wrapper .bp3-icon-multi-select::before{content:\"\ue680\"}.jupyter-wrapper .bp3-icon-music::before{content:\"\ue6a6\"}.jupyter-wrapper .bp3-icon-new-drawing::before{content:\"\ue905\"}.jupyter-wrapper .bp3-icon-new-grid-item::before{content:\"\ue747\"}.jupyter-wrapper .bp3-icon-new-layer::before{content:\"\ue902\"}.jupyter-wrapper .bp3-icon-new-layers::before{content:\"\ue903\"}.jupyter-wrapper .bp3-icon-new-link::before{content:\"\ue65c\"}.jupyter-wrapper .bp3-icon-new-object::before{content:\"\ue65d\"}.jupyter-wrapper .bp3-icon-new-person::before{content:\"\ue6e9\"}.jupyter-wrapper .bp3-icon-new-prescription::before{content:\"\ue78b\"}.jupyter-wrapper .bp3-icon-new-text-box::before{content:\"\ue65b\"}.jupyter-wrapper .bp3-icon-ninja::before{content:\"\ue675\"}.jupyter-wrapper .bp3-icon-not-equal-to::before{content:\"\ue7e0\"}.jupyter-wrapper .bp3-icon-notifications::before{content:\"\ue624\"}.jupyter-wrapper .bp3-icon-notifications-updated::before{content:\"\ue7b8\"}.jupyter-wrapper .bp3-icon-numbered-list::before{content:\"\ue746\"}.jupyter-wrapper .bp3-icon-numerical::before{content:\"\ue756\"}.jupyter-wrapper .bp3-icon-office::before{content:\"\ue69b\"}.jupyter-wrapper .bp3-icon-offline::before{content:\"\ue67a\"}.jupyter-wrapper .bp3-icon-oil-field::before{content:\"\ue73f\"}.jupyter-wrapper .bp3-icon-one-column::before{content:\"\ue658\"}.jupyter-wrapper .bp3-icon-outdated::before{content:\"\ue7a8\"}.jupyter-wrapper .bp3-icon-page-layout::before{content:\"\ue660\"}.jupyter-wrapper .bp3-icon-panel-stats::before{content:\"\ue777\"}.jupyter-wrapper .bp3-icon-panel-table::before{content:\"\ue778\"}.jupyter-wrapper .bp3-icon-paperclip::before{content:\"\ue664\"}.jupyter-wrapper .bp3-icon-paragraph::before{content:\"\ue76c\"}.jupyter-wrapper .bp3-icon-path::before{content:\"\ue753\"}.jupyter-wrapper .bp3-icon-path-search::before{content:\"\ue65e\"}.jupyter-wrapper .bp3-icon-pause::before{content:\"\ue6a9\"}.jupyter-wrapper .bp3-icon-people::before{content:\"\ue63d\"}.jupyter-wrapper .bp3-icon-percentage::before{content:\"\ue76a\"}.jupyter-wrapper .bp3-icon-person::before{content:\"\ue63c\"}.jupyter-wrapper .bp3-icon-phone::before{content:\"\u260e\"}.jupyter-wrapper .bp3-icon-pie-chart::before{content:\"\ue684\"}.jupyter-wrapper .bp3-icon-pin::before{content:\"\ue646\"}.jupyter-wrapper .bp3-icon-pivot::before{content:\"\ue6f1\"}.jupyter-wrapper .bp3-icon-pivot-table::before{content:\"\ue6eb\"}.jupyter-wrapper .bp3-icon-play::before{content:\"\ue6ab\"}.jupyter-wrapper .bp3-icon-plus::before{content:\"+\"}.jupyter-wrapper .bp3-icon-polygon-filter::before{content:\"\ue6d1\"}.jupyter-wrapper .bp3-icon-power::before{content:\"\ue6d9\"}.jupyter-wrapper .bp3-icon-predictive-analysis::before{content:\"\ue617\"}.jupyter-wrapper .bp3-icon-prescription::before{content:\"\ue78a\"}.jupyter-wrapper .bp3-icon-presentation::before{content:\"\ue687\"}.jupyter-wrapper .bp3-icon-print::before{content:\"\u2399\"}.jupyter-wrapper .bp3-icon-projects::before{content:\"\ue622\"}.jupyter-wrapper .bp3-icon-properties::before{content:\"\ue631\"}.jupyter-wrapper .bp3-icon-property::before{content:\"\ue65a\"}.jupyter-wrapper .bp3-icon-publish-function::before{content:\"\ue752\"}.jupyter-wrapper .bp3-icon-pulse::before{content:\"\ue6e8\"}.jupyter-wrapper .bp3-icon-random::before{content:\"\ue698\"}.jupyter-wrapper .bp3-icon-record::before{content:\"\ue6ae\"}.jupyter-wrapper .bp3-icon-redo::before{content:\"\ue6c4\"}.jupyter-wrapper .bp3-icon-refresh::before{content:\"\ue643\"}.jupyter-wrapper .bp3-icon-regression-chart::before{content:\"\ue758\"}.jupyter-wrapper .bp3-icon-remove::before{content:\"\ue63f\"}.jupyter-wrapper .bp3-icon-remove-column::before{content:\"\ue755\"}.jupyter-wrapper .bp3-icon-remove-column-left::before{content:\"\ue6fd\"}.jupyter-wrapper .bp3-icon-remove-column-right::before{content:\"\ue6fe\"}.jupyter-wrapper .bp3-icon-remove-row-bottom::before{content:\"\ue6fc\"}.jupyter-wrapper .bp3-icon-remove-row-top::before{content:\"\ue6fb\"}.jupyter-wrapper .bp3-icon-repeat::before{content:\"\ue692\"}.jupyter-wrapper .bp3-icon-reset::before{content:\"\ue7d6\"}.jupyter-wrapper .bp3-icon-resolve::before{content:\"\ue672\"}.jupyter-wrapper .bp3-icon-rig::before{content:\"\ue740\"}.jupyter-wrapper .bp3-icon-right-join::before{content:\"\ue7a5\"}.jupyter-wrapper .bp3-icon-ring::before{content:\"\ue6f2\"}.jupyter-wrapper .bp3-icon-rotate-document::before{content:\"\ue6e1\"}.jupyter-wrapper .bp3-icon-rotate-page::before{content:\"\ue6e2\"}.jupyter-wrapper .bp3-icon-satellite::before{content:\"\ue76b\"}.jupyter-wrapper .bp3-icon-saved::before{content:\"\ue6b6\"}.jupyter-wrapper .bp3-icon-scatter-plot::before{content:\"\ue73e\"}.jupyter-wrapper .bp3-icon-search::before{content:\"\ue64b\"}.jupyter-wrapper .bp3-icon-search-around::before{content:\"\ue608\"}.jupyter-wrapper .bp3-icon-search-template::before{content:\"\ue628\"}.jupyter-wrapper .bp3-icon-search-text::before{content:\"\ue663\"}.jupyter-wrapper .bp3-icon-segmented-control::before{content:\"\ue6ec\"}.jupyter-wrapper .bp3-icon-select::before{content:\"\ue616\"}.jupyter-wrapper .bp3-icon-selection::before{content:\"\u29bf\"}.jupyter-wrapper .bp3-icon-send-to::before{content:\"\ue66e\"}.jupyter-wrapper .bp3-icon-send-to-graph::before{content:\"\ue736\"}.jupyter-wrapper .bp3-icon-send-to-map::before{content:\"\ue737\"}.jupyter-wrapper .bp3-icon-series-add::before{content:\"\ue796\"}.jupyter-wrapper .bp3-icon-series-configuration::before{content:\"\ue79a\"}.jupyter-wrapper .bp3-icon-series-derived::before{content:\"\ue799\"}.jupyter-wrapper .bp3-icon-series-filtered::before{content:\"\ue798\"}.jupyter-wrapper .bp3-icon-series-search::before{content:\"\ue797\"}.jupyter-wrapper .bp3-icon-settings::before{content:\"\ue6a2\"}.jupyter-wrapper .bp3-icon-share::before{content:\"\ue62e\"}.jupyter-wrapper .bp3-icon-shield::before{content:\"\ue7b2\"}.jupyter-wrapper .bp3-icon-shop::before{content:\"\ue6c2\"}.jupyter-wrapper .bp3-icon-shopping-cart::before{content:\"\ue6c1\"}.jupyter-wrapper .bp3-icon-signal-search::before{content:\"\ue909\"}.jupyter-wrapper .bp3-icon-sim-card::before{content:\"\ue718\"}.jupyter-wrapper .bp3-icon-slash::before{content:\"\ue769\"}.jupyter-wrapper .bp3-icon-small-cross::before{content:\"\ue6d7\"}.jupyter-wrapper .bp3-icon-small-minus::before{content:\"\ue70e\"}.jupyter-wrapper .bp3-icon-small-plus::before{content:\"\ue70d\"}.jupyter-wrapper .bp3-icon-small-tick::before{content:\"\ue6d8\"}.jupyter-wrapper .bp3-icon-snowflake::before{content:\"\ue7b6\"}.jupyter-wrapper .bp3-icon-social-media::before{content:\"\ue671\"}.jupyter-wrapper .bp3-icon-sort::before{content:\"\ue64f\"}.jupyter-wrapper .bp3-icon-sort-alphabetical::before{content:\"\ue64d\"}.jupyter-wrapper .bp3-icon-sort-alphabetical-desc::before{content:\"\ue6c8\"}.jupyter-wrapper .bp3-icon-sort-asc::before{content:\"\ue6d5\"}.jupyter-wrapper .bp3-icon-sort-desc::before{content:\"\ue6d6\"}.jupyter-wrapper .bp3-icon-sort-numerical::before{content:\"\ue64e\"}.jupyter-wrapper .bp3-icon-sort-numerical-desc::before{content:\"\ue6c9\"}.jupyter-wrapper .bp3-icon-split-columns::before{content:\"\ue750\"}.jupyter-wrapper .bp3-icon-square::before{content:\"\ue686\"}.jupyter-wrapper .bp3-icon-stacked-chart::before{content:\"\ue6e7\"}.jupyter-wrapper .bp3-icon-star::before{content:\"\u2605\"}.jupyter-wrapper .bp3-icon-star-empty::before{content:\"\u2606\"}.jupyter-wrapper .bp3-icon-step-backward::before{content:\"\ue6a7\"}.jupyter-wrapper .bp3-icon-step-chart::before{content:\"\ue70f\"}.jupyter-wrapper .bp3-icon-step-forward::before{content:\"\ue6ad\"}.jupyter-wrapper .bp3-icon-stop::before{content:\"\ue6aa\"}.jupyter-wrapper .bp3-icon-stopwatch::before{content:\"\ue901\"}.jupyter-wrapper .bp3-icon-strikethrough::before{content:\"\ue7a6\"}.jupyter-wrapper .bp3-icon-style::before{content:\"\ue601\"}.jupyter-wrapper .bp3-icon-swap-horizontal::before{content:\"\ue745\"}.jupyter-wrapper .bp3-icon-swap-vertical::before{content:\"\ue744\"}.jupyter-wrapper .bp3-icon-symbol-circle::before{content:\"\ue72e\"}.jupyter-wrapper .bp3-icon-symbol-cross::before{content:\"\ue731\"}.jupyter-wrapper .bp3-icon-symbol-diamond::before{content:\"\ue730\"}.jupyter-wrapper .bp3-icon-symbol-square::before{content:\"\ue72f\"}.jupyter-wrapper .bp3-icon-symbol-triangle-down::before{content:\"\ue733\"}.jupyter-wrapper .bp3-icon-symbol-triangle-up::before{content:\"\ue732\"}.jupyter-wrapper .bp3-icon-tag::before{content:\"\ue61c\"}.jupyter-wrapper .bp3-icon-take-action::before{content:\"\ue6ca\"}.jupyter-wrapper .bp3-icon-taxi::before{content:\"\ue79e\"}.jupyter-wrapper .bp3-icon-text-highlight::before{content:\"\ue6dd\"}.jupyter-wrapper .bp3-icon-th::before{content:\"\ue667\"}.jupyter-wrapper .bp3-icon-th-derived::before{content:\"\ue669\"}.jupyter-wrapper .bp3-icon-th-disconnect::before{content:\"\ue7d8\"}.jupyter-wrapper .bp3-icon-th-filtered::before{content:\"\ue7c6\"}.jupyter-wrapper .bp3-icon-th-list::before{content:\"\ue668\"}.jupyter-wrapper .bp3-icon-thumbs-down::before{content:\"\ue6be\"}.jupyter-wrapper .bp3-icon-thumbs-up::before{content:\"\ue6bd\"}.jupyter-wrapper .bp3-icon-tick::before{content:\"\u2713\"}.jupyter-wrapper .bp3-icon-tick-circle::before{content:\"\ue779\"}.jupyter-wrapper .bp3-icon-time::before{content:\"\u23f2\"}.jupyter-wrapper .bp3-icon-timeline-area-chart::before{content:\"\ue6cd\"}.jupyter-wrapper .bp3-icon-timeline-bar-chart::before{content:\"\ue620\"}.jupyter-wrapper .bp3-icon-timeline-events::before{content:\"\ue61e\"}.jupyter-wrapper .bp3-icon-timeline-line-chart::before{content:\"\ue61f\"}.jupyter-wrapper .bp3-icon-tint::before{content:\"\ue6b2\"}.jupyter-wrapper .bp3-icon-torch::before{content:\"\ue677\"}.jupyter-wrapper .bp3-icon-tractor::before{content:\"\ue90c\"}.jupyter-wrapper .bp3-icon-train::before{content:\"\ue79f\"}.jupyter-wrapper .bp3-icon-translate::before{content:\"\ue759\"}.jupyter-wrapper .bp3-icon-trash::before{content:\"\ue63b\"}.jupyter-wrapper .bp3-icon-tree::before{content:\"\ue7b7\"}.jupyter-wrapper .bp3-icon-trending-down::before{content:\"\ue71a\"}.jupyter-wrapper .bp3-icon-trending-up::before{content:\"\ue719\"}.jupyter-wrapper .bp3-icon-truck::before{content:\"\ue90b\"}.jupyter-wrapper .bp3-icon-two-columns::before{content:\"\ue657\"}.jupyter-wrapper .bp3-icon-unarchive::before{content:\"\ue906\"}.jupyter-wrapper .bp3-icon-underline::before{content:\"\u2381\"}.jupyter-wrapper .bp3-icon-undo::before{content:\"\u238c\"}.jupyter-wrapper .bp3-icon-ungroup-objects::before{content:\"\ue688\"}.jupyter-wrapper .bp3-icon-unknown-vehicle::before{content:\"\ue73d\"}.jupyter-wrapper .bp3-icon-unlock::before{content:\"\ue626\"}.jupyter-wrapper .bp3-icon-unpin::before{content:\"\ue650\"}.jupyter-wrapper .bp3-icon-unresolve::before{content:\"\ue679\"}.jupyter-wrapper .bp3-icon-updated::before{content:\"\ue7a7\"}.jupyter-wrapper .bp3-icon-upload::before{content:\"\ue68f\"}.jupyter-wrapper .bp3-icon-user::before{content:\"\ue627\"}.jupyter-wrapper .bp3-icon-variable::before{content:\"\ue6f5\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-asc::before{content:\"\ue75b\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-desc::before{content:\"\ue71c\"}.jupyter-wrapper .bp3-icon-vertical-distribution::before{content:\"\ue721\"}.jupyter-wrapper .bp3-icon-video::before{content:\"\ue6a0\"}.jupyter-wrapper .bp3-icon-volume-down::before{content:\"\ue6a4\"}.jupyter-wrapper .bp3-icon-volume-off::before{content:\"\ue6a3\"}.jupyter-wrapper .bp3-icon-volume-up::before{content:\"\ue6a5\"}.jupyter-wrapper .bp3-icon-walk::before{content:\"\ue79d\"}.jupyter-wrapper .bp3-icon-warning-sign::before{content:\"\ue647\"}.jupyter-wrapper .bp3-icon-waterfall-chart::before{content:\"\ue6e6\"}.jupyter-wrapper .bp3-icon-widget::before{content:\"\ue678\"}.jupyter-wrapper .bp3-icon-widget-button::before{content:\"\ue790\"}.jupyter-wrapper .bp3-icon-widget-footer::before{content:\"\ue792\"}.jupyter-wrapper .bp3-icon-widget-header::before{content:\"\ue791\"}.jupyter-wrapper .bp3-icon-wrench::before{content:\"\ue734\"}.jupyter-wrapper .bp3-icon-zoom-in::before{content:\"\ue641\"}.jupyter-wrapper .bp3-icon-zoom-out::before{content:\"\ue642\"}.jupyter-wrapper .bp3-icon-zoom-to-fit::before{content:\"\ue67b\"}.jupyter-wrapper .bp3-submenu>.bp3-popover-wrapper{display:block}.jupyter-wrapper .bp3-submenu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-submenu.bp3-popover{-webkit-box-shadow:none;box-shadow:none;padding:0 5px}.jupyter-wrapper .bp3-submenu.bp3-popover>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover>.bp3-popover-content,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-menu{margin:0;border-radius:3px;background:#fff;min-width:180px;padding:5px;list-style:none;text-align:left;color:#182026}.jupyter-wrapper .bp3-menu-divider{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-divider{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;border-radius:2px;padding:5px 7px;text-decoration:none;line-height:20px;color:inherit;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-menu-item>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>*{margin-right:7px}.jupyter-wrapper .bp3-menu-item:empty::before,.jupyter-wrapper .bp3-menu-item>:last-child{margin-right:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{word-break:break-word}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(167,182,194,.3);cursor:pointer;text-decoration:none}.jupyter-wrapper .bp3-menu-item.bp3-disabled{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(138,155,168,.15);color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{background-color:inherit;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-right:7px}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>.bp3-icon{margin-top:2px;color:#5c7080}.jupyter-wrapper .bp3-menu-item .bp3-menu-item-label{color:#5c7080}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-menu-item:active{background-color:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-menu-item.bp3-disabled{outline:none !important;background-color:inherit !important;cursor:not-allowed !important;color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-large .bp3-menu-item{padding:9px 7px;line-height:22px;font-size:16px}.jupyter-wrapper .bp3-large .bp3-menu-item .bp3-icon{margin-top:3px}.jupyter-wrapper .bp3-large .bp3-menu-item::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-top:1px;margin-right:10px}.jupyter-wrapper button.bp3-menu-item{border:none;background:none;width:100%;text-align:left}.jupyter-wrapper .bp3-menu-header{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15);cursor:default;padding-left:2px}.jupyter-wrapper .bp3-dark .bp3-menu-header{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-header:first-of-type{border-top:none}.jupyter-wrapper .bp3-menu-header>h6{color:#182026;font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;margin:0;padding:10px 7px 0 1px;line-height:17px}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-large .bp3-menu-header>h6{padding-top:15px;padding-bottom:5px;font-size:18px}.jupyter-wrapper .bp3-large .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-dark .bp3-menu{background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item .bp3-menu-item-label{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item:active{background-color:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-divider,.jupyter-wrapper .bp3-dark .bp3-menu-header{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-label .bp3-menu{margin-top:5px}.jupyter-wrapper .bp3-navbar{position:relative;z-index:10;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background-color:#fff;width:100%;height:50px;padding:0 15px}.jupyter-wrapper .bp3-navbar.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-navbar{background-color:#394b59}.jupyter-wrapper .bp3-navbar.bp3-dark{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-navbar{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-navbar.bp3-fixed-top{position:fixed;top:0;right:0;left:0}.jupyter-wrapper .bp3-navbar-heading{margin-right:15px;font-size:16px}.jupyter-wrapper .bp3-navbar-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:50px}.jupyter-wrapper .bp3-navbar-group.bp3-align-left{float:left}.jupyter-wrapper .bp3-navbar-group.bp3-align-right{float:right}.jupyter-wrapper .bp3-navbar-divider{margin:0 10px;border-left:1px solid rgba(16,22,26,.15);height:20px}.jupyter-wrapper .bp3-dark .bp3-navbar-divider{border-left-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-non-ideal-state{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%;text-align:center}.jupyter-wrapper .bp3-non-ideal-state>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-non-ideal-state>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-non-ideal-state::before,.jupyter-wrapper .bp3-non-ideal-state>*{margin-bottom:20px}.jupyter-wrapper .bp3-non-ideal-state:empty::before,.jupyter-wrapper .bp3-non-ideal-state>:last-child{margin-bottom:0}.jupyter-wrapper .bp3-non-ideal-state>*{max-width:400px}.jupyter-wrapper .bp3-non-ideal-state-visual{color:rgba(92,112,128,.6);font-size:60px}.jupyter-wrapper .bp3-dark .bp3-non-ideal-state-visual{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-overflow-list{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;min-width:0}.jupyter-wrapper .bp3-overflow-list-spacer{-ms-flex-negative:1;flex-shrink:1;width:1px}.jupyter-wrapper body.bp3-overlay-open{overflow:hidden}.jupyter-wrapper .bp3-overlay{position:static;top:0;right:0;bottom:0;left:0;z-index:20}.jupyter-wrapper .bp3-overlay:not(.bp3-overlay-open){pointer-events:none}.jupyter-wrapper .bp3-overlay.bp3-overlay-container{position:fixed;overflow:hidden}.jupyter-wrapper .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container{position:fixed;overflow:auto}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-inline{display:inline;overflow:visible}.jupyter-wrapper .bp3-overlay-content{position:fixed;z-index:20}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-content,.jupyter-wrapper .bp3-overlay-scroll-container .bp3-overlay-content{position:absolute}.jupyter-wrapper .bp3-overlay-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;opacity:1;z-index:20;background-color:rgba(16,22,26,.7);overflow:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear{opacity:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter-active,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit{opacity:1}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop:focus{outline:none}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-backdrop{position:absolute}.jupyter-wrapper .bp3-panel-stack{position:relative;overflow:hidden}.jupyter-wrapper .bp3-panel-stack-header{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-negative:0;flex-shrink:0;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:1;-webkit-box-shadow:0 1px rgba(16,22,26,.15);box-shadow:0 1px rgba(16,22,26,.15);height:30px}.jupyter-wrapper .bp3-dark .bp3-panel-stack-header{-webkit-box-shadow:0 1px rgba(255,255,255,.15);box-shadow:0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-panel-stack-header>span{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1;flex:1;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-panel-stack-header .bp3-heading{margin:0 5px}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back{margin-left:5px;padding-left:0;white-space:nowrap}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back .bp3-icon{margin:0 2px}.jupyter-wrapper .bp3-panel-stack-view{position:absolute;top:0;right:0;bottom:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-right:-1px;border-right:1px solid rgba(16,22,26,.15);background-color:#fff;overflow-y:auto}.jupyter-wrapper .bp3-dark .bp3-panel-stack-view{background-color:#30404d}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit-active{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1);display:inline-block;z-index:20;border-radius:3px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow{position:absolute;width:30px;height:30px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{margin:5px;width:20px;height:20px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover{margin-top:-17px;margin-bottom:17px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{bottom:-11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover{margin-left:17px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{left:-11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover{margin-top:17px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{top:-11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover{margin-right:17px;margin-left:-17px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{right:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-popover>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-popover>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{top:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{right:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{left:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{bottom:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-popover .bp3-popover-content{background:#fff;color:inherit}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-fill{fill:#fff}.jupyter-wrapper .bp3-popover-enter>.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover .bp3-popover-content{position:relative;border-radius:3px}.jupyter-wrapper .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{max-width:350px;padding:20px}.jupyter-wrapper .bp3-popover-target+.bp3-overlay .bp3-popover.bp3-popover-content-sizing{width:350px}.jupyter-wrapper .bp3-popover.bp3-minimal{margin:0 !important}.jupyter-wrapper .bp3-popover.bp3-minimal .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-content{background:#30404d;color:inherit}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-fill{fill:#30404d}.jupyter-wrapper .bp3-popover-arrow::before{display:block;position:absolute;-webkit-transform:rotate(45deg);transform:rotate(45deg);border-radius:2px;content:\"\"}.jupyter-wrapper .bp3-tether-pinned .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover-backdrop{background:rgba(255,255,255,0)}.jupyter-wrapper .bp3-transition-container{opacity:1;display:-webkit-box;display:-ms-flexbox;display:flex;z-index:20}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear{opacity:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter-active,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit{opacity:1}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container:focus{outline:none}.jupyter-wrapper .bp3-transition-container.bp3-popover-leave .bp3-popover-content{pointer-events:none}.jupyter-wrapper .bp3-transition-container[data-x-out-of-boundaries]{display:none}.jupyter-wrapper span.bp3-popover-target{display:inline-block}.jupyter-wrapper .bp3-popover-wrapper.bp3-fill{width:100%}.jupyter-wrapper .bp3-portal{position:absolute;top:0;right:0;left:0}@-webkit-keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}@keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}.jupyter-wrapper .bp3-progress-bar{display:block;position:relative;border-radius:40px;background:rgba(92,112,128,.2);width:100%;height:8px;overflow:hidden}.jupyter-wrapper .bp3-progress-bar .bp3-progress-meter{position:absolute;border-radius:40px;background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);background-color:rgba(92,112,128,.8);background-size:30px 30px;width:100%;height:100%;-webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{animation:linear-progress-bar-stripes 300ms linear infinite reverse}.jupyter-wrapper .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{background-image:none}.jupyter-wrapper .bp3-dark .bp3-progress-bar{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-progress-bar .bp3-progress-meter{background-color:#8a9ba8}.jupyter-wrapper .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{background-color:#137cbd}.jupyter-wrapper .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{background-color:#0f9960}.jupyter-wrapper .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{background-color:#d9822b}.jupyter-wrapper .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{background-color:#db3737}@-webkit-keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}@keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}.jupyter-wrapper .bp3-skeleton{border-color:rgba(206,217,224,.2) !important;border-radius:2px;-webkit-box-shadow:none !important;box-shadow:none !important;background:rgba(206,217,224,.2);background-clip:padding-box !important;cursor:default;color:rgba(0,0,0,0) !important;-webkit-animation:1000ms linear infinite alternate skeleton-glow;animation:1000ms linear infinite alternate skeleton-glow;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-skeleton::before,.jupyter-wrapper .bp3-skeleton::after,.jupyter-wrapper .bp3-skeleton *{visibility:hidden !important}.jupyter-wrapper .bp3-slider{width:100%;min-width:150px;height:40px;position:relative;outline:none;cursor:default;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-slider:hover{cursor:pointer}.jupyter-wrapper .bp3-slider:active{cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-slider.bp3-disabled{opacity:.5;cursor:not-allowed}.jupyter-wrapper .bp3-slider.bp3-slider-unlabeled{height:16px}.jupyter-wrapper .bp3-slider-track,.jupyter-wrapper .bp3-slider-progress{top:5px;right:0;left:0;height:6px;position:absolute}.jupyter-wrapper .bp3-slider-track{border-radius:3px;overflow:hidden}.jupyter-wrapper .bp3-slider-progress{background:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-dark .bp3-slider-progress{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-slider-progress.bp3-intent-primary{background-color:#137cbd}.jupyter-wrapper .bp3-slider-progress.bp3-intent-success{background-color:#0f9960}.jupyter-wrapper .bp3-slider-progress.bp3-intent-warning{background-color:#d9822b}.jupyter-wrapper .bp3-slider-progress.bp3-intent-danger{background-color:#db3737}.jupyter-wrapper .bp3-slider-handle{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;position:absolute;top:0;left:0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:pointer;width:16px;height:16px}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-slider-handle:active,.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-slider-handle.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active:hover,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-slider-handle:focus{z-index:1}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5;z-index:2;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:-webkit-grab;cursor:grab}.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-disabled .bp3-slider-handle{-webkit-box-shadow:none;box-shadow:none;background:#bfccd6;pointer-events:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover,.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-slider-handle,.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{background-color:#394b59}.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{background-color:#293742}.jupyter-wrapper .bp3-dark .bp3-disabled .bp3-slider-handle{border-color:#5c7080;-webkit-box-shadow:none;box-shadow:none;background:#5c7080}.jupyter-wrapper .bp3-slider-handle .bp3-slider-label{margin-left:8px;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-disabled .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-slider-handle.bp3-start,.jupyter-wrapper .bp3-slider-handle.bp3-end{width:8px}.jupyter-wrapper .bp3-slider-handle.bp3-start{border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end{margin-left:8px;border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end .bp3-slider-label{margin-left:0}.jupyter-wrapper .bp3-slider-label{-webkit-transform:translate(-50%, 20px);transform:translate(-50%, 20px);display:inline-block;position:absolute;padding:2px 5px;vertical-align:top;line-height:1;font-size:12px}.jupyter-wrapper .bp3-slider.bp3-vertical{width:40px;min-width:40px;height:150px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-track,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:0;bottom:0;left:5px;width:6px;height:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-label{-webkit-transform:translate(20px, 50%);transform:translate(20px, 50%)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{margin-top:-8px;margin-left:0}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{margin-left:0;width:16px;height:8px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{border-top-left-radius:0;border-bottom-right-radius:3px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{-webkit-transform:translate(20px);transform:translate(20px)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{margin-bottom:8px;border-top-left-radius:3px;border-bottom-left-radius:0;border-bottom-right-radius:0}@-webkit-keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.jupyter-wrapper .bp3-spinner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;overflow:visible;vertical-align:middle}.jupyter-wrapper .bp3-spinner svg{display:block}.jupyter-wrapper .bp3-spinner path{fill-opacity:0}.jupyter-wrapper .bp3-spinner .bp3-spinner-head{-webkit-transform-origin:center;transform-origin:center;-webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);stroke:rgba(92,112,128,.8);stroke-linecap:round}.jupyter-wrapper .bp3-spinner .bp3-spinner-track{stroke:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-spinner-animation{-webkit-animation:pt-spinner-animation 500ms linear infinite;animation:pt-spinner-animation 500ms linear infinite}.jupyter-wrapper .bp3-no-spin>.bp3-spinner-animation{-webkit-animation:none;animation:none}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-track{stroke:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-spinner.bp3-intent-primary .bp3-spinner-head{stroke:#137cbd}.jupyter-wrapper .bp3-spinner.bp3-intent-success .bp3-spinner-head{stroke:#0f9960}.jupyter-wrapper .bp3-spinner.bp3-intent-warning .bp3-spinner-head{stroke:#d9822b}.jupyter-wrapper .bp3-spinner.bp3-intent-danger .bp3-spinner-head{stroke:#db3737}.jupyter-wrapper .bp3-tabs.bp3-vertical{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab{border-radius:3px;width:100%;padding:0 10px}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab[aria-selected=true]{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.2)}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{top:0;right:0;bottom:0;left:0;border-radius:3px;background-color:rgba(19,124,189,.2);height:auto}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-panel{margin-top:0;padding-left:20px}.jupyter-wrapper .bp3-tab-list{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:relative;margin:0;border:none;padding:0;list-style:none}.jupyter-wrapper .bp3-tab-list>*:not(:last-child){margin-right:20px}.jupyter-wrapper .bp3-tab{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;cursor:pointer;max-width:100%;vertical-align:top;line-height:30px;color:#182026;font-size:14px}.jupyter-wrapper .bp3-tab a{display:block;text-decoration:none;color:inherit}.jupyter-wrapper .bp3-tab-indicator-wrapper~.bp3-tab{-webkit-box-shadow:none !important;box-shadow:none !important;background-color:rgba(0,0,0,0) !important}.jupyter-wrapper .bp3-tab[aria-disabled=true]{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tab[aria-selected=true]{border-radius:0;-webkit-box-shadow:inset 0 -3px 0 #106ba3;box-shadow:inset 0 -3px 0 #106ba3}.jupyter-wrapper .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-tab:not([aria-disabled=true]):hover{color:#106ba3}.jupyter-wrapper .bp3-tab:focus{-moz-outline-radius:0}.jupyter-wrapper .bp3-large>.bp3-tab{line-height:40px;font-size:16px}.jupyter-wrapper .bp3-tab-panel{margin-top:20px}.jupyter-wrapper .bp3-tab-panel[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-tab-indicator-wrapper{position:absolute;top:0;left:0;-webkit-transform:translateX(0),translateY(0);transform:translateX(0),translateY(0);-webkit-transition:height,width,-webkit-transform;transition:height,width,-webkit-transform;transition:height,transform,width;transition:height,transform,width,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);pointer-events:none}.jupyter-wrapper .bp3-tab-indicator-wrapper .bp3-tab-indicator{position:absolute;right:0;bottom:0;left:0;background-color:#106ba3;height:3px}.jupyter-wrapper .bp3-tab-indicator-wrapper.bp3-no-animation{-webkit-transition:none;transition:none}.jupyter-wrapper .bp3-dark .bp3-tab{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tab[aria-disabled=true]{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true]{-webkit-box-shadow:inset 0 -3px 0 #48aff0;box-shadow:inset 0 -3px 0 #48aff0}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-dark .bp3-tab:not([aria-disabled=true]):hover{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tab-indicator{background-color:#48aff0}.jupyter-wrapper .bp3-flex-expander{-webkit-box-flex:1;-ms-flex:1 1;flex:1 1}.jupyter-wrapper .bp3-tag{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border:none;border-radius:3px;-webkit-box-shadow:none;box-shadow:none;background-color:#5c7080;min-width:20px;max-width:100%;min-height:20px;padding:2px 6px;line-height:16px;color:#f5f8fa;font-size:12px}.jupyter-wrapper .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-interactive:hover{background-color:rgba(92,112,128,.85)}.jupyter-wrapper .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-interactive:active{background-color:rgba(92,112,128,.7)}.jupyter-wrapper .bp3-tag>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag::before,.jupyter-wrapper .bp3-tag>*{margin-right:4px}.jupyter-wrapper .bp3-tag:empty::before,.jupyter-wrapper .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag:focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag.bp3-round{border-radius:30px;padding-right:8px;padding-left:8px}.jupyter-wrapper .bp3-dark .bp3-tag{background-color:#bfccd6;color:#182026}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:hover{background-color:rgba(191,204,214,.85)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:active{background-color:rgba(191,204,214,.7)}.jupyter-wrapper .bp3-dark .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-large{fill:currentColor}.jupyter-wrapper .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-tag .bp3-icon-large{fill:#fff}.jupyter-wrapper .bp3-tag.bp3-large,.jupyter-wrapper .bp3-large .bp3-tag{min-width:30px;min-height:30px;padding:0 10px;line-height:20px;font-size:14px}.jupyter-wrapper .bp3-tag.bp3-large::before,.jupyter-wrapper .bp3-tag.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-tag::before,.jupyter-wrapper .bp3-large .bp3-tag>*{margin-right:7px}.jupyter-wrapper .bp3-tag.bp3-large:empty::before,.jupyter-wrapper .bp3-tag.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-tag:empty::before,.jupyter-wrapper .bp3-large .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag.bp3-large.bp3-round,.jupyter-wrapper .bp3-large .bp3-tag.bp3-round{padding-right:12px;padding-left:12px}.jupyter-wrapper .bp3-tag.bp3-intent-primary{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-success{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-warning{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-danger{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.7)}.jupyter-wrapper .bp3-tag.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-tag.bp3-minimal>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-large{fill:#5c7080}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){background-color:rgba(138,155,168,.2);color:#182026}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(191,204,214,.3)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-])>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-large{fill:#a7b6c2}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{fill:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.25);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{fill:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.25);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{fill:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.25);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{fill:#db3737}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.25);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.45)}.jupyter-wrapper .bp3-tag-remove{display:-webkit-box;display:-ms-flexbox;display:flex;opacity:.5;margin-top:-2px;margin-right:-6px !important;margin-bottom:-2px;border:none;background:none;cursor:pointer;padding:2px;padding-left:0;color:inherit}.jupyter-wrapper .bp3-tag-remove:hover{opacity:.8;background:none;text-decoration:none}.jupyter-wrapper .bp3-tag-remove:active{opacity:1}.jupyter-wrapper .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6d7\"}.jupyter-wrapper .bp3-large .bp3-tag-remove{margin-right:-10px !important;padding:5px;padding-left:0}.jupyter-wrapper .bp3-large .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal}.jupyter-wrapper .bp3-tag-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;cursor:text;height:auto;min-height:30px;padding-right:0;padding-left:5px;line-height:inherit}.jupyter-wrapper .bp3-tag-input>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input>.bp3-tag-input-values{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-icon{margin-top:7px;margin-right:7px;margin-left:2px;color:#5c7080}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-item-align:stretch;align-self:stretch;margin-top:5px;margin-right:7px;min-width:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-right:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:empty::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{padding-left:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-bottom:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag{overflow-wrap:break-word}.jupyter-wrapper .bp3-tag-input .bp3-tag.bp3-active{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:80px;line-height:20px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost:disabled,.jupyter-wrapper .bp3-tag-input .bp3-input-ghost.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-tag-input .bp3-button,.jupyter-wrapper .bp3-tag-input .bp3-spinner{margin:3px;margin-left:0}.jupyter-wrapper .bp3-tag-input .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-tag-input.bp3-large{height:auto;min-height:40px}.jupyter-wrapper .bp3-tag-input.bp3-large::before,.jupyter-wrapper .bp3-tag-input.bp3-large>*{margin-right:10px}.jupyter-wrapper .bp3-tag-input.bp3-large:empty::before,.jupyter-wrapper .bp3-tag-input.bp3-large>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-tag-input-icon{margin-top:10px;margin-left:5px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-input-ghost{line-height:30px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-button{min-width:30px;min-height:30px;padding:5px 10px;margin:5px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-spinner{margin:8px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-tag-input-icon,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-tag-input-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-input-ghost{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;padding:0}.jupyter-wrapper .bp3-input-ghost::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:focus{outline:none !important}.jupyter-wrapper .bp3-toast{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;position:relative !important;margin:20px 0 0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background-color:#fff;min-width:300px;max-width:500px;pointer-events:all}.jupyter-wrapper .bp3-toast.bp3-toast-enter,.jupyter-wrapper .bp3-toast.bp3-toast-appear{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-enter~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit{opacity:1;-webkit-filter:blur(0);filter:blur(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active{opacity:0;-webkit-filter:blur(10px);filter:blur(10px);-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:opacity,filter;transition-property:opacity,filter,-webkit-filter;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:50ms;transition-delay:50ms}.jupyter-wrapper .bp3-toast .bp3-button-group{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;padding-left:0}.jupyter-wrapper .bp3-toast>.bp3-icon{margin:12px;margin-right:0;color:#5c7080}.jupyter-wrapper .bp3-toast.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-toast{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background-color:#394b59}.jupyter-wrapper .bp3-toast.bp3-dark>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-toast>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a:hover{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-]>.bp3-icon{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::before,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button .bp3-icon,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{color:rgba(255,255,255,.7) !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:focus{outline-color:rgba(255,255,255,.5)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:hover{background-color:rgba(255,255,255,.15) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{background-color:rgba(255,255,255,.3) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::after{background:rgba(255,255,255,.3) !important}.jupyter-wrapper .bp3-toast.bp3-intent-primary{background-color:#137cbd;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-success{background-color:#0f9960;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-warning{background-color:#d9822b;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-danger{background-color:#db3737;color:#fff}.jupyter-wrapper .bp3-toast-message{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;padding:11px;word-break:break-word}.jupyter-wrapper .bp3-toast-container{display:-webkit-box !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;right:0;left:0;z-index:40;overflow:hidden;padding:0 20px 20px;pointer-events:none}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-top{top:0;bottom:auto}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-bottom{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;top:auto;bottom:0}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-left{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-right{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active~.bp3-toast{-webkit-transform:translateY(60px);transform:translateY(60px)}.jupyter-wrapper .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow{position:absolute;width:22px;height:22px}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{margin:4px;width:14px;height:14px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip{margin-top:-11px;margin-bottom:11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{bottom:-8px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip{margin-left:11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{left:-8px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip{margin-top:11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{top:-8px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip{margin-right:11px;margin-left:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{right:-8px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-tooltip>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-tooltip>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{top:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{right:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{left:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{bottom:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-fill{fill:#394b59}.jupyter-wrapper .bp3-popover-enter>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear-active>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{padding:10px 12px}.jupyter-wrapper .bp3-tooltip.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-content{background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{fill:#e1e8ed}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-content{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{fill:#137cbd}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-content{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{fill:#0f9960}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-content{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{fill:#d9822b}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-content{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{fill:#db3737}.jupyter-wrapper .bp3-tooltip-indicator{border-bottom:dotted 1px;cursor:help}.jupyter-wrapper .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-tree .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-tree-node-list{margin:0;padding-left:0;list-style:none}.jupyter-wrapper .bp3-tree-root{position:relative;background-color:rgba(0,0,0,0);cursor:default;padding-left:0}.jupyter-wrapper .bp3-tree-node-content-0{padding-left:0px}.jupyter-wrapper .bp3-tree-node-content-1{padding-left:23px}.jupyter-wrapper .bp3-tree-node-content-2{padding-left:46px}.jupyter-wrapper .bp3-tree-node-content-3{padding-left:69px}.jupyter-wrapper .bp3-tree-node-content-4{padding-left:92px}.jupyter-wrapper .bp3-tree-node-content-5{padding-left:115px}.jupyter-wrapper .bp3-tree-node-content-6{padding-left:138px}.jupyter-wrapper .bp3-tree-node-content-7{padding-left:161px}.jupyter-wrapper .bp3-tree-node-content-8{padding-left:184px}.jupyter-wrapper .bp3-tree-node-content-9{padding-left:207px}.jupyter-wrapper .bp3-tree-node-content-10{padding-left:230px}.jupyter-wrapper .bp3-tree-node-content-11{padding-left:253px}.jupyter-wrapper .bp3-tree-node-content-12{padding-left:276px}.jupyter-wrapper .bp3-tree-node-content-13{padding-left:299px}.jupyter-wrapper .bp3-tree-node-content-14{padding-left:322px}.jupyter-wrapper .bp3-tree-node-content-15{padding-left:345px}.jupyter-wrapper .bp3-tree-node-content-16{padding-left:368px}.jupyter-wrapper .bp3-tree-node-content-17{padding-left:391px}.jupyter-wrapper .bp3-tree-node-content-18{padding-left:414px}.jupyter-wrapper .bp3-tree-node-content-19{padding-left:437px}.jupyter-wrapper .bp3-tree-node-content-20{padding-left:460px}.jupyter-wrapper .bp3-tree-node-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:30px;padding-right:5px}.jupyter-wrapper .bp3-tree-node-content:hover{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node-caret-none{min-width:30px}.jupyter-wrapper .bp3-tree-node-caret{color:#5c7080;-webkit-transform:rotate(0deg);transform:rotate(0deg);cursor:pointer;padding:7px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-tree-node-caret:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret:hover{color:#f5f8fa}.jupyter-wrapper .bp3-tree-node-caret.bp3-tree-node-caret-open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tree-node-caret.bp3-icon-standard::before{content:\"\ue695\"}.jupyter-wrapper .bp3-tree-node-icon{position:relative;margin-right:7px}.jupyter-wrapper .bp3-tree-node-label{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-label span{display:inline}.jupyter-wrapper .bp3-tree-node-secondary-label{padding:0 5px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-wrapper,.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-content{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-standard,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-large{color:#fff}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret::before{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret:hover::before{color:#fff}.jupyter-wrapper .bp3-dark .bp3-tree-node-content:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-dark .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-omnibar{-webkit-filter:blur(0);filter:blur(0);opacity:1;top:20vh;left:calc(50% - 250px);z-index:21;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background-color:#fff;width:500px}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter-active,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear-active{-webkit-filter:blur(0);filter:blur(0);opacity:1;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit{-webkit-filter:blur(0);filter:blur(0);opacity:1}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit-active{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar .bp3-input{border-radius:0;background-color:rgba(0,0,0,0)}.jupyter-wrapper .bp3-omnibar .bp3-input,.jupyter-wrapper .bp3-omnibar .bp3-input:focus{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-omnibar .bp3-menu{border-radius:0;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);background-color:rgba(0,0,0,0);max-height:calc(60vh - 40px);overflow:auto}.jupyter-wrapper .bp3-omnibar .bp3-menu:empty{display:none}.jupyter-wrapper .bp3-dark .bp3-omnibar,.jupyter-wrapper .bp3-omnibar.bp3-dark{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-omnibar-overlay .bp3-overlay-backdrop{background-color:rgba(16,22,26,.2)}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper .bp3-multi-select{min-width:150px}.jupyter-wrapper .bp3-multi-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper :root{--jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-build: url(data:image/svg+xml;base64,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);--jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);--jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);--jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);--jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);--jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);--jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter: url(data:image/svg+xml;base64,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);--jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);--jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);--jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);--jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);--jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);--jp-icon-listings-info: url(data:image/svg+xml;base64,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);--jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);--jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);--jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);--jp-icon-react: url(data:image/svg+xml;base64,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);--jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);--jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);--jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-terminal: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0IiA+CiAgICA8cmVjdCBjbGFzcz0ianAtaWNvbjIganAtaWNvbi1zZWxlY3RhYmxlIiB3aWR0aD0iMjAiIGhlaWdodD0iMjAiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMikiIGZpbGw9IiMzMzMzMzMiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uLWFjY2VudDIganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGQ9Ik01LjA1NjY0IDguNzYxNzJDNS4wNTY2NCA4LjU5NzY2IDUuMDMxMjUgOC40NTMxMiA0Ljk4MDQ3IDguMzI4MTJDNC45MzM1OSA4LjE5OTIyIDQuODU1NDcgOC4wODIwMyA0Ljc0NjA5IDcuOTc2NTZDNC42NDA2MiA3Ljg3MTA5IDQuNSA3Ljc3NTM5IDQuMzI0MjIgNy42ODk0NUM0LjE1MjM0IDcuNTk5NjEgMy45NDMzNiA3LjUxMTcyIDMuNjk3MjcgNy40MjU3OEMzLjMwMjczIDcuMjg1MTYgMi45NDMzNiA3LjEzNjcyIDIuNjE5MTQgNi45ODA0N0MyLjI5NDkyIDYuODI0MjIgMi4wMTc1OCA2LjY0MjU4IDEuNzg3MTEgNi40MzU1NUMxLjU2MDU1IDYuMjI4NTIgMS4zODQ3NyA1Ljk4ODI4IDEuMjU5NzcgNS43MTQ4NEMxLjEzNDc3IDUuNDM3NSAxLjA3MjI3IDUuMTA5MzggMS4wNzIyNyA0LjczMDQ3QzEuMDcyMjcgNC4zOTg0NCAxLjEyODkxIDQuMDk1NyAxLjI0MjE5IDMuODIyMjdDMS4zNTU0NyAzLjU0NDkyIDEuNTE1NjIgMy4zMDQ2OSAxLjcyMjY2IDMuMTAxNTZDMS45Mjk2OSAyLjg5ODQ0IDIuMTc5NjkgMi43MzQzNyAyLjQ3MjY2IDIuNjA5MzhDMi43NjU2MiAyLjQ4NDM4IDMuMDkxOCAyLjQwNDMgMy40NTExNyAyLjM2OTE0VjEuMTA5MzhINC4zODg2N1YyLjM4MDg2QzQuNzQwMjMgMi40Mjc3MyA1LjA1NjY0IDIuNTIzNDQgNS4zMzc4OSAyLjY2Nzk3QzUuNjE5MTQgMi44MTI1IDUuODU3NDIgMy4wMDE5NSA2LjA1MjczIDMuMjM2MzNDNi4yNTE5NSAzLjQ2NjggNi40MDQzIDMuNzQwMjMgNi41MDk3NyA0LjA1NjY0QzYuNjE5MTQgNC4zNjkxNCA2LjY3MzgzIDQuNzIwNyA2LjY3MzgzIDUuMTExMzNINS4wNDQ5MkM1LjA0NDkyIDQuNjM4NjcgNC45Mzc1IDQuMjgxMjUgNC43MjI2NiA0LjAzOTA2QzQuNTA3ODEgMy43OTI5NyA0LjIxNjggMy42Njk5MiAzLjg0OTYxIDMuNjY5OTJDMy42NTAzOSAzLjY2OTkyIDMuNDc2NTYgMy42OTcyNyAzLjMyODEyIDMuNzUxOTVDMy4xODM1OSAzLjgwMjczIDMuMDY0NDUgMy44NzY5NSAyLjk3MDcgMy45NzQ2MUMyLjg3Njk1IDQuMDY4MzYgMi44MDY2NCA0LjE3OTY5IDIuNzU5NzcgNC4zMDg1OUMyLjcxNjggNC40Mzc1IDIuNjk1MzEgNC41NzgxMiAyLjY5NTMxIDQuNzMwNDdDMi42OTUzMSA0Ljg4MjgxIDIuNzE2OCA1LjAxOTUzIDIuNzU5NzcgNS4xNDA2MkMyLjgwNjY0IDUuMjU3ODEgMi44ODI4MSA1LjM2NzE5IDIuOTg4MjggNS40Njg3NUMzLjA5NzY2IDUuNTcwMzEgMy4yNDAyMyA1LjY2Nzk3IDMuNDE2MDIgNS43NjE3MkMzLjU5MTggNS44NTE1NiAzLjgxMDU1IDUuOTQzMzYgNC4wNzIyNyA2LjAzNzExQzQuNDY2OCA2LjE4NTU1IDQuODI0MjIgNi4zMzk4NCA1LjE0NDUzIDYuNUM1LjQ2NDg0IDYuNjU2MjUgNS43MzgyOCA2LjgzOTg0IDUuOTY0ODQgNy4wNTA3OEM2LjE5NTMxIDcuMjU3ODEgNi4zNzEwOSA3LjUgNi40OTIxOSA3Ljc3NzM0QzYuNjE3MTkgOC4wNTA3OCA2LjY3OTY5IDguMzc1IDYuNjc5NjkgOC43NUM2LjY3OTY5IDkuMDkzNzUgNi42MjMwNSA5LjQwNDMgNi41MDk3NyA5LjY4MTY0QzYuMzk2NDggOS45NTUwOCA2LjIzNDM4IDEwLjE5MTQgNi4wMjM0NCAxMC4zOTA2QzUuODEyNSAxMC41ODk4IDUuNTU4NTkgMTAuNzUgNS4yNjE3MiAxMC44NzExQzQuOTY0ODQgMTAuOTg4MyA0LjYzMjgxIDExLjA2NDUgNC4yNjU2MiAxMS4wOTk2VjEyLjI0OEgzLjMzMzk4VjExLjA5OTZDMy4wMDE5NSAxMS4wNjg0IDIuNjc5NjkgMTAuOTk2MSAyLjM2NzE5IDEwLjg4MjhDMi4wNTQ2OSAxMC43NjU2IDEuNzc3MzQgMTAuNTk3NyAxLjUzNTE2IDEwLjM3ODlDMS4yOTY4OCAxMC4xNjAyIDEuMTA1NDcgOS44ODQ3NyAwLjk2MDkzOCA5LjU1MjczQzAuODE2NDA2IDkuMjE2OCAwLjc0NDE0MSA4LjgxNDQ1IDAuNzQ0MTQxIDguMzQ1N0gyLjM3ODkxQzIuMzc4OTEgOC42MjY5NSAyLjQxOTkyIDguODYzMjggMi41MDE5NSA5LjA1NDY5QzIuNTgzOTggOS4yNDIxOSAyLjY4OTQ1IDkuMzkyNTggMi44MTgzNiA5LjUwNTg2QzIuOTUxMTcgOS42MTUyMyAzLjEwMTU2IDkuNjkzMzYgMy4yNjk1MyA5Ljc0MDIzQzMuNDM3NSA5Ljc4NzExIDMuNjA5MzggOS44MTA1NSAzLjc4NTE2IDkuODEwNTVDNC4yMDMxMiA5LjgxMDU1IDQuNTE5NTMgOS43MTI4OSA0LjczNDM4IDkuNTE3NThDNC45NDkyMiA5LjMyMjI3IDUuMDU2NjQgOS4wNzAzMSA1LjA1NjY0IDguNzYxNzJaTTEzLjQxOCAxMi4yNzE1SDguMDc0MjJWMTFIMTMuNDE4VjEyLjI3MTVaIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgzLjk1MjY0IDYpIiBmaWxsPSJ3aGl0ZSIvPgo8L3N2Zz4K);--jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);--jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);--jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K)}.jupyter-wrapper .jp-AddIcon{background-image:var(--jp-icon-add)}.jupyter-wrapper .jp-BugIcon{background-image:var(--jp-icon-bug)}.jupyter-wrapper .jp-BuildIcon{background-image:var(--jp-icon-build)}.jupyter-wrapper .jp-CaretDownEmptyIcon{background-image:var(--jp-icon-caret-down-empty)}.jupyter-wrapper .jp-CaretDownEmptyThinIcon{background-image:var(--jp-icon-caret-down-empty-thin)}.jupyter-wrapper .jp-CaretDownIcon{background-image:var(--jp-icon-caret-down)}.jupyter-wrapper .jp-CaretLeftIcon{background-image:var(--jp-icon-caret-left)}.jupyter-wrapper .jp-CaretRightIcon{background-image:var(--jp-icon-caret-right)}.jupyter-wrapper .jp-CaretUpEmptyThinIcon{background-image:var(--jp-icon-caret-up-empty-thin)}.jupyter-wrapper .jp-CaretUpIcon{background-image:var(--jp-icon-caret-up)}.jupyter-wrapper .jp-CaseSensitiveIcon{background-image:var(--jp-icon-case-sensitive)}.jupyter-wrapper .jp-CheckIcon{background-image:var(--jp-icon-check)}.jupyter-wrapper .jp-CircleEmptyIcon{background-image:var(--jp-icon-circle-empty)}.jupyter-wrapper .jp-CircleIcon{background-image:var(--jp-icon-circle)}.jupyter-wrapper .jp-ClearIcon{background-image:var(--jp-icon-clear)}.jupyter-wrapper .jp-CloseIcon{background-image:var(--jp-icon-close)}.jupyter-wrapper .jp-ConsoleIcon{background-image:var(--jp-icon-console)}.jupyter-wrapper .jp-CopyIcon{background-image:var(--jp-icon-copy)}.jupyter-wrapper .jp-CutIcon{background-image:var(--jp-icon-cut)}.jupyter-wrapper .jp-DownloadIcon{background-image:var(--jp-icon-download)}.jupyter-wrapper .jp-EditIcon{background-image:var(--jp-icon-edit)}.jupyter-wrapper .jp-EllipsesIcon{background-image:var(--jp-icon-ellipses)}.jupyter-wrapper .jp-ExtensionIcon{background-image:var(--jp-icon-extension)}.jupyter-wrapper .jp-FastForwardIcon{background-image:var(--jp-icon-fast-forward)}.jupyter-wrapper .jp-FileIcon{background-image:var(--jp-icon-file)}.jupyter-wrapper .jp-FileUploadIcon{background-image:var(--jp-icon-file-upload)}.jupyter-wrapper .jp-FilterListIcon{background-image:var(--jp-icon-filter-list)}.jupyter-wrapper .jp-FolderIcon{background-image:var(--jp-icon-folder)}.jupyter-wrapper .jp-Html5Icon{background-image:var(--jp-icon-html5)}.jupyter-wrapper .jp-ImageIcon{background-image:var(--jp-icon-image)}.jupyter-wrapper .jp-InspectorIcon{background-image:var(--jp-icon-inspector)}.jupyter-wrapper .jp-JsonIcon{background-image:var(--jp-icon-json)}.jupyter-wrapper .jp-JupyterFaviconIcon{background-image:var(--jp-icon-jupyter-favicon)}.jupyter-wrapper .jp-JupyterIcon{background-image:var(--jp-icon-jupyter)}.jupyter-wrapper .jp-JupyterlabWordmarkIcon{background-image:var(--jp-icon-jupyterlab-wordmark)}.jupyter-wrapper .jp-KernelIcon{background-image:var(--jp-icon-kernel)}.jupyter-wrapper .jp-KeyboardIcon{background-image:var(--jp-icon-keyboard)}.jupyter-wrapper .jp-LauncherIcon{background-image:var(--jp-icon-launcher)}.jupyter-wrapper .jp-LineFormIcon{background-image:var(--jp-icon-line-form)}.jupyter-wrapper .jp-LinkIcon{background-image:var(--jp-icon-link)}.jupyter-wrapper .jp-ListIcon{background-image:var(--jp-icon-list)}.jupyter-wrapper .jp-ListingsInfoIcon{background-image:var(--jp-icon-listings-info)}.jupyter-wrapper .jp-MarkdownIcon{background-image:var(--jp-icon-markdown)}.jupyter-wrapper .jp-NewFolderIcon{background-image:var(--jp-icon-new-folder)}.jupyter-wrapper .jp-NotTrustedIcon{background-image:var(--jp-icon-not-trusted)}.jupyter-wrapper .jp-NotebookIcon{background-image:var(--jp-icon-notebook)}.jupyter-wrapper .jp-PaletteIcon{background-image:var(--jp-icon-palette)}.jupyter-wrapper .jp-PasteIcon{background-image:var(--jp-icon-paste)}.jupyter-wrapper .jp-PythonIcon{background-image:var(--jp-icon-python)}.jupyter-wrapper .jp-RKernelIcon{background-image:var(--jp-icon-r-kernel)}.jupyter-wrapper .jp-ReactIcon{background-image:var(--jp-icon-react)}.jupyter-wrapper .jp-RefreshIcon{background-image:var(--jp-icon-refresh)}.jupyter-wrapper .jp-RegexIcon{background-image:var(--jp-icon-regex)}.jupyter-wrapper .jp-RunIcon{background-image:var(--jp-icon-run)}.jupyter-wrapper .jp-RunningIcon{background-image:var(--jp-icon-running)}.jupyter-wrapper .jp-SaveIcon{background-image:var(--jp-icon-save)}.jupyter-wrapper .jp-SearchIcon{background-image:var(--jp-icon-search)}.jupyter-wrapper .jp-SettingsIcon{background-image:var(--jp-icon-settings)}.jupyter-wrapper .jp-SpreadsheetIcon{background-image:var(--jp-icon-spreadsheet)}.jupyter-wrapper .jp-StopIcon{background-image:var(--jp-icon-stop)}.jupyter-wrapper .jp-TabIcon{background-image:var(--jp-icon-tab)}.jupyter-wrapper .jp-TerminalIcon{background-image:var(--jp-icon-terminal)}.jupyter-wrapper .jp-TextEditorIcon{background-image:var(--jp-icon-text-editor)}.jupyter-wrapper .jp-TrustedIcon{background-image:var(--jp-icon-trusted)}.jupyter-wrapper .jp-UndoIcon{background-image:var(--jp-icon-undo)}.jupyter-wrapper .jp-VegaIcon{background-image:var(--jp-icon-vega)}.jupyter-wrapper .jp-YamlIcon{background-image:var(--jp-icon-yaml)}.jupyter-wrapper :root{--jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==)}.jupyter-wrapper .jp-Icon,.jupyter-wrapper .jp-MaterialIcon{background-position:center;background-repeat:no-repeat;background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-cover{background-position:center;background-repeat:no-repeat;background-size:cover}.jupyter-wrapper .jp-Icon-16{background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-18{background-size:18px;min-width:18px;min-height:18px}.jupyter-wrapper .jp-Icon-20{background-size:20px;min-width:20px;min-height:20px}.jupyter-wrapper .jp-icon0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-accent0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-accent0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-none[fill]{fill:none}.jupyter-wrapper .jp-icon-none[stroke]{stroke:none}.jupyter-wrapper .jp-icon-brand0[fill]{fill:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[fill]{fill:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[fill]{fill:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[fill]{fill:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-brand0[stroke]{stroke:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[stroke]{stroke:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[stroke]{stroke:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[stroke]{stroke:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[stroke]{stroke:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-warn0[fill]{fill:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[fill]{fill:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[fill]{fill:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[fill]{fill:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-warn0[stroke]{stroke:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[stroke]{stroke:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[stroke]{stroke:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[stroke]{stroke:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-contrast0[fill]{fill:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[fill]{fill:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[fill]{fill:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[fill]{fill:var(--jp-icon-contrast-color3)}.jupyter-wrapper .jp-icon-contrast0[stroke]{stroke:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[stroke]{stroke:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[stroke]{stroke:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[stroke]{stroke:var(--jp-icon-contrast-color3)}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:#fff}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper :root{--jp-warn-color0: var(--md-orange-700)}.jupyter-wrapper .jp-DragIcon{margin-right:4px}.jupyter-wrapper .jp-icon-alt .jp-icon0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hoverShow:not(:hover) svg{display:none !important}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[fill]{fill:none}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[stroke]{stroke:none}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper :focus{outline:unset;outline-offset:unset;-moz-outline-radius:unset}.jupyter-wrapper .jp-Button{border-radius:var(--jp-border-radius);padding:0px 12px;font-size:var(--jp-ui-font-size1)}.jupyter-wrapper button.jp-Button.bp3-button.bp3-minimal:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Button.minimal{color:unset !important}.jupyter-wrapper .jp-Button.jp-ToolbarButtonComponent{text-transform:none}.jupyter-wrapper .jp-InputGroup input{box-sizing:border-box;border-radius:0;background-color:rgba(0,0,0,0);color:var(--jp-ui-font-color0);box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .jp-InputGroup input:focus{box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .jp-InputGroup input::placeholder,.jupyter-wrapper input::placeholder{color:var(--jp-ui-font-color3)}.jupyter-wrapper .jp-BPIcon{display:inline-block;vertical-align:middle;margin:auto}.jupyter-wrapper .bp3-icon.jp-BPIcon>svg:not([fill]){fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-InputGroupAction{padding:6px}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select{background-color:initial;border:none;border-radius:0;box-shadow:none;color:var(--jp-ui-font-color0);display:block;font-size:var(--jp-ui-font-size1);height:24px;line-height:14px;padding:0 25px 0 10px;text-align:left;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select:hover,.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select>option{background-color:var(--jp-layout-color2);color:var(--jp-ui-font-color0)}.jupyter-wrapper select{box-sizing:border-box}.jupyter-wrapper .jp-Collapse{display:flex;flex-direction:column;align-items:stretch;border-top:1px solid var(--jp-border-color2);border-bottom:1px solid var(--jp-border-color2)}.jupyter-wrapper .jp-Collapse-header{padding:1px 12px;color:var(--jp-ui-font-color1);background-color:var(--jp-layout-color1);font-size:var(--jp-ui-font-size2)}.jupyter-wrapper .jp-Collapse-header:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Collapse-contents{padding:0px 12px 0px 12px;background-color:var(--jp-layout-color1);color:var(--jp-ui-font-color1);overflow:auto}.jupyter-wrapper :root{--jp-private-commandpalette-search-height: 28px}.jupyter-wrapper .lm-CommandPalette{padding-bottom:0px;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-search{padding:4px;background-color:var(--jp-layout-color1);z-index:2}.jupyter-wrapper .lm-CommandPalette-wrapper{overflow:overlay;padding:0px 9px;background-color:var(--jp-input-active-background);height:30px;box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper{box-shadow:inset 0 0 0 1px var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .lm-CommandPalette-wrapper::after{content:\" \";color:#fff;background-color:var(--jp-brand-color1);position:absolute;top:4px;right:4px;height:30px;width:10px;padding:0px 10px;background-image:var(--jp-icon-search-white);background-size:20px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .lm-CommandPalette-input{background:rgba(0,0,0,0);width:calc(100% - 18px);float:left;border:none;outline:none;font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);line-height:var(--jp-private-commandpalette-search-height)}.jupyter-wrapper .lm-CommandPalette-input::-webkit-input-placeholder,.jupyter-wrapper .lm-CommandPalette-input::-moz-placeholder,.jupyter-wrapper .lm-CommandPalette-input:-ms-input-placeholder{color:var(--jp-ui-font-color3);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-header:first-child{margin-top:0px}.jupyter-wrapper .lm-CommandPalette-header{border-bottom:solid var(--jp-border-width) var(--jp-border-color2);color:var(--jp-ui-font-color1);cursor:pointer;display:flex;font-size:var(--jp-ui-font-size0);font-weight:600;letter-spacing:1px;margin-top:8px;padding:8px 0 8px 12px;text-transform:uppercase}.jupyter-wrapper .lm-CommandPalette-header.lm-mod-active{background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-header>mark{background-color:rgba(0,0,0,0);font-weight:bold;color:var(--jp-ui-font-color1)}.jupyter-wrapper .lm-CommandPalette-item{padding:4px 12px 4px 4px;color:var(--jp-ui-font-color1);font-size:var(--jp-ui-font-size1);font-weight:400;display:flex}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active{background:var(--jp-layout-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled){background:var(--jp-layout-color4)}.jupyter-wrapper .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled){background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-itemContent{overflow:hidden}.jupyter-wrapper .lm-CommandPalette-itemLabel>mark{color:var(--jp-ui-font-color0);background-color:rgba(0,0,0,0);font-weight:bold}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled mark{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemIcon{margin:0 4px 0 0;position:relative;width:16px;top:2px;flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon{opacity:.4}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-itemCaption{display:none}.jupyter-wrapper .lm-CommandPalette-content{background-color:var(--jp-layout-color1)}.jupyter-wrapper .lm-CommandPalette-content:empty:after{content:\"No results\";margin:auto;margin-top:20px;width:100px;display:block;font-size:var(--jp-ui-font-size2);font-family:var(--jp-ui-font-family);font-weight:lighter}.jupyter-wrapper .lm-CommandPalette-emptyMessage{text-align:center;margin-top:24px;line-height:1.32;padding:0px 8px;color:var(--jp-content-font-color3)}.jupyter-wrapper .jp-Dialog{position:absolute;z-index:10000;display:flex;flex-direction:column;align-items:center;justify-content:center;top:0px;left:0px;margin:0;padding:0;width:100%;height:100%;background:var(--jp-dialog-background)}.jupyter-wrapper .jp-Dialog-content{display:flex;flex-direction:column;margin-left:auto;margin-right:auto;background:var(--jp-layout-color1);padding:24px;padding-bottom:12px;min-width:300px;min-height:150px;max-width:1000px;max-height:500px;box-sizing:border-box;box-shadow:var(--jp-elevation-z20);word-wrap:break-word;border-radius:var(--jp-border-radius);font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color1)}.jupyter-wrapper .jp-Dialog-button{overflow:visible}.jupyter-wrapper button.jp-Dialog-button:focus{outline:1px solid var(--jp-brand-color1);outline-offset:4px;-moz-outline-radius:0px}.jupyter-wrapper button.jp-Dialog-button:focus::-moz-focus-inner{border:0}.jupyter-wrapper .jp-Dialog-header{flex:0 0 auto;padding-bottom:12px;font-size:var(--jp-ui-font-size3);font-weight:400;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-body{display:flex;flex-direction:column;flex:1 1 auto;font-size:var(--jp-ui-font-size1);background:var(--jp-layout-color1);overflow:auto}.jupyter-wrapper .jp-Dialog-footer{display:flex;flex-direction:row;justify-content:flex-end;flex:0 0 auto;margin-left:-12px;margin-right:-12px;padding:12px}.jupyter-wrapper .jp-Dialog-title{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .jp-Dialog-body>.jp-select-wrapper{width:100%}.jupyter-wrapper .jp-Dialog-body>button{padding:0px 16px}.jupyter-wrapper .jp-Dialog-body>label{line-height:1.4;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-button.jp-mod-styled:not(:last-child){margin-right:12px}.jupyter-wrapper .jp-HoverBox{position:fixed}.jupyter-wrapper .jp-HoverBox.jp-mod-outofview{display:none}.jupyter-wrapper .jp-IFrame{width:100%;height:100%}.jupyter-wrapper .jp-IFrame>iframe{border:none}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MainAreaWidget>:focus{outline:none}.jupyter-wrapper :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper .jp-Spinner{position:absolute;display:flex;justify-content:center;align-items:center;z-index:10;left:0;top:0;width:100%;height:100%;background:var(--jp-layout-color0);outline:none}.jupyter-wrapper .jp-SpinnerContent{font-size:10px;margin:50px auto;text-indent:-9999em;width:3em;height:3em;border-radius:50%;background:var(--jp-brand-color3);background:linear-gradient(to right, #f37626 10%, rgba(255, 255, 255, 0) 42%);position:relative;animation:load3 1s infinite linear,fadeIn 1s}.jupyter-wrapper .jp-SpinnerContent:before{width:50%;height:50%;background:#f37626;border-radius:100% 0 0 0;position:absolute;top:0;left:0;content:\"\"}.jupyter-wrapper .jp-SpinnerContent:after{background:var(--jp-layout-color0);width:75%;height:75%;border-radius:50%;content:\"\";margin:auto;position:absolute;top:0;left:0;bottom:0;right:0}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}@keyframes load3{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.jupyter-wrapper button.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:none;box-sizing:border-box;text-align:center;line-height:32px;height:32px;padding:0px 12px;letter-spacing:.8px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled{background:var(--jp-input-background);height:28px;box-sizing:border-box;border:var(--jp-border-width) solid var(--jp-border-color1);padding-left:7px;padding-right:7px;font-size:var(--jp-ui-font-size2);color:var(--jp-ui-font-color0);outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled:focus{border:var(--jp-border-width) solid var(--md-blue-500);box-shadow:inset 0 0 4px var(--md-blue-300)}.jupyter-wrapper .jp-select-wrapper{display:flex;position:relative;flex-direction:column;padding:1px;background-color:var(--jp-layout-color1);height:28px;box-sizing:border-box;margin-bottom:12px}.jupyter-wrapper .jp-select-wrapper.jp-mod-focused select.jp-mod-styled{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-input-active-background)}.jupyter-wrapper select.jp-mod-styled:hover{background-color:var(--jp-layout-color1);cursor:pointer;color:var(--jp-ui-font-color0);background-color:var(--jp-input-hover-background);box-shadow:inset 0 0px 1px rgba(0,0,0,.5)}.jupyter-wrapper select.jp-mod-styled{flex:1 1 auto;height:32px;width:100%;font-size:var(--jp-ui-font-size2);background:var(--jp-input-background);color:var(--jp-ui-font-color0);padding:0 25px 0 8px;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper :root{--jp-private-toolbar-height: calc( 28px + var(--jp-border-width) )}.jupyter-wrapper .jp-Toolbar{color:var(--jp-ui-font-color1);flex:0 0 auto;display:flex;flex-direction:row;border-bottom:var(--jp-border-width) solid var(--jp-toolbar-border-color);box-shadow:var(--jp-toolbar-box-shadow);background:var(--jp-toolbar-background);min-height:var(--jp-toolbar-micro-height);padding:2px;z-index:1}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item.jp-Toolbar-spacer{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-Toolbar-item.jp-Toolbar-kernelStatus{display:inline-block;width:32px;background-repeat:no-repeat;background-position:center;background-size:16px}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item{flex:0 0 auto;display:flex;padding-left:1px;padding-right:1px;font-size:var(--jp-ui-font-size1);line-height:var(--jp-private-toolbar-height);height:100%}.jupyter-wrapper div.jp-ToolbarButton{color:rgba(0,0,0,0);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px;margin:0px}.jupyter-wrapper button.jp-ToolbarButtonComponent{background:var(--jp-layout-color1);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px 6px;margin:0px;height:24px;border-radius:var(--jp-border-radius);display:flex;align-items:center;text-align:center;font-size:14px;min-width:unset;min-height:unset}.jupyter-wrapper button.jp-ToolbarButtonComponent:disabled{opacity:.4}.jupyter-wrapper button.jp-ToolbarButtonComponent span{padding:0px;flex:0 0 auto}.jupyter-wrapper button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label{font-size:var(--jp-ui-font-size1);line-height:100%;padding-left:2px;color:var(--jp-ui-font-color1)}.jupyter-wrapper body.p-mod-override-cursor *,.jupyter-wrapper body.lm-mod-override-cursor *{cursor:inherit !important}.jupyter-wrapper .jp-JSONEditor{display:flex;flex-direction:column;width:100%}.jupyter-wrapper .jp-JSONEditor-host{flex:1 1 auto;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;background:var(--jp-layout-color0);min-height:50px;padding:1px}.jupyter-wrapper .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host{border-color:red;outline-color:red}.jupyter-wrapper .jp-JSONEditor-header{display:flex;flex:1 0 auto;padding:0 0 0 12px}.jupyter-wrapper .jp-JSONEditor-header label{flex:0 0 auto}.jupyter-wrapper .jp-JSONEditor-commitButton{height:16px;width:16px;background-size:18px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .jp-JSONEditor-host.jp-mod-focused{background-color:var(--jp-input-active-background);border:1px solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .jp-Editor.jp-mod-dropTarget{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.jupyter-wrapper .CodeMirror-lines{padding:4px 0}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{padding:0 4px}.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{background-color:#fff}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.jupyter-wrapper .CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.jupyter-wrapper .CodeMirror-guttermarker{color:#000}.jupyter-wrapper .CodeMirror-guttermarker-subtle{color:#999}.jupyter-wrapper .CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.jupyter-wrapper .CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.jupyter-wrapper .cm-fat-cursor .CodeMirror-cursor{width:auto;border:0 !important;background:#7e7}.jupyter-wrapper .cm-fat-cursor div.CodeMirror-cursors{z-index:1}.jupyter-wrapper .cm-fat-cursor-mark{background-color:rgba(20,255,20,.5);-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite}.jupyter-wrapper .cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@-webkit-keyframes blink{50%{background-color:rgba(0,0,0,0)}}@keyframes blink{50%{background-color:rgba(0,0,0,0)}}.jupyter-wrapper .cm-tab{display:inline-block;text-decoration:inherit}.jupyter-wrapper .CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:0;overflow:hidden}.jupyter-wrapper .CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.jupyter-wrapper .cm-s-default .cm-header{color:blue}.jupyter-wrapper .cm-s-default .cm-quote{color:#090}.jupyter-wrapper .cm-negative{color:#d44}.jupyter-wrapper .cm-positive{color:#292}.jupyter-wrapper .cm-header,.jupyter-wrapper .cm-strong{font-weight:bold}.jupyter-wrapper .cm-em{font-style:italic}.jupyter-wrapper .cm-link{text-decoration:underline}.jupyter-wrapper .cm-strikethrough{text-decoration:line-through}.jupyter-wrapper .cm-s-default .cm-keyword{color:#708}.jupyter-wrapper .cm-s-default .cm-atom{color:#219}.jupyter-wrapper .cm-s-default .cm-number{color:#164}.jupyter-wrapper .cm-s-default .cm-def{color:blue}.jupyter-wrapper .cm-s-default .cm-variable-2{color:#05a}.jupyter-wrapper .cm-s-default .cm-variable-3,.jupyter-wrapper .cm-s-default .cm-type{color:#085}.jupyter-wrapper .cm-s-default .cm-comment{color:#a50}.jupyter-wrapper .cm-s-default .cm-string{color:#a11}.jupyter-wrapper .cm-s-default .cm-string-2{color:#f50}.jupyter-wrapper .cm-s-default .cm-meta{color:#555}.jupyter-wrapper .cm-s-default .cm-qualifier{color:#555}.jupyter-wrapper .cm-s-default .cm-builtin{color:#30a}.jupyter-wrapper .cm-s-default .cm-bracket{color:#997}.jupyter-wrapper .cm-s-default .cm-tag{color:#170}.jupyter-wrapper .cm-s-default .cm-attribute{color:#00c}.jupyter-wrapper .cm-s-default .cm-hr{color:#999}.jupyter-wrapper .cm-s-default .cm-link{color:#00c}.jupyter-wrapper .cm-s-default .cm-error{color:red}.jupyter-wrapper .cm-invalidchar{color:red}.jupyter-wrapper .CodeMirror-composing{border-bottom:2px solid}.jupyter-wrapper div.CodeMirror span.CodeMirror-matchingbracket{color:#0b0}.jupyter-wrapper div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#a22}.jupyter-wrapper .CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.jupyter-wrapper .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .CodeMirror{position:relative;overflow:hidden;background:#fff}.jupyter-wrapper .CodeMirror-scroll{overflow:scroll !important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:none;position:relative}.jupyter-wrapper .CodeMirror-sizer{position:relative;border-right:30px solid rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-vscrollbar,.jupyter-wrapper .CodeMirror-hscrollbar,.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{position:absolute;z-index:6;display:none}.jupyter-wrapper .CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.jupyter-wrapper .CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.jupyter-wrapper .CodeMirror-scrollbar-filler{right:0;bottom:0}.jupyter-wrapper .CodeMirror-gutter-filler{left:0;bottom:0}.jupyter-wrapper .CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.jupyter-wrapper .CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.jupyter-wrapper .CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:none !important;border:none !important}.jupyter-wrapper .CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.jupyter-wrapper .CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.jupyter-wrapper .CodeMirror-gutter-wrapper ::selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-gutter-wrapper ::-moz-selection{background-color:rgba(0,0,0,0)}.jupyter-wrapper .CodeMirror-lines{cursor:text;min-height:1px}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:rgba(0,0,0,0);font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:rgba(0,0,0,0);-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line,.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line-like{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.jupyter-wrapper .CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.jupyter-wrapper .CodeMirror-linewidget{position:relative;z-index:2;padding:.1px}.jupyter-wrapper .CodeMirror-rtl pre{direction:rtl}.jupyter-wrapper .CodeMirror-code{outline:none}.jupyter-wrapper .CodeMirror-scroll,.jupyter-wrapper .CodeMirror-sizer,.jupyter-wrapper .CodeMirror-gutter,.jupyter-wrapper .CodeMirror-gutters,.jupyter-wrapper .CodeMirror-linenumber{-moz-box-sizing:content-box;box-sizing:content-box}.jupyter-wrapper .CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.jupyter-wrapper .CodeMirror-cursor{position:absolute;pointer-events:none}.jupyter-wrapper .CodeMirror-measure pre{position:static}.jupyter-wrapper div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}.jupyter-wrapper div.CodeMirror-dragcursors{visibility:visible}.jupyter-wrapper .CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.jupyter-wrapper .CodeMirror-selected{background:#d9d9d9}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.jupyter-wrapper .CodeMirror-crosshair{cursor:crosshair}.jupyter-wrapper .CodeMirror-line::selection,.jupyter-wrapper .CodeMirror-line>span::selection,.jupyter-wrapper .CodeMirror-line>span>span::selection{background:#d7d4f0}.jupyter-wrapper .CodeMirror-line::-moz-selection,.jupyter-wrapper .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.jupyter-wrapper .cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.jupyter-wrapper .cm-force-border{padding-right:.1px}@media print{.jupyter-wrapper .CodeMirror div.CodeMirror-cursors{visibility:hidden}}.jupyter-wrapper .cm-tab-wrap-hack:after{content:\"\"}.jupyter-wrapper span.CodeMirror-selectedtext{background:none}.jupyter-wrapper .CodeMirror-dialog{position:absolute;left:0;right:0;background:inherit;z-index:15;padding:.1em .8em;overflow:hidden;color:inherit}.jupyter-wrapper .CodeMirror-dialog-top{border-bottom:1px solid #eee;top:0}.jupyter-wrapper .CodeMirror-dialog-bottom{border-top:1px solid #eee;bottom:0}.jupyter-wrapper .CodeMirror-dialog input{border:none;outline:none;background:rgba(0,0,0,0);width:20em;color:inherit;font-family:monospace}.jupyter-wrapper .CodeMirror-dialog button{font-size:70%}.jupyter-wrapper .CodeMirror-foldmarker{color:blue;text-shadow:#b9f 1px 1px 2px,#b9f -1px -1px 2px,#b9f 1px -1px 2px,#b9f -1px 1px 2px;font-family:arial;line-height:.3;cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter{width:.7em}.jupyter-wrapper .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter-open:after{content:\"\u25be\"}.jupyter-wrapper .CodeMirror-foldgutter-folded:after{content:\"\u25b8\"}.jupyter-wrapper .cm-s-material.CodeMirror{background-color:#263238;color:#eff}.jupyter-wrapper .cm-s-material .CodeMirror-gutters{background:#263238;color:#546e7a;border:none}.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker,.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker-subtle,.jupyter-wrapper .cm-s-material .CodeMirror-linenumber{color:#546e7a}.jupyter-wrapper .cm-s-material .CodeMirror-cursor{border-left:1px solid #fc0}.jupyter-wrapper .cm-s-material div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material.CodeMirror-focused div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::-moz-selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-activeline-background{background:rgba(0,0,0,.5)}.jupyter-wrapper .cm-s-material .cm-keyword{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-operator{color:#89ddff}.jupyter-wrapper .cm-s-material .cm-variable-2{color:#eff}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#f07178}.jupyter-wrapper .cm-s-material .cm-builtin{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-atom{color:#f78c6c}.jupyter-wrapper .cm-s-material .cm-number{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-def{color:#82aaff}.jupyter-wrapper .cm-s-material .cm-string{color:#c3e88d}.jupyter-wrapper .cm-s-material .cm-string-2{color:#f07178}.jupyter-wrapper .cm-s-material .cm-comment{color:#546e7a}.jupyter-wrapper .cm-s-material .cm-variable{color:#f07178}.jupyter-wrapper .cm-s-material .cm-tag{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-meta{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-attribute{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-property{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-qualifier{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-error{color:#fff;background-color:#ff5370}.jupyter-wrapper .cm-s-material .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-gutters{background:#3f3f3f !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{color:#999}.jupyter-wrapper .cm-s-zenburn .CodeMirror-cursor{border-left:1px solid #fff}.jupyter-wrapper .cm-s-zenburn{background-color:#3f3f3f;color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-builtin{color:#dcdccc;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-comment{color:#7f9f7f}.jupyter-wrapper .cm-s-zenburn span.cm-keyword{color:#f0dfaf;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-atom{color:#bfebbf}.jupyter-wrapper .cm-s-zenburn span.cm-def{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-variable{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-variable-2{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-string{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-string-2{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-number{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-tag{color:#93e0e3}.jupyter-wrapper .cm-s-zenburn span.cm-property{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-attribute{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-qualifier{color:#7cb8bb}.jupyter-wrapper .cm-s-zenburn span.cm-meta{color:#f0dfaf}.jupyter-wrapper .cm-s-zenburn span.cm-header{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.cm-operator{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-matchingbracket{box-sizing:border-box;background:rgba(0,0,0,0);border-bottom:1px solid}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-nonmatchingbracket{border-bottom:1px solid;background:none}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline{background:#000}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline-background{background:#000}.jupyter-wrapper .cm-s-zenburn div.CodeMirror-selected{background:#545454}.jupyter-wrapper .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected{background:#4f4f4f}.jupyter-wrapper .cm-s-abcdef.CodeMirror{background:#0f0f0f;color:#defdef}.jupyter-wrapper .cm-s-abcdef div.CodeMirror-selected{background:#515151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::-moz-selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-gutters{background:#555;border-right:2px solid #314151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker{color:#222}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker-subtle{color:azure}.jupyter-wrapper .cm-s-abcdef .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-abcdef .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-abcdef span.cm-keyword{color:#b8860b;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-atom{color:#77f}.jupyter-wrapper .cm-s-abcdef span.cm-number{color:violet}.jupyter-wrapper .cm-s-abcdef span.cm-def{color:#fffabc}.jupyter-wrapper .cm-s-abcdef span.cm-variable{color:#abcdef}.jupyter-wrapper .cm-s-abcdef span.cm-variable-2{color:#cacbcc}.jupyter-wrapper .cm-s-abcdef span.cm-variable-3,.jupyter-wrapper .cm-s-abcdef span.cm-type{color:#def}.jupyter-wrapper .cm-s-abcdef span.cm-property{color:#fedcba}.jupyter-wrapper .cm-s-abcdef span.cm-operator{color:#ff0}.jupyter-wrapper .cm-s-abcdef span.cm-comment{color:#7a7b7c;font-style:italic}.jupyter-wrapper .cm-s-abcdef span.cm-string{color:#2b4}.jupyter-wrapper .cm-s-abcdef span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-abcdef span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-abcdef span.cm-builtin{color:#30aabc}.jupyter-wrapper .cm-s-abcdef span.cm-bracket{color:#8a8a8a}.jupyter-wrapper .cm-s-abcdef span.cm-tag{color:#fd4}.jupyter-wrapper .cm-s-abcdef span.cm-attribute{color:#df0}.jupyter-wrapper .cm-s-abcdef span.cm-error{color:red}.jupyter-wrapper .cm-s-abcdef span.cm-header{color:#7fffd4;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-link{color:#8a2be2}.jupyter-wrapper .cm-s-abcdef .CodeMirror-activeline-background{background:#314151}.jupyter-wrapper .cm-s-base16-light.CodeMirror{background:#f5f5f5;color:#202020}.jupyter-wrapper .cm-s-base16-light div.CodeMirror-selected{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::-moz-selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-gutters{background:#f5f5f5;border-right:0px}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker-subtle{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-linenumber{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-cursor{border-left:1px solid #505050}.jupyter-wrapper .cm-s-base16-light span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-light span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-property,.jupyter-wrapper .cm-s-base16-light span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-light span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-light span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-light span.cm-bracket{color:#202020}.jupyter-wrapper .cm-s-base16-light span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-error{background:#ac4142;color:#505050}.jupyter-wrapper .cm-s-base16-light .CodeMirror-activeline-background{background:#dddcdc}.jupyter-wrapper .cm-s-base16-light .CodeMirror-matchingbracket{color:#f5f5f5 !important;background-color:#6a9fb5 !important}.jupyter-wrapper .cm-s-base16-dark.CodeMirror{background:#151515;color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark div.CodeMirror-selected{background:#303030}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-gutters{background:#151515;border-right:0px}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker-subtle{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-linenumber{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-cursor{border-left:1px solid #b0b0b0}.jupyter-wrapper .cm-s-base16-dark span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-dark span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-property,.jupyter-wrapper .cm-s-base16-dark span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-dark span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-dark span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-dark span.cm-bracket{color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-error{background:#ac4142;color:#b0b0b0}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-activeline-background{background:#202020}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-dracula.CodeMirror,.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{background-color:#282a36 !important;color:#f8f8f2 !important;border:none}.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{color:#282a36}.jupyter-wrapper .cm-s-dracula .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-dracula .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-dracula .CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula span.cm-comment{color:#6272a4}.jupyter-wrapper .cm-s-dracula span.cm-string,.jupyter-wrapper .cm-s-dracula span.cm-string-2{color:#f1fa8c}.jupyter-wrapper .cm-s-dracula span.cm-number{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-variable{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-2{color:#fff}.jupyter-wrapper .cm-s-dracula span.cm-def{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-operator{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-atom{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-meta{color:#f8f8f2}.jupyter-wrapper .cm-s-dracula span.cm-tag{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-attribute{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-qualifier{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-property{color:#66d9ef}.jupyter-wrapper .cm-s-dracula span.cm-builtin{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-3,.jupyter-wrapper .cm-s-dracula span.cm-type{color:#ffb86c}.jupyter-wrapper .cm-s-dracula .CodeMirror-activeline-background{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch.CodeMirror{background:#322931;color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch div.CodeMirror-selected{background:#433b42 !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-gutters{background:#322931;border-right:0px}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-linenumber{color:#797379}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-cursor{border-left:1px solid #989498 !important}.jupyter-wrapper .cm-s-hopscotch span.cm-comment{color:#b33508}.jupyter-wrapper .cm-s-hopscotch span.cm-atom{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-number{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-property,.jupyter-wrapper .cm-s-hopscotch span.cm-attribute{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-keyword{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-string{color:#fdcc59}.jupyter-wrapper .cm-s-hopscotch span.cm-variable{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-variable-2{color:#1290bf}.jupyter-wrapper .cm-s-hopscotch span.cm-def{color:#fd8b19}.jupyter-wrapper .cm-s-hopscotch span.cm-error{background:#dd464c;color:#989498}.jupyter-wrapper .cm-s-hopscotch span.cm-bracket{color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch span.cm-tag{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-link{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-activeline-background{background:#302020}.jupyter-wrapper .cm-s-mbo.CodeMirror{background:#2c2c2c;color:#ffffec}.jupyter-wrapper .cm-s-mbo div.CodeMirror-selected{background:#716c62}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::-moz-selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-gutters{background:#4e4e4e;border-right:0px}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker{color:#fff}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker-subtle{color:gray}.jupyter-wrapper .cm-s-mbo .CodeMirror-linenumber{color:#dadada}.jupyter-wrapper .cm-s-mbo .CodeMirror-cursor{border-left:1px solid #ffffec}.jupyter-wrapper .cm-s-mbo span.cm-comment{color:#95958a}.jupyter-wrapper .cm-s-mbo span.cm-atom{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-number{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-property,.jupyter-wrapper .cm-s-mbo span.cm-attribute{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-keyword{color:#ffb928}.jupyter-wrapper .cm-s-mbo span.cm-string{color:#ffcf6c}.jupyter-wrapper .cm-s-mbo span.cm-string.cm-property{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable-2{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-def{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-bracket{color:#fffffc;font-weight:bold}.jupyter-wrapper .cm-s-mbo span.cm-tag{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-link{color:#f54b07}.jupyter-wrapper .cm-s-mbo span.cm-error{border-bottom:#636363;color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-qualifier{color:#ffffec}.jupyter-wrapper .cm-s-mbo .CodeMirror-activeline-background{background:#494b41}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingbracket{color:#ffb928 !important}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingtag{background:rgba(255,255,255,.37)}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{color:#999;background-color:#fff}.jupyter-wrapper .cm-s-mdn-like div.CodeMirror-selected{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::-moz-selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-gutters{background:#f8f8f8;border-left:6px solid rgba(0,83,159,.65);color:#333}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-linenumber{color:#aaa;padding-left:8px}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-cursor{border-left:2px solid #222}.jupyter-wrapper .cm-s-mdn-like .cm-keyword{color:#6262ff}.jupyter-wrapper .cm-s-mdn-like .cm-atom{color:#f90}.jupyter-wrapper .cm-s-mdn-like .cm-number{color:#ca7841}.jupyter-wrapper .cm-s-mdn-like .cm-def{color:#8da6ce}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-2,.jupyter-wrapper .cm-s-mdn-like span.cm-tag{color:#690}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-3,.jupyter-wrapper .cm-s-mdn-like span.cm-def,.jupyter-wrapper .cm-s-mdn-like span.cm-type{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-variable{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-property{color:#905}.jupyter-wrapper .cm-s-mdn-like .cm-qualifier{color:#690}.jupyter-wrapper .cm-s-mdn-like .cm-operator{color:#cda869}.jupyter-wrapper .cm-s-mdn-like .cm-comment{color:#777;font-weight:normal}.jupyter-wrapper .cm-s-mdn-like .cm-string{color:#07a;font-style:italic}.jupyter-wrapper .cm-s-mdn-like .cm-string-2{color:#bd6b18}.jupyter-wrapper .cm-s-mdn-like .cm-meta{color:#000}.jupyter-wrapper .cm-s-mdn-like .cm-builtin{color:#9b7536}.jupyter-wrapper .cm-s-mdn-like .cm-tag{color:#997643}.jupyter-wrapper .cm-s-mdn-like .cm-attribute{color:#d6bb6d}.jupyter-wrapper .cm-s-mdn-like .cm-header{color:#ff6400}.jupyter-wrapper .cm-s-mdn-like .cm-hr{color:#aeaeae}.jupyter-wrapper .cm-s-mdn-like .cm-link{color:#ad9361;font-style:italic;text-decoration:none}.jupyter-wrapper .cm-s-mdn-like .cm-error{border-bottom:1px solid red}.jupyter-wrapper div.cm-s-mdn-like .CodeMirror-activeline-background{background:#efefff}.jupyter-wrapper div.cm-s-mdn-like span.CodeMirror-matchingbracket{outline:1px solid gray;color:inherit}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{background-image:url(data:image/png;base64,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)}.jupyter-wrapper .cm-s-seti.CodeMirror{background-color:#151718 !important;color:#cfd2d1 !important;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-gutters{color:#404b53;background-color:#0e1112;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-seti .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-seti.CodeMirror-focused div.CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti span.cm-comment{color:#41535b}.jupyter-wrapper .cm-s-seti span.cm-string,.jupyter-wrapper .cm-s-seti span.cm-string-2{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-number{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-variable{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-variable-2{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-def{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-seti span.cm-operator{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#e6cd69}.jupyter-wrapper .cm-s-seti span.cm-atom{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-meta{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-tag{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-attribute{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-qualifier{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-property{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-variable-3,.jupyter-wrapper .cm-s-seti span.cm-type{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-builtin{color:#9fca56}.jupyter-wrapper .cm-s-seti .CodeMirror-activeline-background{background:#101213}.jupyter-wrapper .cm-s-seti .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .solarized.base03{color:#002b36}.jupyter-wrapper .solarized.base02{color:#073642}.jupyter-wrapper .solarized.base01{color:#586e75}.jupyter-wrapper .solarized.base00{color:#657b83}.jupyter-wrapper .solarized.base0{color:#839496}.jupyter-wrapper .solarized.base1{color:#93a1a1}.jupyter-wrapper .solarized.base2{color:#eee8d5}.jupyter-wrapper .solarized.base3{color:#fdf6e3}.jupyter-wrapper .solarized.solar-yellow{color:#b58900}.jupyter-wrapper .solarized.solar-orange{color:#cb4b16}.jupyter-wrapper .solarized.solar-red{color:#dc322f}.jupyter-wrapper .solarized.solar-magenta{color:#d33682}.jupyter-wrapper .solarized.solar-violet{color:#6c71c4}.jupyter-wrapper .solarized.solar-blue{color:#268bd2}.jupyter-wrapper .solarized.solar-cyan{color:#2aa198}.jupyter-wrapper .solarized.solar-green{color:#859900}.jupyter-wrapper .cm-s-solarized{line-height:1.45em;color-profile:sRGB;rendering-intent:auto}.jupyter-wrapper .cm-s-solarized.cm-s-dark{color:#839496;background-color:#002b36;text-shadow:#002b36 0 1px}.jupyter-wrapper .cm-s-solarized.cm-s-light{background-color:#fdf6e3;color:#657b83;text-shadow:#eee8d5 0 1px}.jupyter-wrapper .cm-s-solarized .CodeMirror-widget{text-shadow:none}.jupyter-wrapper .cm-s-solarized .cm-header{color:#586e75}.jupyter-wrapper .cm-s-solarized .cm-quote{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-keyword{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .cm-atom{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-number{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-def{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-variable{color:#839496}.jupyter-wrapper .cm-s-solarized .cm-variable-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-variable-3,.jupyter-wrapper .cm-s-solarized .cm-type{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-property{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-operator{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-comment{color:#586e75;font-style:italic}.jupyter-wrapper .cm-s-solarized .cm-string{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-string-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-meta{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-qualifier{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-builtin{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-bracket{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-matchingbracket{color:#859900}.jupyter-wrapper .cm-s-solarized .CodeMirror-nonmatchingbracket{color:#dc322f}.jupyter-wrapper .cm-s-solarized .cm-tag{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-attribute{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-hr{color:rgba(0,0,0,0);border-top:1px solid #586e75;display:block}.jupyter-wrapper .cm-s-solarized .cm-link{color:#93a1a1;cursor:pointer}.jupyter-wrapper .cm-s-solarized .cm-special{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-em{color:#999;text-decoration:underline;text-decoration-style:dotted}.jupyter-wrapper .cm-s-solarized .cm-error,.jupyter-wrapper .cm-s-solarized .cm-invalidchar{color:#586e75;border-bottom:1px dotted #dc322f}.jupyter-wrapper .cm-s-solarized.cm-s-dark div.CodeMirror-selected{background:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark.CodeMirror ::selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-light div.CodeMirror-selected{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span>span::selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span>span::-moz-selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.CodeMirror{-moz-box-shadow:inset 7px 0 12px -6px #000;-webkit-box-shadow:inset 7px 0 12px -6px #000;box-shadow:inset 7px 0 12px -6px #000}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutters{border-right:0}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-gutters{background-color:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-linenumber{color:#586e75;text-shadow:#021014 0 -1px}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-gutters{background-color:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-linenumber{color:#839496}.jupyter-wrapper .cm-s-solarized .CodeMirror-linenumber{padding:0 5px}.jupyter-wrapper .cm-s-solarized .CodeMirror-guttermarker-subtle{color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker{color:#ddd}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-guttermarker{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text{color:#586e75}.jupyter-wrapper .cm-s-solarized .CodeMirror-cursor{border-left:1px solid #819090}.jupyter-wrapper .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor{background:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-light .cm-animate-fat-cursor{background-color:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor{background:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor{background-color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background{background:rgba(255,255,255,.06)}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-activeline-background{background:rgba(0,0,0,.06)}.jupyter-wrapper .cm-s-the-matrix.CodeMirror{background:#000;color:lime}.jupyter-wrapper .cm-s-the-matrix div.CodeMirror-selected{background:#2d2d2d}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::-moz-selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-gutters{background:#060;border-right:2px solid lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker{color:lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker-subtle{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-the-matrix span.cm-keyword{color:#008803;font-weight:bold}.jupyter-wrapper .cm-s-the-matrix span.cm-atom{color:#3ff}.jupyter-wrapper .cm-s-the-matrix span.cm-number{color:#ffb94f}.jupyter-wrapper .cm-s-the-matrix span.cm-def{color:#99c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable{color:#f6c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-2{color:#c6f}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-3,.jupyter-wrapper .cm-s-the-matrix span.cm-type{color:#96f}.jupyter-wrapper .cm-s-the-matrix span.cm-property{color:#62ffa0}.jupyter-wrapper .cm-s-the-matrix span.cm-operator{color:#999}.jupyter-wrapper .cm-s-the-matrix span.cm-comment{color:#ccc}.jupyter-wrapper .cm-s-the-matrix span.cm-string{color:#39c}.jupyter-wrapper .cm-s-the-matrix span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-the-matrix span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-builtin{color:#30a}.jupyter-wrapper .cm-s-the-matrix span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-the-matrix span.cm-tag{color:#ffbd40}.jupyter-wrapper .cm-s-the-matrix span.cm-attribute{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-error{color:red}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-activeline-background{background:#040}.jupyter-wrapper .cm-s-xq-light span.cm-keyword{line-height:1em;font-weight:bold;color:#5a5cad}.jupyter-wrapper .cm-s-xq-light span.cm-atom{color:#6c8cd5}.jupyter-wrapper .cm-s-xq-light span.cm-number{color:#164}.jupyter-wrapper .cm-s-xq-light span.cm-def{text-decoration:underline}.jupyter-wrapper .cm-s-xq-light span.cm-variable{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-2{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-3,.jupyter-wrapper .cm-s-xq-light span.cm-type{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-comment{color:#0080ff;font-style:italic}.jupyter-wrapper .cm-s-xq-light span.cm-string{color:red}.jupyter-wrapper .cm-s-xq-light span.cm-meta{color:#ff0}.jupyter-wrapper .cm-s-xq-light span.cm-qualifier{color:gray}.jupyter-wrapper .cm-s-xq-light span.cm-builtin{color:#7ea656}.jupyter-wrapper .cm-s-xq-light span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-xq-light span.cm-tag{color:#3f7f7f}.jupyter-wrapper .cm-s-xq-light span.cm-attribute{color:#7f007f}.jupyter-wrapper .cm-s-xq-light span.cm-error{color:red}.jupyter-wrapper .cm-s-xq-light .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .cm-s-xq-light .CodeMirror-matchingbracket{outline:1px solid gray;color:#000 !important;background:#ff0}.jupyter-wrapper .CodeMirror{line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);border:0;border-radius:0;height:auto}.jupyter-wrapper .CodeMirror pre{padding:0 var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-dialog{background-color:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .CodeMirror-lines{padding:var(--jp-code-padding) 0}.jupyter-wrapper .CodeMirror-linenumber{padding:0 8px}.jupyter-wrapper .jp-CodeMirrorEditor-static{margin:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor,.jupyter-wrapper .jp-CodeMirrorEditor-static{cursor:text}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}@media screen and (min-width: 2138px)and (max-width: 4319px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color)}}@media screen and (min-width: 4320px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color)}}.jupyter-wrapper .CodeMirror.jp-mod-readOnly .CodeMirror-cursor{display:none}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid var(--jp-border-color2);background-color:var(--jp-layout-color0)}.jupyter-wrapper .jp-CollaboratorCursor{border-left:5px solid rgba(0,0,0,0);border-right:5px solid rgba(0,0,0,0);border-top:none;border-bottom:3px solid;background-clip:content-box;margin-left:-5px;margin-right:-5px}.jupyter-wrapper .CodeMirror-selectedtext.cm-searching{background-color:var(--jp-search-selected-match-background-color) !important;color:var(--jp-search-selected-match-color) !important}.jupyter-wrapper .cm-searching{background-color:var(--jp-search-unselected-match-background-color) !important;color:var(--jp-search-unselected-match-color) !important}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background-color:var(--jp-editor-selected-focused-background)}.jupyter-wrapper .CodeMirror-selected{background-color:var(--jp-editor-selected-background)}.jupyter-wrapper .jp-CollaboratorCursor-hover{position:absolute;z-index:1;transform:translateX(-50%);color:#fff;border-radius:3px;padding-left:4px;padding-right:4px;padding-top:1px;padding-bottom:1px;text-align:center;font-size:var(--jp-ui-font-size1);white-space:nowrap}.jupyter-wrapper .jp-CodeMirror-ruler{border-left:1px dashed var(--jp-border-color2)}.jupyter-wrapper .CodeMirror.cm-s-jupyter{background:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .jp-CodeConsole .CodeMirror.cm-s-jupyter,.jupyter-wrapper .jp-Notebook .CodeMirror.cm-s-jupyter{background:rgba(0,0,0,0)}.jupyter-wrapper .cm-s-jupyter .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}.jupyter-wrapper .cm-s-jupyter span.cm-keyword{color:var(--jp-mirror-editor-keyword-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-atom{color:var(--jp-mirror-editor-atom-color)}.jupyter-wrapper .cm-s-jupyter span.cm-number{color:var(--jp-mirror-editor-number-color)}.jupyter-wrapper .cm-s-jupyter span.cm-def{color:var(--jp-mirror-editor-def-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable{color:var(--jp-mirror-editor-variable-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-2{color:var(--jp-mirror-editor-variable-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-3{color:var(--jp-mirror-editor-variable-3-color)}.jupyter-wrapper .cm-s-jupyter span.cm-punctuation{color:var(--jp-mirror-editor-punctuation-color)}.jupyter-wrapper .cm-s-jupyter span.cm-property{color:var(--jp-mirror-editor-property-color)}.jupyter-wrapper .cm-s-jupyter span.cm-operator{color:var(--jp-mirror-editor-operator-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-comment{color:var(--jp-mirror-editor-comment-color);font-style:italic}.jupyter-wrapper .cm-s-jupyter span.cm-string{color:var(--jp-mirror-editor-string-color)}.jupyter-wrapper .cm-s-jupyter span.cm-string-2{color:var(--jp-mirror-editor-string-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-meta{color:var(--jp-mirror-editor-meta-color)}.jupyter-wrapper .cm-s-jupyter span.cm-qualifier{color:var(--jp-mirror-editor-qualifier-color)}.jupyter-wrapper .cm-s-jupyter span.cm-builtin{color:var(--jp-mirror-editor-builtin-color)}.jupyter-wrapper .cm-s-jupyter span.cm-bracket{color:var(--jp-mirror-editor-bracket-color)}.jupyter-wrapper .cm-s-jupyter span.cm-tag{color:var(--jp-mirror-editor-tag-color)}.jupyter-wrapper .cm-s-jupyter span.cm-attribute{color:var(--jp-mirror-editor-attribute-color)}.jupyter-wrapper .cm-s-jupyter span.cm-header{color:var(--jp-mirror-editor-header-color)}.jupyter-wrapper .cm-s-jupyter span.cm-quote{color:var(--jp-mirror-editor-quote-color)}.jupyter-wrapper .cm-s-jupyter span.cm-link{color:var(--jp-mirror-editor-link-color)}.jupyter-wrapper .cm-s-jupyter span.cm-error{color:var(--jp-mirror-editor-error-color)}.jupyter-wrapper .cm-s-jupyter span.cm-hr{color:#999}.jupyter-wrapper .cm-s-jupyter span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}.jupyter-wrapper .cm-s-jupyter .CodeMirror-activeline-background,.jupyter-wrapper .cm-s-jupyter .CodeMirror-gutter{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-RenderedLatex{color:var(--jp-content-font-color1);font-size:var(--jp-content-font-size1);line-height:var(--jp-content-line-height)}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedLatex{padding:var(--jp-code-padding);text-align:left}.jupyter-wrapper .jp-MimeDocument{outline:none}.jupyter-wrapper :root{--jp-private-filebrowser-button-height: 28px;--jp-private-filebrowser-button-width: 48px}.jupyter-wrapper .jp-FileBrowser{display:flex;flex-direction:column;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{border-bottom:none;height:auto;margin:var(--jp-toolbar-header-margin);box-shadow:none}.jupyter-wrapper .jp-BreadCrumbs{flex:0 0 auto;margin:4px 12px}.jupyter-wrapper .jp-BreadCrumbs-item{margin:0px 2px;padding:0px 2px;border-radius:var(--jp-border-radius);cursor:pointer}.jupyter-wrapper .jp-BreadCrumbs-item:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-BreadCrumbs-item:first-child{margin-left:0px}.jupyter-wrapper .jp-BreadCrumbs-item.jp-mod-dropTarget{background-color:var(--jp-brand-color2);opacity:.7}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{padding:0px}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{justify-content:space-evenly}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item{flex:1}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent{width:100%}.jupyter-wrapper .jp-DirListing{flex:1 1 auto;display:flex;flex-direction:column;outline:0}.jupyter-wrapper .jp-DirListing-header{flex:0 0 auto;display:flex;flex-direction:row;overflow:hidden;border-top:var(--jp-border-width) solid var(--jp-border-color2);border-bottom:var(--jp-border-width) solid var(--jp-border-color1);box-shadow:var(--jp-toolbar-box-shadow);z-index:2}.jupyter-wrapper .jp-DirListing-headerItem{padding:4px 12px 2px 12px;font-weight:500}.jupyter-wrapper .jp-DirListing-headerItem:hover{background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-name{flex:1 0 84px}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-modified{flex:0 0 112px;border-left:var(--jp-border-width) solid var(--jp-border-color2);text-align:right}.jupyter-wrapper .jp-DirListing-narrow .jp-id-modified,.jupyter-wrapper .jp-DirListing-narrow .jp-DirListing-itemModified{display:none}.jupyter-wrapper .jp-DirListing-headerItem.jp-mod-selected{font-weight:600}.jupyter-wrapper .jp-DirListing-content{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-DirListing.jp-mod-native-drop .jp-DirListing-content{outline:5px dashed rgba(128,128,128,.5);outline-offset:-10px;cursor:copy}.jupyter-wrapper .jp-DirListing-item{display:flex;flex-direction:row;padding:4px 12px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected{color:#fff;background:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-dropTarget{background:var(--jp-brand-color3)}.jupyter-wrapper .jp-DirListing-item:hover:not(.jp-mod-selected){background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-itemIcon{flex:0 0 20px;margin-right:4px}.jupyter-wrapper .jp-DirListing-itemText{flex:1 0 64px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;user-select:none}.jupyter-wrapper .jp-DirListing-itemModified{flex:0 0 125px;text-align:right}.jupyter-wrapper .jp-DirListing-editor{flex:1 0 64px;outline:none;border:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before{color:#32cd32;content:\"\u25cf\";font-size:8px;position:absolute;left:-8px}.jupyter-wrapper .jp-DirListing-item.lm-mod-drag-image,.jupyter-wrapper .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image{font-size:var(--jp-ui-font-size1);padding-left:4px;margin-left:4px;width:160px;background-color:var(--jp-ui-inverse-font-color2);box-shadow:var(--jp-elevation-z2);border-radius:0px;color:var(--jp-ui-font-color1);transform:translateX(-40%) translateY(-58%)}.jupyter-wrapper .jp-DirListing-deadSpace{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-Document{min-width:120px;min-height:120px;outline:none}.jupyter-wrapper .jp-FileDialog.jp-mod-conflict input{color:red}.jupyter-wrapper .jp-FileDialog .jp-new-name-title{margin-top:12px}.jupyter-wrapper .jp-OutputArea{overflow-y:auto}.jupyter-wrapper .jp-OutputArea-child{display:flex;flex-direction:row}.jupyter-wrapper .jp-OutputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-outprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-OutputArea-output{height:auto;overflow:auto;user-select:text;-moz-user-select:text;-webkit-user-select:text;-ms-user-select:text}.jupyter-wrapper .jp-OutputArea-child .jp-OutputArea-output{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-OutputArea-output.jp-mod-isolated{width:100%;display:block}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-OutputArea-output pre{border:none;margin:0px;padding:0px;overflow-x:auto;overflow-y:auto;word-break:break-all;word-wrap:break-word;white-space:pre-wrap}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedHTMLCommon table{margin-left:0;margin-right:0}.jupyter-wrapper .jp-OutputArea-output dl,.jupyter-wrapper .jp-OutputArea-output dt,.jupyter-wrapper .jp-OutputArea-output dd{display:block}.jupyter-wrapper .jp-OutputArea-output dl{width:100%;overflow:hidden;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dt{font-weight:bold;float:left;width:20%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dd{float:left;width:80%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt{display:none}.jupyter-wrapper .jp-OutputArea-output.jp-OutputArea-executeResult{margin-left:0px;flex:1 1 auto}.jupyter-wrapper .jp-OutputArea-executeResult.jp-RenderedText{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-OutputArea-stdin{line-height:var(--jp-code-line-height);padding-top:var(--jp-code-padding);display:flex}.jupyter-wrapper .jp-Stdin-prompt{color:var(--jp-content-font-color0);padding-right:var(--jp-code-padding);vertical-align:baseline;flex:0 0 auto}.jupyter-wrapper .jp-Stdin-input{font-family:var(--jp-code-font-family);font-size:inherit;color:inherit;background-color:inherit;width:42%;min-width:200px;vertical-align:baseline;padding:0em .25em;margin:0em .25em;flex:0 0 70%}.jupyter-wrapper .jp-Stdin-input:focus{box-shadow:none}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea{height:100%;display:block}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea-output:only-child{height:100%}.jupyter-wrapper .jp-Collapser{flex:0 0 var(--jp-cell-collapser-width);padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0);border-radius:var(--jp-border-radius);opacity:1}.jupyter-wrapper .jp-Collapser-child{display:block;width:100%;box-sizing:border-box;position:absolute;top:0px;bottom:0px}.jupyter-wrapper .jp-CellHeader,.jupyter-wrapper .jp-CellFooter{height:0px;width:100%;padding:0px;margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-InputArea{display:flex;flex-direction:row}.jupyter-wrapper .jp-InputArea-editor{flex:1 1 auto}.jupyter-wrapper .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);border-radius:0px;background:var(--jp-cell-editor-background)}.jupyter-wrapper .jp-InputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-inprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);opacity:var(--jp-cell-prompt-opacity);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0);opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-Placeholder{display:flex;flex-direction:row;flex:1 1 auto}.jupyter-wrapper .jp-Placeholder-prompt{box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content{flex:1 1 auto;border:none;background:rgba(0,0,0,0);height:20px;box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon{width:32px;height:16px;border:1px solid rgba(0,0,0,0);border-radius:var(--jp-border-radius)}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon:hover{border:1px solid var(--jp-border-color1);box-shadow:0px 0px 2px 0px rgba(0,0,0,.25);background-color:var(--jp-layout-color0)}.jupyter-wrapper :root{--jp-private-cell-scrolling-output-offset: 5px}.jupyter-wrapper .jp-Cell{padding:var(--jp-cell-padding);margin:0px;border:none;outline:none;background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Cell-inputWrapper,.jupyter-wrapper .jp-Cell-outputWrapper{display:flex;flex-direction:row;padding:0px;margin:0px;overflow:visible}.jupyter-wrapper .jp-Cell-inputArea,.jupyter-wrapper .jp-Cell-outputArea{flex:1 1 auto}.jupyter-wrapper .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser{border:none !important;background:rgba(0,0,0,0) !important}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser{min-height:var(--jp-cell-collapser-min-height)}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper{margin-top:5px}.jupyter-wrapper .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea{overflow-y:auto;max-height:200px;box-shadow:inset 0 0 6px 2px rgba(0,0,0,.3);margin-left:var(--jp-private-cell-scrolling-output-offset)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt{flex:0 0 calc(var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset))}.jupyter-wrapper .jp-MarkdownOutput{flex:1 1 auto;margin-top:0;margin-bottom:0;padding-left:var(--jp-code-padding)}.jupyter-wrapper .jp-MarkdownOutput.jp-RenderedHTMLCommon{overflow:auto}.jupyter-wrapper .jp-NotebookPanel-toolbar{padding:2px}.jupyter-wrapper .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused{border:none;box-shadow:none}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown select{height:24px;font-size:var(--jp-ui-font-size1);line-height:14px;border-radius:0;display:block}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown span{top:5px !important}.jupyter-wrapper :root{--jp-private-notebook-dragImage-width: 304px;--jp-private-notebook-dragImage-height: 36px;--jp-private-notebook-selected-color: var(--md-blue-400);--jp-private-notebook-active-color: var(--md-green-400)}.jupyter-wrapper .jp-NotebookPanel{display:block;height:100%}.jupyter-wrapper .jp-NotebookPanel.jp-Document{min-width:240px;min-height:120px}.jupyter-wrapper .jp-Notebook{padding:var(--jp-notebook-padding);outline:none;overflow:auto;background:var(--jp-layout-color0)}.jupyter-wrapper .jp-Notebook.jp-mod-scrollPastEnd::after{display:block;content:\"\";min-height:var(--jp-notebook-scroll-padding)}.jupyter-wrapper .jp-Notebook .jp-Cell{overflow:visible}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-InputPrompt{cursor:move}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser{background:var(--jp-brand-color1)}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-Collapser:hover{box-shadow:var(--jp-elevation-z2);background:var(--jp-brand-color1);opacity:var(--jp-cell-collapser-not-active-hover-opacity)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover{background:var(--jp-brand-color0);opacity:1}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected{background:var(--jp-notebook-multiselected-color)}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected){background:rgba(0,0,0,0)}.jupyter-wrapper .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-cell-editor-active-background)}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropSource{opacity:.5}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropTarget,.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget{border-top-color:var(--jp-private-notebook-selected-color);border-top-style:solid;border-top-width:2px}.jupyter-wrapper .jp-dragImage{display:flex;flex-direction:row;width:var(--jp-private-notebook-dragImage-width);height:var(--jp-private-notebook-dragImage-height);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background);overflow:visible}.jupyter-wrapper .jp-dragImage-singlePrompt{box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-dragImage .jp-dragImage-content{flex:1 1 auto;z-index:2;font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);line-height:var(--jp-code-line-height);padding:var(--jp-code-padding);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background-color);color:var(--jp-content-font-color3);text-align:left;margin:4px 4px 4px 0px}.jupyter-wrapper .jp-dragImage .jp-dragImage-prompt{flex:0 0 auto;min-width:36px;color:var(--jp-cell-inprompt-font-color);padding:var(--jp-code-padding);padding-left:12px;font-family:var(--jp-cell-prompt-font-family);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:1.9;font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid rgba(0,0,0,0)}.jupyter-wrapper .jp-dragImage-multipleBack{z-index:-1;position:absolute;height:32px;width:300px;top:8px;left:8px;background:var(--jp-layout-color2);border:var(--jp-border-width) solid var(--jp-input-border-color);box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-NotebookTools{display:block;min-width:var(--jp-sidebar-min-width);color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1);overflow:auto}.jupyter-wrapper .jp-NotebookTools-tool{padding:0px 12px 0 12px}.jupyter-wrapper .jp-ActiveCellTool{padding:12px;background-color:var(--jp-layout-color1);border-top:none !important}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-prompt{flex:0 0 auto;padding-left:0px}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor{flex:1 1 auto;background:var(--jp-cell-editor-background);border-color:var(--jp-cell-editor-border-color)}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror{background:rgba(0,0,0,0)}.jupyter-wrapper .jp-MetadataEditorTool{flex-direction:column;padding:12px 0px 12px 0px}.jupyter-wrapper .jp-RankedPanel>:not(:first-child){margin-top:12px}.jupyter-wrapper .jp-KeySelector select.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:var(--jp-border-width) solid var(--jp-border-color1)}.jupyter-wrapper .jp-KeySelector label,.jupyter-wrapper .jp-MetadataEditorTool label{line-height:1.4}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook{--jp-content-font-size1: var(--jp-content-presentation-font-size1);--jp-code-font-size: var(--jp-code-presentation-font-size)}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt{flex:0 0 110px}.jupyter-wrapper .md-typeset__scrollwrap{margin:0}.jupyter-wrapper .jp-MarkdownOutput{padding:0}.jupyter-wrapper h1 .anchor-link,.jupyter-wrapper h2 .anchor-link,.jupyter-wrapper h3 .anchor-link,.jupyter-wrapper h4 .anchor-link,.jupyter-wrapper h5 .anchor-link,.jupyter-wrapper h6 .anchor-link{display:none;margin-left:.5rem;color:var(--md-default-fg-color--lighter)}.jupyter-wrapper h1 .anchor-link:hover,.jupyter-wrapper h2 .anchor-link:hover,.jupyter-wrapper h3 .anchor-link:hover,.jupyter-wrapper h4 .anchor-link:hover,.jupyter-wrapper h5 .anchor-link:hover,.jupyter-wrapper h6 .anchor-link:hover{text-decoration:none;color:var(--md-accent-fg-color)}.jupyter-wrapper h1:hover .anchor-link,.jupyter-wrapper h2:hover .anchor-link,.jupyter-wrapper h3:hover .anchor-link,.jupyter-wrapper h4:hover .anchor-link,.jupyter-wrapper h5:hover .anchor-link,.jupyter-wrapper h6:hover .anchor-link{display:inline-block}.jupyter-wrapper .jp-InputArea{width:100%}.jupyter-wrapper .jp-Cell-inputArea{width:100%}.jupyter-wrapper .jp-RenderedHTMLCommon{width:100%}.jupyter-wrapper .jp-Cell-inputWrapper .jp-InputPrompt{display:none}.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt{display:block}.jupyter-wrapper .highlight pre{overflow:auto}.jupyter-wrapper .celltoolbar{border:none;background:#eee;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;box-pack:end;justify-content:flex-start;display:-webkit-flex}.jupyter-wrapper .celltoolbar .tags_button_container{display:flex}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container{display:flex;flex-direction:row;flex-grow:1;overflow:hidden;position:relative}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;box-shadow:none;width:inherit;font-size:11px;font-family:\"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace;height:22px;display:inline-block}.jupyter-wrapper .jp-InputArea-editor{width:1px}.jupyter-wrapper .jp-InputPrompt{overflow:unset}.jupyter-wrapper .jp-OutputPrompt{overflow:unset}.jupyter-wrapper .jp-RenderedText{font-size:var(--jp-code-font-size)}.jupyter-wrapper .highlight-ipynb{overflow:auto}.jupyter-wrapper .highlight-ipynb pre{margin:0;padding:5px 10px}.jupyter-wrapper table{width:max-content}.jupyter-wrapper table.dataframe{margin-left:auto;margin-right:auto;border:none;border-collapse:collapse;border-spacing:0;color:#000;font-size:12px;table-layout:fixed}.jupyter-wrapper table.dataframe thead{border-bottom:1px solid #000;vertical-align:bottom}.jupyter-wrapper table.dataframe tr,.jupyter-wrapper table.dataframe th,.jupyter-wrapper table.dataframe td{text-align:right;vertical-align:middle;padding:.5em .5em;line-height:normal;white-space:normal;max-width:none;border:none}.jupyter-wrapper table.dataframe th{font-weight:bold}.jupyter-wrapper table.dataframe tbody tr:nth-child(odd){background:#f5f5f5}.jupyter-wrapper table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}.jupyter-wrapper *+table{margin-top:1em}.jupyter-wrapper .jp-InputArea-editor{position:relative}.jupyter-wrapper .zeroclipboard-container{position:absolute;top:-3px;right:0;z-index:1000}.jupyter-wrapper .zeroclipboard-container clipboard-copy{-webkit-appearance:button;-moz-appearance:button;padding:7px 5px;font:11px system-ui,sans-serif;display:inline-block;cursor:default}.jupyter-wrapper .zeroclipboard-container .clipboard-copy-icon{padding:4px 4px 2px;color:#57606a;vertical-align:text-bottom}.jupyter-wrapper .clipboard-copy-txt{display:none}[data-md-color-scheme=slate] .clipboard-copy-icon{color:#fff !important}[data-md-color-scheme=slate] table.dataframe{color:#e9ebfc}[data-md-color-scheme=slate] table.dataframe thead{border-bottom:1px solid rgba(233,235,252,.12)}[data-md-color-scheme=slate] table.dataframe tbody tr:nth-child(odd){background:#222}[data-md-color-scheme=slate] table.dataframe tbody tr:hover{background:rgba(66,165,245,.2)}table{width:max-content} /*# sourceMappingURL=mkdocs-jupyter.css.map*/ init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Nvidia GPU INT-8 quantization on any transformers model (encoder based) \u00b6 For some context and explanations, please check our documentation here: https://els-rd.github.io/transformer-deploy/quantization/quantization_intro/ . Project setup \u00b6 Dependencies installation \u00b6 Your machine should have Nvidia CUDA 11.X, TensorRT 8.2.1 and cuBLAS installed. It's said to be tricky to install, in my experience, just follow Nvidia download page instructions and nothing else , it should work out of the box. Nvidia Docker image could be a good choice too. In [1]: Copied! #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com Collecting datasets Downloading datasets-1.18.4-py3-none-any.whl (312 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 312 kB 12.2 MB/s eta 0:00:01 Collecting sklearn Downloading sklearn-0.0.tar.gz (1.1 kB) Collecting dill Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86 kB 70.0 MB/s eta 0:00:01 Requirement already satisfied: requests>=2.19.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (2.27.1) Requirement already satisfied: numpy>=1.17 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (1.21.5) Requirement already satisfied: packaging in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (21.3) Collecting pyarrow!=4.0.0,>=3.0.0 Downloading pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.7 MB 20.6 MB/s eta 0:00:01 Collecting xxhash Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 211 kB 59.3 MB/s eta 0:00:01 Collecting fsspec[http]>=2021.05.0 Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 134 kB 73.4 MB/s eta 0:00:01 Collecting responses<0.19 Downloading responses-0.18.0-py3-none-any.whl (38 kB) Collecting pandas Downloading pandas-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.7 MB 90.8 MB/s eta 0:00:01 Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (0.4.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 100.2 MB/s eta 0:00:01 Collecting multiprocess Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 78.2 MB/s eta 0:00:01 Requirement already satisfied: tqdm>=4.62.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (4.62.3) Requirement already satisfied: pyyaml in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0) Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1) Requirement already satisfied: filelock in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from packaging->datasets) (3.0.7) Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.8) Requirement already satisfied: certifi>=2017.4.17 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8) Requirement already satisfied: idna<4,>=2.5 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3) Requirement already satisfied: charset-normalizer~=2.0.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.11) Requirement already satisfied: scikit-learn in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from sklearn) (1.0.2) Requirement already satisfied: attrs>=17.3.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0) Collecting multidict<7.0,>=4.5 Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 114 kB 96.7 MB/s eta 0:00:01 Collecting aiosignal>=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting frozenlist>=1.1.1 Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 156 kB 86.6 MB/s eta 0:00:01 Collecting yarl<2.0,>=1.0 Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 304 kB 95.1 MB/s eta 0:00:01 Collecting async-timeout<5.0,>=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Requirement already satisfied: python-dateutil>=2.8.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from pandas->datasets) (2.8.2) Collecting pytz>=2020.1 Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 503 kB 68.0 MB/s eta 0:00:01 Requirement already satisfied: six>=1.5 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0) Requirement already satisfied: joblib>=0.11 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0) Requirement already satisfied: scipy>=1.1.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0) Requirement already satisfied: threadpoolctl>=2.0.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0) Building wheels for collected packages: sklearn Building wheel for sklearn (setup.py) ... done Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=3ac0638e4032964c7ba9d014645ce13935d501fa40fff5bf77431ac4cffa5e14 Stored in directory: /tmp/pip-ephem-wheel-cache-m57vxdol/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c Successfully built sklearn Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, pytz, fsspec, dill, aiohttp, xxhash, responses, pyarrow, pandas, multiprocess, sklearn, datasets Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-1.18.4 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 multiprocess-0.70.12.2 pandas-1.4.1 pyarrow-7.0.0 pytz-2021.3 responses-0.18.0 sklearn-0.0 xxhash-3.0.0 yarl-1.7.2 WARNING: You are using pip version 21.1.2; however, version 22.0.4 is available. You should consider upgrading via the '/home/geantvert/.local/share/virtualenvs/fast_transformer/bin/python -m pip install --upgrade pip' command. Check the GPU is enabled and usable. In [2]: Copied! ! nvidia - smi ! nvidia-smi Wed Mar 9 19:14:33 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 48% 46C P8 41W / 350W | 366MiB / 24576MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1895 G /usr/lib/xorg/Xorg 182MiB | | 0 N/A N/A 7706 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 8916 G ...on/Bin/AgentConnectix.bin 4MiB | | 0 N/A N/A 10507 G ...884829189831830011,131072 53MiB | | 0 N/A N/A 1025897 G ...AAAAAAAAA= --shared-files 39MiB | | 0 N/A N/A 3867171 G ...947156.log --shared-files 37MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import os from collections import OrderedDict from typing import Dict , List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import tensorrt as trt import torch import transformers from datasets import load_dataset , load_metric from tensorrt.tensorrt import IExecutionContext , Logger , Runtime from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , IntervalStrategy , PreTrainedModel , PreTrainedTokenizer , Trainer , TrainingArguments , ) from transformer_deploy.backends.ort_utils import ( cpu_quantization , create_model_for_provider , optimize_onnx , ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine , get_binding_idxs , infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings , track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate import logging import os from collections import OrderedDict from typing import Dict, List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import tensorrt as trt import torch import transformers from datasets import load_dataset, load_metric from tensorrt.tensorrt import IExecutionContext, Logger, Runtime from transformers import ( AutoModelForSequenceClassification, AutoTokenizer, IntervalStrategy, PreTrainedModel, PreTrainedTokenizer, Trainer, TrainingArguments, ) from transformer_deploy.backends.ort_utils import ( cpu_quantization, create_model_for_provider, optimize_onnx, ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings, track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate Set logging to error level to ease readability of this notebook on Github. In [4]: Copied! log_level = logging . ERROR logging . getLogger () . setLevel ( log_level ) datasets . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . enable_default_handler () transformers . utils . logging . enable_explicit_format () trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) transformers . logging . set_verbosity_error () log_level = logging.ERROR logging.getLogger().setLevel(log_level) datasets.utils.logging.set_verbosity(log_level) transformers.utils.logging.set_verbosity(log_level) transformers.utils.logging.enable_default_handler() transformers.utils.logging.enable_explicit_format() trt_logger: Logger = trt.Logger(trt.Logger.ERROR) transformers.logging.set_verbosity_error() Preprocess data \u00b6 This part is inspired from an official Notebooks from Hugging Face . There is nothing special to do. Define the task: In [5]: Copied! model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings : Dict [ str , List [ float ]] = dict () runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings: Dict[str, List[float]] = dict() runtime: Runtime = trt.Runtime(trt_logger) profile_index = 0 Preprocess data (task specific): In [6]: Copied! def preprocess_function ( examples ): return tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True , padding = \"max_length\" , max_length = max_seq_len ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) def convert_tensor ( data : OD [ str , List [ List [ int ]]], output : str ) -> OD [ str , Union [ np . ndarray , torch . Tensor ]]: input : OD [ str , Union [ np . ndarray , torch . Tensor ]] = OrderedDict () for k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ]: if k in data : v = data [ k ] if output == \"torch\" : value = torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) elif output == \"np\" : value = np . asarray ( v , dtype = np . int32 ) else : raise Exception ( f \"unknown output type: { output } \" ) input [ k ] = value return input def measure_accuracy ( infer , tensor_type : str ) -> float : outputs = list () for start_index in range ( 0 , len ( encoded_dataset [ validation_key ]), batch_size ): end_index = start_index + batch_size data = encoded_dataset [ validation_key ][ start_index : end_index ] inputs : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = tensor_type ) output = infer ( inputs )[ 0 ] if tensor_type == \"torch\" : output = output . detach () . cpu () . numpy () output = np . argmax ( output , axis = 1 ) . astype ( int ) . tolist () outputs . extend ( output ) return np . mean ( np . array ( outputs ) == np . array ( validation_labels )) def get_trainer ( model : PreTrainedModel ) -> Trainer : trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics , ) transformers . logging . set_verbosity_error () return trainer def preprocess_function(examples): return tokenizer( examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_seq_len ) def compute_metrics(eval_pred): predictions, labels = eval_pred if task != \"stsb\": predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) def convert_tensor(data: OD[str, List[List[int]]], output: str) -> OD[str, Union[np.ndarray, torch.Tensor]]: input: OD[str, Union[np.ndarray, torch.Tensor]] = OrderedDict() for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]: if k in data: v = data[k] if output == \"torch\": value = torch.tensor(v, dtype=torch.long, device=\"cuda\") elif output == \"np\": value = np.asarray(v, dtype=np.int32) else: raise Exception(f\"unknown output type: {output}\") input[k] = value return input def measure_accuracy(infer, tensor_type: str) -> float: outputs = list() for start_index in range(0, len(encoded_dataset[validation_key]), batch_size): end_index = start_index + batch_size data = encoded_dataset[validation_key][start_index:end_index] inputs: OD[str, np.ndarray] = convert_tensor(data=data, output=tensor_type) output = infer(inputs)[0] if tensor_type == \"torch\": output = output.detach().cpu().numpy() output = np.argmax(output, axis=1).astype(int).tolist() outputs.extend(output) return np.mean(np.array(outputs) == np.array(validation_labels)) def get_trainer(model: PreTrainedModel) -> Trainer: trainer = Trainer( model, args, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics, ) transformers.logging.set_verbosity_error() return trainer In [7]: Copied! tokenizer : PreTrainedTokenizer = AutoTokenizer . from_pretrained ( model_name , use_fast = True ) dataset = load_dataset ( \"glue\" , task ) metric = load_metric ( \"glue\" , task ) encoded_dataset = dataset . map ( preprocess_function , batched = True ) validation_labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] nb_step = 1000 strategy = IntervalStrategy . STEPS args = TrainingArguments ( f \" { model_name } - { task } \" , evaluation_strategy = strategy , eval_steps = nb_step , logging_steps = nb_step , save_steps = nb_step , save_strategy = strategy , learning_rate = 1e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size * 2 , num_train_epochs = 1 , fp16 = True , group_by_length = True , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = \"accuracy\" , report_to = [], ) tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) dataset = load_dataset(\"glue\", task) metric = load_metric(\"glue\", task) encoded_dataset = dataset.map(preprocess_function, batched=True) validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]] nb_step = 1000 strategy = IntervalStrategy.STEPS args = TrainingArguments( f\"{model_name}-{task}\", evaluation_strategy=strategy, eval_steps=nb_step, logging_steps=nb_step, save_steps=nb_step, save_strategy=strategy, learning_rate=1e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, num_train_epochs=1, fp16=True, group_by_length=True, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=\"accuracy\", report_to=[], ) Downloading: 0%| | 0.00/7.78k [00:00<?, ?B/s] Downloading: 0%| | 0.00/4.47k [00:00<?, ?B/s] 0%| | 0/5 [00:00<?, ?it/s] Downloading: 0%| | 0.00/1.84k [00:00<?, ?B/s] 0%| | 0/393 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] (Standard) fine-tuning model \u00b6 Now that our data are ready, we can download/fine tune the pretrained model. In [8]: Copied! model_fp16 : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = num_labels ) trainer = get_trainer ( model_fp16 ) transformers . logging . set_verbosity_error () trainer . train () print ( trainer . evaluate ()) model_fp16 . save_pretrained ( \"model_trained_fp16\" ) model_fp16: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) trainer = get_trainer(model_fp16) transformers.logging.set_verbosity_error() trainer.train() print(trainer.evaluate()) model_fp16.save_pretrained(\"model_trained_fp16\") [INFO|trainer.py:457] 2022-03-09 19:15:29,964 >> Using amp half precision backend /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( {'loss': 0.6865, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08} {'eval_loss': 0.4732713997364044, 'eval_accuracy': 0.8161996943453897, 'eval_runtime': 18.5941, 'eval_samples_per_second': 527.856, 'eval_steps_per_second': 8.282, 'epoch': 0.08} {'loss': 0.4992, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16} {'eval_loss': 0.42749494314193726, 'eval_accuracy': 0.8335201222618441, 'eval_runtime': 18.4454, 'eval_samples_per_second': 532.112, 'eval_steps_per_second': 8.349, 'epoch': 0.16} {'loss': 0.4684, 'learning_rate': 7.557855280312908e-06, 'epoch': 0.24} {'eval_loss': 0.41849666833877563, 'eval_accuracy': 0.8358634742740703, 'eval_runtime': 18.4936, 'eval_samples_per_second': 530.726, 'eval_steps_per_second': 8.327, 'epoch': 0.24} {'loss': 0.4457, 'learning_rate': 6.743807040417211e-06, 'epoch': 0.33} {'eval_loss': 0.3834249973297119, 'eval_accuracy': 0.8508405501782985, 'eval_runtime': 18.4657, 'eval_samples_per_second': 531.527, 'eval_steps_per_second': 8.34, 'epoch': 0.33} {'loss': 0.4302, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41} {'eval_loss': 0.3902352452278137, 'eval_accuracy': 0.8499235863474274, 'eval_runtime': 18.5403, 'eval_samples_per_second': 529.388, 'eval_steps_per_second': 8.306, 'epoch': 0.41} {'loss': 0.4227, 'learning_rate': 5.114080834419818e-06, 'epoch': 0.49} {'eval_loss': 0.38703474402427673, 'eval_accuracy': 0.8512480896586857, 'eval_runtime': 18.5155, 'eval_samples_per_second': 530.097, 'eval_steps_per_second': 8.317, 'epoch': 0.49} {'loss': 0.4165, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57} {'eval_loss': 0.36435264348983765, 'eval_accuracy': 0.8603158430973, 'eval_runtime': 18.5529, 'eval_samples_per_second': 529.029, 'eval_steps_per_second': 8.301, 'epoch': 0.57} {'loss': 0.4132, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65} {'eval_loss': 0.35739392042160034, 'eval_accuracy': 0.8606214977075904, 'eval_runtime': 18.5392, 'eval_samples_per_second': 529.418, 'eval_steps_per_second': 8.307, 'epoch': 0.65} {'loss': 0.4015, 'learning_rate': 2.670306388526728e-06, 'epoch': 0.73} {'eval_loss': 0.36193060874938965, 'eval_accuracy': 0.8631686194600102, 'eval_runtime': 18.5487, 'eval_samples_per_second': 529.149, 'eval_steps_per_second': 8.302, 'epoch': 0.73} {'loss': 0.3948, 'learning_rate': 1.8554432855280313e-06, 'epoch': 0.81} {'eval_loss': 0.3555583655834198, 'eval_accuracy': 0.8639836984207845, 'eval_runtime': 18.5639, 'eval_samples_per_second': 528.713, 'eval_steps_per_second': 8.296, 'epoch': 0.81} {'loss': 0.3967, 'learning_rate': 1.0413950456323338e-06, 'epoch': 0.9} {'eval_loss': 0.35024452209472656, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 18.5496, 'eval_samples_per_second': 529.121, 'eval_steps_per_second': 8.302, 'epoch': 0.9} {'loss': 0.3979, 'learning_rate': 2.2734680573663624e-07, 'epoch': 0.98} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.5647, 'eval_samples_per_second': 528.693, 'eval_steps_per_second': 8.295, 'epoch': 0.98} {'train_runtime': 2621.6937, 'train_samples_per_second': 149.789, 'train_steps_per_second': 4.681, 'train_loss': 0.4466879855855969, 'epoch': 1.0} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.571, 'eval_samples_per_second': 528.511, 'eval_steps_per_second': 8.292, 'epoch': 1.0} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.571, 'eval_samples_per_second': 528.511, 'eval_steps_per_second': 8.292, 'epoch': 1.0} Add quantization support to any model \u00b6 The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. If you want to know more, check our documentation on: https://els-rd.github.io/transformer-deploy/quantization/quantization_ast/ In [9]: Copied! for percentile in [ 99.9 , 99.99 , 99.999 , 99.9999 ]: with QATCalibrate ( method = \"histogram\" , percentile = percentile ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( f \"percentile: { percentile } \" ) print ( trainer . evaluate ()) for percentile in [99.9, 99.99, 99.999, 99.9999]: with QATCalibrate(method=\"histogram\", percentile=percentile) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(f\"percentile: {percentile}\") print(trainer.evaluate()) [INFO|trainer.py:457] 2022-03-09 20:04:30,756 >> Using amp half precision backend percentile: 99.9 {'eval_loss': 0.4834432005882263, 'eval_accuracy': 0.8129393785022924, 'eval_runtime': 46.3749, 'eval_samples_per_second': 211.645, 'eval_steps_per_second': 3.321} {'eval_loss': 0.4834432005882263, 'eval_accuracy': 0.8129393785022924, 'eval_runtime': 46.3749, 'eval_samples_per_second': 211.645, 'eval_steps_per_second': 3.321} [INFO|trainer.py:457] 2022-03-09 20:08:47,875 >> Using amp half precision backend percentile: 99.99 {'eval_loss': 0.3795105814933777, 'eval_accuracy': 0.8555272542027509, 'eval_runtime': 46.4677, 'eval_samples_per_second': 211.222, 'eval_steps_per_second': 3.314} {'eval_loss': 0.3795105814933777, 'eval_accuracy': 0.8555272542027509, 'eval_runtime': 46.4677, 'eval_samples_per_second': 211.222, 'eval_steps_per_second': 3.314} [INFO|trainer.py:457] 2022-03-09 20:13:04,907 >> Using amp half precision backend percentile: 99.999 {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4646, 'eval_samples_per_second': 211.236, 'eval_steps_per_second': 3.314} {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4646, 'eval_samples_per_second': 211.236, 'eval_steps_per_second': 3.314} [INFO|trainer.py:457] 2022-03-09 20:17:16,064 >> Using amp half precision backend percentile: 99.9999 {'eval_loss': 0.9809716939926147, 'eval_accuracy': 0.5092205807437595, 'eval_runtime': 46.4947, 'eval_samples_per_second': 211.099, 'eval_steps_per_second': 3.312} {'eval_loss': 0.9809716939926147, 'eval_accuracy': 0.5092205807437595, 'eval_runtime': 46.4947, 'eval_samples_per_second': 211.099, 'eval_steps_per_second': 3.312} As you can see, the chosen percentile value has a high impact on the final accuracy. For the rest of the notebook, we apply the 99.999 percentile. In [10]: Copied! with QATCalibrate ( method = \"histogram\" , percentile = 99.999 ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( trainer . evaluate ()) with QATCalibrate(method=\"histogram\", percentile=99.999) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(trainer.evaluate()) [INFO|trainer.py:457] 2022-03-09 20:21:33,073 >> Using amp half precision backend {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4748, 'eval_samples_per_second': 211.19, 'eval_steps_per_second': 3.314} {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4748, 'eval_samples_per_second': 211.19, 'eval_steps_per_second': 3.314} Per layer quantization analysis \u00b6 Below we will run a sensitivity analysis, by enabling quantization of one layer at a time and measuring the accuracy. That way we will be able to detect if the quantization of a specific layer has a larger cost on accuracy than other layers. In [11]: Copied! from pytorch_quantization import nn as quant_nn for i in range ( 12 ): layer_name = f \"layer. { i } \" print ( layer_name ) for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if layer_name in name : module . enable_quant () else : module . disable_quant () trainer . evaluate () print ( \"----\" ) from pytorch_quantization import nn as quant_nn for i in range(12): layer_name = f\"layer.{i}\" print(layer_name) for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if layer_name in name: module.enable_quant() else: module.disable_quant() trainer.evaluate() print(\"----\") layer.0 {'eval_loss': 0.34956735372543335, 'eval_accuracy': 0.86571574121243, 'eval_runtime': 20.7064, 'eval_samples_per_second': 474.009, 'eval_steps_per_second': 7.437} ---- layer.1 {'eval_loss': 0.3523275852203369, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 25.4843, 'eval_samples_per_second': 385.14, 'eval_steps_per_second': 6.043} ---- layer.2 {'eval_loss': 0.356509268283844, 'eval_accuracy': 0.8622516556291391, 'eval_runtime': 20.7496, 'eval_samples_per_second': 473.021, 'eval_steps_per_second': 7.422} ---- layer.3 {'eval_loss': 0.36036217212677, 'eval_accuracy': 0.8617422312786551, 'eval_runtime': 20.7815, 'eval_samples_per_second': 472.296, 'eval_steps_per_second': 7.41} ---- layer.4 {'eval_loss': 0.35000357031822205, 'eval_accuracy': 0.8643912379011717, 'eval_runtime': 20.7921, 'eval_samples_per_second': 472.053, 'eval_steps_per_second': 7.407} ---- layer.5 {'eval_loss': 0.354992538690567, 'eval_accuracy': 0.8644931227712684, 'eval_runtime': 20.7938, 'eval_samples_per_second': 472.016, 'eval_steps_per_second': 7.406} ---- layer.6 {'eval_loss': 0.35205718874931335, 'eval_accuracy': 0.8645950076413652, 'eval_runtime': 20.7918, 'eval_samples_per_second': 472.061, 'eval_steps_per_second': 7.407} ---- layer.7 {'eval_loss': 0.35065746307373047, 'eval_accuracy': 0.8655119714722364, 'eval_runtime': 20.8011, 'eval_samples_per_second': 471.849, 'eval_steps_per_second': 7.403} ---- layer.8 {'eval_loss': 0.3491470217704773, 'eval_accuracy': 0.8659195109526235, 'eval_runtime': 20.8112, 'eval_samples_per_second': 471.621, 'eval_steps_per_second': 7.4} ---- layer.9 {'eval_loss': 0.3492998480796814, 'eval_accuracy': 0.8659195109526235, 'eval_runtime': 20.8695, 'eval_samples_per_second': 470.303, 'eval_steps_per_second': 7.379} ---- layer.10 {'eval_loss': 0.3501480221748352, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 20.8698, 'eval_samples_per_second': 470.296, 'eval_steps_per_second': 7.379} ---- layer.11 {'eval_loss': 0.3497083783149719, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 20.9345, 'eval_samples_per_second': 468.843, 'eval_steps_per_second': 7.356} ---- It seems that quantization of layers 2 to 6 has the largest accuracy impact. Operator quantization analysis \u00b6 Below we will run a sensitivity analysis, by enabling quantization of one operator type at a time and measuring the accuracy. That way we will be able to detect if a specific operator has a larger cost on accuracy. On Roberta we only quantize matmul and LayerNorm , so we test both candidates. In [12]: Copied! for op in [ \"matmul\" , \"layernorm\" ]: for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if op in name : module . enable_quant () else : module . disable_quant () print ( op ) trainer . evaluate () print ( \"----\" ) for op in [\"matmul\", \"layernorm\"]: for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if op in name: module.enable_quant() else: module.disable_quant() print(op) trainer.evaluate() print(\"----\") matmul {'eval_loss': 0.3494793176651001, 'eval_accuracy': 0.8654100866021396, 'eval_runtime': 26.892, 'eval_samples_per_second': 364.978, 'eval_steps_per_second': 5.727} ---- layernorm {'eval_loss': 0.3585323095321655, 'eval_accuracy': 0.8587875700458482, 'eval_runtime': 24.0982, 'eval_samples_per_second': 407.293, 'eval_steps_per_second': 6.391} ---- It appears that LayerNorm quantization has a significant accuracy cost. Our goal is to disable quantization for as few operations as possible while preserving accuracy as much as possible. Therefore we will try to only disable quantization for LayerNorm on Layers 2 to 6. In [13]: Copied! disable_layer_names = [ \"layer.2\" , \"layer.3\" , \"layer.4\" , \"layer.6\" ] for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if any ([ f \" { l } .output.layernorm\" in name for l in disable_layer_names ]): print ( f \"disable { name } \" ) module . disable_quant () else : module . enable_quant () trainer . evaluate () disable_layer_names = [\"layer.2\", \"layer.3\", \"layer.4\", \"layer.6\"] for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if any([f\"{l}.output.layernorm\" in name for l in disable_layer_names]): print(f\"disable {name}\") module.disable_quant() else: module.enable_quant() trainer.evaluate() disable roberta.encoder.layer.2.output.layernorm_quantizer_0 disable roberta.encoder.layer.2.output.layernorm_quantizer_1 disable roberta.encoder.layer.3.output.layernorm_quantizer_0 disable roberta.encoder.layer.3.output.layernorm_quantizer_1 disable roberta.encoder.layer.4.output.layernorm_quantizer_0 disable roberta.encoder.layer.4.output.layernorm_quantizer_1 disable roberta.encoder.layer.6.output.layernorm_quantizer_0 disable roberta.encoder.layer.6.output.layernorm_quantizer_1 {'eval_loss': 0.3617263436317444, 'eval_accuracy': 0.8614365766683647, 'eval_runtime': 46.0379, 'eval_samples_per_second': 213.194, 'eval_steps_per_second': 3.345} Out[13]: {'eval_loss': 0.3617263436317444, 'eval_accuracy': 0.8614365766683647, 'eval_runtime': 46.0379, 'eval_samples_per_second': 213.194, 'eval_steps_per_second': 3.345} By just disabling quantization for a single operator on a few layers, we keep most of the performance boost (quantization) but retrieve more than 1 point of accuracy. It's also possible to perform an analysis per quantizer to get a smaller granularity but it's a bit slow to run. If we stop here, it's called a Post Training Quantization (PTQ). Below, we will try to retrieve even more accuracy. Quantization Aware Training (QAT) \u00b6 We retrain the model with 1/10 or 1/100 of the original learning rate. Our goal is to retrieve most of the original accuracy. In [14]: Copied! args . learning_rate = 1e-7 trainer = get_trainer ( model_q ) trainer . train () print ( trainer . evaluate ()) model_q . save_pretrained ( \"model-qat\" ) args.learning_rate = 1e-7 trainer = get_trainer(model_q) trainer.train() print(trainer.evaluate()) model_q.save_pretrained(\"model-qat\") [INFO|trainer.py:457] 2022-03-09 20:28:11,049 >> Using amp half precision backend /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( {'loss': 0.3628, 'learning_rate': 9.1867666232073e-08, 'epoch': 0.08} {'eval_loss': 0.37072187662124634, 'eval_accuracy': 0.8610290371879776, 'eval_runtime': 46.0646, 'eval_samples_per_second': 213.071, 'eval_steps_per_second': 3.343, 'epoch': 0.08} {'loss': 0.3159, 'learning_rate': 8.371903520208604e-08, 'epoch': 0.16} {'eval_loss': 0.3802173137664795, 'eval_accuracy': 0.8589913397860418, 'eval_runtime': 46.1087, 'eval_samples_per_second': 212.866, 'eval_steps_per_second': 3.34, 'epoch': 0.16} {'loss': 0.3038, 'learning_rate': 7.557855280312907e-08, 'epoch': 0.24} {'eval_loss': 0.38381442427635193, 'eval_accuracy': 0.8597045338767193, 'eval_runtime': 46.1153, 'eval_samples_per_second': 212.836, 'eval_steps_per_second': 3.339, 'epoch': 0.24} {'loss': 0.2981, 'learning_rate': 6.74299217731421e-08, 'epoch': 0.33} {'eval_loss': 0.39187753200531006, 'eval_accuracy': 0.8617422312786551, 'eval_runtime': 46.0955, 'eval_samples_per_second': 212.927, 'eval_steps_per_second': 3.341, 'epoch': 0.33} {'loss': 0.2979, 'learning_rate': 5.9289439374185136e-08, 'epoch': 0.41} {'eval_loss': 0.39416825771331787, 'eval_accuracy': 0.8601120733571065, 'eval_runtime': 46.1939, 'eval_samples_per_second': 212.474, 'eval_steps_per_second': 3.334, 'epoch': 0.41} {'loss': 0.3041, 'learning_rate': 5.114080834419817e-08, 'epoch': 0.49} {'eval_loss': 0.39381393790245056, 'eval_accuracy': 0.8609271523178808, 'eval_runtime': 46.1092, 'eval_samples_per_second': 212.864, 'eval_steps_per_second': 3.34, 'epoch': 0.49} {'loss': 0.3122, 'learning_rate': 4.3008474576271185e-08, 'epoch': 0.57} {'eval_loss': 0.39094462990760803, 'eval_accuracy': 0.8600101884870097, 'eval_runtime': 46.1377, 'eval_samples_per_second': 212.733, 'eval_steps_per_second': 3.338, 'epoch': 0.57} {'loss': 0.3297, 'learning_rate': 3.485984354628422e-08, 'epoch': 0.65} {'eval_loss': 0.3906724452972412, 'eval_accuracy': 0.8616403464085584, 'eval_runtime': 46.2006, 'eval_samples_per_second': 212.443, 'eval_steps_per_second': 3.333, 'epoch': 0.65} {'loss': 0.338, 'learning_rate': 2.671121251629726e-08, 'epoch': 0.73} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 46.1088, 'eval_samples_per_second': 212.866, 'eval_steps_per_second': 3.34, 'epoch': 0.73} {'loss': 0.3557, 'learning_rate': 1.8570730117340286e-08, 'epoch': 0.81} {'eval_loss': 0.39177224040031433, 'eval_accuracy': 0.8593988792664289, 'eval_runtime': 49.4531, 'eval_samples_per_second': 198.471, 'eval_steps_per_second': 3.114, 'epoch': 0.81} {'loss': 0.385, 'learning_rate': 1.0430247718383311e-08, 'epoch': 0.9} {'eval_loss': 0.38946154713630676, 'eval_accuracy': 0.8593988792664289, 'eval_runtime': 51.1685, 'eval_samples_per_second': 191.817, 'eval_steps_per_second': 3.01, 'epoch': 0.9} {'loss': 0.4133, 'learning_rate': 2.2816166883963493e-09, 'epoch': 0.98} {'eval_loss': 0.3868817687034607, 'eval_accuracy': 0.8611309220580744, 'eval_runtime': 46.8425, 'eval_samples_per_second': 209.532, 'eval_steps_per_second': 3.288, 'epoch': 0.98} {'train_runtime': 5020.8729, 'train_samples_per_second': 78.214, 'train_steps_per_second': 2.444, 'train_loss': 0.33702789975580366, 'epoch': 1.0} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 47.1812, 'eval_samples_per_second': 208.028, 'eval_steps_per_second': 3.264, 'epoch': 1.0} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 47.1812, 'eval_samples_per_second': 208.028, 'eval_steps_per_second': 3.264, 'epoch': 1.0} Export a QDQ Pytorch model to ONNX \u00b6 We need to enable fake quantization mode from Pytorch. In [15]: Copied! data = encoded_dataset [ \"train\" ][ 1 : 3 ] input_torch = convert_tensor ( data , output = \"torch\" ) convert_to_onnx ( model_pytorch = model_q , output_path = \"model_qat.onnx\" , inputs_pytorch = input_torch , quantization = True , var_output_seq = False , ) data = encoded_dataset[\"train\"][1:3] input_torch = convert_tensor(data, output=\"torch\") convert_to_onnx( model_pytorch=model_q, output_path=\"model_qat.onnx\", inputs_pytorch=input_torch, quantization=True, var_output_seq=False, ) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! inputs, amax.item() / bound, 0, /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0]) In [16]: Copied! del model_q QATCalibrate . restore () del model_q QATCalibrate.restore() Benchmark \u00b6 Convert ONNX graph to TensorRT engine \u00b6 In [17]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) engine = build_engine( runtime=runtime, onnx_file_path=\"model_qat.onnx\", logger=trt_logger, min_shape=(1, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=True, ) In [18]: Copied! # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" Prepare input and output buffer \u00b6 In [19]: Copied! context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async( profile_index=profile_index, stream_handle=torch.cuda.current_stream().cuda_stream ) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] In [20]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") Inference on TensorRT \u00b6 We first check that inference is working correctly: In [21]: Copied! tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print ( tensorrt_output ) tensorrt_output = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print(tensorrt_output) [tensor([[ 1.2287, 1.3706, -2.4623], [ 2.4742, -0.7816, -1.8427], [ 2.4837, -0.3966, -2.2666], [ 2.9077, -0.3062, -2.9778], [ 2.3437, 0.0488, -2.6377], [ 3.7914, -1.1918, -3.1387], [-3.6134, 2.7432, 0.8490], [ 3.6679, -1.5787, -2.6408], [ 1.0155, -1.2787, 0.4250], [-3.4514, -0.4434, 4.2748], [ 3.5201, -1.1297, -2.8988], [-3.0225, -0.4062, 3.8606], [-2.7311, 3.5470, -0.4632], [-2.0741, 1.6613, 0.5798], [-0.4047, -0.8650, 1.6144], [ 2.8432, -1.3301, -1.8994], [ 3.7722, -0.9103, -3.3070], [-2.4204, -2.1432, 4.6537], [-3.1179, -1.3207, 4.6400], [-1.8794, 4.1075, -1.8630], [ 3.7726, -1.2056, -3.0701], [ 1.8645, 1.9744, -3.8743], [-3.1448, -1.2497, 4.5782], [ 3.5385, -0.2421, -3.6629], [ 3.7501, -1.6469, -2.7108], [-0.6568, 0.9046, -0.0228], [-3.2998, 0.0867, 3.3673], [-2.1030, 4.0461, -1.6705], [-3.7080, 0.4164, 3.4332], [ 3.6850, -0.9984, -3.0304], [ 3.4525, -0.5405, -3.2981], [ 3.6128, -0.9298, -3.0746]], device='cuda:0')] Measure of the accuracy: In [22]: Copied! infer_trt = lambda inputs : infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) measure_accuracy ( infer = infer_trt , tensor_type = \"torch\" ) infer_trt = lambda inputs: infer_tensorrt( context=context, host_inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) measure_accuracy(infer=infer_trt, tensor_type=\"torch\") Out[22]: 0.8618441161487519 Latency measures: In [23]: Copied! time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print_timings ( name = \"TensorRT (INT-8)\" , timings = time_buffer ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer) [TensorRT (INT-8)] mean=18.04ms, sd=2.02ms, min=16.67ms, max=28.88ms, median=17.27ms, 95p=20.04ms, 99p=28.86ms In [24]: Copied! del engine , context del engine, context Pytorch baseline \u00b6 Time to get some numbers to compare with. GPU execution \u00b6 We will measure vanilla Pytorch inference on both FP32 and FP16 precision on GPU, it will be our baseline: In [25]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) with torch . inference_mode (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32)\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") with torch.inference_mode(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32)\", timings=time_buffer) [Pytorch (FP32)] mean=82.28ms, sd=7.72ms, min=75.77ms, max=120.13ms, median=79.73ms, 95p=97.02ms, 99p=110.68ms In [26]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16)\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = [] for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16)\", timings=time_buffer) del baseline_model [Pytorch (FP16)] mean=58.73ms, sd=1.88ms, min=55.56ms, max=69.69ms, median=58.03ms, 95p=62.28ms, 99p=64.42ms CPU execution \u00b6 In [27]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_torch_cpu = { k : v . to ( \"cpu\" ) for k , v in input_torch . items ()} torch . set_num_threads ( os . cpu_count ()) with torch . inference_mode (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32) - CPU\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_torch_cpu = {k: v.to(\"cpu\") for k, v in input_torch.items()} torch.set_num_threads(os.cpu_count()) with torch.inference_mode(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32) - CPU\", timings=time_buffer) [Pytorch (FP32) - CPU] mean=5141.10ms, sd=654.18ms, min=4095.59ms, max=6229.84ms, median=5347.27ms, 95p=5958.43ms, 99p=6175.56ms In [28]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16) - CPU\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = [] for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16) - CPU\", timings=time_buffer) del baseline_model [Pytorch (FP16) - CPU] mean=4422.39ms, sd=170.45ms, min=4250.10ms, max=4744.97ms, median=4337.85ms, 95p=4727.72ms, 99p=4741.52ms Below, we will perform dynamic quantization on CPU. In [29]: Copied! quantized_baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) quantized_baseline_model = quantized_baseline_model . eval () quantized_baseline_model = torch . quantization . quantize_dynamic ( quantized_baseline_model , { torch . nn . Linear }, dtype = torch . qint8 ) with torch . inference_mode (): for _ in range ( 3 ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (INT-8) - CPU\" , timings = time_buffer ) quantized_baseline_model = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) quantized_baseline_model = quantized_baseline_model.eval() quantized_baseline_model = torch.quantization.quantize_dynamic( quantized_baseline_model, {torch.nn.Linear}, dtype=torch.qint8 ) with torch.inference_mode(): for _ in range(3): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (INT-8) - CPU\", timings=time_buffer) [Pytorch (INT-8) - CPU] mean=3818.99ms, sd=137.98ms, min=3616.11ms, max=4049.00ms, median=3807.33ms, 95p=4024.45ms, 99p=4044.09ms TensorRT baseline \u00b6 Below we export our finetuned model, the purpose is to only check the performance on mixed precision (FP16, no quantization). In [30]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () convert_to_onnx ( baseline_model , output_path = \"baseline.onnx\" , inputs_pytorch = input_torch , quantization = False , var_output_seq = False ) del baseline_model baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() convert_to_onnx( baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch, quantization=False, var_output_seq=False ) del baseline_model In [31]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"baseline.onnx\" , logger = trt_logger , min_shape = ( batch_size , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = False , ) input_torch : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"torch\" ) context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] for _ in range ( 30 ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print_timings ( name = \"TensorRT (FP16)\" , timings = time_buffer ) del engine , context engine = build_engine( runtime=runtime, onnx_file_path=\"baseline.onnx\", logger=trt_logger, min_shape=(batch_size, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=False, ) input_torch: OD[str, np.ndarray] = convert_tensor(data=data, output=\"torch\") context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async( profile_index=profile_index, stream_handle=torch.cuda.current_stream().cuda_stream ) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] for _ in range(30): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print_timings(name=\"TensorRT (FP16)\", timings=time_buffer) del engine, context [TensorRT (FP16)] mean=32.36ms, sd=1.47ms, min=29.92ms, max=38.08ms, median=32.47ms, 95p=34.38ms, 99p=36.50ms ONNX Runtime baseline \u00b6 ONNX Runtime is the go to inference solution from Microsoft. The recent 1.10 version of ONNX Runtime (with TensorRT support) is still a bit buggy on transformer models, that is why we use the 1.9.0 version in the measures below. As before, CPU quantization is dynamic. Function will set ONNX Runtime to use all cores available and enable any possible optimizations. In [32]: Copied! optimize_onnx ( onnx_path = \"baseline.onnx\" , onnx_optim_model_path = \"baseline-optimized.onnx\" , fp16 = True , use_cuda = True , num_attention_heads = 12 , hidden_size = 768 , architecture = \"bert\" , ) optimize_onnx( onnx_path=\"baseline.onnx\", onnx_optim_model_path=\"baseline-optimized.onnx\", fp16=True, use_cuda=True, num_attention_heads=12, hidden_size=768, architecture=\"bert\", ) failed in shape inference <class 'AssertionError'> In [33]: Copied! cpu_quantization ( input_model_path = \"baseline.onnx\" , output_model_path = \"baseline-quantized.onnx\" ) cpu_quantization(input_model_path=\"baseline.onnx\", output_model_path=\"baseline-quantized.onnx\") Ignore MatMul due to non constant B: /[MatMul_108] Ignore MatMul due to non constant B: /[MatMul_113] Ignore MatMul due to non constant B: /[MatMul_210] Ignore MatMul due to non constant B: /[MatMul_215] Ignore MatMul due to non constant B: /[MatMul_312] Ignore MatMul due to non constant B: /[MatMul_317] Ignore MatMul due to non constant B: /[MatMul_414] Ignore MatMul due to non constant B: /[MatMul_419] Ignore MatMul due to non constant B: /[MatMul_516] Ignore MatMul due to non constant B: /[MatMul_521] Ignore MatMul due to non constant B: /[MatMul_618] Ignore MatMul due to non constant B: /[MatMul_623] Ignore MatMul due to non constant B: /[MatMul_720] Ignore MatMul due to non constant B: /[MatMul_725] Ignore MatMul due to non constant B: /[MatMul_822] Ignore MatMul due to non constant B: /[MatMul_827] Ignore MatMul due to non constant B: /[MatMul_924] Ignore MatMul due to non constant B: /[MatMul_929] Ignore MatMul due to non constant B: /[MatMul_1026] Ignore MatMul due to non constant B: /[MatMul_1031] Ignore MatMul due to non constant B: /[MatMul_1128] Ignore MatMul due to non constant B: /[MatMul_1133] Ignore MatMul due to non constant B: /[MatMul_1230] Ignore MatMul due to non constant B: /[MatMul_1235] In [34]: Copied! labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] data = encoded_dataset [ validation_key ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) output = model . run ( None , inputs_onnx ) labels = [item[\"label\"] for item in encoded_dataset[validation_key]] data = encoded_dataset[validation_key][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") output = model.run(None, inputs_onnx) In [35]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for provider , model_path , benchmark_name , warmup , nb_inference in [ ( \"CUDAExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime GPU (FP32)\" , 10 , 100 ), ( \"CUDAExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime GPU (FP16)\" , 10 , 100 ), ( \"CPUExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime CPU (FP32)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime CPU (FP16)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-quantized.onnx\" , \"ONNX Runtime CPU (INT-8)\" , 3 , 10 ), ]: model = create_model_for_provider ( path = model_path , provider_to_use = provider ) for _ in range ( warmup ): _ = model . run ( None , inputs_onnx ) time_buffer = [] for _ in range ( nb_inference ): with track_infer_time ( time_buffer ): _ = model . run ( None , inputs_onnx ) print_timings ( name = benchmark_name , timings = time_buffer ) del model data = encoded_dataset[\"train\"][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for provider, model_path, benchmark_name, warmup, nb_inference in [ (\"CUDAExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime GPU (FP32)\", 10, 100), (\"CUDAExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime GPU (FP16)\", 10, 100), (\"CPUExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime CPU (FP32)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime CPU (FP16)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-quantized.onnx\", \"ONNX Runtime CPU (INT-8)\", 3, 10), ]: model = create_model_for_provider(path=model_path, provider_to_use=provider) for _ in range(warmup): _ = model.run(None, inputs_onnx) time_buffer = [] for _ in range(nb_inference): with track_infer_time(time_buffer): _ = model.run(None, inputs_onnx) print_timings(name=benchmark_name, timings=time_buffer) del model [ONNX Runtime GPU (FP32)] mean=82.50ms, sd=11.53ms, min=74.12ms, max=118.73ms, median=77.62ms, 95p=112.51ms, 99p=117.81ms [ONNX Runtime GPU (FP16)] mean=36.23ms, sd=4.01ms, min=33.47ms, max=55.75ms, median=35.35ms, 95p=46.89ms, 99p=53.95ms [ONNX Runtime CPU (FP32)] mean=4608.31ms, sd=468.16ms, min=3954.09ms, max=5226.20ms, median=4531.47ms, 95p=5189.71ms, 99p=5218.90ms [ONNX Runtime CPU (FP16)] mean=3987.95ms, sd=223.00ms, min=3755.70ms, max=4548.57ms, median=3907.74ms, 95p=4375.23ms, 99p=4513.90ms [ONNX Runtime CPU (INT-8)] mean=3506.39ms, sd=125.07ms, min=3425.41ms, max=3872.95ms, median=3463.65ms, 95p=3713.08ms, 99p=3840.98ms Measure of the accuracy with ONNX Runtime engine and CUDA provider: In [36]: Copied! model = create_model_for_provider ( path = \"baseline.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[36]: 0.8665308201732043 In [37]: Copied! model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[37]: 0.8663270504330107 In [38]: Copied! model = create_model_for_provider ( path = \"baseline-quantized.onnx\" , provider_to_use = \"CPUExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline-quantized.onnx\", provider_to_use=\"CPUExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[38]: 0.8604177279673968 In [39]: Copied! del model del model In [40]: Copied!","title":"End to end demo"},{"location":"quantization/quantization/#nvidia-gpu-int-8-quantization-on-any-transformers-model-encoder-based","text":"For some context and explanations, please check our documentation here: https://els-rd.github.io/transformer-deploy/quantization/quantization_intro/ .","title":"Nvidia GPU INT-8 quantization on any transformers model (encoder based)"},{"location":"quantization/quantization/#project-setup","text":"","title":"Project setup"},{"location":"quantization/quantization/#dependencies-installation","text":"Your machine should have Nvidia CUDA 11.X, TensorRT 8.2.1 and cuBLAS installed. It's said to be tricky to install, in my experience, just follow Nvidia download page instructions and nothing else , it should work out of the box. Nvidia Docker image could be a good choice too. In [1]: Copied! #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com Collecting datasets Downloading datasets-1.18.4-py3-none-any.whl (312 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 312 kB 12.2 MB/s eta 0:00:01 Collecting sklearn Downloading sklearn-0.0.tar.gz (1.1 kB) Collecting dill Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86 kB 70.0 MB/s eta 0:00:01 Requirement already satisfied: requests>=2.19.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (2.27.1) Requirement already satisfied: numpy>=1.17 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (1.21.5) Requirement already satisfied: packaging in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (21.3) Collecting pyarrow!=4.0.0,>=3.0.0 Downloading pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26.7 MB 20.6 MB/s eta 0:00:01 Collecting xxhash Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 211 kB 59.3 MB/s eta 0:00:01 Collecting fsspec[http]>=2021.05.0 Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 134 kB 73.4 MB/s eta 0:00:01 Collecting responses<0.19 Downloading responses-0.18.0-py3-none-any.whl (38 kB) Collecting pandas Downloading pandas-1.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 11.7 MB 90.8 MB/s eta 0:00:01 Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (0.4.0) Collecting aiohttp Downloading aiohttp-3.8.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.2 MB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.2 MB 100.2 MB/s eta 0:00:01 Collecting multiprocess Downloading multiprocess-0.70.12.2-py39-none-any.whl (128 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128 kB 78.2 MB/s eta 0:00:01 Requirement already satisfied: tqdm>=4.62.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from datasets) (4.62.3) Requirement already satisfied: pyyaml in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0) Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.1) Requirement already satisfied: filelock in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2) Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from packaging->datasets) (3.0.7) Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.8) Requirement already satisfied: certifi>=2017.4.17 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8) Requirement already satisfied: idna<4,>=2.5 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3) Requirement already satisfied: charset-normalizer~=2.0.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.11) Requirement already satisfied: scikit-learn in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from sklearn) (1.0.2) Requirement already satisfied: attrs>=17.3.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0) Collecting multidict<7.0,>=4.5 Downloading multidict-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 114 kB 96.7 MB/s eta 0:00:01 Collecting aiosignal>=1.1.2 Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB) Collecting frozenlist>=1.1.1 Downloading frozenlist-1.3.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (156 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 156 kB 86.6 MB/s eta 0:00:01 Collecting yarl<2.0,>=1.0 Downloading yarl-1.7.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (304 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 304 kB 95.1 MB/s eta 0:00:01 Collecting async-timeout<5.0,>=4.0.0a3 Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB) Requirement already satisfied: python-dateutil>=2.8.1 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from pandas->datasets) (2.8.2) Collecting pytz>=2020.1 Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB) |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 503 kB 68.0 MB/s eta 0:00:01 Requirement already satisfied: six>=1.5 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0) Requirement already satisfied: joblib>=0.11 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0) Requirement already satisfied: scipy>=1.1.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0) Requirement already satisfied: threadpoolctl>=2.0.0 in /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0) Building wheels for collected packages: sklearn Building wheel for sklearn (setup.py) ... done Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=3ac0638e4032964c7ba9d014645ce13935d501fa40fff5bf77431ac4cffa5e14 Stored in directory: /tmp/pip-ephem-wheel-cache-m57vxdol/wheels/e4/7b/98/b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c Successfully built sklearn Installing collected packages: multidict, frozenlist, yarl, async-timeout, aiosignal, pytz, fsspec, dill, aiohttp, xxhash, responses, pyarrow, pandas, multiprocess, sklearn, datasets Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-1.18.4 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.2.0 multidict-6.0.2 multiprocess-0.70.12.2 pandas-1.4.1 pyarrow-7.0.0 pytz-2021.3 responses-0.18.0 sklearn-0.0 xxhash-3.0.0 yarl-1.7.2 WARNING: You are using pip version 21.1.2; however, version 22.0.4 is available. You should consider upgrading via the '/home/geantvert/.local/share/virtualenvs/fast_transformer/bin/python -m pip install --upgrade pip' command. Check the GPU is enabled and usable. In [2]: Copied! ! nvidia - smi ! nvidia-smi Wed Mar 9 19:14:33 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 510.47.03 Driver Version: 510.47.03 CUDA Version: 11.6 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... On | 00000000:03:00.0 On | N/A | | 48% 46C P8 41W / 350W | 366MiB / 24576MiB | 0% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1895 G /usr/lib/xorg/Xorg 182MiB | | 0 N/A N/A 7706 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 8916 G ...on/Bin/AgentConnectix.bin 4MiB | | 0 N/A N/A 10507 G ...884829189831830011,131072 53MiB | | 0 N/A N/A 1025897 G ...AAAAAAAAA= --shared-files 39MiB | | 0 N/A N/A 3867171 G ...947156.log --shared-files 37MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import os from collections import OrderedDict from typing import Dict , List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import tensorrt as trt import torch import transformers from datasets import load_dataset , load_metric from tensorrt.tensorrt import IExecutionContext , Logger , Runtime from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , IntervalStrategy , PreTrainedModel , PreTrainedTokenizer , Trainer , TrainingArguments , ) from transformer_deploy.backends.ort_utils import ( cpu_quantization , create_model_for_provider , optimize_onnx , ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine , get_binding_idxs , infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings , track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate import logging import os from collections import OrderedDict from typing import Dict, List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import tensorrt as trt import torch import transformers from datasets import load_dataset, load_metric from tensorrt.tensorrt import IExecutionContext, Logger, Runtime from transformers import ( AutoModelForSequenceClassification, AutoTokenizer, IntervalStrategy, PreTrainedModel, PreTrainedTokenizer, Trainer, TrainingArguments, ) from transformer_deploy.backends.ort_utils import ( cpu_quantization, create_model_for_provider, optimize_onnx, ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings, track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate Set logging to error level to ease readability of this notebook on Github. In [4]: Copied! log_level = logging . ERROR logging . getLogger () . setLevel ( log_level ) datasets . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . enable_default_handler () transformers . utils . logging . enable_explicit_format () trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) transformers . logging . set_verbosity_error () log_level = logging.ERROR logging.getLogger().setLevel(log_level) datasets.utils.logging.set_verbosity(log_level) transformers.utils.logging.set_verbosity(log_level) transformers.utils.logging.enable_default_handler() transformers.utils.logging.enable_explicit_format() trt_logger: Logger = trt.Logger(trt.Logger.ERROR) transformers.logging.set_verbosity_error()","title":"Dependencies installation"},{"location":"quantization/quantization/#preprocess-data","text":"This part is inspired from an official Notebooks from Hugging Face . There is nothing special to do. Define the task: In [5]: Copied! model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings : Dict [ str , List [ float ]] = dict () runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings: Dict[str, List[float]] = dict() runtime: Runtime = trt.Runtime(trt_logger) profile_index = 0 Preprocess data (task specific): In [6]: Copied! def preprocess_function ( examples ): return tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True , padding = \"max_length\" , max_length = max_seq_len ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) def convert_tensor ( data : OD [ str , List [ List [ int ]]], output : str ) -> OD [ str , Union [ np . ndarray , torch . Tensor ]]: input : OD [ str , Union [ np . ndarray , torch . Tensor ]] = OrderedDict () for k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ]: if k in data : v = data [ k ] if output == \"torch\" : value = torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) elif output == \"np\" : value = np . asarray ( v , dtype = np . int32 ) else : raise Exception ( f \"unknown output type: { output } \" ) input [ k ] = value return input def measure_accuracy ( infer , tensor_type : str ) -> float : outputs = list () for start_index in range ( 0 , len ( encoded_dataset [ validation_key ]), batch_size ): end_index = start_index + batch_size data = encoded_dataset [ validation_key ][ start_index : end_index ] inputs : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = tensor_type ) output = infer ( inputs )[ 0 ] if tensor_type == \"torch\" : output = output . detach () . cpu () . numpy () output = np . argmax ( output , axis = 1 ) . astype ( int ) . tolist () outputs . extend ( output ) return np . mean ( np . array ( outputs ) == np . array ( validation_labels )) def get_trainer ( model : PreTrainedModel ) -> Trainer : trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics , ) transformers . logging . set_verbosity_error () return trainer def preprocess_function(examples): return tokenizer( examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_seq_len ) def compute_metrics(eval_pred): predictions, labels = eval_pred if task != \"stsb\": predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) def convert_tensor(data: OD[str, List[List[int]]], output: str) -> OD[str, Union[np.ndarray, torch.Tensor]]: input: OD[str, Union[np.ndarray, torch.Tensor]] = OrderedDict() for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]: if k in data: v = data[k] if output == \"torch\": value = torch.tensor(v, dtype=torch.long, device=\"cuda\") elif output == \"np\": value = np.asarray(v, dtype=np.int32) else: raise Exception(f\"unknown output type: {output}\") input[k] = value return input def measure_accuracy(infer, tensor_type: str) -> float: outputs = list() for start_index in range(0, len(encoded_dataset[validation_key]), batch_size): end_index = start_index + batch_size data = encoded_dataset[validation_key][start_index:end_index] inputs: OD[str, np.ndarray] = convert_tensor(data=data, output=tensor_type) output = infer(inputs)[0] if tensor_type == \"torch\": output = output.detach().cpu().numpy() output = np.argmax(output, axis=1).astype(int).tolist() outputs.extend(output) return np.mean(np.array(outputs) == np.array(validation_labels)) def get_trainer(model: PreTrainedModel) -> Trainer: trainer = Trainer( model, args, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics, ) transformers.logging.set_verbosity_error() return trainer In [7]: Copied! tokenizer : PreTrainedTokenizer = AutoTokenizer . from_pretrained ( model_name , use_fast = True ) dataset = load_dataset ( \"glue\" , task ) metric = load_metric ( \"glue\" , task ) encoded_dataset = dataset . map ( preprocess_function , batched = True ) validation_labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] nb_step = 1000 strategy = IntervalStrategy . STEPS args = TrainingArguments ( f \" { model_name } - { task } \" , evaluation_strategy = strategy , eval_steps = nb_step , logging_steps = nb_step , save_steps = nb_step , save_strategy = strategy , learning_rate = 1e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size * 2 , num_train_epochs = 1 , fp16 = True , group_by_length = True , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = \"accuracy\" , report_to = [], ) tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) dataset = load_dataset(\"glue\", task) metric = load_metric(\"glue\", task) encoded_dataset = dataset.map(preprocess_function, batched=True) validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]] nb_step = 1000 strategy = IntervalStrategy.STEPS args = TrainingArguments( f\"{model_name}-{task}\", evaluation_strategy=strategy, eval_steps=nb_step, logging_steps=nb_step, save_steps=nb_step, save_strategy=strategy, learning_rate=1e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, num_train_epochs=1, fp16=True, group_by_length=True, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=\"accuracy\", report_to=[], ) Downloading: 0%| | 0.00/7.78k [00:00<?, ?B/s] Downloading: 0%| | 0.00/4.47k [00:00<?, ?B/s] 0%| | 0/5 [00:00<?, ?it/s] Downloading: 0%| | 0.00/1.84k [00:00<?, ?B/s] 0%| | 0/393 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s] 0%| | 0/10 [00:00<?, ?ba/s]","title":"Preprocess data"},{"location":"quantization/quantization/#standard-fine-tuning-model","text":"Now that our data are ready, we can download/fine tune the pretrained model. In [8]: Copied! model_fp16 : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = num_labels ) trainer = get_trainer ( model_fp16 ) transformers . logging . set_verbosity_error () trainer . train () print ( trainer . evaluate ()) model_fp16 . save_pretrained ( \"model_trained_fp16\" ) model_fp16: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) trainer = get_trainer(model_fp16) transformers.logging.set_verbosity_error() trainer.train() print(trainer.evaluate()) model_fp16.save_pretrained(\"model_trained_fp16\") [INFO|trainer.py:457] 2022-03-09 19:15:29,964 >> Using amp half precision backend /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( {'loss': 0.6865, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08} {'eval_loss': 0.4732713997364044, 'eval_accuracy': 0.8161996943453897, 'eval_runtime': 18.5941, 'eval_samples_per_second': 527.856, 'eval_steps_per_second': 8.282, 'epoch': 0.08} {'loss': 0.4992, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16} {'eval_loss': 0.42749494314193726, 'eval_accuracy': 0.8335201222618441, 'eval_runtime': 18.4454, 'eval_samples_per_second': 532.112, 'eval_steps_per_second': 8.349, 'epoch': 0.16} {'loss': 0.4684, 'learning_rate': 7.557855280312908e-06, 'epoch': 0.24} {'eval_loss': 0.41849666833877563, 'eval_accuracy': 0.8358634742740703, 'eval_runtime': 18.4936, 'eval_samples_per_second': 530.726, 'eval_steps_per_second': 8.327, 'epoch': 0.24} {'loss': 0.4457, 'learning_rate': 6.743807040417211e-06, 'epoch': 0.33} {'eval_loss': 0.3834249973297119, 'eval_accuracy': 0.8508405501782985, 'eval_runtime': 18.4657, 'eval_samples_per_second': 531.527, 'eval_steps_per_second': 8.34, 'epoch': 0.33} {'loss': 0.4302, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41} {'eval_loss': 0.3902352452278137, 'eval_accuracy': 0.8499235863474274, 'eval_runtime': 18.5403, 'eval_samples_per_second': 529.388, 'eval_steps_per_second': 8.306, 'epoch': 0.41} {'loss': 0.4227, 'learning_rate': 5.114080834419818e-06, 'epoch': 0.49} {'eval_loss': 0.38703474402427673, 'eval_accuracy': 0.8512480896586857, 'eval_runtime': 18.5155, 'eval_samples_per_second': 530.097, 'eval_steps_per_second': 8.317, 'epoch': 0.49} {'loss': 0.4165, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57} {'eval_loss': 0.36435264348983765, 'eval_accuracy': 0.8603158430973, 'eval_runtime': 18.5529, 'eval_samples_per_second': 529.029, 'eval_steps_per_second': 8.301, 'epoch': 0.57} {'loss': 0.4132, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65} {'eval_loss': 0.35739392042160034, 'eval_accuracy': 0.8606214977075904, 'eval_runtime': 18.5392, 'eval_samples_per_second': 529.418, 'eval_steps_per_second': 8.307, 'epoch': 0.65} {'loss': 0.4015, 'learning_rate': 2.670306388526728e-06, 'epoch': 0.73} {'eval_loss': 0.36193060874938965, 'eval_accuracy': 0.8631686194600102, 'eval_runtime': 18.5487, 'eval_samples_per_second': 529.149, 'eval_steps_per_second': 8.302, 'epoch': 0.73} {'loss': 0.3948, 'learning_rate': 1.8554432855280313e-06, 'epoch': 0.81} {'eval_loss': 0.3555583655834198, 'eval_accuracy': 0.8639836984207845, 'eval_runtime': 18.5639, 'eval_samples_per_second': 528.713, 'eval_steps_per_second': 8.296, 'epoch': 0.81} {'loss': 0.3967, 'learning_rate': 1.0413950456323338e-06, 'epoch': 0.9} {'eval_loss': 0.35024452209472656, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 18.5496, 'eval_samples_per_second': 529.121, 'eval_steps_per_second': 8.302, 'epoch': 0.9} {'loss': 0.3979, 'learning_rate': 2.2734680573663624e-07, 'epoch': 0.98} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.5647, 'eval_samples_per_second': 528.693, 'eval_steps_per_second': 8.295, 'epoch': 0.98} {'train_runtime': 2621.6937, 'train_samples_per_second': 149.789, 'train_steps_per_second': 4.681, 'train_loss': 0.4466879855855969, 'epoch': 1.0} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.571, 'eval_samples_per_second': 528.511, 'eval_steps_per_second': 8.292, 'epoch': 1.0} {'eval_loss': 0.34961390495300293, 'eval_accuracy': 0.8664289353031075, 'eval_runtime': 18.571, 'eval_samples_per_second': 528.511, 'eval_steps_per_second': 8.292, 'epoch': 1.0}","title":"(Standard) fine-tuning model"},{"location":"quantization/quantization/#add-quantization-support-to-any-model","text":"The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. If you want to know more, check our documentation on: https://els-rd.github.io/transformer-deploy/quantization/quantization_ast/ In [9]: Copied! for percentile in [ 99.9 , 99.99 , 99.999 , 99.9999 ]: with QATCalibrate ( method = \"histogram\" , percentile = percentile ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( f \"percentile: { percentile } \" ) print ( trainer . evaluate ()) for percentile in [99.9, 99.99, 99.999, 99.9999]: with QATCalibrate(method=\"histogram\", percentile=percentile) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(f\"percentile: {percentile}\") print(trainer.evaluate()) [INFO|trainer.py:457] 2022-03-09 20:04:30,756 >> Using amp half precision backend percentile: 99.9 {'eval_loss': 0.4834432005882263, 'eval_accuracy': 0.8129393785022924, 'eval_runtime': 46.3749, 'eval_samples_per_second': 211.645, 'eval_steps_per_second': 3.321} {'eval_loss': 0.4834432005882263, 'eval_accuracy': 0.8129393785022924, 'eval_runtime': 46.3749, 'eval_samples_per_second': 211.645, 'eval_steps_per_second': 3.321} [INFO|trainer.py:457] 2022-03-09 20:08:47,875 >> Using amp half precision backend percentile: 99.99 {'eval_loss': 0.3795105814933777, 'eval_accuracy': 0.8555272542027509, 'eval_runtime': 46.4677, 'eval_samples_per_second': 211.222, 'eval_steps_per_second': 3.314} {'eval_loss': 0.3795105814933777, 'eval_accuracy': 0.8555272542027509, 'eval_runtime': 46.4677, 'eval_samples_per_second': 211.222, 'eval_steps_per_second': 3.314} [INFO|trainer.py:457] 2022-03-09 20:13:04,907 >> Using amp half precision backend percentile: 99.999 {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4646, 'eval_samples_per_second': 211.236, 'eval_steps_per_second': 3.314} {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4646, 'eval_samples_per_second': 211.236, 'eval_steps_per_second': 3.314} [INFO|trainer.py:457] 2022-03-09 20:17:16,064 >> Using amp half precision backend percentile: 99.9999 {'eval_loss': 0.9809716939926147, 'eval_accuracy': 0.5092205807437595, 'eval_runtime': 46.4947, 'eval_samples_per_second': 211.099, 'eval_steps_per_second': 3.312} {'eval_loss': 0.9809716939926147, 'eval_accuracy': 0.5092205807437595, 'eval_runtime': 46.4947, 'eval_samples_per_second': 211.099, 'eval_steps_per_second': 3.312} As you can see, the chosen percentile value has a high impact on the final accuracy. For the rest of the notebook, we apply the 99.999 percentile. In [10]: Copied! with QATCalibrate ( method = \"histogram\" , percentile = 99.999 ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( trainer . evaluate ()) with QATCalibrate(method=\"histogram\", percentile=99.999) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(trainer.evaluate()) [INFO|trainer.py:457] 2022-03-09 20:21:33,073 >> Using amp half precision backend {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4748, 'eval_samples_per_second': 211.19, 'eval_steps_per_second': 3.314} {'eval_loss': 0.38251793384552, 'eval_accuracy': 0.8548140601120734, 'eval_runtime': 46.4748, 'eval_samples_per_second': 211.19, 'eval_steps_per_second': 3.314}","title":"Add quantization support to any model"},{"location":"quantization/quantization/#per-layer-quantization-analysis","text":"Below we will run a sensitivity analysis, by enabling quantization of one layer at a time and measuring the accuracy. That way we will be able to detect if the quantization of a specific layer has a larger cost on accuracy than other layers. In [11]: Copied! from pytorch_quantization import nn as quant_nn for i in range ( 12 ): layer_name = f \"layer. { i } \" print ( layer_name ) for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if layer_name in name : module . enable_quant () else : module . disable_quant () trainer . evaluate () print ( \"----\" ) from pytorch_quantization import nn as quant_nn for i in range(12): layer_name = f\"layer.{i}\" print(layer_name) for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if layer_name in name: module.enable_quant() else: module.disable_quant() trainer.evaluate() print(\"----\") layer.0 {'eval_loss': 0.34956735372543335, 'eval_accuracy': 0.86571574121243, 'eval_runtime': 20.7064, 'eval_samples_per_second': 474.009, 'eval_steps_per_second': 7.437} ---- layer.1 {'eval_loss': 0.3523275852203369, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 25.4843, 'eval_samples_per_second': 385.14, 'eval_steps_per_second': 6.043} ---- layer.2 {'eval_loss': 0.356509268283844, 'eval_accuracy': 0.8622516556291391, 'eval_runtime': 20.7496, 'eval_samples_per_second': 473.021, 'eval_steps_per_second': 7.422} ---- layer.3 {'eval_loss': 0.36036217212677, 'eval_accuracy': 0.8617422312786551, 'eval_runtime': 20.7815, 'eval_samples_per_second': 472.296, 'eval_steps_per_second': 7.41} ---- layer.4 {'eval_loss': 0.35000357031822205, 'eval_accuracy': 0.8643912379011717, 'eval_runtime': 20.7921, 'eval_samples_per_second': 472.053, 'eval_steps_per_second': 7.407} ---- layer.5 {'eval_loss': 0.354992538690567, 'eval_accuracy': 0.8644931227712684, 'eval_runtime': 20.7938, 'eval_samples_per_second': 472.016, 'eval_steps_per_second': 7.406} ---- layer.6 {'eval_loss': 0.35205718874931335, 'eval_accuracy': 0.8645950076413652, 'eval_runtime': 20.7918, 'eval_samples_per_second': 472.061, 'eval_steps_per_second': 7.407} ---- layer.7 {'eval_loss': 0.35065746307373047, 'eval_accuracy': 0.8655119714722364, 'eval_runtime': 20.8011, 'eval_samples_per_second': 471.849, 'eval_steps_per_second': 7.403} ---- layer.8 {'eval_loss': 0.3491470217704773, 'eval_accuracy': 0.8659195109526235, 'eval_runtime': 20.8112, 'eval_samples_per_second': 471.621, 'eval_steps_per_second': 7.4} ---- layer.9 {'eval_loss': 0.3492998480796814, 'eval_accuracy': 0.8659195109526235, 'eval_runtime': 20.8695, 'eval_samples_per_second': 470.303, 'eval_steps_per_second': 7.379} ---- layer.10 {'eval_loss': 0.3501480221748352, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 20.8698, 'eval_samples_per_second': 470.296, 'eval_steps_per_second': 7.379} ---- layer.11 {'eval_loss': 0.3497083783149719, 'eval_accuracy': 0.866225165562914, 'eval_runtime': 20.9345, 'eval_samples_per_second': 468.843, 'eval_steps_per_second': 7.356} ---- It seems that quantization of layers 2 to 6 has the largest accuracy impact.","title":"Per layer quantization analysis"},{"location":"quantization/quantization/#operator-quantization-analysis","text":"Below we will run a sensitivity analysis, by enabling quantization of one operator type at a time and measuring the accuracy. That way we will be able to detect if a specific operator has a larger cost on accuracy. On Roberta we only quantize matmul and LayerNorm , so we test both candidates. In [12]: Copied! for op in [ \"matmul\" , \"layernorm\" ]: for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if op in name : module . enable_quant () else : module . disable_quant () print ( op ) trainer . evaluate () print ( \"----\" ) for op in [\"matmul\", \"layernorm\"]: for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if op in name: module.enable_quant() else: module.disable_quant() print(op) trainer.evaluate() print(\"----\") matmul {'eval_loss': 0.3494793176651001, 'eval_accuracy': 0.8654100866021396, 'eval_runtime': 26.892, 'eval_samples_per_second': 364.978, 'eval_steps_per_second': 5.727} ---- layernorm {'eval_loss': 0.3585323095321655, 'eval_accuracy': 0.8587875700458482, 'eval_runtime': 24.0982, 'eval_samples_per_second': 407.293, 'eval_steps_per_second': 6.391} ---- It appears that LayerNorm quantization has a significant accuracy cost. Our goal is to disable quantization for as few operations as possible while preserving accuracy as much as possible. Therefore we will try to only disable quantization for LayerNorm on Layers 2 to 6. In [13]: Copied! disable_layer_names = [ \"layer.2\" , \"layer.3\" , \"layer.4\" , \"layer.6\" ] for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if any ([ f \" { l } .output.layernorm\" in name for l in disable_layer_names ]): print ( f \"disable { name } \" ) module . disable_quant () else : module . enable_quant () trainer . evaluate () disable_layer_names = [\"layer.2\", \"layer.3\", \"layer.4\", \"layer.6\"] for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if any([f\"{l}.output.layernorm\" in name for l in disable_layer_names]): print(f\"disable {name}\") module.disable_quant() else: module.enable_quant() trainer.evaluate() disable roberta.encoder.layer.2.output.layernorm_quantizer_0 disable roberta.encoder.layer.2.output.layernorm_quantizer_1 disable roberta.encoder.layer.3.output.layernorm_quantizer_0 disable roberta.encoder.layer.3.output.layernorm_quantizer_1 disable roberta.encoder.layer.4.output.layernorm_quantizer_0 disable roberta.encoder.layer.4.output.layernorm_quantizer_1 disable roberta.encoder.layer.6.output.layernorm_quantizer_0 disable roberta.encoder.layer.6.output.layernorm_quantizer_1 {'eval_loss': 0.3617263436317444, 'eval_accuracy': 0.8614365766683647, 'eval_runtime': 46.0379, 'eval_samples_per_second': 213.194, 'eval_steps_per_second': 3.345} Out[13]: {'eval_loss': 0.3617263436317444, 'eval_accuracy': 0.8614365766683647, 'eval_runtime': 46.0379, 'eval_samples_per_second': 213.194, 'eval_steps_per_second': 3.345} By just disabling quantization for a single operator on a few layers, we keep most of the performance boost (quantization) but retrieve more than 1 point of accuracy. It's also possible to perform an analysis per quantizer to get a smaller granularity but it's a bit slow to run. If we stop here, it's called a Post Training Quantization (PTQ). Below, we will try to retrieve even more accuracy.","title":"Operator quantization analysis"},{"location":"quantization/quantization/#quantization-aware-training-qat","text":"We retrain the model with 1/10 or 1/100 of the original learning rate. Our goal is to retrieve most of the original accuracy. In [14]: Copied! args . learning_rate = 1e-7 trainer = get_trainer ( model_q ) trainer . train () print ( trainer . evaluate ()) model_q . save_pretrained ( \"model-qat\" ) args.learning_rate = 1e-7 trainer = get_trainer(model_q) trainer.train() print(trainer.evaluate()) model_q.save_pretrained(\"model-qat\") [INFO|trainer.py:457] 2022-03-09 20:28:11,049 >> Using amp half precision backend /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( {'loss': 0.3628, 'learning_rate': 9.1867666232073e-08, 'epoch': 0.08} {'eval_loss': 0.37072187662124634, 'eval_accuracy': 0.8610290371879776, 'eval_runtime': 46.0646, 'eval_samples_per_second': 213.071, 'eval_steps_per_second': 3.343, 'epoch': 0.08} {'loss': 0.3159, 'learning_rate': 8.371903520208604e-08, 'epoch': 0.16} {'eval_loss': 0.3802173137664795, 'eval_accuracy': 0.8589913397860418, 'eval_runtime': 46.1087, 'eval_samples_per_second': 212.866, 'eval_steps_per_second': 3.34, 'epoch': 0.16} {'loss': 0.3038, 'learning_rate': 7.557855280312907e-08, 'epoch': 0.24} {'eval_loss': 0.38381442427635193, 'eval_accuracy': 0.8597045338767193, 'eval_runtime': 46.1153, 'eval_samples_per_second': 212.836, 'eval_steps_per_second': 3.339, 'epoch': 0.24} {'loss': 0.2981, 'learning_rate': 6.74299217731421e-08, 'epoch': 0.33} {'eval_loss': 0.39187753200531006, 'eval_accuracy': 0.8617422312786551, 'eval_runtime': 46.0955, 'eval_samples_per_second': 212.927, 'eval_steps_per_second': 3.341, 'epoch': 0.33} {'loss': 0.2979, 'learning_rate': 5.9289439374185136e-08, 'epoch': 0.41} {'eval_loss': 0.39416825771331787, 'eval_accuracy': 0.8601120733571065, 'eval_runtime': 46.1939, 'eval_samples_per_second': 212.474, 'eval_steps_per_second': 3.334, 'epoch': 0.41} {'loss': 0.3041, 'learning_rate': 5.114080834419817e-08, 'epoch': 0.49} {'eval_loss': 0.39381393790245056, 'eval_accuracy': 0.8609271523178808, 'eval_runtime': 46.1092, 'eval_samples_per_second': 212.864, 'eval_steps_per_second': 3.34, 'epoch': 0.49} {'loss': 0.3122, 'learning_rate': 4.3008474576271185e-08, 'epoch': 0.57} {'eval_loss': 0.39094462990760803, 'eval_accuracy': 0.8600101884870097, 'eval_runtime': 46.1377, 'eval_samples_per_second': 212.733, 'eval_steps_per_second': 3.338, 'epoch': 0.57} {'loss': 0.3297, 'learning_rate': 3.485984354628422e-08, 'epoch': 0.65} {'eval_loss': 0.3906724452972412, 'eval_accuracy': 0.8616403464085584, 'eval_runtime': 46.2006, 'eval_samples_per_second': 212.443, 'eval_steps_per_second': 3.333, 'epoch': 0.65} {'loss': 0.338, 'learning_rate': 2.671121251629726e-08, 'epoch': 0.73} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 46.1088, 'eval_samples_per_second': 212.866, 'eval_steps_per_second': 3.34, 'epoch': 0.73} {'loss': 0.3557, 'learning_rate': 1.8570730117340286e-08, 'epoch': 0.81} {'eval_loss': 0.39177224040031433, 'eval_accuracy': 0.8593988792664289, 'eval_runtime': 49.4531, 'eval_samples_per_second': 198.471, 'eval_steps_per_second': 3.114, 'epoch': 0.81} {'loss': 0.385, 'learning_rate': 1.0430247718383311e-08, 'epoch': 0.9} {'eval_loss': 0.38946154713630676, 'eval_accuracy': 0.8593988792664289, 'eval_runtime': 51.1685, 'eval_samples_per_second': 191.817, 'eval_steps_per_second': 3.01, 'epoch': 0.9} {'loss': 0.4133, 'learning_rate': 2.2816166883963493e-09, 'epoch': 0.98} {'eval_loss': 0.3868817687034607, 'eval_accuracy': 0.8611309220580744, 'eval_runtime': 46.8425, 'eval_samples_per_second': 209.532, 'eval_steps_per_second': 3.288, 'epoch': 0.98} {'train_runtime': 5020.8729, 'train_samples_per_second': 78.214, 'train_steps_per_second': 2.444, 'train_loss': 0.33702789975580366, 'epoch': 1.0} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 47.1812, 'eval_samples_per_second': 208.028, 'eval_steps_per_second': 3.264, 'epoch': 1.0} {'eval_loss': 0.38998448848724365, 'eval_accuracy': 0.8624554253693326, 'eval_runtime': 47.1812, 'eval_samples_per_second': 208.028, 'eval_steps_per_second': 3.264, 'epoch': 1.0}","title":"Quantization Aware Training (QAT)"},{"location":"quantization/quantization/#export-a-qdq-pytorch-model-to-onnx","text":"We need to enable fake quantization mode from Pytorch. In [15]: Copied! data = encoded_dataset [ \"train\" ][ 1 : 3 ] input_torch = convert_tensor ( data , output = \"torch\" ) convert_to_onnx ( model_pytorch = model_q , output_path = \"model_qat.onnx\" , inputs_pytorch = input_torch , quantization = True , var_output_seq = False , ) data = encoded_dataset[\"train\"][1:3] input_torch = convert_tensor(data, output=\"torch\") convert_to_onnx( model_pytorch=model_q, output_path=\"model_qat.onnx\", inputs_pytorch=input_torch, quantization=True, var_output_seq=False, ) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! inputs, amax.item() / bound, 0, /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0]) In [16]: Copied! del model_q QATCalibrate . restore () del model_q QATCalibrate.restore()","title":"Export a QDQ Pytorch model to ONNX"},{"location":"quantization/quantization/#benchmark","text":"","title":"Benchmark"},{"location":"quantization/quantization/#convert-onnx-graph-to-tensorrt-engine","text":"In [17]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) engine = build_engine( runtime=runtime, onnx_file_path=\"model_qat.onnx\", logger=trt_logger, min_shape=(1, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=True, ) In [18]: Copied! # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\"","title":"Convert ONNX graph to TensorRT engine"},{"location":"quantization/quantization/#prepare-input-and-output-buffer","text":"In [19]: Copied! context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async( profile_index=profile_index, stream_handle=torch.cuda.current_stream().cuda_stream ) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] In [20]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")","title":"Prepare input and output buffer"},{"location":"quantization/quantization/#inference-on-tensorrt","text":"We first check that inference is working correctly: In [21]: Copied! tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print ( tensorrt_output ) tensorrt_output = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print(tensorrt_output) [tensor([[ 1.2287, 1.3706, -2.4623], [ 2.4742, -0.7816, -1.8427], [ 2.4837, -0.3966, -2.2666], [ 2.9077, -0.3062, -2.9778], [ 2.3437, 0.0488, -2.6377], [ 3.7914, -1.1918, -3.1387], [-3.6134, 2.7432, 0.8490], [ 3.6679, -1.5787, -2.6408], [ 1.0155, -1.2787, 0.4250], [-3.4514, -0.4434, 4.2748], [ 3.5201, -1.1297, -2.8988], [-3.0225, -0.4062, 3.8606], [-2.7311, 3.5470, -0.4632], [-2.0741, 1.6613, 0.5798], [-0.4047, -0.8650, 1.6144], [ 2.8432, -1.3301, -1.8994], [ 3.7722, -0.9103, -3.3070], [-2.4204, -2.1432, 4.6537], [-3.1179, -1.3207, 4.6400], [-1.8794, 4.1075, -1.8630], [ 3.7726, -1.2056, -3.0701], [ 1.8645, 1.9744, -3.8743], [-3.1448, -1.2497, 4.5782], [ 3.5385, -0.2421, -3.6629], [ 3.7501, -1.6469, -2.7108], [-0.6568, 0.9046, -0.0228], [-3.2998, 0.0867, 3.3673], [-2.1030, 4.0461, -1.6705], [-3.7080, 0.4164, 3.4332], [ 3.6850, -0.9984, -3.0304], [ 3.4525, -0.5405, -3.2981], [ 3.6128, -0.9298, -3.0746]], device='cuda:0')] Measure of the accuracy: In [22]: Copied! infer_trt = lambda inputs : infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) measure_accuracy ( infer = infer_trt , tensor_type = \"torch\" ) infer_trt = lambda inputs: infer_tensorrt( context=context, host_inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) measure_accuracy(infer=infer_trt, tensor_type=\"torch\") Out[22]: 0.8618441161487519 Latency measures: In [23]: Copied! time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print_timings ( name = \"TensorRT (INT-8)\" , timings = time_buffer ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer) [TensorRT (INT-8)] mean=18.04ms, sd=2.02ms, min=16.67ms, max=28.88ms, median=17.27ms, 95p=20.04ms, 99p=28.86ms In [24]: Copied! del engine , context del engine, context","title":"Inference on TensorRT"},{"location":"quantization/quantization/#pytorch-baseline","text":"Time to get some numbers to compare with.","title":"Pytorch baseline"},{"location":"quantization/quantization/#gpu-execution","text":"We will measure vanilla Pytorch inference on both FP32 and FP16 precision on GPU, it will be our baseline: In [25]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) with torch . inference_mode (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32)\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") with torch.inference_mode(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32)\", timings=time_buffer) [Pytorch (FP32)] mean=82.28ms, sd=7.72ms, min=75.77ms, max=120.13ms, median=79.73ms, 95p=97.02ms, 99p=110.68ms In [26]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16)\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = [] for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16)\", timings=time_buffer) del baseline_model [Pytorch (FP16)] mean=58.73ms, sd=1.88ms, min=55.56ms, max=69.69ms, median=58.03ms, 95p=62.28ms, 99p=64.42ms","title":"GPU execution"},{"location":"quantization/quantization/#cpu-execution","text":"In [27]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_torch_cpu = { k : v . to ( \"cpu\" ) for k , v in input_torch . items ()} torch . set_num_threads ( os . cpu_count ()) with torch . inference_mode (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32) - CPU\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_torch_cpu = {k: v.to(\"cpu\") for k, v in input_torch.items()} torch.set_num_threads(os.cpu_count()) with torch.inference_mode(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32) - CPU\", timings=time_buffer) [Pytorch (FP32) - CPU] mean=5141.10ms, sd=654.18ms, min=4095.59ms, max=6229.84ms, median=5347.27ms, 95p=5958.43ms, 99p=6175.56ms In [28]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16) - CPU\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = [] for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16) - CPU\", timings=time_buffer) del baseline_model [Pytorch (FP16) - CPU] mean=4422.39ms, sd=170.45ms, min=4250.10ms, max=4744.97ms, median=4337.85ms, 95p=4727.72ms, 99p=4741.52ms Below, we will perform dynamic quantization on CPU. In [29]: Copied! quantized_baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) quantized_baseline_model = quantized_baseline_model . eval () quantized_baseline_model = torch . quantization . quantize_dynamic ( quantized_baseline_model , { torch . nn . Linear }, dtype = torch . qint8 ) with torch . inference_mode (): for _ in range ( 3 ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (INT-8) - CPU\" , timings = time_buffer ) quantized_baseline_model = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) quantized_baseline_model = quantized_baseline_model.eval() quantized_baseline_model = torch.quantization.quantize_dynamic( quantized_baseline_model, {torch.nn.Linear}, dtype=torch.qint8 ) with torch.inference_mode(): for _ in range(3): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (INT-8) - CPU\", timings=time_buffer) [Pytorch (INT-8) - CPU] mean=3818.99ms, sd=137.98ms, min=3616.11ms, max=4049.00ms, median=3807.33ms, 95p=4024.45ms, 99p=4044.09ms","title":"CPU execution"},{"location":"quantization/quantization/#tensorrt-baseline","text":"Below we export our finetuned model, the purpose is to only check the performance on mixed precision (FP16, no quantization). In [30]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () convert_to_onnx ( baseline_model , output_path = \"baseline.onnx\" , inputs_pytorch = input_torch , quantization = False , var_output_seq = False ) del baseline_model baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() convert_to_onnx( baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch, quantization=False, var_output_seq=False ) del baseline_model In [31]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"baseline.onnx\" , logger = trt_logger , min_shape = ( batch_size , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = False , ) input_torch : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"torch\" ) context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = torch . cuda . current_stream () . cuda_stream ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] for _ in range ( 30 ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_torch , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , ) print_timings ( name = \"TensorRT (FP16)\" , timings = time_buffer ) del engine , context engine = build_engine( runtime=runtime, onnx_file_path=\"baseline.onnx\", logger=trt_logger, min_shape=(batch_size, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=False, ) input_torch: OD[str, np.ndarray] = convert_tensor(data=data, output=\"torch\") context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async( profile_index=profile_index, stream_handle=torch.cuda.current_stream().cuda_stream ) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] for _ in range(30): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_torch, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, ) print_timings(name=\"TensorRT (FP16)\", timings=time_buffer) del engine, context [TensorRT (FP16)] mean=32.36ms, sd=1.47ms, min=29.92ms, max=38.08ms, median=32.47ms, 95p=34.38ms, 99p=36.50ms","title":"TensorRT baseline"},{"location":"quantization/quantization/#onnx-runtime-baseline","text":"ONNX Runtime is the go to inference solution from Microsoft. The recent 1.10 version of ONNX Runtime (with TensorRT support) is still a bit buggy on transformer models, that is why we use the 1.9.0 version in the measures below. As before, CPU quantization is dynamic. Function will set ONNX Runtime to use all cores available and enable any possible optimizations. In [32]: Copied! optimize_onnx ( onnx_path = \"baseline.onnx\" , onnx_optim_model_path = \"baseline-optimized.onnx\" , fp16 = True , use_cuda = True , num_attention_heads = 12 , hidden_size = 768 , architecture = \"bert\" , ) optimize_onnx( onnx_path=\"baseline.onnx\", onnx_optim_model_path=\"baseline-optimized.onnx\", fp16=True, use_cuda=True, num_attention_heads=12, hidden_size=768, architecture=\"bert\", ) failed in shape inference <class 'AssertionError'> In [33]: Copied! cpu_quantization ( input_model_path = \"baseline.onnx\" , output_model_path = \"baseline-quantized.onnx\" ) cpu_quantization(input_model_path=\"baseline.onnx\", output_model_path=\"baseline-quantized.onnx\") Ignore MatMul due to non constant B: /[MatMul_108] Ignore MatMul due to non constant B: /[MatMul_113] Ignore MatMul due to non constant B: /[MatMul_210] Ignore MatMul due to non constant B: /[MatMul_215] Ignore MatMul due to non constant B: /[MatMul_312] Ignore MatMul due to non constant B: /[MatMul_317] Ignore MatMul due to non constant B: /[MatMul_414] Ignore MatMul due to non constant B: /[MatMul_419] Ignore MatMul due to non constant B: /[MatMul_516] Ignore MatMul due to non constant B: /[MatMul_521] Ignore MatMul due to non constant B: /[MatMul_618] Ignore MatMul due to non constant B: /[MatMul_623] Ignore MatMul due to non constant B: /[MatMul_720] Ignore MatMul due to non constant B: /[MatMul_725] Ignore MatMul due to non constant B: /[MatMul_822] Ignore MatMul due to non constant B: /[MatMul_827] Ignore MatMul due to non constant B: /[MatMul_924] Ignore MatMul due to non constant B: /[MatMul_929] Ignore MatMul due to non constant B: /[MatMul_1026] Ignore MatMul due to non constant B: /[MatMul_1031] Ignore MatMul due to non constant B: /[MatMul_1128] Ignore MatMul due to non constant B: /[MatMul_1133] Ignore MatMul due to non constant B: /[MatMul_1230] Ignore MatMul due to non constant B: /[MatMul_1235] In [34]: Copied! labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] data = encoded_dataset [ validation_key ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) output = model . run ( None , inputs_onnx ) labels = [item[\"label\"] for item in encoded_dataset[validation_key]] data = encoded_dataset[validation_key][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") output = model.run(None, inputs_onnx) In [35]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for provider , model_path , benchmark_name , warmup , nb_inference in [ ( \"CUDAExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime GPU (FP32)\" , 10 , 100 ), ( \"CUDAExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime GPU (FP16)\" , 10 , 100 ), ( \"CPUExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime CPU (FP32)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime CPU (FP16)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-quantized.onnx\" , \"ONNX Runtime CPU (INT-8)\" , 3 , 10 ), ]: model = create_model_for_provider ( path = model_path , provider_to_use = provider ) for _ in range ( warmup ): _ = model . run ( None , inputs_onnx ) time_buffer = [] for _ in range ( nb_inference ): with track_infer_time ( time_buffer ): _ = model . run ( None , inputs_onnx ) print_timings ( name = benchmark_name , timings = time_buffer ) del model data = encoded_dataset[\"train\"][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for provider, model_path, benchmark_name, warmup, nb_inference in [ (\"CUDAExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime GPU (FP32)\", 10, 100), (\"CUDAExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime GPU (FP16)\", 10, 100), (\"CPUExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime CPU (FP32)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime CPU (FP16)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-quantized.onnx\", \"ONNX Runtime CPU (INT-8)\", 3, 10), ]: model = create_model_for_provider(path=model_path, provider_to_use=provider) for _ in range(warmup): _ = model.run(None, inputs_onnx) time_buffer = [] for _ in range(nb_inference): with track_infer_time(time_buffer): _ = model.run(None, inputs_onnx) print_timings(name=benchmark_name, timings=time_buffer) del model [ONNX Runtime GPU (FP32)] mean=82.50ms, sd=11.53ms, min=74.12ms, max=118.73ms, median=77.62ms, 95p=112.51ms, 99p=117.81ms [ONNX Runtime GPU (FP16)] mean=36.23ms, sd=4.01ms, min=33.47ms, max=55.75ms, median=35.35ms, 95p=46.89ms, 99p=53.95ms [ONNX Runtime CPU (FP32)] mean=4608.31ms, sd=468.16ms, min=3954.09ms, max=5226.20ms, median=4531.47ms, 95p=5189.71ms, 99p=5218.90ms [ONNX Runtime CPU (FP16)] mean=3987.95ms, sd=223.00ms, min=3755.70ms, max=4548.57ms, median=3907.74ms, 95p=4375.23ms, 99p=4513.90ms [ONNX Runtime CPU (INT-8)] mean=3506.39ms, sd=125.07ms, min=3425.41ms, max=3872.95ms, median=3463.65ms, 95p=3713.08ms, 99p=3840.98ms Measure of the accuracy with ONNX Runtime engine and CUDA provider: In [36]: Copied! model = create_model_for_provider ( path = \"baseline.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[36]: 0.8665308201732043 In [37]: Copied! model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[37]: 0.8663270504330107 In [38]: Copied! model = create_model_for_provider ( path = \"baseline-quantized.onnx\" , provider_to_use = \"CPUExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , tensor_type = \"np\" ) model = create_model_for_provider(path=\"baseline-quantized.onnx\", provider_to_use=\"CPUExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, tensor_type=\"np\") Out[38]: 0.8604177279673968 In [39]: Copied! del model del model In [40]: Copied!","title":"ONNX Runtime baseline"},{"location":"quantization/quantization_ast/","text":"Add quantization support to any model # The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. That way, quantization will work out of the box for the final user. The process is based on Python AST modification, basically we parse the model source code in RAM, we convert it to a tree, then we patch the tree to add the QDQ nodes and we replace, still in RAM, the original module source code. Our library also offer the option to restore original behavior. In theory it works for any model. However, not related to quantization, some models are not fully compliant with TensorRT (unsupported operators, etc.). For those models, we rewrite some part of the source code, these patches are manually written but are applied to the model at run time (like the AST manipulation). Info concrete examples on Roberta architecture: in Hugging Face library, there is a cumsum operator used during the position embedding generation. Something very simple. It takes as input an integer tensor and output an integer tensor. It happens that the cumsum operator from TensorRT supports float but not integer (https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md). It leads to a crash during the model conversion with a strange error message. Converting the input to float tensor fixes the issue. The process is simple: Calibrate, after that you have a PTQ second fine tuning, it's the QAT (optional) Info there are many ways to get a QDQ model, you can modify Pytorch source code (including doing it at runtime like here), patch ONNX graph (this approach is used at Microsoft for instance but only support PTQ , not QAT as ONNX file can't be trained on Pytorch for now) or leverage the new FX Pytorch interface (it's a bit experimental and it seems to miss some feature to support Nvidia QAT library). Modifying the source code is the most straightforward, and doing it through AST is the least intrusive (no need to duplicate the work of HF). Concretly, minimal user code looks like that: apply_quantization.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from transformers import AutoModelForSequenceClassification from transformer_deploy.QDQModels.calibration_utils import QATCalibrate my_data_loader = ... with QATCalibrate () as qat : model_q = AutoModelForSequenceClassification . from_pretrained ( \"my/model\" ) model_q . cuda () qat . setup_model_qat ( model_q ) with torch . no_grad (): for data in my_data_loader : model_q ( ** data ) # <- calibration happens here model_q . save_model_to_file ( \"path/to/somewhere\" ) ... and then in shell convert_model -m path/to/somewhere --quantization ... Info The first context manager will enable quantization support, and the setup_model_qat() method will add the QDQ nodes.","title":"How is it implemented in this library?"},{"location":"quantization/quantization_ast/#add-quantization-support-to-any-model","text":"The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. That way, quantization will work out of the box for the final user. The process is based on Python AST modification, basically we parse the model source code in RAM, we convert it to a tree, then we patch the tree to add the QDQ nodes and we replace, still in RAM, the original module source code. Our library also offer the option to restore original behavior. In theory it works for any model. However, not related to quantization, some models are not fully compliant with TensorRT (unsupported operators, etc.). For those models, we rewrite some part of the source code, these patches are manually written but are applied to the model at run time (like the AST manipulation). Info concrete examples on Roberta architecture: in Hugging Face library, there is a cumsum operator used during the position embedding generation. Something very simple. It takes as input an integer tensor and output an integer tensor. It happens that the cumsum operator from TensorRT supports float but not integer (https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md). It leads to a crash during the model conversion with a strange error message. Converting the input to float tensor fixes the issue. The process is simple: Calibrate, after that you have a PTQ second fine tuning, it's the QAT (optional) Info there are many ways to get a QDQ model, you can modify Pytorch source code (including doing it at runtime like here), patch ONNX graph (this approach is used at Microsoft for instance but only support PTQ , not QAT as ONNX file can't be trained on Pytorch for now) or leverage the new FX Pytorch interface (it's a bit experimental and it seems to miss some feature to support Nvidia QAT library). Modifying the source code is the most straightforward, and doing it through AST is the least intrusive (no need to duplicate the work of HF). Concretly, minimal user code looks like that: apply_quantization.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from transformers import AutoModelForSequenceClassification from transformer_deploy.QDQModels.calibration_utils import QATCalibrate my_data_loader = ... with QATCalibrate () as qat : model_q = AutoModelForSequenceClassification . from_pretrained ( \"my/model\" ) model_q . cuda () qat . setup_model_qat ( model_q ) with torch . no_grad (): for data in my_data_loader : model_q ( ** data ) # <- calibration happens here model_q . save_model_to_file ( \"path/to/somewhere\" ) ... and then in shell convert_model -m path/to/somewhere --quantization ... Info The first context manager will enable quantization support, and the setup_model_qat() method will add the QDQ nodes.","title":"Add quantization support to any model"},{"location":"quantization/quantization_intro/","text":"Nvidia GPU INT-8 quantization # What is it about? # Quantization is one of the most effective and generic approaches to make model inference faster. Basically, it replaces high precision float numbers in model tensors encoded in 32 or 16 bits by lower precision ones encoded in 8 bits or less: it takes less memory computation is easier / faster It can be applied to any model in theory, and, if done well, it should maintain accuracy. The purpose of this notebook is to show a process to perform quantization on any Transformer architectures. Moreover, the library is designed to offer a simple API and still let advanced users tweak the algorithm. Benchmark # TL;DR We benchmarked Pytorch and Nvidia TensorRT, on both CPU and GPU , with/without quantization, our methods provide the fastest inference by large margin. Framework Precision Latency (ms) Accuracy Speedup Hardware Pytorch FP32 4267 86.6 % X 0.02 CPU Pytorch FP16 4428 86.6 % X 0.02 CPU Pytorch INT-8 3300 85.9 % X 0.02 CPU Pytorch FP32 77 86.6 % X 1 GPU Pytorch FP16 56 86.6 % X 1.38 GPU ONNX Runtime FP32 76 86.6 % X 1.01 GPU ONNX Runtime FP16 34 86.6 % X 2.26 GPU ONNX Runtime FP32 4023 86.6 % X 0.02 CPU ONNX Runtime FP16 3957 86.6 % X 0.02 CPU ONNX Runtime INT-8 3336 86.5 % X 0.02 CPU TensorRT FP16 30 86.6 % X 2.57 GPU TensorRT ( our method ) INT-8 17 86.2 % X 4.53 GPU Note measures done on a Nvidia RTX 3090 GPU + 12 cores i7 Intel CPU (support AVX-2 instruction) Roberta base architecture flavor with batch of size 32 / seq len 256, similar results obtained for other sizes/seq len not included in the table. Accuracy obtained after a single epoch, no LR search or any hyper parameter optimization Check the end to end demo to see where these numbers are from.","title":"Why using quantization?"},{"location":"quantization/quantization_intro/#nvidia-gpu-int-8-quantization","text":"","title":"Nvidia GPU INT-8 quantization"},{"location":"quantization/quantization_intro/#what-is-it-about","text":"Quantization is one of the most effective and generic approaches to make model inference faster. Basically, it replaces high precision float numbers in model tensors encoded in 32 or 16 bits by lower precision ones encoded in 8 bits or less: it takes less memory computation is easier / faster It can be applied to any model in theory, and, if done well, it should maintain accuracy. The purpose of this notebook is to show a process to perform quantization on any Transformer architectures. Moreover, the library is designed to offer a simple API and still let advanced users tweak the algorithm.","title":"What is it about?"},{"location":"quantization/quantization_intro/#benchmark","text":"TL;DR We benchmarked Pytorch and Nvidia TensorRT, on both CPU and GPU , with/without quantization, our methods provide the fastest inference by large margin. Framework Precision Latency (ms) Accuracy Speedup Hardware Pytorch FP32 4267 86.6 % X 0.02 CPU Pytorch FP16 4428 86.6 % X 0.02 CPU Pytorch INT-8 3300 85.9 % X 0.02 CPU Pytorch FP32 77 86.6 % X 1 GPU Pytorch FP16 56 86.6 % X 1.38 GPU ONNX Runtime FP32 76 86.6 % X 1.01 GPU ONNX Runtime FP16 34 86.6 % X 2.26 GPU ONNX Runtime FP32 4023 86.6 % X 0.02 CPU ONNX Runtime FP16 3957 86.6 % X 0.02 CPU ONNX Runtime INT-8 3336 86.5 % X 0.02 CPU TensorRT FP16 30 86.6 % X 2.57 GPU TensorRT ( our method ) INT-8 17 86.2 % X 4.53 GPU Note measures done on a Nvidia RTX 3090 GPU + 12 cores i7 Intel CPU (support AVX-2 instruction) Roberta base architecture flavor with batch of size 32 / seq len 256, similar results obtained for other sizes/seq len not included in the table. Accuracy obtained after a single epoch, no LR search or any hyper parameter optimization Check the end to end demo to see where these numbers are from.","title":"Benchmark"},{"location":"quantization/quantization_ptq/","text":"Post Training Quantization ( PTQ ) and Quantization Aware Training ( QAT ) # What is it? # A PTQ is basically a fine tuned model where we add quantization nodes and that we calibrate. Calibration is a key step in the static quantization process. Its quality depends on the final accuracy (the inference speed will stay the same). Moreover, a good PTQ is a good basis for a good Quantization Aware Training ( QAT ). By calling with QATCalibrate(...) as qat: , the lib will patch Transformer model AST (source code) in RAM, basically adding quantization support to each model. How to tune a PTQ ? # One of the things we try to guess during the calibration is what range of tensor values capture most of the information stored in the tensor. Indeed, a FP32 tensor can store at the same time very large and very small values, we obviously can't do the same with a 8-bits integer tensors and a scale. An 8-bits integer can only encode 255 values so we need to fix some limits and say, if a value is outside our limits, it just takes a maximum value instead of its real one. For instance, if we say our range is -1000 to +1000 and a tensor contains the value +4000, it will be replaced by the maximum value, +1000. As said before, we will use the histogram method to find the perfect range. We also need to choose a percentile. Usually, you will choose something very close to 100. Danger If the percentile is too small, we put too many values outside the covered range. Values outside the range will be replaced by a single maximum value and you lose some granularity in model weights. If the percentile is too big, your range will be very large and because 8-bits signed integers can only encode values between -127 to +127, even when you use a scale you lose in granularity. Therefore, we in our demo, we launched a grid search on percentile hyper parameter and retrived 1 accuracy point with very little effort. One step further, the QAT # If it's not enough, the last step is to just fine tune a second the model. It helps to retrieve a bit of accuracy.","title":"PTQ and QAT, what are they?"},{"location":"quantization/quantization_ptq/#post-training-quantization-ptq-and-quantization-aware-training-qat","text":"","title":"Post Training Quantization (PTQ) and Quantization Aware Training (QAT)"},{"location":"quantization/quantization_ptq/#what-is-it","text":"A PTQ is basically a fine tuned model where we add quantization nodes and that we calibrate. Calibration is a key step in the static quantization process. Its quality depends on the final accuracy (the inference speed will stay the same). Moreover, a good PTQ is a good basis for a good Quantization Aware Training ( QAT ). By calling with QATCalibrate(...) as qat: , the lib will patch Transformer model AST (source code) in RAM, basically adding quantization support to each model.","title":"What is it?"},{"location":"quantization/quantization_ptq/#how-to-tune-a-ptq","text":"One of the things we try to guess during the calibration is what range of tensor values capture most of the information stored in the tensor. Indeed, a FP32 tensor can store at the same time very large and very small values, we obviously can't do the same with a 8-bits integer tensors and a scale. An 8-bits integer can only encode 255 values so we need to fix some limits and say, if a value is outside our limits, it just takes a maximum value instead of its real one. For instance, if we say our range is -1000 to +1000 and a tensor contains the value +4000, it will be replaced by the maximum value, +1000. As said before, we will use the histogram method to find the perfect range. We also need to choose a percentile. Usually, you will choose something very close to 100. Danger If the percentile is too small, we put too many values outside the covered range. Values outside the range will be replaced by a single maximum value and you lose some granularity in model weights. If the percentile is too big, your range will be very large and because 8-bits signed integers can only encode values between -127 to +127, even when you use a scale you lose in granularity. Therefore, we in our demo, we launched a grid search on percentile hyper parameter and retrived 1 accuracy point with very little effort.","title":"How to tune a PTQ?"},{"location":"quantization/quantization_ptq/#one-step-further-the-qat","text":"If it's not enough, the last step is to just fine tune a second the model. It helps to retrieve a bit of accuracy.","title":"One step further, the QAT"},{"location":"quantization/quantization_theory/","text":"Some theory # A (very) short intro to INT-8 quantization # Basic idea behind model quantization is to replace tensors made of float numbers (usually encoded on 32 bits) by lower precision representation (integers encoded on 8 bits for Nvidia GPUs). Therefore computation is faster and model memory footprint is lower. Making tensor storage smaller makes memory transfer faster... and is also a source of computation acceleration. This approach is very interesting for its trade-off: you reduce inference time significantly, and it costs close to nothing in accuracy. Replacing float numbers by integers is done through a mapping. This step is called calibration , and its purpose is to compute for each tensor or each channel of a tensor (one of its dimensions) a range covering most weights and then define a scale and a distribution center to map float numbers to 8 bits integers. There are several ways to perform quantization, depending of how and when the calibration is performed: dynamically: the mapping is done online, during the inference, there are some overhead but it's usually the easiest to leverage, end user has very few configuration to set, statically, after training ( post training quantization or PTQ ): this way is efficient because quantization is done offline, before inference, but it may have an accuracy cost, statically, after training ( quantization aware training or QAT ): like a PTQ followed by a second fine tuning. Same efficiency but usually slightly better accuracy. Nvidia GPUs don't support dynamic quantization, CPU supports all types of quantization. Compared to PTQ , QAT better preserves accuracy and should be preferred in most cases. During the quantization aware training : in the inside, Pytorch will train with high precision float numbers, on the outside, Pytorch will simulate that a quantization has already been applied and output results accordingly (for loss computation for instance) The simulation process is done through the add of quantization / dequantization nodes, most often called QDQ , it's an abbreviation you will see often in the quantization world. Want to learn more about quantization? You can check this high quality blog post for more information. The process is well described in this Nvidia presentation Why does it matter? # CPU quantization is supported out of the box by Pytorch and ONNX Runtime . GPU quantization on the other side requires specific tools and process to be applied . In the specific case of Transformer models, few demos from Nvidia and Microsoft exist; they are all for the old vanilla Bert architecture. It doesn't support modern architectures out of the box, like Albert , Roberta , Deberta or Electra .","title":"Quantization theory"},{"location":"quantization/quantization_theory/#some-theory","text":"","title":"Some theory"},{"location":"quantization/quantization_theory/#a-very-short-intro-to-int-8-quantization","text":"Basic idea behind model quantization is to replace tensors made of float numbers (usually encoded on 32 bits) by lower precision representation (integers encoded on 8 bits for Nvidia GPUs). Therefore computation is faster and model memory footprint is lower. Making tensor storage smaller makes memory transfer faster... and is also a source of computation acceleration. This approach is very interesting for its trade-off: you reduce inference time significantly, and it costs close to nothing in accuracy. Replacing float numbers by integers is done through a mapping. This step is called calibration , and its purpose is to compute for each tensor or each channel of a tensor (one of its dimensions) a range covering most weights and then define a scale and a distribution center to map float numbers to 8 bits integers. There are several ways to perform quantization, depending of how and when the calibration is performed: dynamically: the mapping is done online, during the inference, there are some overhead but it's usually the easiest to leverage, end user has very few configuration to set, statically, after training ( post training quantization or PTQ ): this way is efficient because quantization is done offline, before inference, but it may have an accuracy cost, statically, after training ( quantization aware training or QAT ): like a PTQ followed by a second fine tuning. Same efficiency but usually slightly better accuracy. Nvidia GPUs don't support dynamic quantization, CPU supports all types of quantization. Compared to PTQ , QAT better preserves accuracy and should be preferred in most cases. During the quantization aware training : in the inside, Pytorch will train with high precision float numbers, on the outside, Pytorch will simulate that a quantization has already been applied and output results accordingly (for loss computation for instance) The simulation process is done through the add of quantization / dequantization nodes, most often called QDQ , it's an abbreviation you will see often in the quantization world. Want to learn more about quantization? You can check this high quality blog post for more information. The process is well described in this Nvidia presentation","title":"A (very) short intro to INT-8 quantization"},{"location":"quantization/quantization_theory/#why-does-it-matter","text":"CPU quantization is supported out of the box by Pytorch and ONNX Runtime . GPU quantization on the other side requires specific tools and process to be applied . In the specific case of Transformer models, few demos from Nvidia and Microsoft exist; they are all for the old vanilla Bert architecture. It doesn't support modern architectures out of the box, like Albert , Roberta , Deberta or Electra .","title":"Why does it matter?"},{"location":"reference/convert/","text":"This module contains code related to client interface. check_accuracy ( engine_name , pytorch_output , engine_output , tolerance ) # Compare engine predictions with a reference. Assert that the difference is under a threshold. Parameters: Name Type Description Default engine_name str string used in error message, if any required pytorch_output List[torch.Tensor] reference output used for the comparaison required engine_output List[Union[numpy.ndarray, torch.Tensor]] output from the engine required tolerance float if difference in outputs is above threshold, an error will be raised required Source code in src/transformer_deploy/convert.py def check_accuracy ( engine_name : str , pytorch_output : List [ torch . Tensor ], engine_output : List [ Union [ np . ndarray , torch . Tensor ]], tolerance : float , ) -> None : \"\"\" Compare engine predictions with a reference. Assert that the difference is under a threshold. :param engine_name: string used in error message, if any :param pytorch_output: reference output used for the comparaison :param engine_output: output from the engine :param tolerance: if difference in outputs is above threshold, an error will be raised \"\"\" pytorch_output = to_numpy ( pytorch_output ) engine_output = to_numpy ( engine_output ) discrepency = compare_outputs ( pytorch_output = pytorch_output , engine_output = engine_output ) assert discrepency <= tolerance , ( f \" { engine_name } discrepency is too high ( { discrepency : .2f } >= { tolerance } ): \\n \" f \"Pythorch: \\n { pytorch_output } \\n \" f \"VS \\n \" f \"Engine: \\n { engine_output } \\n \" f \"Diff: \\n \" f \" { torch . asarray ( pytorch_output ) - torch . asarray ( engine_output ) } \\n \" \"Tolerance can be increased with --atol parameter.\" ) launch_inference ( infer , inputs , nb_measures ) # Perform inference and measure latency. Parameters: Name Type Description Default infer Callable a lambda which will perform the inference required inputs List[Dict[str, Union[numpy.ndarray, torch.Tensor]]] tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) required nb_measures int number of measures to perform for the latency measure required Returns: Type Description Tuple[List[Union[numpy.ndarray, torch.Tensor]], List[float]] a tuple of model output and inference latencies Source code in src/transformer_deploy/convert.py def launch_inference ( infer : Callable , inputs : List [ Dict [ str , Union [ np . ndarray , torch . Tensor ]]], nb_measures : int ) -> Tuple [ List [ Union [ np . ndarray , torch . Tensor ]], List [ float ]]: \"\"\" Perform inference and measure latency. :param infer: a lambda which will perform the inference :param inputs: tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) :param nb_measures: number of measures to perform for the latency measure :return: a tuple of model output and inference latencies \"\"\" assert type ( inputs ) == list assert len ( inputs ) > 0 outputs = list () for batch_input in inputs : output = infer ( batch_input ) outputs . append ( output ) time_buffer : List [ int ] = list () for _ in range ( nb_measures ): with track_infer_time ( time_buffer ): _ = infer ( inputs [ 0 ]) return outputs , time_buffer","title":"Convert"},{"location":"reference/convert/#src.transformer_deploy.convert.check_accuracy","text":"Compare engine predictions with a reference. Assert that the difference is under a threshold. Parameters: Name Type Description Default engine_name str string used in error message, if any required pytorch_output List[torch.Tensor] reference output used for the comparaison required engine_output List[Union[numpy.ndarray, torch.Tensor]] output from the engine required tolerance float if difference in outputs is above threshold, an error will be raised required Source code in src/transformer_deploy/convert.py def check_accuracy ( engine_name : str , pytorch_output : List [ torch . Tensor ], engine_output : List [ Union [ np . ndarray , torch . Tensor ]], tolerance : float , ) -> None : \"\"\" Compare engine predictions with a reference. Assert that the difference is under a threshold. :param engine_name: string used in error message, if any :param pytorch_output: reference output used for the comparaison :param engine_output: output from the engine :param tolerance: if difference in outputs is above threshold, an error will be raised \"\"\" pytorch_output = to_numpy ( pytorch_output ) engine_output = to_numpy ( engine_output ) discrepency = compare_outputs ( pytorch_output = pytorch_output , engine_output = engine_output ) assert discrepency <= tolerance , ( f \" { engine_name } discrepency is too high ( { discrepency : .2f } >= { tolerance } ): \\n \" f \"Pythorch: \\n { pytorch_output } \\n \" f \"VS \\n \" f \"Engine: \\n { engine_output } \\n \" f \"Diff: \\n \" f \" { torch . asarray ( pytorch_output ) - torch . asarray ( engine_output ) } \\n \" \"Tolerance can be increased with --atol parameter.\" )","title":"check_accuracy()"},{"location":"reference/convert/#src.transformer_deploy.convert.launch_inference","text":"Perform inference and measure latency. Parameters: Name Type Description Default infer Callable a lambda which will perform the inference required inputs List[Dict[str, Union[numpy.ndarray, torch.Tensor]]] tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) required nb_measures int number of measures to perform for the latency measure required Returns: Type Description Tuple[List[Union[numpy.ndarray, torch.Tensor]], List[float]] a tuple of model output and inference latencies Source code in src/transformer_deploy/convert.py def launch_inference ( infer : Callable , inputs : List [ Dict [ str , Union [ np . ndarray , torch . Tensor ]]], nb_measures : int ) -> Tuple [ List [ Union [ np . ndarray , torch . Tensor ]], List [ float ]]: \"\"\" Perform inference and measure latency. :param infer: a lambda which will perform the inference :param inputs: tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) :param nb_measures: number of measures to perform for the latency measure :return: a tuple of model output and inference latencies \"\"\" assert type ( inputs ) == list assert len ( inputs ) > 0 outputs = list () for batch_input in inputs : output = infer ( batch_input ) outputs . append ( output ) time_buffer : List [ int ] = list () for _ in range ( nb_measures ): with track_infer_time ( time_buffer ): _ = infer ( inputs [ 0 ]) return outputs , time_buffer","title":"launch_inference()"},{"location":"reference/QDQModels/QDQAlbert/","text":"This module add quantization support to all Albert architecture based models.","title":"QDQAlbert"},{"location":"reference/QDQModels/QDQBert/","text":"This module add quantization support to all Bert architecture based models.","title":"QDQBert"},{"location":"reference/QDQModels/QDQDeberta/","text":"This module add quantization support to all Deberta architecture based models. For now, Deberta export to ONNX doesn't work well. This PR may help: https://github.com/microsoft/DeBERTa/pull/6 get_attention_mask ( self , attention_mask ) # Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def get_attention_mask ( self , attention_mask ): \"\"\" Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. \"\"\" if attention_mask . dim () <= 2 : extended_attention_mask = attention_mask . unsqueeze ( 1 ) . unsqueeze ( 2 ) attention_mask = extended_attention_mask * extended_attention_mask . squeeze ( - 2 ) . unsqueeze ( - 1 ) # unecessary conversion, byte == unsigned integer -> not supported by TensorRT # attention_mask = attention_mask.byte() elif attention_mask . dim () == 3 : attention_mask = attention_mask . unsqueeze ( 1 ) return attention_mask symbolic ( g , self , mask , dim ) # Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def symbolic ( g , self , mask , dim ): \"\"\" Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. \"\"\" import torch.onnx.symbolic_helper as sym_help from torch.onnx.symbolic_opset9 import masked_fill , softmax mask_cast_value = g . op ( \"Cast\" , mask , to_i = sym_help . cast_pytorch_to_onnx [ \"Long\" ]) # r_mask = g.op(\"Sub\", g.op(\"Constant\", value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value) # replace Byte by Char to get signed numbers r_mask = g . op ( \"Cast\" , g . op ( \"Sub\" , g . op ( \"Constant\" , value_t = torch . tensor ( 1 , dtype = torch . int64 )), mask_cast_value ), to_i = sym_help . cast_pytorch_to_onnx [ \"Char\" ], ) output = masked_fill ( g , self , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( float ( \"-inf\" )))) output = softmax ( g , output , dim ) return masked_fill ( g , output , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( 0 , dtype = torch . int8 )))","title":"QDQDeberta"},{"location":"reference/QDQModels/QDQDeberta/#src.transformer_deploy.QDQModels.QDQDeberta.get_attention_mask","text":"Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def get_attention_mask ( self , attention_mask ): \"\"\" Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. \"\"\" if attention_mask . dim () <= 2 : extended_attention_mask = attention_mask . unsqueeze ( 1 ) . unsqueeze ( 2 ) attention_mask = extended_attention_mask * extended_attention_mask . squeeze ( - 2 ) . unsqueeze ( - 1 ) # unecessary conversion, byte == unsigned integer -> not supported by TensorRT # attention_mask = attention_mask.byte() elif attention_mask . dim () == 3 : attention_mask = attention_mask . unsqueeze ( 1 ) return attention_mask","title":"get_attention_mask()"},{"location":"reference/QDQModels/QDQDeberta/#src.transformer_deploy.QDQModels.QDQDeberta.symbolic","text":"Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def symbolic ( g , self , mask , dim ): \"\"\" Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. \"\"\" import torch.onnx.symbolic_helper as sym_help from torch.onnx.symbolic_opset9 import masked_fill , softmax mask_cast_value = g . op ( \"Cast\" , mask , to_i = sym_help . cast_pytorch_to_onnx [ \"Long\" ]) # r_mask = g.op(\"Sub\", g.op(\"Constant\", value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value) # replace Byte by Char to get signed numbers r_mask = g . op ( \"Cast\" , g . op ( \"Sub\" , g . op ( \"Constant\" , value_t = torch . tensor ( 1 , dtype = torch . int64 )), mask_cast_value ), to_i = sym_help . cast_pytorch_to_onnx [ \"Char\" ], ) output = masked_fill ( g , self , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( float ( \"-inf\" )))) output = softmax ( g , output , dim ) return masked_fill ( g , output , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( 0 , dtype = torch . int8 )))","title":"symbolic()"},{"location":"reference/QDQModels/QDQDistilbert/","text":"This module add quantization support to all Distilbert architecture based models.","title":"QDQDistilbert"},{"location":"reference/QDQModels/QDQElectra/","text":"This module add quantization support to all Electra architecture based models.","title":"QDQElectra"},{"location":"reference/QDQModels/QDQRoberta/","text":"This module add quantization support to all Roberta architecture based models. qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ) # Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. Source code in src/transformer_deploy/QDQModels/QDQRoberta.py def qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ): \"\"\" Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. \"\"\" # QDQ change below # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA. # int() -> float() because of a limitations in cumsum operator implementation in TensorRT mask = input_ids . ne ( padding_idx ) . float () incremental_indices = ( torch . cumsum ( mask , dim = 1 ) . type_as ( mask ) + past_key_values_length ) * mask return incremental_indices . long () + padding_idx","title":"QDQRoberta"},{"location":"reference/QDQModels/QDQRoberta/#src.transformer_deploy.QDQModels.QDQRoberta.qdq_create_position_tensorrt","text":"Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. Source code in src/transformer_deploy/QDQModels/QDQRoberta.py def qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ): \"\"\" Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. \"\"\" # QDQ change below # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA. # int() -> float() because of a limitations in cumsum operator implementation in TensorRT mask = input_ids . ne ( padding_idx ) . float () incremental_indices = ( torch . cumsum ( mask , dim = 1 ) . type_as ( mask ) + past_key_values_length ) * mask return incremental_indices . long () + padding_idx","title":"qdq_create_position_tensorrt()"},{"location":"reference/QDQModels/ast_operator_patch/","text":"Contains code to match and patch specific AST patterns. Patch2ArgsNode ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class Patch2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names __init__ ( self , op ) special # Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) PatchAdd2ArgsNode ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchAdd2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ] __init__ ( self , op ) special # Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ] should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) PatchLayer ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchLayer ( PatchNode ): def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return [] __init__ ( self , origin_module , origin_layer , target_module , target_layer ) special # Patch source code in the form a.b(...) to c.d(...) Parameters: Name Type Description Default origin_module str module to patch required origin_layer str layer/method to patch required target_module str new module to use required target_layer str new layer/method to use required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return [] should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) PatchNode # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchNode ( object ): __metaclass__ = abc . ABCMeta torch_op_to_quantize : str @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" ) @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) @staticmethod def _wrap_attr ( quantizer_name : str , tensor_var : ast . expr ) -> ast . Call : \"\"\" Generate quantization wrapping each attribute of a torch operation to optimize (matmul, add, etc.) :param quantizer_name: generated quantization name :param tensor_var: the variable to wrap :return: the ast tree to replace the original variable \"\"\" return ast . Call ( func = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = quantizer_name , ctx = ast . Load ()), args = [ tensor_var ], keywords = [], ) def get_quant_name ( self , node_id : int ) -> str : return f \" { self . torch_op_to_quantize . lower () } _quantizer_ { node_id } \" __metaclass__ ( type ) # Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special # Override for isinstance(instance, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod # Create and return a new object. See help(type) for accurate signature. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special # Override for issubclass(subclass, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) # Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" )","title":"Ast operator patch"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class Patch2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names","title":"Patch2ArgsNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.__init__","text":"Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchAdd2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ]","title":"PatchAdd2ArgsNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.__init__","text":"Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ]","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchLayer ( PatchNode ): def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return []","title":"PatchLayer"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.__init__","text":"Patch source code in the form a.b(...) to c.d(...) Parameters: Name Type Description Default origin_module str module to patch required origin_layer str layer/method to patch required target_module str new module to use required target_layer str new layer/method to use required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return []","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchNode ( object ): __metaclass__ = abc . ABCMeta torch_op_to_quantize : str @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" ) @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) @staticmethod def _wrap_attr ( quantizer_name : str , tensor_var : ast . expr ) -> ast . Call : \"\"\" Generate quantization wrapping each attribute of a torch operation to optimize (matmul, add, etc.) :param quantizer_name: generated quantization name :param tensor_var: the variable to wrap :return: the ast tree to replace the original variable \"\"\" return ast . Call ( func = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = quantizer_name , ctx = ast . Load ()), args = [ tensor_var ], keywords = [], ) def get_quant_name ( self , node_id : int ) -> str : return f \" { self . torch_op_to_quantize . lower () } _quantizer_ { node_id } \"","title":"PatchNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__metaclass__"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" )","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" )","title":"should_patch()"},{"location":"reference/QDQModels/ast_utils/","text":"Contains the code to patch model AST in RAM. PatchModule dataclass # PatchModule(module: str, monkey_patch: Dict[str, Tuple[Callable, str]] = ) Source code in src/transformer_deploy/QDQModels/ast_utils.py @dataclass class PatchModule : module : str monkey_patch : Dict [ str , Tuple [ Callable , str ]] = field ( default_factory = dict ) def print_code ( self ): for class_name , cl in self . monkey_patch . items (): print ( \"---------\" ) print ( class_name ) inspect . getsource ( cl ) def restore ( self ): model_module = importlib . import_module ( name = self . module ) importlib . reload ( model_module ) add_init_quantizer ( head_node , q_attr_names ) # Add initialization of quantizer to init () Parameters: Name Type Description Default head_node Module node related to a class to optimize required q_attr_names List[str] list of quantizer names to init required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_init_quantizer ( head_node : ast . Module , q_attr_names : List [ str ]) -> ast . Module : \"\"\" Add initialization of quantizer to __init__() :param head_node: node related to a class to optimize :param q_attr_names: list of quantizer names to init :return: modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.FunctionDef if isinstance ( node , ast . FunctionDef ) and node . name == \"__init__\" : for name in q_attr_names : quantizer = init_quantizer ( name ) node . body . append ( quantizer ) return head_node add_qdq_to_class_name ( head_node , new_class_name ) # Change the name of the class to optimize (may help in debugging / error messages) Parameters: Name Type Description Default head_node Module node related to the class to optimize required new_class_name str new name to use required Returns: Type Description Module the modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_qdq_to_class_name ( head_node : ast . Module , new_class_name : str ) -> ast . Module : \"\"\" Change the name of the class to optimize (may help in debugging / error messages) :param head_node: node related to the class to optimize :param new_class_name: new name to use :return: the modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.ClassDef if isinstance ( node , ast . ClassDef ): node . name = new_class_name return head_node add_quant_to_module ( module_to_patch , new_module_name ) # Modify a class to add quantization operations around each torch operation to optimize. Parameters: Name Type Description Default module_to_patch type Pytorch module to patch required new_module_name str new name for the module required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quant_to_module ( module_to_patch : type , new_module_name : str ) -> ast . Module : \"\"\" Modify a class to add quantization operations around each torch operation to optimize. :param module_to_patch: Pytorch module to patch :param new_module_name: new name for the module :return: modified ast tree \"\"\" source_code = inspect . getsource ( module_to_patch ) head = ast . parse ( source_code ) head , nodes_to_add = patch_nodes ( head ) add_init_quantizer ( head_node = head , q_attr_names = nodes_to_add ) head = add_qdq_to_class_name ( head_node = head , new_class_name = new_module_name ) return head add_quantization_to_model ( module_path , class_to_patch ) # Add quantization support to a model. Parameters: Name Type Description Default module_path str model module to optimize required class_to_patch Optional[List[str]] name of modules to patch, if None it will be auto-detected. required Returns: Type Description backup of original classes Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quantization_to_model ( module_path : str , class_to_patch : Optional [ List [ str ]], ): \"\"\" Add quantization support to a model. :param module_path: model module to optimize :param class_to_patch: name of modules to patch, if None it will be auto-detected. :return: backup of original classes \"\"\" model_module = importlib . import_module ( name = module_path ) load_missing_imports ( model_module ) if class_to_patch is None or len ( class_to_patch ) == 0 : class_to_patch = list_class_to_patch ( model_module = model_module ) logging . info ( f \"modify class { ', ' . join ( class_to_patch ) } \" ) for class_name in class_to_patch : module_to_patch = getattr ( model_module , class_name ) head = add_quant_to_module ( module_to_patch = module_to_patch , new_module_name = class_name ) head = ast . fix_missing_locations ( head ) module_patched : code = compile ( head , filename = \"<ast modif - transformer deploy>\" , mode = \"exec\" ) # execute the code in the module context so it overrides the original classes and leverage existing imports exec ( module_patched , model_module . __dict__ , model_module . __dict__ ) contains_op ( node ) # Check if a tree contains some operations to optimize. Parameters: Name Type Description Default node AST Head of the ast tree required Returns: Type Description bool True if ast tree contains operations to optimize Source code in src/transformer_deploy/QDQModels/ast_utils.py def contains_op ( node : ast . AST ) -> bool : \"\"\" Check if a tree contains some operations to optimize. :param node: Head of the ast tree :return: True if ast tree contains operations to optimize \"\"\" for node in ast . walk ( node ): for op in op_to_quant : if op . should_patch ( node = node ): return True return False init_quantizer ( name ) # Generate quantization node initialization to add to the end of init () Parameters: Name Type Description Default name str generated name of the node required Returns: Type Description Assign quantization init ast node Source code in src/transformer_deploy/QDQModels/ast_utils.py def init_quantizer ( name : str ) -> ast . Assign : \"\"\" Generate quantization node initialization to add to the end of __init__() :param name: generated name of the node :return: quantization init ast node \"\"\" quant_linear = ast . Attribute ( value = ast . Name ( id = \"quant_nn\" , ctx = ast . Load ()), attr = \"QuantLinear\" , ctx = ast . Load ()) default_quant_desc_input = ast . Attribute ( value = quant_linear , attr = \"default_quant_desc_input\" , ctx = ast . Load ()) tensor_quant = ast . Name ( id = \"TensorQuantizer\" , ctx = ast . Load ()) quant_value = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = name , ctx = ast . Store ()) return ast . Assign ( targets = [ quant_value ], value = ast . Call ( func = tensor_quant , args = [ default_quant_desc_input ], keywords = []), ) list_class_to_patch ( model_module ) # List all classes which contain operations to be optimized. Parameters: Name Type Description Default model_module Pytorch module required Returns: Type Description List[str] the list of module names to be optimized Source code in src/transformer_deploy/QDQModels/ast_utils.py def list_class_to_patch ( model_module ) -> List [ str ]: \"\"\" List all classes which contain operations to be optimized. :param model_module: Pytorch module :return: the list of module names to be optimized \"\"\" module_names : List [ str ] = list () module_source_code = inspect . getsource ( model_module ) head_node = ast . parse ( module_source_code ) for node in ast . walk ( head_node ): if isinstance ( node , ast . ClassDef ) and contains_op ( node = node ): module_names . append ( node . name ) return module_names load_missing_imports ( model_module ) # Execute some imports in the context of a module. Override Linear layer by its quantized version Parameters: Name Type Description Default model_module module to use for the imports required Source code in src/transformer_deploy/QDQModels/ast_utils.py def load_missing_imports ( model_module ) -> None : \"\"\" Execute some imports in the context of a module. Override Linear layer by its quantized version :param model_module: module to use for the imports \"\"\" import_code = \"\"\" from pytorch_quantization import nn as quant_nn from pytorch_quantization.nn import TensorQuantizer \"\"\" # remove extra spaces import_code = inspect . cleandoc ( import_code ) # execute the code in the module context exec ( import_code , model_module . __dict__ , model_module . __dict__ ) patch_nodes ( head_node ) # Replace an operation to optimize by its optimized version. May have to generate some quantization node names. Parameters: Name Type Description Default head_node Module ast node to modify required Returns: Type Description Tuple[ast.Module, List[str]] the modified ast tree and the list of generated quantization nodes Source code in src/transformer_deploy/QDQModels/ast_utils.py def patch_nodes ( head_node : ast . Module ) -> Tuple [ ast . Module , List [ str ]]: \"\"\" Replace an operation to optimize by its optimized version. May have to generate some quantization node names. :param head_node: ast node to modify :return: the modified ast tree and the list of generated quantization nodes \"\"\" q_attr_names : List [ str ] = list () for node in ast . walk ( head_node ): # type: ast.Call for op in op_to_quant : if op . should_patch ( node = node ): quant_names = op . patch ( node = node , nb_quant_node = len ( q_attr_names )) q_attr_names . extend ( quant_names ) return head_node , q_attr_names","title":"Ast utils"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.PatchModule","text":"PatchModule(module: str, monkey_patch: Dict[str, Tuple[Callable, str]] = ) Source code in src/transformer_deploy/QDQModels/ast_utils.py @dataclass class PatchModule : module : str monkey_patch : Dict [ str , Tuple [ Callable , str ]] = field ( default_factory = dict ) def print_code ( self ): for class_name , cl in self . monkey_patch . items (): print ( \"---------\" ) print ( class_name ) inspect . getsource ( cl ) def restore ( self ): model_module = importlib . import_module ( name = self . module ) importlib . reload ( model_module )","title":"PatchModule"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_init_quantizer","text":"Add initialization of quantizer to init () Parameters: Name Type Description Default head_node Module node related to a class to optimize required q_attr_names List[str] list of quantizer names to init required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_init_quantizer ( head_node : ast . Module , q_attr_names : List [ str ]) -> ast . Module : \"\"\" Add initialization of quantizer to __init__() :param head_node: node related to a class to optimize :param q_attr_names: list of quantizer names to init :return: modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.FunctionDef if isinstance ( node , ast . FunctionDef ) and node . name == \"__init__\" : for name in q_attr_names : quantizer = init_quantizer ( name ) node . body . append ( quantizer ) return head_node","title":"add_init_quantizer()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_qdq_to_class_name","text":"Change the name of the class to optimize (may help in debugging / error messages) Parameters: Name Type Description Default head_node Module node related to the class to optimize required new_class_name str new name to use required Returns: Type Description Module the modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_qdq_to_class_name ( head_node : ast . Module , new_class_name : str ) -> ast . Module : \"\"\" Change the name of the class to optimize (may help in debugging / error messages) :param head_node: node related to the class to optimize :param new_class_name: new name to use :return: the modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.ClassDef if isinstance ( node , ast . ClassDef ): node . name = new_class_name return head_node","title":"add_qdq_to_class_name()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_quant_to_module","text":"Modify a class to add quantization operations around each torch operation to optimize. Parameters: Name Type Description Default module_to_patch type Pytorch module to patch required new_module_name str new name for the module required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quant_to_module ( module_to_patch : type , new_module_name : str ) -> ast . Module : \"\"\" Modify a class to add quantization operations around each torch operation to optimize. :param module_to_patch: Pytorch module to patch :param new_module_name: new name for the module :return: modified ast tree \"\"\" source_code = inspect . getsource ( module_to_patch ) head = ast . parse ( source_code ) head , nodes_to_add = patch_nodes ( head ) add_init_quantizer ( head_node = head , q_attr_names = nodes_to_add ) head = add_qdq_to_class_name ( head_node = head , new_class_name = new_module_name ) return head","title":"add_quant_to_module()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_quantization_to_model","text":"Add quantization support to a model. Parameters: Name Type Description Default module_path str model module to optimize required class_to_patch Optional[List[str]] name of modules to patch, if None it will be auto-detected. required Returns: Type Description backup of original classes Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quantization_to_model ( module_path : str , class_to_patch : Optional [ List [ str ]], ): \"\"\" Add quantization support to a model. :param module_path: model module to optimize :param class_to_patch: name of modules to patch, if None it will be auto-detected. :return: backup of original classes \"\"\" model_module = importlib . import_module ( name = module_path ) load_missing_imports ( model_module ) if class_to_patch is None or len ( class_to_patch ) == 0 : class_to_patch = list_class_to_patch ( model_module = model_module ) logging . info ( f \"modify class { ', ' . join ( class_to_patch ) } \" ) for class_name in class_to_patch : module_to_patch = getattr ( model_module , class_name ) head = add_quant_to_module ( module_to_patch = module_to_patch , new_module_name = class_name ) head = ast . fix_missing_locations ( head ) module_patched : code = compile ( head , filename = \"<ast modif - transformer deploy>\" , mode = \"exec\" ) # execute the code in the module context so it overrides the original classes and leverage existing imports exec ( module_patched , model_module . __dict__ , model_module . __dict__ )","title":"add_quantization_to_model()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.contains_op","text":"Check if a tree contains some operations to optimize. Parameters: Name Type Description Default node AST Head of the ast tree required Returns: Type Description bool True if ast tree contains operations to optimize Source code in src/transformer_deploy/QDQModels/ast_utils.py def contains_op ( node : ast . AST ) -> bool : \"\"\" Check if a tree contains some operations to optimize. :param node: Head of the ast tree :return: True if ast tree contains operations to optimize \"\"\" for node in ast . walk ( node ): for op in op_to_quant : if op . should_patch ( node = node ): return True return False","title":"contains_op()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.init_quantizer","text":"Generate quantization node initialization to add to the end of init () Parameters: Name Type Description Default name str generated name of the node required Returns: Type Description Assign quantization init ast node Source code in src/transformer_deploy/QDQModels/ast_utils.py def init_quantizer ( name : str ) -> ast . Assign : \"\"\" Generate quantization node initialization to add to the end of __init__() :param name: generated name of the node :return: quantization init ast node \"\"\" quant_linear = ast . Attribute ( value = ast . Name ( id = \"quant_nn\" , ctx = ast . Load ()), attr = \"QuantLinear\" , ctx = ast . Load ()) default_quant_desc_input = ast . Attribute ( value = quant_linear , attr = \"default_quant_desc_input\" , ctx = ast . Load ()) tensor_quant = ast . Name ( id = \"TensorQuantizer\" , ctx = ast . Load ()) quant_value = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = name , ctx = ast . Store ()) return ast . Assign ( targets = [ quant_value ], value = ast . Call ( func = tensor_quant , args = [ default_quant_desc_input ], keywords = []), )","title":"init_quantizer()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.list_class_to_patch","text":"List all classes which contain operations to be optimized. Parameters: Name Type Description Default model_module Pytorch module required Returns: Type Description List[str] the list of module names to be optimized Source code in src/transformer_deploy/QDQModels/ast_utils.py def list_class_to_patch ( model_module ) -> List [ str ]: \"\"\" List all classes which contain operations to be optimized. :param model_module: Pytorch module :return: the list of module names to be optimized \"\"\" module_names : List [ str ] = list () module_source_code = inspect . getsource ( model_module ) head_node = ast . parse ( module_source_code ) for node in ast . walk ( head_node ): if isinstance ( node , ast . ClassDef ) and contains_op ( node = node ): module_names . append ( node . name ) return module_names","title":"list_class_to_patch()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.load_missing_imports","text":"Execute some imports in the context of a module. Override Linear layer by its quantized version Parameters: Name Type Description Default model_module module to use for the imports required Source code in src/transformer_deploy/QDQModels/ast_utils.py def load_missing_imports ( model_module ) -> None : \"\"\" Execute some imports in the context of a module. Override Linear layer by its quantized version :param model_module: module to use for the imports \"\"\" import_code = \"\"\" from pytorch_quantization import nn as quant_nn from pytorch_quantization.nn import TensorQuantizer \"\"\" # remove extra spaces import_code = inspect . cleandoc ( import_code ) # execute the code in the module context exec ( import_code , model_module . __dict__ , model_module . __dict__ )","title":"load_missing_imports()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.patch_nodes","text":"Replace an operation to optimize by its optimized version. May have to generate some quantization node names. Parameters: Name Type Description Default head_node Module ast node to modify required Returns: Type Description Tuple[ast.Module, List[str]] the modified ast tree and the list of generated quantization nodes Source code in src/transformer_deploy/QDQModels/ast_utils.py def patch_nodes ( head_node : ast . Module ) -> Tuple [ ast . Module , List [ str ]]: \"\"\" Replace an operation to optimize by its optimized version. May have to generate some quantization node names. :param head_node: ast node to modify :return: the modified ast tree and the list of generated quantization nodes \"\"\" q_attr_names : List [ str ] = list () for node in ast . walk ( head_node ): # type: ast.Call for op in op_to_quant : if op . should_patch ( node = node ): quant_names = op . patch ( node = node , nb_quant_node = len ( q_attr_names )) q_attr_names . extend ( quant_names ) return head_node , q_attr_names","title":"patch_nodes()"},{"location":"reference/QDQModels/calibration_utils/","text":"Setup and run quantization calibration QATCalibrate # Source code in src/transformer_deploy/QDQModels/calibration_utils.py class QATCalibrate : def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc ) def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () def __enter__ ( self ): add_qdq () self . setup_nvidia_qat () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : self . finalize_calibration () __init__ ( self , method = 'histogram' , percentile = 99.999 , per_channel = True ) special # Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. Parameters: Name Type Description Default method str the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". 'histogram' percentile float for histogram method, what do you define as an outlier value 99.999 per_channel bool calibration granularity. per channel == per dimension. True Source code in src/transformer_deploy/QDQModels/calibration_utils.py def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel finalize_calibration ( self ) # Disable calibration process and enable quantized nodes. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () restore () staticmethod # Restore behavior without quantization support. Source code in src/transformer_deploy/QDQModels/calibration_utils.py @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () setup_model_qat ( self , model ) # Enable calibration on each tensor to quantize. Parameters: Name Type Description Default model PreTrainedModel model to optimize required Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () setup_nvidia_qat ( self ) # Setup Nvidia QAT library global variables. Should be called before initializing a model. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc )","title":"Calibration utils"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate","text":"Source code in src/transformer_deploy/QDQModels/calibration_utils.py class QATCalibrate : def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc ) def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () def __enter__ ( self ): add_qdq () self . setup_nvidia_qat () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : self . finalize_calibration ()","title":"QATCalibrate"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.__init__","text":"Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. Parameters: Name Type Description Default method str the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". 'histogram' percentile float for histogram method, what do you define as an outlier value 99.999 per_channel bool calibration granularity. per channel == per dimension. True Source code in src/transformer_deploy/QDQModels/calibration_utils.py def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel","title":"__init__()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.finalize_calibration","text":"Disable calibration process and enable quantized nodes. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda ()","title":"finalize_calibration()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.restore","text":"Restore behavior without quantization support. Source code in src/transformer_deploy/QDQModels/calibration_utils.py @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq ()","title":"restore()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.setup_model_qat","text":"Enable calibration on each tensor to quantize. Parameters: Name Type Description Default model PreTrainedModel model to optimize required Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable ()","title":"setup_model_qat()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.setup_nvidia_qat","text":"Setup Nvidia QAT library global variables. Should be called before initializing a model. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc )","title":"setup_nvidia_qat()"},{"location":"reference/QDQModels/patch/","text":"Simple to use wrapper to patch transformer models AST add_qdq ( modules_to_patch = None ) # Add quantization support to each tested model by modifyin their AST. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def add_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Add quantization support to each tested model by modifyin their AST. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"add quantization to module { patch . module } \" ) patch_model ( patch ) patch_model ( patch ) # Perform modifications to model to make it work with ONNX export and quantization. Parameters: Name Type Description Default patch PatchModule an object containing all the information to perform a modification required Source code in src/transformer_deploy/QDQModels/patch.py def patch_model ( patch : PatchModule ) -> None : \"\"\" Perform modifications to model to make it work with ONNX export and quantization. :param patch: an object containing all the information to perform a modification \"\"\" add_quantization_to_model ( module_path = patch . module , class_to_patch = None ) model_module = importlib . import_module ( patch . module ) for target , ( modified_object , object_name ) in patch . monkey_patch . items (): source_code = inspect . getsource ( modified_object ) source_code += f \" \\n { target } = { object_name } \" exec ( source_code , model_module . __dict__ , model_module . __dict__ ) remove_qdq ( modules_to_patch = None ) # Restore AST of modified modules. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def remove_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Restore AST of modified modules. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"restore module { patch . module } \" ) patch . restore ()","title":"Patch"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.add_qdq","text":"Add quantization support to each tested model by modifyin their AST. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def add_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Add quantization support to each tested model by modifyin their AST. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"add quantization to module { patch . module } \" ) patch_model ( patch )","title":"add_qdq()"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.patch_model","text":"Perform modifications to model to make it work with ONNX export and quantization. Parameters: Name Type Description Default patch PatchModule an object containing all the information to perform a modification required Source code in src/transformer_deploy/QDQModels/patch.py def patch_model ( patch : PatchModule ) -> None : \"\"\" Perform modifications to model to make it work with ONNX export and quantization. :param patch: an object containing all the information to perform a modification \"\"\" add_quantization_to_model ( module_path = patch . module , class_to_patch = None ) model_module = importlib . import_module ( patch . module ) for target , ( modified_object , object_name ) in patch . monkey_patch . items (): source_code = inspect . getsource ( modified_object ) source_code += f \" \\n { target } = { object_name } \" exec ( source_code , model_module . __dict__ , model_module . __dict__ )","title":"patch_model()"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.remove_qdq","text":"Restore AST of modified modules. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def remove_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Restore AST of modified modules. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"restore module { patch . module } \" ) patch . restore ()","title":"remove_qdq()"},{"location":"reference/backends/ort_utils/","text":"All the tooling to ease ONNX Runtime usage. add_output_nodes ( model ) # Set each node as output node for debugging purpose. Parameters: Name Type Description Default model ModelProto ONNX model in protobuf format required Returns: Type Description ModelProto modified ONNX model Source code in src/transformer_deploy/backends/ort_utils.py def add_output_nodes ( model : ModelProto ) -> ModelProto : \"\"\" Set each node as output node for debugging purpose. :param model: ONNX model in protobuf format :return: modified ONNX model \"\"\" model = copy . deepcopy ( model ) output_nodes = list () for n in model . graph . node : for output_name in n . output : output_nodes . append ( onnx . ValueInfoProto ( name = output_name )) # clear output array (protobuff way...) while model . graph . output : model . graph . output . pop () model . graph . output . extend ( output_nodes ) return model convert_fp16 ( onnx_model , nodes_to_exclude ) # Convert ONNX model in FP16, and still being able to exclude a list of nodes. Parameters: Name Type Description Default onnx_model str original FP32 model required nodes_to_exclude List[str] nodes that should stay in FP32 required Returns: Type Description ModelProto mostly FP16 model Source code in src/transformer_deploy/backends/ort_utils.py def convert_fp16 ( onnx_model : str , nodes_to_exclude : List [ str ]) -> ModelProto : \"\"\" Convert ONNX model in FP16, and still being able to exclude a list of nodes. :param onnx_model: original FP32 model :param nodes_to_exclude: nodes that should stay in FP32 :return: mostly FP16 model \"\"\" # add value info related to each node, required for the conversion output_path = onnx_model + \"_shape_inference.onnx\" infer_shapes_path ( model_path = onnx_model , output_path = output_path ) model_fp16 = onnx . load_model ( output_path ) model_fp16 = convert_float_to_float16 ( model = model_fp16 , keep_io_types = False , node_block_list = nodes_to_exclude ) # clean casting nodes before returning the model wrapped_fp16_model = OnnxModel ( model_fp16 ) fusion_utils = FusionUtils ( wrapped_fp16_model ) fusion_utils . remove_cascaded_cast_nodes () fusion_utils . remove_useless_cast_nodes () wrapped_fp16_model . topological_sort () return wrapped_fp16_model . model cpu_quantization ( input_model_path , output_model_path ) # ONNX CPU only dynamic quantization. Parameters: Name Type Description Default input_model_path str ONNX graph (float) to quantize required output_model_path str where to save quantized model required Source code in src/transformer_deploy/backends/ort_utils.py def cpu_quantization ( input_model_path : str , output_model_path : str ) -> None : \"\"\" ONNX CPU only dynamic quantization. :param input_model_path: ONNX graph (float) to quantize :param output_model_path: where to save quantized model \"\"\" quantize_dynamic ( model_input = Path ( input_model_path ), model_output = Path ( output_model_path ), op_types_to_quantize = [ \"MatMul\" , \"Attention\" ], weight_type = QuantType . QInt8 , per_channel = True , reduce_range = True , extra_options = { \"WeightSymmetric\" : False , \"MatMulConstBOnly\" : True }, ) create_model_for_provider ( path , provider_to_use , nb_threads = 12 , nb_instances = 0 , optimization_level =< GraphOptimizationLevel . ORT_ENABLE_EXTENDED : 2 > , enable_profiling = False , log_severity = 2 ) # Create an ONNX Runtime instance. Parameters: Name Type Description Default path str path to ONNX file or serialized to string model required provider_to_use Union[str, List] provider to use for inference required nb_threads int intra_op_num_threads to use. You may want to try different parameters, more core does not always provide best performances. 12 nb_instances int inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible. 0 optimization_level GraphOptimizationLevel expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one providing kernel fusion of element wise operations. Enable all level is for CPU inference. see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations <GraphOptimizationLevel.ORT_ENABLE_EXTENDED: 2> enable_profiling bool let Onnx Runtime log each kernel time. False log_severity int Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. 2 Returns: Type Description InferenceSession ONNX Runtime inference session Source code in src/transformer_deploy/backends/ort_utils.py def create_model_for_provider ( path : str , provider_to_use : Union [ str , List ], nb_threads : int = multiprocessing . cpu_count (), nb_instances : int = 0 , optimization_level : GraphOptimizationLevel = GraphOptimizationLevel . ORT_ENABLE_EXTENDED , enable_profiling : bool = False , log_severity : int = 2 , ) -> InferenceSession : \"\"\" Create an ONNX Runtime instance. :param path: path to ONNX file or serialized to string model :param provider_to_use: provider to use for inference :param nb_threads: intra_op_num_threads to use. You may want to try different parameters, more core does not always provide best performances. :param nb_instances: inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible. :param optimization_level: expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one providing kernel fusion of element wise operations. Enable all level is for CPU inference. see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations :param enable_profiling: let Onnx Runtime log each kernel time. :param log_severity: Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. :return: ONNX Runtime inference session \"\"\" options = SessionOptions () options . graph_optimization_level = optimization_level options . enable_profiling = enable_profiling options . log_severity_level = log_severity if isinstance ( provider_to_use , str ): provider_to_use = [ provider_to_use ] if provider_to_use == [ \"CPUExecutionProvider\" ]: options . execution_mode = ExecutionMode . ORT_SEQUENTIAL if nb_instances <= 1 else ExecutionMode . ORT_PARALLEL options . intra_op_num_threads = nb_threads if nb_instances > 1 : options . inter_op_num_threads = nb_instances return InferenceSession ( path , options , providers = provider_to_use ) find_node_fp32 ( graph , output_nodes ) # Identify out of range values in node outputs. Parameters: Name Type Description Default graph Dict[str, str] graph as adjency nodes dict required output_nodes Dict[str, torch.Tensor] output of each node required Returns: Type Description List[str] list of nodes producing outputs outside fp16 tensor Source code in src/transformer_deploy/backends/ort_utils.py def find_node_fp32 ( graph : Dict [ str , str ], output_nodes : Dict [ str , torch . Tensor ]) -> List [ str ]: \"\"\" Identify out of range values in node outputs. :param graph: graph as adjency nodes dict :param output_nodes: output of each node :return: list of nodes producing outputs outside fp16 tensor \"\"\" keep_fp32 = list () min_float16 = torch . finfo ( torch . float16 ) . min max_float16 = torch . finfo ( torch . float16 ) . max resolution = 5.96e-08 # torch.finfo(torch.float16).eps # minimum value that can be represented by FP16 for k , tensor in output_nodes . items (): if tensor . dtype != torch . float32 : continue # out of FP16 range if ( torch . any ( tensor > max_float16 ) or torch . any ( tensor < min_float16 ) or ( torch . any ( tensor < resolution & tensor > - resolution & tensor != 0 )) # limited memory footprint ): keep_fp32 . append ( graph [ k ]) return keep_fp32 get_io_to_node_mapping ( onnx_model ) # Extract output->node and input->node mappings Parameters: Name Type Description Default onnx_model ModelProto ONNX model required Returns: Type Description Tuple[Dict[str, str], Dict[str, str]] 2 mappings, (i->node, o->node) Source code in src/transformer_deploy/backends/ort_utils.py def get_io_to_node_mapping ( onnx_model : ModelProto ) -> Tuple [ Dict [ str , str ], Dict [ str , str ]]: \"\"\" Extract output->node and input->node mappings :param onnx_model: ONNX model :return: 2 mappings, (i->node, o->node) \"\"\" output_mapping : Dict [ str , str ] = dict () input_mapping : Dict [ str , str ] = dict () for node in onnx_model . graph . node : # type: NodeProto assert len ( node . output ) == 1 output_node = node . output [ 0 ] output_mapping [ output_node ] = node . name for i in node . input : input_mapping [ i ] = node . name return input_mapping , output_mapping get_keep_fp32_nodes ( onnx_model_path , get_input , early_stop = 100 , device = 'cuda' ) # Find the list of nodes to keep in FP32 to avoid out of range values Parameters: Name Type Description Default onnx_model_path str ONNX model path required get_input Callable[[], Dict[str, torch.Tensor]] generate input to test the model. Output should change from call to call required early_stop int will test until early_stop tests are done without any new node to keep in FP32 100 device str where to run the inference 'cuda' Returns: Type Description List[str] list of names of nodes to keep in FP32 Source code in src/transformer_deploy/backends/ort_utils.py def get_keep_fp32_nodes ( onnx_model_path : str , get_input : Callable [[], Dict [ str , torch . Tensor ]], early_stop : int = 100 , device : str = \"cuda\" , ) -> List [ str ]: \"\"\" Find the list of nodes to keep in FP32 to avoid out of range values :param onnx_model_path: ONNX model path :param get_input: generate input to test the model. Output should change from call to call :param early_stop: will test until `early_stop` tests are done without any new node to keep in FP32 :param device: where to run the inference :return: list of names of nodes to keep in FP32 \"\"\" # do not load weights on LLM (>2Gb), we only need to modify the computation graph onnx_model : ModelProto = onnx . load_model ( f = onnx_model_path , load_external_data = False ) onnx_model_fp32_all_nodes = add_output_nodes ( model = onnx_model ) path_onnx_model_fp32_all_nodes = onnx_model_path + \"_all_nodes.onnx\" onnx . save_model ( proto = onnx_model_fp32_all_nodes , f = path_onnx_model_fp32_all_nodes , save_as_external_data = False ) provider = \"CUDAExecutionProvider\" if device == \"cuda\" else \"CPUExecutionProvider\" ort_model_fp32_all_nodes = create_model_for_provider ( path_onnx_model_fp32_all_nodes , provider ) ort_binding = ort_model_fp32_all_nodes . io_binding () input_mapping , output_mapping = get_io_to_node_mapping ( onnx_model = onnx_model ) # list all nodes which have an output out of the FP16 range keep_fp32_nodes = list () no_new_node_counter = 0 while no_new_node_counter < early_stop : inputs = get_input () outputs : Dict [ str , torch . Tensor ] = inference_onnx_binding ( model_onnx = ort_model_fp32_all_nodes , inputs = inputs , device = device , binding = ort_binding , clone_tensor = False ) keep_node_io = find_node_fp32 ( graph = output_mapping , output_nodes = outputs ) nodes_to_add = [ n for n in keep_node_io if n not in keep_fp32_nodes ] keep_fp32_nodes += nodes_to_add if len ( nodes_to_add ) == 0 : no_new_node_counter += 1 else : no_new_node_counter = 0 if device == \"cuda\" : torch . cuda . empty_cache () # I/O names that can't be found in the graph nodes_to_skip = ( [ n . name for n in onnx_model . graph . input ] + [ n . name for n in onnx_model . graph . output ] + [ n . name for n in onnx_model . graph . initializer ] ) # for each node to keep in FP32, we keep its children in FP32 too as they will receive FP32 values as input map_children = defaultdict ( list ) for node in onnx_model . graph . node : for o in node . output : if o in nodes_to_skip : continue child = input_mapping [ o ] map_children [ node . name ] . append ( child ) keep_fp32_nodes += [ c for k in keep_fp32_nodes if k in map_children for c in map_children [ k ]] return keep_fp32_nodes inference_onnx_binding ( model_onnx , inputs , device , device_id = 0 , binding = None , clone_tensor = True ) # Performs inference on ONNX Runtime in an optimized way. In particular, it avoids any Onnx Runtime output tensor copy. It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another inference. To avoid any issue, just set clone_tensor to True (default). For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True. Parameters: Name Type Description Default model_onnx InferenceSession ONNX model required inputs Dict[str, torch.Tensor] input torch tensor required device str where to run the inference. One of [cpu, cuda] required device_id int ID of the device where to run the inference, to be used when there are multiple GPUs, etc. 0 binding Optional[onnxruntime.capi.onnxruntime_inference_collection.IOBinding] previously generated binding IO, will be reset. None clone_tensor bool clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime at the next inference call. True Returns: Type Description Dict[str, torch.Tensor] a dict {axis name: output tensor} Source code in src/transformer_deploy/backends/ort_utils.py def inference_onnx_binding ( model_onnx : InferenceSession , inputs : Dict [ str , torch . Tensor ], device : str , device_id : int = 0 , binding : Optional [ IOBinding ] = None , clone_tensor : bool = True , ) -> Dict [ str , torch . Tensor ]: \"\"\" Performs inference on ONNX Runtime in an optimized way. In particular, it avoids any Onnx Runtime output tensor copy. It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another inference. To avoid any issue, just set clone_tensor to True (default). For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True. :param model_onnx: ONNX model :param inputs: input torch tensor :param device: where to run the inference. One of [cpu, cuda] :param device_id: ID of the device where to run the inference, to be used when there are multiple GPUs, etc. :param binding: previously generated binding IO, will be reset. :param clone_tensor: clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime at the next inference call. :return: a dict {axis name: output tensor} \"\"\" assert isinstance ( device , str ) assert device in [ \"cpu\" , \"cuda\" ], f \"unexpected inference device: ' { device } '\" if binding is None : binding : IOBinding = model_onnx . io_binding () else : binding . clear_binding_inputs () binding . clear_binding_outputs () for input_onnx in model_onnx . get_inputs (): if input_onnx . name not in inputs : # some inputs may be optional continue tensor : torch . Tensor = inputs [ input_onnx . name ] tensor = tensor . detach () if tensor . dtype in [ torch . int64 , torch . long ]: # int32 mandatory as input of bindings, int64 not supported tensor = tensor . type ( dtype = torch . int32 ) tensor = tensor . contiguous () binding . bind_input ( name = input_onnx . name , device_type = device , device_id = device_id , element_type = torch_to_numpy_dtype_dict [ tensor . dtype ], shape = tuple ( tensor . shape ), buffer_ptr = tensor . data_ptr (), ) inputs [ input_onnx . name ] = tensor for out in model_onnx . get_outputs (): binding . bind_output ( name = out . name , device_type = device , device_id = device_id , ) binding . synchronize_inputs () model_onnx . run_with_iobinding ( binding ) binding . synchronize_outputs () outputs = dict () assert len ( model_onnx . get_outputs ()) == len ( binding . get_outputs () ), f \" { len ( model_onnx . get_outputs ()) } != { len ( binding . get_outputs ()) } \" for out , t in zip ( model_onnx . get_outputs (), binding . get_outputs ()): outputs [ out . name ] = to_pytorch ( t , clone_tensor = clone_tensor ) return outputs optimize_onnx ( onnx_path , onnx_optim_model_path , fp16 , use_cuda , num_attention_heads = 0 , hidden_size = 0 , architecture = 'bert' ) # ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. Parameters: Name Type Description Default onnx_path str ONNX input path required onnx_optim_model_path str where to save optimized model required fp16 bool use mixed precision (faster inference) required use_cuda bool perform optimization on GPU (should ) required num_attention_heads int number of attention heads of a model (0 -> try to detect) 0 hidden_size int hidden layer size of a model (0 -> try to detect) 0 architecture str model architecture to optimize. One of [bert, bart, gpt2] 'bert' Source code in src/transformer_deploy/backends/ort_utils.py def optimize_onnx ( onnx_path : str , onnx_optim_model_path : str , fp16 : bool , use_cuda : bool , num_attention_heads : int = 0 , hidden_size : int = 0 , architecture : str = \"bert\" , ) -> None : \"\"\" ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. :param onnx_path: ONNX input path :param onnx_optim_model_path: where to save optimized model :param fp16: use mixed precision (faster inference) :param use_cuda: perform optimization on GPU (should ) :param num_attention_heads: number of attention heads of a model (0 -> try to detect) :param hidden_size: hidden layer size of a model (0 -> try to detect) :param architecture: model architecture to optimize. One of [bert, bart, gpt2] \"\"\" assert architecture in [ \"bert\" , \"bart\" , \"gpt2\" ], f \"unsupported architecture: { architecture } \" opt_level = 1 if architecture == \"bert\" else 0 optimization_options = FusionOptions ( model_type = architecture ) optimization_options . enable_gelu_approximation = False # additional optimization optimized_model : BertOnnxModel = optimizer . optimize_model ( input = onnx_path , model_type = architecture , use_gpu = use_cuda , opt_level = opt_level , num_heads = num_attention_heads , # automatic detection with 0 may not work with opset 13 or distilbert models hidden_size = hidden_size , # automatic detection with 0 optimization_options = optimization_options , ) if fp16 : # use_symbolic_shape_infer set to false because doesn't work after ONNX package v1.10.2 optimized_model . convert_float_to_float16 ( use_symbolic_shape_infer = False ) # FP32 -> FP16 logging . info ( f \"optimizations applied: { optimized_model . get_fused_operator_statistics () } \" ) optimized_model . save_model_to_file ( onnx_optim_model_path ) to_pytorch ( ort_tensor , clone_tensor ) # Convert OrtValue output by Onnx Runtime to Pytorch tensor. The process can be done in a zero copy way (depending of clone parameter). Parameters: Name Type Description Default ort_tensor OrtValue output from Onnx Runtime required clone_tensor bool Onnx Runtime owns the storage array and will write on the next inference. By cloning you guarantee that the data won't change. required Returns: Type Description Tensor Pytorch tensor Source code in src/transformer_deploy/backends/ort_utils.py def to_pytorch ( ort_tensor : OrtValue , clone_tensor : bool ) -> torch . Tensor : \"\"\" Convert OrtValue output by Onnx Runtime to Pytorch tensor. The process can be done in a zero copy way (depending of clone parameter). :param ort_tensor: output from Onnx Runtime :param clone_tensor: Onnx Runtime owns the storage array and will write on the next inference. By cloning you guarantee that the data won't change. :return: Pytorch tensor \"\"\" if ort_tensor . device_name () . lower () == \"cuda\" : np_type = ort_to_numpy_dtype_dict [ ort_tensor . data_type ()] fake_owner = 1 # size not used anywhere, so just put 0 memory = cp . cuda . UnownedMemory ( ort_tensor . data_ptr (), 0 , fake_owner ) memory_ptr = cp . cuda . MemoryPointer ( memory , 0 ) # make sure you interpret the array shape/dtype/strides correctly cp_array = cp . ndarray ( shape = ort_tensor . shape (), memptr = memory_ptr , dtype = np_type ) # cloning required otherwise ORT will recycle the storage array and put new values into it if new inf is done. torch_tensor = torch . from_dlpack ( cp_array . toDlpack ()) if clone_tensor : torch_tensor = torch_tensor . clone () return torch_tensor else : np_tensor = ort_tensor . numpy () return torch . from_numpy ( np_tensor ) use_external_data ( path ) # Check if a model uses external data Parameters: Name Type Description Default path str Onnx model path required Returns: Type Description bool True if any initalizer (model weight) is stored in an external file Source code in src/transformer_deploy/backends/ort_utils.py def use_external_data ( path : str ) -> bool : \"\"\" Check if a model uses external data :param path: Onnx model path :return: True if any initalizer (model weight) is stored in an external file \"\"\" model = onnx . load_model ( f = path , load_external_data = False ) for i in model . graph . initializer : if i . HasField ( \"data_location\" ) and i . data_location == onnx . TensorProto . EXTERNAL : return True return False","title":"Ort utils"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.add_output_nodes","text":"Set each node as output node for debugging purpose. Parameters: Name Type Description Default model ModelProto ONNX model in protobuf format required Returns: Type Description ModelProto modified ONNX model Source code in src/transformer_deploy/backends/ort_utils.py def add_output_nodes ( model : ModelProto ) -> ModelProto : \"\"\" Set each node as output node for debugging purpose. :param model: ONNX model in protobuf format :return: modified ONNX model \"\"\" model = copy . deepcopy ( model ) output_nodes = list () for n in model . graph . node : for output_name in n . output : output_nodes . append ( onnx . ValueInfoProto ( name = output_name )) # clear output array (protobuff way...) while model . graph . output : model . graph . output . pop () model . graph . output . extend ( output_nodes ) return model","title":"add_output_nodes()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.convert_fp16","text":"Convert ONNX model in FP16, and still being able to exclude a list of nodes. Parameters: Name Type Description Default onnx_model str original FP32 model required nodes_to_exclude List[str] nodes that should stay in FP32 required Returns: Type Description ModelProto mostly FP16 model Source code in src/transformer_deploy/backends/ort_utils.py def convert_fp16 ( onnx_model : str , nodes_to_exclude : List [ str ]) -> ModelProto : \"\"\" Convert ONNX model in FP16, and still being able to exclude a list of nodes. :param onnx_model: original FP32 model :param nodes_to_exclude: nodes that should stay in FP32 :return: mostly FP16 model \"\"\" # add value info related to each node, required for the conversion output_path = onnx_model + \"_shape_inference.onnx\" infer_shapes_path ( model_path = onnx_model , output_path = output_path ) model_fp16 = onnx . load_model ( output_path ) model_fp16 = convert_float_to_float16 ( model = model_fp16 , keep_io_types = False , node_block_list = nodes_to_exclude ) # clean casting nodes before returning the model wrapped_fp16_model = OnnxModel ( model_fp16 ) fusion_utils = FusionUtils ( wrapped_fp16_model ) fusion_utils . remove_cascaded_cast_nodes () fusion_utils . remove_useless_cast_nodes () wrapped_fp16_model . topological_sort () return wrapped_fp16_model . model","title":"convert_fp16()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.cpu_quantization","text":"ONNX CPU only dynamic quantization. Parameters: Name Type Description Default input_model_path str ONNX graph (float) to quantize required output_model_path str where to save quantized model required Source code in src/transformer_deploy/backends/ort_utils.py def cpu_quantization ( input_model_path : str , output_model_path : str ) -> None : \"\"\" ONNX CPU only dynamic quantization. :param input_model_path: ONNX graph (float) to quantize :param output_model_path: where to save quantized model \"\"\" quantize_dynamic ( model_input = Path ( input_model_path ), model_output = Path ( output_model_path ), op_types_to_quantize = [ \"MatMul\" , \"Attention\" ], weight_type = QuantType . QInt8 , per_channel = True , reduce_range = True , extra_options = { \"WeightSymmetric\" : False , \"MatMulConstBOnly\" : True }, )","title":"cpu_quantization()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.create_model_for_provider","text":"Create an ONNX Runtime instance. Parameters: Name Type Description Default path str path to ONNX file or serialized to string model required provider_to_use Union[str, List] provider to use for inference required nb_threads int intra_op_num_threads to use. You may want to try different parameters, more core does not always provide best performances. 12 nb_instances int inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible. 0 optimization_level GraphOptimizationLevel expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one providing kernel fusion of element wise operations. Enable all level is for CPU inference. see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations <GraphOptimizationLevel.ORT_ENABLE_EXTENDED: 2> enable_profiling bool let Onnx Runtime log each kernel time. False log_severity int Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. 2 Returns: Type Description InferenceSession ONNX Runtime inference session Source code in src/transformer_deploy/backends/ort_utils.py def create_model_for_provider ( path : str , provider_to_use : Union [ str , List ], nb_threads : int = multiprocessing . cpu_count (), nb_instances : int = 0 , optimization_level : GraphOptimizationLevel = GraphOptimizationLevel . ORT_ENABLE_EXTENDED , enable_profiling : bool = False , log_severity : int = 2 , ) -> InferenceSession : \"\"\" Create an ONNX Runtime instance. :param path: path to ONNX file or serialized to string model :param provider_to_use: provider to use for inference :param nb_threads: intra_op_num_threads to use. You may want to try different parameters, more core does not always provide best performances. :param nb_instances: inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible. :param optimization_level: expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one providing kernel fusion of element wise operations. Enable all level is for CPU inference. see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations :param enable_profiling: let Onnx Runtime log each kernel time. :param log_severity: Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal. :return: ONNX Runtime inference session \"\"\" options = SessionOptions () options . graph_optimization_level = optimization_level options . enable_profiling = enable_profiling options . log_severity_level = log_severity if isinstance ( provider_to_use , str ): provider_to_use = [ provider_to_use ] if provider_to_use == [ \"CPUExecutionProvider\" ]: options . execution_mode = ExecutionMode . ORT_SEQUENTIAL if nb_instances <= 1 else ExecutionMode . ORT_PARALLEL options . intra_op_num_threads = nb_threads if nb_instances > 1 : options . inter_op_num_threads = nb_instances return InferenceSession ( path , options , providers = provider_to_use )","title":"create_model_for_provider()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.find_node_fp32","text":"Identify out of range values in node outputs. Parameters: Name Type Description Default graph Dict[str, str] graph as adjency nodes dict required output_nodes Dict[str, torch.Tensor] output of each node required Returns: Type Description List[str] list of nodes producing outputs outside fp16 tensor Source code in src/transformer_deploy/backends/ort_utils.py def find_node_fp32 ( graph : Dict [ str , str ], output_nodes : Dict [ str , torch . Tensor ]) -> List [ str ]: \"\"\" Identify out of range values in node outputs. :param graph: graph as adjency nodes dict :param output_nodes: output of each node :return: list of nodes producing outputs outside fp16 tensor \"\"\" keep_fp32 = list () min_float16 = torch . finfo ( torch . float16 ) . min max_float16 = torch . finfo ( torch . float16 ) . max resolution = 5.96e-08 # torch.finfo(torch.float16).eps # minimum value that can be represented by FP16 for k , tensor in output_nodes . items (): if tensor . dtype != torch . float32 : continue # out of FP16 range if ( torch . any ( tensor > max_float16 ) or torch . any ( tensor < min_float16 ) or ( torch . any ( tensor < resolution & tensor > - resolution & tensor != 0 )) # limited memory footprint ): keep_fp32 . append ( graph [ k ]) return keep_fp32","title":"find_node_fp32()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.get_io_to_node_mapping","text":"Extract output->node and input->node mappings Parameters: Name Type Description Default onnx_model ModelProto ONNX model required Returns: Type Description Tuple[Dict[str, str], Dict[str, str]] 2 mappings, (i->node, o->node) Source code in src/transformer_deploy/backends/ort_utils.py def get_io_to_node_mapping ( onnx_model : ModelProto ) -> Tuple [ Dict [ str , str ], Dict [ str , str ]]: \"\"\" Extract output->node and input->node mappings :param onnx_model: ONNX model :return: 2 mappings, (i->node, o->node) \"\"\" output_mapping : Dict [ str , str ] = dict () input_mapping : Dict [ str , str ] = dict () for node in onnx_model . graph . node : # type: NodeProto assert len ( node . output ) == 1 output_node = node . output [ 0 ] output_mapping [ output_node ] = node . name for i in node . input : input_mapping [ i ] = node . name return input_mapping , output_mapping","title":"get_io_to_node_mapping()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.get_keep_fp32_nodes","text":"Find the list of nodes to keep in FP32 to avoid out of range values Parameters: Name Type Description Default onnx_model_path str ONNX model path required get_input Callable[[], Dict[str, torch.Tensor]] generate input to test the model. Output should change from call to call required early_stop int will test until early_stop tests are done without any new node to keep in FP32 100 device str where to run the inference 'cuda' Returns: Type Description List[str] list of names of nodes to keep in FP32 Source code in src/transformer_deploy/backends/ort_utils.py def get_keep_fp32_nodes ( onnx_model_path : str , get_input : Callable [[], Dict [ str , torch . Tensor ]], early_stop : int = 100 , device : str = \"cuda\" , ) -> List [ str ]: \"\"\" Find the list of nodes to keep in FP32 to avoid out of range values :param onnx_model_path: ONNX model path :param get_input: generate input to test the model. Output should change from call to call :param early_stop: will test until `early_stop` tests are done without any new node to keep in FP32 :param device: where to run the inference :return: list of names of nodes to keep in FP32 \"\"\" # do not load weights on LLM (>2Gb), we only need to modify the computation graph onnx_model : ModelProto = onnx . load_model ( f = onnx_model_path , load_external_data = False ) onnx_model_fp32_all_nodes = add_output_nodes ( model = onnx_model ) path_onnx_model_fp32_all_nodes = onnx_model_path + \"_all_nodes.onnx\" onnx . save_model ( proto = onnx_model_fp32_all_nodes , f = path_onnx_model_fp32_all_nodes , save_as_external_data = False ) provider = \"CUDAExecutionProvider\" if device == \"cuda\" else \"CPUExecutionProvider\" ort_model_fp32_all_nodes = create_model_for_provider ( path_onnx_model_fp32_all_nodes , provider ) ort_binding = ort_model_fp32_all_nodes . io_binding () input_mapping , output_mapping = get_io_to_node_mapping ( onnx_model = onnx_model ) # list all nodes which have an output out of the FP16 range keep_fp32_nodes = list () no_new_node_counter = 0 while no_new_node_counter < early_stop : inputs = get_input () outputs : Dict [ str , torch . Tensor ] = inference_onnx_binding ( model_onnx = ort_model_fp32_all_nodes , inputs = inputs , device = device , binding = ort_binding , clone_tensor = False ) keep_node_io = find_node_fp32 ( graph = output_mapping , output_nodes = outputs ) nodes_to_add = [ n for n in keep_node_io if n not in keep_fp32_nodes ] keep_fp32_nodes += nodes_to_add if len ( nodes_to_add ) == 0 : no_new_node_counter += 1 else : no_new_node_counter = 0 if device == \"cuda\" : torch . cuda . empty_cache () # I/O names that can't be found in the graph nodes_to_skip = ( [ n . name for n in onnx_model . graph . input ] + [ n . name for n in onnx_model . graph . output ] + [ n . name for n in onnx_model . graph . initializer ] ) # for each node to keep in FP32, we keep its children in FP32 too as they will receive FP32 values as input map_children = defaultdict ( list ) for node in onnx_model . graph . node : for o in node . output : if o in nodes_to_skip : continue child = input_mapping [ o ] map_children [ node . name ] . append ( child ) keep_fp32_nodes += [ c for k in keep_fp32_nodes if k in map_children for c in map_children [ k ]] return keep_fp32_nodes","title":"get_keep_fp32_nodes()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.inference_onnx_binding","text":"Performs inference on ONNX Runtime in an optimized way. In particular, it avoids any Onnx Runtime output tensor copy. It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another inference. To avoid any issue, just set clone_tensor to True (default). For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True. Parameters: Name Type Description Default model_onnx InferenceSession ONNX model required inputs Dict[str, torch.Tensor] input torch tensor required device str where to run the inference. One of [cpu, cuda] required device_id int ID of the device where to run the inference, to be used when there are multiple GPUs, etc. 0 binding Optional[onnxruntime.capi.onnxruntime_inference_collection.IOBinding] previously generated binding IO, will be reset. None clone_tensor bool clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime at the next inference call. True Returns: Type Description Dict[str, torch.Tensor] a dict {axis name: output tensor} Source code in src/transformer_deploy/backends/ort_utils.py def inference_onnx_binding ( model_onnx : InferenceSession , inputs : Dict [ str , torch . Tensor ], device : str , device_id : int = 0 , binding : Optional [ IOBinding ] = None , clone_tensor : bool = True , ) -> Dict [ str , torch . Tensor ]: \"\"\" Performs inference on ONNX Runtime in an optimized way. In particular, it avoids any Onnx Runtime output tensor copy. It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another inference. To avoid any issue, just set clone_tensor to True (default). For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True. :param model_onnx: ONNX model :param inputs: input torch tensor :param device: where to run the inference. One of [cpu, cuda] :param device_id: ID of the device where to run the inference, to be used when there are multiple GPUs, etc. :param binding: previously generated binding IO, will be reset. :param clone_tensor: clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime at the next inference call. :return: a dict {axis name: output tensor} \"\"\" assert isinstance ( device , str ) assert device in [ \"cpu\" , \"cuda\" ], f \"unexpected inference device: ' { device } '\" if binding is None : binding : IOBinding = model_onnx . io_binding () else : binding . clear_binding_inputs () binding . clear_binding_outputs () for input_onnx in model_onnx . get_inputs (): if input_onnx . name not in inputs : # some inputs may be optional continue tensor : torch . Tensor = inputs [ input_onnx . name ] tensor = tensor . detach () if tensor . dtype in [ torch . int64 , torch . long ]: # int32 mandatory as input of bindings, int64 not supported tensor = tensor . type ( dtype = torch . int32 ) tensor = tensor . contiguous () binding . bind_input ( name = input_onnx . name , device_type = device , device_id = device_id , element_type = torch_to_numpy_dtype_dict [ tensor . dtype ], shape = tuple ( tensor . shape ), buffer_ptr = tensor . data_ptr (), ) inputs [ input_onnx . name ] = tensor for out in model_onnx . get_outputs (): binding . bind_output ( name = out . name , device_type = device , device_id = device_id , ) binding . synchronize_inputs () model_onnx . run_with_iobinding ( binding ) binding . synchronize_outputs () outputs = dict () assert len ( model_onnx . get_outputs ()) == len ( binding . get_outputs () ), f \" { len ( model_onnx . get_outputs ()) } != { len ( binding . get_outputs ()) } \" for out , t in zip ( model_onnx . get_outputs (), binding . get_outputs ()): outputs [ out . name ] = to_pytorch ( t , clone_tensor = clone_tensor ) return outputs","title":"inference_onnx_binding()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.optimize_onnx","text":"ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. Parameters: Name Type Description Default onnx_path str ONNX input path required onnx_optim_model_path str where to save optimized model required fp16 bool use mixed precision (faster inference) required use_cuda bool perform optimization on GPU (should ) required num_attention_heads int number of attention heads of a model (0 -> try to detect) 0 hidden_size int hidden layer size of a model (0 -> try to detect) 0 architecture str model architecture to optimize. One of [bert, bart, gpt2] 'bert' Source code in src/transformer_deploy/backends/ort_utils.py def optimize_onnx ( onnx_path : str , onnx_optim_model_path : str , fp16 : bool , use_cuda : bool , num_attention_heads : int = 0 , hidden_size : int = 0 , architecture : str = \"bert\" , ) -> None : \"\"\" ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. :param onnx_path: ONNX input path :param onnx_optim_model_path: where to save optimized model :param fp16: use mixed precision (faster inference) :param use_cuda: perform optimization on GPU (should ) :param num_attention_heads: number of attention heads of a model (0 -> try to detect) :param hidden_size: hidden layer size of a model (0 -> try to detect) :param architecture: model architecture to optimize. One of [bert, bart, gpt2] \"\"\" assert architecture in [ \"bert\" , \"bart\" , \"gpt2\" ], f \"unsupported architecture: { architecture } \" opt_level = 1 if architecture == \"bert\" else 0 optimization_options = FusionOptions ( model_type = architecture ) optimization_options . enable_gelu_approximation = False # additional optimization optimized_model : BertOnnxModel = optimizer . optimize_model ( input = onnx_path , model_type = architecture , use_gpu = use_cuda , opt_level = opt_level , num_heads = num_attention_heads , # automatic detection with 0 may not work with opset 13 or distilbert models hidden_size = hidden_size , # automatic detection with 0 optimization_options = optimization_options , ) if fp16 : # use_symbolic_shape_infer set to false because doesn't work after ONNX package v1.10.2 optimized_model . convert_float_to_float16 ( use_symbolic_shape_infer = False ) # FP32 -> FP16 logging . info ( f \"optimizations applied: { optimized_model . get_fused_operator_statistics () } \" ) optimized_model . save_model_to_file ( onnx_optim_model_path )","title":"optimize_onnx()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.to_pytorch","text":"Convert OrtValue output by Onnx Runtime to Pytorch tensor. The process can be done in a zero copy way (depending of clone parameter). Parameters: Name Type Description Default ort_tensor OrtValue output from Onnx Runtime required clone_tensor bool Onnx Runtime owns the storage array and will write on the next inference. By cloning you guarantee that the data won't change. required Returns: Type Description Tensor Pytorch tensor Source code in src/transformer_deploy/backends/ort_utils.py def to_pytorch ( ort_tensor : OrtValue , clone_tensor : bool ) -> torch . Tensor : \"\"\" Convert OrtValue output by Onnx Runtime to Pytorch tensor. The process can be done in a zero copy way (depending of clone parameter). :param ort_tensor: output from Onnx Runtime :param clone_tensor: Onnx Runtime owns the storage array and will write on the next inference. By cloning you guarantee that the data won't change. :return: Pytorch tensor \"\"\" if ort_tensor . device_name () . lower () == \"cuda\" : np_type = ort_to_numpy_dtype_dict [ ort_tensor . data_type ()] fake_owner = 1 # size not used anywhere, so just put 0 memory = cp . cuda . UnownedMemory ( ort_tensor . data_ptr (), 0 , fake_owner ) memory_ptr = cp . cuda . MemoryPointer ( memory , 0 ) # make sure you interpret the array shape/dtype/strides correctly cp_array = cp . ndarray ( shape = ort_tensor . shape (), memptr = memory_ptr , dtype = np_type ) # cloning required otherwise ORT will recycle the storage array and put new values into it if new inf is done. torch_tensor = torch . from_dlpack ( cp_array . toDlpack ()) if clone_tensor : torch_tensor = torch_tensor . clone () return torch_tensor else : np_tensor = ort_tensor . numpy () return torch . from_numpy ( np_tensor )","title":"to_pytorch()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.use_external_data","text":"Check if a model uses external data Parameters: Name Type Description Default path str Onnx model path required Returns: Type Description bool True if any initalizer (model weight) is stored in an external file Source code in src/transformer_deploy/backends/ort_utils.py def use_external_data ( path : str ) -> bool : \"\"\" Check if a model uses external data :param path: Onnx model path :return: True if any initalizer (model weight) is stored in an external file \"\"\" model = onnx . load_model ( f = path , load_external_data = False ) for i in model . graph . initializer : if i . HasField ( \"data_location\" ) and i . data_location == onnx . TensorProto . EXTERNAL : return True return False","title":"use_external_data()"},{"location":"reference/backends/pytorch_utils/","text":"Utils related to Pytorch inference. convert_to_onnx ( model_pytorch , output_path , inputs_pytorch , quantization , var_output_seq ) # Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. Pytorch sometimes fails to infer output tensor shape of models In ONNX graph, some axis name may be marked like \"Divoutput_dim_1\" which is a generated name, and there may be a warning: \"WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\" ex.: https://discuss.pytorch.org/t/bidirectional-lstm-and-onnx-runtime-warnings/136374 Parameters: Name Type Description Default model_pytorch Module Pytorch model (transformers) required output_path str where to save ONNX file required inputs_pytorch Dict[str, torch.Tensor] Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) required quantization bool model is quantized required var_output_seq bool variable size sequence required Source code in src/transformer_deploy/backends/pytorch_utils.py def convert_to_onnx ( model_pytorch : torch . nn . Module , output_path : str , inputs_pytorch : Dict [ str , torch . Tensor ], quantization : bool , var_output_seq : bool , ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. Pytorch sometimes fails to infer output tensor shape of models In ONNX graph, some axis name may be marked like \"Divoutput_dim_1\" which is a generated name, and there may be a warning: ** \"WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\" ** ex.: https://discuss.pytorch.org/t/bidirectional-lstm-and-onnx-runtime-warnings/136374 :param model_pytorch: Pytorch model (transformers) :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param quantization: model is quantized :param var_output_seq: variable size sequence \"\"\" if quantization : try : from pytorch_quantization.nn import TensorQuantizer except ImportError : raise ImportError ( \"It seems that pytorch-quantization is not yet installed. \" \"It is required when you enable the quantization flag and use CUDA device.\" \"Please find installation instructions on \" \"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization or use: \\n \" \"pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization \\\\ &\" \"subdirectory=tools/pytorch-quantization/\" ) TensorQuantizer . use_fb_fake_quant = True if hasattr ( model_pytorch , \"config\" ) and hasattr ( model_pytorch . config , \"use_cache\" ): use_cache = getattr ( model_pytorch . config , \"use_cache\" ) setattr ( model_pytorch . config , \"use_cache\" , False ) # dynamic axis == variable length axis dynamic_axis = dict () for k in inputs_pytorch . keys (): if var_output_seq : # seq axis name is fixed to be matched with output seq axis name (for output shape prediction) dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } else : # if there is no specific requirement, each axis name is unique, fix some issue on T5 model dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : f \"sequence- { k } \" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } if var_output_seq : dynamic_axis [ \"output\" ][ 1 ] = \"sequence\" # replace int64 input tensors by int32 -> for ONNX Runtime binding API and expected by TensorRT engine for k , v in inputs_pytorch . items (): if not isinstance ( v , torch . Tensor ): continue if v . dtype in [ torch . long , torch . int64 ]: inputs_pytorch [ k ] = v . type ( torch . int32 ) with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = 13 , # the ONNX version to use, >= 13 supports channel quantized model do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name, hard coded so only 1 output supported dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) if quantization : TensorQuantizer . use_fb_fake_quant = False if hasattr ( model_pytorch , \"config\" ) and hasattr ( model_pytorch . config , \"use_cache\" ): setattr ( model_pytorch . config , \"use_cache\" , use_cache ) get_model_size ( path ) # Find number of attention heads and hidden layer size of a model Parameters: Name Type Description Default path str path to model required Returns: Type Description Tuple[int, int] tupple of # of attention heads and hidden layer size (0 if not found) Source code in src/transformer_deploy/backends/pytorch_utils.py def get_model_size ( path : str ) -> Tuple [ int , int ]: \"\"\" Find number of attention heads and hidden layer size of a model :param path: path to model :return: tupple of # of attention heads and hidden layer size (0 if not found) \"\"\" config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = path ) num_attention_heads = getattr ( config , \"num_attention_heads\" , 0 ) hidden_size = getattr ( config , \"hidden_size\" , 0 ) return num_attention_heads , hidden_size infer_classification_pytorch ( model , run_on_cuda ) # Perform Pytorch inference for classification task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_classification_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Perform Pytorch inference for classification task :param model: Pytorch model (transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : model_output = model ( ** inputs ) . logits . detach () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer infer_feature_extraction_pytorch ( model , run_on_cuda ) # Perform Pytorch inference for feature extraction task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (sentence-transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_feature_extraction_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Perform Pytorch inference for feature extraction task :param model: Pytorch model (sentence-transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : model_output = model ( ** inputs ) . detach () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"Pytorch utils"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.convert_to_onnx","text":"Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. Pytorch sometimes fails to infer output tensor shape of models In ONNX graph, some axis name may be marked like \"Divoutput_dim_1\" which is a generated name, and there may be a warning: \"WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\" ex.: https://discuss.pytorch.org/t/bidirectional-lstm-and-onnx-runtime-warnings/136374 Parameters: Name Type Description Default model_pytorch Module Pytorch model (transformers) required output_path str where to save ONNX file required inputs_pytorch Dict[str, torch.Tensor] Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) required quantization bool model is quantized required var_output_seq bool variable size sequence required Source code in src/transformer_deploy/backends/pytorch_utils.py def convert_to_onnx ( model_pytorch : torch . nn . Module , output_path : str , inputs_pytorch : Dict [ str , torch . Tensor ], quantization : bool , var_output_seq : bool , ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. Pytorch sometimes fails to infer output tensor shape of models In ONNX graph, some axis name may be marked like \"Divoutput_dim_1\" which is a generated name, and there may be a warning: ** \"WARNING: The shape inference of prim::Constant type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\" ** ex.: https://discuss.pytorch.org/t/bidirectional-lstm-and-onnx-runtime-warnings/136374 :param model_pytorch: Pytorch model (transformers) :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param quantization: model is quantized :param var_output_seq: variable size sequence \"\"\" if quantization : try : from pytorch_quantization.nn import TensorQuantizer except ImportError : raise ImportError ( \"It seems that pytorch-quantization is not yet installed. \" \"It is required when you enable the quantization flag and use CUDA device.\" \"Please find installation instructions on \" \"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization or use: \\n \" \"pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization \\\\ &\" \"subdirectory=tools/pytorch-quantization/\" ) TensorQuantizer . use_fb_fake_quant = True if hasattr ( model_pytorch , \"config\" ) and hasattr ( model_pytorch . config , \"use_cache\" ): use_cache = getattr ( model_pytorch . config , \"use_cache\" ) setattr ( model_pytorch . config , \"use_cache\" , False ) # dynamic axis == variable length axis dynamic_axis = dict () for k in inputs_pytorch . keys (): if var_output_seq : # seq axis name is fixed to be matched with output seq axis name (for output shape prediction) dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } else : # if there is no specific requirement, each axis name is unique, fix some issue on T5 model dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : f \"sequence- { k } \" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } if var_output_seq : dynamic_axis [ \"output\" ][ 1 ] = \"sequence\" # replace int64 input tensors by int32 -> for ONNX Runtime binding API and expected by TensorRT engine for k , v in inputs_pytorch . items (): if not isinstance ( v , torch . Tensor ): continue if v . dtype in [ torch . long , torch . int64 ]: inputs_pytorch [ k ] = v . type ( torch . int32 ) with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = 13 , # the ONNX version to use, >= 13 supports channel quantized model do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name, hard coded so only 1 output supported dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) if quantization : TensorQuantizer . use_fb_fake_quant = False if hasattr ( model_pytorch , \"config\" ) and hasattr ( model_pytorch . config , \"use_cache\" ): setattr ( model_pytorch . config , \"use_cache\" , use_cache )","title":"convert_to_onnx()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.get_model_size","text":"Find number of attention heads and hidden layer size of a model Parameters: Name Type Description Default path str path to model required Returns: Type Description Tuple[int, int] tupple of # of attention heads and hidden layer size (0 if not found) Source code in src/transformer_deploy/backends/pytorch_utils.py def get_model_size ( path : str ) -> Tuple [ int , int ]: \"\"\" Find number of attention heads and hidden layer size of a model :param path: path to model :return: tupple of # of attention heads and hidden layer size (0 if not found) \"\"\" config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = path ) num_attention_heads = getattr ( config , \"num_attention_heads\" , 0 ) hidden_size = getattr ( config , \"hidden_size\" , 0 ) return num_attention_heads , hidden_size","title":"get_model_size()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.infer_classification_pytorch","text":"Perform Pytorch inference for classification task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_classification_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Perform Pytorch inference for classification task :param model: Pytorch model (transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : model_output = model ( ** inputs ) . logits . detach () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"infer_classification_pytorch()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.infer_feature_extraction_pytorch","text":"Perform Pytorch inference for feature extraction task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (sentence-transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_feature_extraction_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Perform Pytorch inference for feature extraction task :param model: Pytorch model (sentence-transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : model_output = model ( ** inputs ) . detach () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"infer_feature_extraction_pytorch()"},{"location":"reference/backends/st_utils/","text":"Utils related to sentence-transformers. STransformerWrapper ( Module ) # Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. Source code in src/transformer_deploy/backends/st_utils.py class STransformerWrapper ( nn . Module ): \"\"\" Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. \"\"\" def __init__ ( self , model : \"SentenceTransformer\" ): super () . __init__ () self . model = model def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ] forward ( self , * kargs , ** kwargs ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/backends/st_utils.py def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ] load_sentence_transformers ( path ) # Load sentence-transformers model and wrap it to make it behave like any other transformers model Parameters: Name Type Description Default path str path to the model required Returns: Type Description STransformerWrapper wrapped sentence-transformers model Source code in src/transformer_deploy/backends/st_utils.py def load_sentence_transformers ( path : str ) -> STransformerWrapper : \"\"\" Load sentence-transformers model and wrap it to make it behave like any other transformers model :param path: path to the model :return: wrapped sentence-transformers model \"\"\" try : from sentence_transformers import SentenceTransformer except ImportError : raise Exception ( \"sentence-transformers library is not present, please install it: pip install sentence-transformers\" ) model : SentenceTransformer = SentenceTransformer ( path ) return STransformerWrapper ( model = model )","title":"St utils"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.STransformerWrapper","text":"Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. Source code in src/transformer_deploy/backends/st_utils.py class STransformerWrapper ( nn . Module ): \"\"\" Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. \"\"\" def __init__ ( self , model : \"SentenceTransformer\" ): super () . __init__ () self . model = model def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ]","title":"STransformerWrapper"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.STransformerWrapper.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/backends/st_utils.py def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ]","title":"forward()"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.load_sentence_transformers","text":"Load sentence-transformers model and wrap it to make it behave like any other transformers model Parameters: Name Type Description Default path str path to the model required Returns: Type Description STransformerWrapper wrapped sentence-transformers model Source code in src/transformer_deploy/backends/st_utils.py def load_sentence_transformers ( path : str ) -> STransformerWrapper : \"\"\" Load sentence-transformers model and wrap it to make it behave like any other transformers model :param path: path to the model :return: wrapped sentence-transformers model \"\"\" try : from sentence_transformers import SentenceTransformer except ImportError : raise Exception ( \"sentence-transformers library is not present, please install it: pip install sentence-transformers\" ) model : SentenceTransformer = SentenceTransformer ( path ) return STransformerWrapper ( model = model )","title":"load_sentence_transformers()"},{"location":"reference/backends/trt_utils/","text":"All the tooling to ease TensorRT usage. TensorRTShape dataclass # Store input shapes for TensorRT build. 3 shapes per input tensor are required (as tuple of integers): minimum input shape optimal size used for the benchmarks during building maximum input shape Set input name to None for default shape. Source code in src/transformer_deploy/backends/trt_utils.py @dataclass class TensorRTShape : \"\"\" Store input shapes for TensorRT build. 3 shapes per input tensor are required (as tuple of integers): * minimum input shape * optimal size used for the benchmarks during building * maximum input shape Set input name to None for default shape. \"\"\" min_shape : List [ int ] optimal_shape : List [ int ] max_shape : List [ int ] input_name : Optional [ str ] def check_validity ( self ) -> None : \"\"\" Basic checks of provided shapes \"\"\" assert len ( self . min_shape ) == len ( self . optimal_shape ) == len ( self . max_shape ) assert len ( self . min_shape ) > 0 assert self . min_shape [ 0 ] > 0 and self . optimal_shape [ 0 ] > 0 and self . max_shape [ 0 ] > 0 assert self . input_name is not None def make_copy ( self , input_name : str ) -> \"TensorRTShape\" : \"\"\" Make a copy of the current instance, with a different input name. :param input_name: new input name to use :return: a copy of the current shape with a different name \"\"\" instance_copy = dataclasses . replace ( self ) instance_copy . input_name = input_name return instance_copy def generate_multiple_shapes ( self , input_names : List [ str ]) -> List [ \"TensorRTShape\" ]: \"\"\" Generate multiple shapes when only a single default one is defined. :param input_names: input names used by the model :return: a list of shapes \"\"\" assert self . input_name is None , f \"input name is not None: { self . input_name } \" result = list () for name in input_names : shape = self . make_copy ( input_name = name ) result . append ( shape ) return result check_validity ( self ) # Basic checks of provided shapes Source code in src/transformer_deploy/backends/trt_utils.py def check_validity ( self ) -> None : \"\"\" Basic checks of provided shapes \"\"\" assert len ( self . min_shape ) == len ( self . optimal_shape ) == len ( self . max_shape ) assert len ( self . min_shape ) > 0 assert self . min_shape [ 0 ] > 0 and self . optimal_shape [ 0 ] > 0 and self . max_shape [ 0 ] > 0 assert self . input_name is not None generate_multiple_shapes ( self , input_names ) # Generate multiple shapes when only a single default one is defined. Parameters: Name Type Description Default input_names List[str] input names used by the model required Returns: Type Description List[TensorRTShape] a list of shapes Source code in src/transformer_deploy/backends/trt_utils.py def generate_multiple_shapes ( self , input_names : List [ str ]) -> List [ \"TensorRTShape\" ]: \"\"\" Generate multiple shapes when only a single default one is defined. :param input_names: input names used by the model :return: a list of shapes \"\"\" assert self . input_name is None , f \"input name is not None: { self . input_name } \" result = list () for name in input_names : shape = self . make_copy ( input_name = name ) result . append ( shape ) return result make_copy ( self , input_name ) # Make a copy of the current instance, with a different input name. Parameters: Name Type Description Default input_name str new input name to use required Returns: Type Description TensorRTShape a copy of the current shape with a different name Source code in src/transformer_deploy/backends/trt_utils.py def make_copy ( self , input_name : str ) -> \"TensorRTShape\" : \"\"\" Make a copy of the current instance, with a different input name. :param input_name: new input name to use :return: a copy of the current shape with a different name \"\"\" instance_copy = dataclasses . replace ( self ) instance_copy . input_name = input_name return instance_copy build_engine ( runtime , onnx_file_path , logger , workspace_size , fp16 , int8 , fp16_fix =< function fix_fp16_network at 0x7fe0424540d0 > , ** kwargs ) # Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size doesn't hurt performance and is highly advised. Batch size can provided through different ways: min_shape , optimal_shape , max_shape : for simple case, 3 tuples of int when all input tensors have the same shape input_shapes : a list of TensorRTShape with names if there are several input tensors with different shapes TIP : minimum batch size should be 1 in most cases. Parameters: Name Type Description Default runtime Runtime global variable shared accross inference call / model building required onnx_file_path str path to the ONNX file required logger Logger specific logger to TensorRT required workspace_size int GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. required fp16 bool enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. required int8 bool enable INT-8 quantization, best performance but model should have been quantized. required fp16_fix Callable[[tensorrt.tensorrt.INetworkDefinition], tensorrt.tensorrt.INetworkDefinition] a function to set FP32 precision on some nodes to fix FP16 overflow <function fix_fp16_network at 0x7fe0424540d0> Returns: Type Description ICudaEngine TensorRT engine to use during inference Source code in src/transformer_deploy/backends/trt_utils.py def build_engine ( runtime : Runtime , onnx_file_path : str , logger : Logger , workspace_size : int , fp16 : bool , int8 : bool , fp16_fix : Callable [[ INetworkDefinition ], INetworkDefinition ] = fix_fp16_network , ** kwargs , ) -> ICudaEngine : \"\"\" Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size doesn't hurt performance and is highly advised. Batch size can provided through different ways: * **min_shape**, **optimal_shape**, **max_shape**: for simple case, 3 tuples of int when all input tensors have the same shape * **input_shapes**: a list of TensorRTShape with names if there are several input tensors with different shapes **TIP**: minimum batch size should be 1 in most cases. :param runtime: global variable shared accross inference call / model building :param onnx_file_path: path to the ONNX file :param logger: specific logger to TensorRT :param workspace_size: GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. :param fp16: enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. :param int8: enable INT-8 quantization, best performance but model should have been quantized. :param fp16_fix: a function to set FP32 precision on some nodes to fix FP16 overflow :return: TensorRT engine to use during inference \"\"\" # default input shape if \"min_shape\" in kwargs and \"optimal_shape\" in kwargs and \"max_shape\" in kwargs : default_shape = TensorRTShape ( min_shape = kwargs [ \"min_shape\" ], optimal_shape = kwargs [ \"optimal_shape\" ], max_shape = kwargs [ \"max_shape\" ], input_name = None , ) input_shapes = [ default_shape ] else : assert \"input_shapes\" in kwargs , \"missing input shapes\" input_shapes : List [ TensorRTShape ] = kwargs [ \"input_shapes\" ] with trt . Builder ( logger ) as builder : # type: Builder with builder . create_network ( flags = 1 << int ( trt . NetworkDefinitionCreationFlag . EXPLICIT_BATCH ) ) as network_def : # type: INetworkDefinition with trt . OnnxParser ( network_def , logger ) as parser : # type: OnnxParser # The maximum batch size which can be used at execution time, # and also the batch size for which the ICudaEngine will be optimized. builder . max_batch_size = max ([ s . max_shape [ 0 ] for s in input_shapes ]) config : IBuilderConfig = builder . create_builder_config () config . max_workspace_size = workspace_size # to enable complete trt inspector debugging, only for TensorRT >= 8.2 # config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED # disable CUDNN optimizations config . set_tactic_sources ( tactic_sources = 1 << int ( trt . TacticSource . CUBLAS ) | 1 << int ( trt . TacticSource . CUBLAS_LT ) ) if int8 : config . set_flag ( trt . BuilderFlag . INT8 ) if fp16 : config . set_flag ( trt . BuilderFlag . FP16 ) config . set_flag ( trt . BuilderFlag . DISABLE_TIMING_CACHE ) # https://github.com/NVIDIA/TensorRT/issues/1196 (sometimes big diff in output when using FP16) config . set_flag ( trt . BuilderFlag . OBEY_PRECISION_CONSTRAINTS ) with open ( onnx_file_path , \"rb\" ) as f : parser . parse ( f . read ()) profile : IOptimizationProfile = builder . create_optimization_profile () # duplicate default shape (one for each input) if len ( input_shapes ) == 1 and input_shapes [ 0 ] . input_name is None : names = [ network_def . get_input ( num_input ) . name for num_input in range ( network_def . num_inputs )] input_shapes = input_shapes [ 0 ] . generate_multiple_shapes ( input_names = names ) for shape in input_shapes : shape . check_validity () profile . set_shape ( input = shape . input_name , min = shape . min_shape , opt = shape . optimal_shape , max = shape . max_shape , ) if \"shape_tensors\" in kwargs : for shape in kwargs [ \"shape_tensors\" ]: profile . set_shape_input ( input = shape . input_name , min = shape . min_shape , opt = shape . optimal_shape , max = shape . max_shape , ) config . add_optimization_profile ( profile ) if fp16 : network_def = fp16_fix ( network_def ) trt_engine = builder . build_serialized_network ( network_def , config ) engine : ICudaEngine = runtime . deserialize_cuda_engine ( trt_engine ) assert engine is not None , \"error during engine generation, check error messages above :-(\" return engine fix_fp16_network ( network_definition ) # Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. Parameters: Name Type Description Default network_definition INetworkDefinition graph generated by TensorRT after parsing ONNX file (during the model building) required Returns: Type Description INetworkDefinition patched network definition Source code in src/transformer_deploy/backends/trt_utils.py def fix_fp16_network ( network_definition : INetworkDefinition ) -> INetworkDefinition : \"\"\" Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. :param network_definition: graph generated by TensorRT after parsing ONNX file (during the model building) :return: patched network definition \"\"\" # search for patterns which may overflow in FP16 precision, we force FP32 precisions for those nodes for layer_index in range ( network_definition . num_layers - 1 ): layer : ILayer = network_definition . get_layer ( layer_index ) next_layer : ILayer = network_definition . get_layer ( layer_index + 1 ) # POW operation usually followed by mean reduce if layer . type == trt . LayerType . ELEMENTWISE and next_layer . type == trt . LayerType . REDUCE : # casting to get access to op attribute layer . __class__ = IElementWiseLayer next_layer . __class__ = IReduceLayer if layer . op == trt . ElementWiseOperation . POW : layer . precision = trt . DataType . FLOAT next_layer . precision = trt . DataType . FLOAT layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) next_layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition get_binding_idxs ( engine , profile_index ) # Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings Parameters: Name Type Description Default engine ICudaEngine TensorRT engine generated during the model building required profile_index int profile to use (several profiles can be set during building) required Returns: Type Description input and output tensor indexes Source code in src/transformer_deploy/backends/trt_utils.py def get_binding_idxs ( engine : trt . ICudaEngine , profile_index : int ): \"\"\" Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings :param engine: TensorRT engine generated during the model building :param profile_index: profile to use (several profiles can be set during building) :return: input and output tensor indexes \"\"\" num_bindings_per_profile = engine . num_bindings // engine . num_optimization_profiles start_binding = profile_index * num_bindings_per_profile end_binding = start_binding + num_bindings_per_profile # Separate input and output binding indices for convenience input_binding_idxs : List [ int ] = [] output_binding_idxs : List [ int ] = [] for binding_index in range ( start_binding , end_binding ): if engine . binding_is_input ( binding_index ): input_binding_idxs . append ( binding_index ) else : output_binding_idxs . append ( binding_index ) return input_binding_idxs , output_binding_idxs get_fix_fp16_network_func ( keep_fp32 ) # Generate a function for TensorRT engine to set precision of specific nodes to FP32 to keep tensorrt FP16 output close to FP32 nodes. Parameters: Name Type Description Default keep_fp32 List[str] nodes to keep in FP32 required Returns: Type Description Callable[[tensorrt.tensorrt.INetworkDefinition], tensorrt.tensorrt.INetworkDefinition] a function to set node precisions Source code in src/transformer_deploy/backends/trt_utils.py def get_fix_fp16_network_func ( keep_fp32 : List [ str ]) -> Callable [[ INetworkDefinition ], INetworkDefinition ]: \"\"\" Generate a function for TensorRT engine to set precision of specific nodes to FP32 to keep tensorrt FP16 output close to FP32 nodes. :param keep_fp32: nodes to keep in FP32 :return: a function to set node precisions \"\"\" def f ( network_definition : INetworkDefinition ) -> INetworkDefinition : for layer_index in range ( network_definition . num_layers ): layer : ILayer = network_definition . get_layer ( layer_index ) # identity function is mainly used for casting # https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Graph/Layers.html#iidentitylayer # if layer.type == LayerType.IDENTITY: # continue if layer . name in keep_fp32 : layer . precision = trt . DataType . FLOAT assert layer . num_outputs == 1 , f \"unexpected # output: { layer . num_outputs } \" layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition return f get_output_tensors ( context , host_inputs , input_binding_idxs , output_binding_idxs ) # Reserve memory in GPU for input and output tensors. Parameters: Name Type Description Default context IExecutionContext TensorRT context shared accross inference steps required host_inputs List[torch.Tensor] input tensor required input_binding_idxs List[int] indexes of each input vector (should be the same than during building) required output_binding_idxs List[int] indexes of each output vector (should be the same than during building) required Returns: Type Description List[torch.Tensor] tensors where output will be stored Source code in src/transformer_deploy/backends/trt_utils.py def get_output_tensors ( context : trt . IExecutionContext , host_inputs : List [ torch . Tensor ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> List [ torch . Tensor ]: \"\"\" Reserve memory in GPU for input and output tensors. :param context: TensorRT context shared accross inference steps :param host_inputs: input tensor :param input_binding_idxs: indexes of each input vector (should be the same than during building) :param output_binding_idxs: indexes of each output vector (should be the same than during building) :return: tensors where output will be stored \"\"\" # explicitly set dynamic input shapes, so dynamic output shapes can be computed internally for host_input , binding_index in zip ( host_inputs , input_binding_idxs ): context . set_binding_shape ( binding_index , tuple ( host_input . shape )) # assert context.all_binding_shapes_specified device_outputs : List [ torch . Tensor ] = [] for binding_index in output_binding_idxs : # TensorRT computes output shape based on input shape provided above output_shape = context . get_binding_shape ( binding_index ) # allocate buffers to hold output results output = torch . empty ( tuple ( output_shape ), device = \"cuda\" ) device_outputs . append ( output ) return device_outputs infer_tensorrt ( context , host_inputs , input_binding_idxs , output_binding_idxs ) # Perform inference with TensorRT. Parameters: Name Type Description Default context IExecutionContext shared variable required host_inputs Dict[str, torch.Tensor] input tensor required input_binding_idxs List[int] input tensor indexes required output_binding_idxs List[int] output tensor indexes required Returns: Type Description List[torch.Tensor] output tensor Source code in src/transformer_deploy/backends/trt_utils.py def infer_tensorrt ( context : IExecutionContext , host_inputs : Dict [ str , torch . Tensor ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> List [ torch . Tensor ]: \"\"\" Perform inference with TensorRT. :param context: shared variable :param host_inputs: input tensor :param input_binding_idxs: input tensor indexes :param output_binding_idxs: output tensor indexes :return: output tensor \"\"\" input_tensors : List [ torch . Tensor ] = list () for i in range ( context . engine . num_bindings ): if not context . engine . binding_is_input ( index = i ): continue tensor_name = context . engine . get_binding_name ( i ) assert tensor_name in host_inputs , f \"missing input: { tensor_name } \" tensor = host_inputs [ tensor_name ] assert isinstance ( tensor , torch . Tensor ), f \"unexpected tensor class: { type ( tensor ) } \" # warning: small changes in output if int64 is used instead of int32 if tensor . dtype in [ torch . int64 , torch . long ]: tensor = tensor . type ( torch . int32 ) if tensor . device . type != \"cuda\" : tensor = tensor . to ( \"cuda\" ) input_tensors . append ( tensor ) # calculate input shape, bind it, allocate GPU memory for the output output_tensors : List [ torch . Tensor ] = get_output_tensors ( context , input_tensors , input_binding_idxs , output_binding_idxs ) bindings = [ int ( i . data_ptr ()) for i in input_tensors + output_tensors ] assert context . execute_async_v2 ( bindings , torch . cuda . current_stream () . cuda_stream ), \"failure during execution of inference\" torch . cuda . current_stream () . synchronize () # sync all CUDA ops return output_tensors load_engine ( runtime , engine_file_path , profile_index = 0 ) # Load serialized TensorRT engine. Parameters: Name Type Description Default runtime Runtime shared variable required engine_file_path str path to the serialized engine required profile_index int which profile to load, 0 if you have not used multiple profiles 0 Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] A function to perform inference Source code in src/transformer_deploy/backends/trt_utils.py def load_engine ( runtime : Runtime , engine_file_path : str , profile_index : int = 0 ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Load serialized TensorRT engine. :param runtime: shared variable :param engine_file_path: path to the serialized engine :param profile_index: which profile to load, 0 if you have not used multiple profiles :return: A function to perform inference \"\"\" with open ( file = engine_file_path , mode = \"rb\" ) as f : engine : ICudaEngine = runtime . deserialize_cuda_engine ( f . read ()) stream : int = torch . cuda . current_stream () . cuda_stream context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream ) # retrieve input/output IDs input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] def tensorrt_model ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : return infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , )[ 0 ] return tensorrt_model save_engine ( engine , engine_file_path ) # Serialize TensorRT engine to file. Parameters: Name Type Description Default engine ICudaEngine TensorRT engine required engine_file_path str output path required Source code in src/transformer_deploy/backends/trt_utils.py def save_engine ( engine : ICudaEngine , engine_file_path : str ) -> None : \"\"\" Serialize TensorRT engine to file. :param engine: TensorRT engine :param engine_file_path: output path \"\"\" with open ( engine_file_path , \"wb\" ) as f : f . write ( engine . serialize ())","title":"Trt utils"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.TensorRTShape","text":"Store input shapes for TensorRT build. 3 shapes per input tensor are required (as tuple of integers): minimum input shape optimal size used for the benchmarks during building maximum input shape Set input name to None for default shape. Source code in src/transformer_deploy/backends/trt_utils.py @dataclass class TensorRTShape : \"\"\" Store input shapes for TensorRT build. 3 shapes per input tensor are required (as tuple of integers): * minimum input shape * optimal size used for the benchmarks during building * maximum input shape Set input name to None for default shape. \"\"\" min_shape : List [ int ] optimal_shape : List [ int ] max_shape : List [ int ] input_name : Optional [ str ] def check_validity ( self ) -> None : \"\"\" Basic checks of provided shapes \"\"\" assert len ( self . min_shape ) == len ( self . optimal_shape ) == len ( self . max_shape ) assert len ( self . min_shape ) > 0 assert self . min_shape [ 0 ] > 0 and self . optimal_shape [ 0 ] > 0 and self . max_shape [ 0 ] > 0 assert self . input_name is not None def make_copy ( self , input_name : str ) -> \"TensorRTShape\" : \"\"\" Make a copy of the current instance, with a different input name. :param input_name: new input name to use :return: a copy of the current shape with a different name \"\"\" instance_copy = dataclasses . replace ( self ) instance_copy . input_name = input_name return instance_copy def generate_multiple_shapes ( self , input_names : List [ str ]) -> List [ \"TensorRTShape\" ]: \"\"\" Generate multiple shapes when only a single default one is defined. :param input_names: input names used by the model :return: a list of shapes \"\"\" assert self . input_name is None , f \"input name is not None: { self . input_name } \" result = list () for name in input_names : shape = self . make_copy ( input_name = name ) result . append ( shape ) return result","title":"TensorRTShape"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.TensorRTShape.check_validity","text":"Basic checks of provided shapes Source code in src/transformer_deploy/backends/trt_utils.py def check_validity ( self ) -> None : \"\"\" Basic checks of provided shapes \"\"\" assert len ( self . min_shape ) == len ( self . optimal_shape ) == len ( self . max_shape ) assert len ( self . min_shape ) > 0 assert self . min_shape [ 0 ] > 0 and self . optimal_shape [ 0 ] > 0 and self . max_shape [ 0 ] > 0 assert self . input_name is not None","title":"check_validity()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.TensorRTShape.generate_multiple_shapes","text":"Generate multiple shapes when only a single default one is defined. Parameters: Name Type Description Default input_names List[str] input names used by the model required Returns: Type Description List[TensorRTShape] a list of shapes Source code in src/transformer_deploy/backends/trt_utils.py def generate_multiple_shapes ( self , input_names : List [ str ]) -> List [ \"TensorRTShape\" ]: \"\"\" Generate multiple shapes when only a single default one is defined. :param input_names: input names used by the model :return: a list of shapes \"\"\" assert self . input_name is None , f \"input name is not None: { self . input_name } \" result = list () for name in input_names : shape = self . make_copy ( input_name = name ) result . append ( shape ) return result","title":"generate_multiple_shapes()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.TensorRTShape.make_copy","text":"Make a copy of the current instance, with a different input name. Parameters: Name Type Description Default input_name str new input name to use required Returns: Type Description TensorRTShape a copy of the current shape with a different name Source code in src/transformer_deploy/backends/trt_utils.py def make_copy ( self , input_name : str ) -> \"TensorRTShape\" : \"\"\" Make a copy of the current instance, with a different input name. :param input_name: new input name to use :return: a copy of the current shape with a different name \"\"\" instance_copy = dataclasses . replace ( self ) instance_copy . input_name = input_name return instance_copy","title":"make_copy()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.build_engine","text":"Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size doesn't hurt performance and is highly advised. Batch size can provided through different ways: min_shape , optimal_shape , max_shape : for simple case, 3 tuples of int when all input tensors have the same shape input_shapes : a list of TensorRTShape with names if there are several input tensors with different shapes TIP : minimum batch size should be 1 in most cases. Parameters: Name Type Description Default runtime Runtime global variable shared accross inference call / model building required onnx_file_path str path to the ONNX file required logger Logger specific logger to TensorRT required workspace_size int GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. required fp16 bool enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. required int8 bool enable INT-8 quantization, best performance but model should have been quantized. required fp16_fix Callable[[tensorrt.tensorrt.INetworkDefinition], tensorrt.tensorrt.INetworkDefinition] a function to set FP32 precision on some nodes to fix FP16 overflow <function fix_fp16_network at 0x7fe0424540d0> Returns: Type Description ICudaEngine TensorRT engine to use during inference Source code in src/transformer_deploy/backends/trt_utils.py def build_engine ( runtime : Runtime , onnx_file_path : str , logger : Logger , workspace_size : int , fp16 : bool , int8 : bool , fp16_fix : Callable [[ INetworkDefinition ], INetworkDefinition ] = fix_fp16_network , ** kwargs , ) -> ICudaEngine : \"\"\" Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size doesn't hurt performance and is highly advised. Batch size can provided through different ways: * **min_shape**, **optimal_shape**, **max_shape**: for simple case, 3 tuples of int when all input tensors have the same shape * **input_shapes**: a list of TensorRTShape with names if there are several input tensors with different shapes **TIP**: minimum batch size should be 1 in most cases. :param runtime: global variable shared accross inference call / model building :param onnx_file_path: path to the ONNX file :param logger: specific logger to TensorRT :param workspace_size: GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. :param fp16: enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. :param int8: enable INT-8 quantization, best performance but model should have been quantized. :param fp16_fix: a function to set FP32 precision on some nodes to fix FP16 overflow :return: TensorRT engine to use during inference \"\"\" # default input shape if \"min_shape\" in kwargs and \"optimal_shape\" in kwargs and \"max_shape\" in kwargs : default_shape = TensorRTShape ( min_shape = kwargs [ \"min_shape\" ], optimal_shape = kwargs [ \"optimal_shape\" ], max_shape = kwargs [ \"max_shape\" ], input_name = None , ) input_shapes = [ default_shape ] else : assert \"input_shapes\" in kwargs , \"missing input shapes\" input_shapes : List [ TensorRTShape ] = kwargs [ \"input_shapes\" ] with trt . Builder ( logger ) as builder : # type: Builder with builder . create_network ( flags = 1 << int ( trt . NetworkDefinitionCreationFlag . EXPLICIT_BATCH ) ) as network_def : # type: INetworkDefinition with trt . OnnxParser ( network_def , logger ) as parser : # type: OnnxParser # The maximum batch size which can be used at execution time, # and also the batch size for which the ICudaEngine will be optimized. builder . max_batch_size = max ([ s . max_shape [ 0 ] for s in input_shapes ]) config : IBuilderConfig = builder . create_builder_config () config . max_workspace_size = workspace_size # to enable complete trt inspector debugging, only for TensorRT >= 8.2 # config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED # disable CUDNN optimizations config . set_tactic_sources ( tactic_sources = 1 << int ( trt . TacticSource . CUBLAS ) | 1 << int ( trt . TacticSource . CUBLAS_LT ) ) if int8 : config . set_flag ( trt . BuilderFlag . INT8 ) if fp16 : config . set_flag ( trt . BuilderFlag . FP16 ) config . set_flag ( trt . BuilderFlag . DISABLE_TIMING_CACHE ) # https://github.com/NVIDIA/TensorRT/issues/1196 (sometimes big diff in output when using FP16) config . set_flag ( trt . BuilderFlag . OBEY_PRECISION_CONSTRAINTS ) with open ( onnx_file_path , \"rb\" ) as f : parser . parse ( f . read ()) profile : IOptimizationProfile = builder . create_optimization_profile () # duplicate default shape (one for each input) if len ( input_shapes ) == 1 and input_shapes [ 0 ] . input_name is None : names = [ network_def . get_input ( num_input ) . name for num_input in range ( network_def . num_inputs )] input_shapes = input_shapes [ 0 ] . generate_multiple_shapes ( input_names = names ) for shape in input_shapes : shape . check_validity () profile . set_shape ( input = shape . input_name , min = shape . min_shape , opt = shape . optimal_shape , max = shape . max_shape , ) if \"shape_tensors\" in kwargs : for shape in kwargs [ \"shape_tensors\" ]: profile . set_shape_input ( input = shape . input_name , min = shape . min_shape , opt = shape . optimal_shape , max = shape . max_shape , ) config . add_optimization_profile ( profile ) if fp16 : network_def = fp16_fix ( network_def ) trt_engine = builder . build_serialized_network ( network_def , config ) engine : ICudaEngine = runtime . deserialize_cuda_engine ( trt_engine ) assert engine is not None , \"error during engine generation, check error messages above :-(\" return engine","title":"build_engine()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.fix_fp16_network","text":"Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. Parameters: Name Type Description Default network_definition INetworkDefinition graph generated by TensorRT after parsing ONNX file (during the model building) required Returns: Type Description INetworkDefinition patched network definition Source code in src/transformer_deploy/backends/trt_utils.py def fix_fp16_network ( network_definition : INetworkDefinition ) -> INetworkDefinition : \"\"\" Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. :param network_definition: graph generated by TensorRT after parsing ONNX file (during the model building) :return: patched network definition \"\"\" # search for patterns which may overflow in FP16 precision, we force FP32 precisions for those nodes for layer_index in range ( network_definition . num_layers - 1 ): layer : ILayer = network_definition . get_layer ( layer_index ) next_layer : ILayer = network_definition . get_layer ( layer_index + 1 ) # POW operation usually followed by mean reduce if layer . type == trt . LayerType . ELEMENTWISE and next_layer . type == trt . LayerType . REDUCE : # casting to get access to op attribute layer . __class__ = IElementWiseLayer next_layer . __class__ = IReduceLayer if layer . op == trt . ElementWiseOperation . POW : layer . precision = trt . DataType . FLOAT next_layer . precision = trt . DataType . FLOAT layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) next_layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition","title":"fix_fp16_network()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.get_binding_idxs","text":"Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings Parameters: Name Type Description Default engine ICudaEngine TensorRT engine generated during the model building required profile_index int profile to use (several profiles can be set during building) required Returns: Type Description input and output tensor indexes Source code in src/transformer_deploy/backends/trt_utils.py def get_binding_idxs ( engine : trt . ICudaEngine , profile_index : int ): \"\"\" Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings :param engine: TensorRT engine generated during the model building :param profile_index: profile to use (several profiles can be set during building) :return: input and output tensor indexes \"\"\" num_bindings_per_profile = engine . num_bindings // engine . num_optimization_profiles start_binding = profile_index * num_bindings_per_profile end_binding = start_binding + num_bindings_per_profile # Separate input and output binding indices for convenience input_binding_idxs : List [ int ] = [] output_binding_idxs : List [ int ] = [] for binding_index in range ( start_binding , end_binding ): if engine . binding_is_input ( binding_index ): input_binding_idxs . append ( binding_index ) else : output_binding_idxs . append ( binding_index ) return input_binding_idxs , output_binding_idxs","title":"get_binding_idxs()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.get_fix_fp16_network_func","text":"Generate a function for TensorRT engine to set precision of specific nodes to FP32 to keep tensorrt FP16 output close to FP32 nodes. Parameters: Name Type Description Default keep_fp32 List[str] nodes to keep in FP32 required Returns: Type Description Callable[[tensorrt.tensorrt.INetworkDefinition], tensorrt.tensorrt.INetworkDefinition] a function to set node precisions Source code in src/transformer_deploy/backends/trt_utils.py def get_fix_fp16_network_func ( keep_fp32 : List [ str ]) -> Callable [[ INetworkDefinition ], INetworkDefinition ]: \"\"\" Generate a function for TensorRT engine to set precision of specific nodes to FP32 to keep tensorrt FP16 output close to FP32 nodes. :param keep_fp32: nodes to keep in FP32 :return: a function to set node precisions \"\"\" def f ( network_definition : INetworkDefinition ) -> INetworkDefinition : for layer_index in range ( network_definition . num_layers ): layer : ILayer = network_definition . get_layer ( layer_index ) # identity function is mainly used for casting # https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Graph/Layers.html#iidentitylayer # if layer.type == LayerType.IDENTITY: # continue if layer . name in keep_fp32 : layer . precision = trt . DataType . FLOAT assert layer . num_outputs == 1 , f \"unexpected # output: { layer . num_outputs } \" layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition return f","title":"get_fix_fp16_network_func()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.get_output_tensors","text":"Reserve memory in GPU for input and output tensors. Parameters: Name Type Description Default context IExecutionContext TensorRT context shared accross inference steps required host_inputs List[torch.Tensor] input tensor required input_binding_idxs List[int] indexes of each input vector (should be the same than during building) required output_binding_idxs List[int] indexes of each output vector (should be the same than during building) required Returns: Type Description List[torch.Tensor] tensors where output will be stored Source code in src/transformer_deploy/backends/trt_utils.py def get_output_tensors ( context : trt . IExecutionContext , host_inputs : List [ torch . Tensor ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> List [ torch . Tensor ]: \"\"\" Reserve memory in GPU for input and output tensors. :param context: TensorRT context shared accross inference steps :param host_inputs: input tensor :param input_binding_idxs: indexes of each input vector (should be the same than during building) :param output_binding_idxs: indexes of each output vector (should be the same than during building) :return: tensors where output will be stored \"\"\" # explicitly set dynamic input shapes, so dynamic output shapes can be computed internally for host_input , binding_index in zip ( host_inputs , input_binding_idxs ): context . set_binding_shape ( binding_index , tuple ( host_input . shape )) # assert context.all_binding_shapes_specified device_outputs : List [ torch . Tensor ] = [] for binding_index in output_binding_idxs : # TensorRT computes output shape based on input shape provided above output_shape = context . get_binding_shape ( binding_index ) # allocate buffers to hold output results output = torch . empty ( tuple ( output_shape ), device = \"cuda\" ) device_outputs . append ( output ) return device_outputs","title":"get_output_tensors()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.infer_tensorrt","text":"Perform inference with TensorRT. Parameters: Name Type Description Default context IExecutionContext shared variable required host_inputs Dict[str, torch.Tensor] input tensor required input_binding_idxs List[int] input tensor indexes required output_binding_idxs List[int] output tensor indexes required Returns: Type Description List[torch.Tensor] output tensor Source code in src/transformer_deploy/backends/trt_utils.py def infer_tensorrt ( context : IExecutionContext , host_inputs : Dict [ str , torch . Tensor ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> List [ torch . Tensor ]: \"\"\" Perform inference with TensorRT. :param context: shared variable :param host_inputs: input tensor :param input_binding_idxs: input tensor indexes :param output_binding_idxs: output tensor indexes :return: output tensor \"\"\" input_tensors : List [ torch . Tensor ] = list () for i in range ( context . engine . num_bindings ): if not context . engine . binding_is_input ( index = i ): continue tensor_name = context . engine . get_binding_name ( i ) assert tensor_name in host_inputs , f \"missing input: { tensor_name } \" tensor = host_inputs [ tensor_name ] assert isinstance ( tensor , torch . Tensor ), f \"unexpected tensor class: { type ( tensor ) } \" # warning: small changes in output if int64 is used instead of int32 if tensor . dtype in [ torch . int64 , torch . long ]: tensor = tensor . type ( torch . int32 ) if tensor . device . type != \"cuda\" : tensor = tensor . to ( \"cuda\" ) input_tensors . append ( tensor ) # calculate input shape, bind it, allocate GPU memory for the output output_tensors : List [ torch . Tensor ] = get_output_tensors ( context , input_tensors , input_binding_idxs , output_binding_idxs ) bindings = [ int ( i . data_ptr ()) for i in input_tensors + output_tensors ] assert context . execute_async_v2 ( bindings , torch . cuda . current_stream () . cuda_stream ), \"failure during execution of inference\" torch . cuda . current_stream () . synchronize () # sync all CUDA ops return output_tensors","title":"infer_tensorrt()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.load_engine","text":"Load serialized TensorRT engine. Parameters: Name Type Description Default runtime Runtime shared variable required engine_file_path str path to the serialized engine required profile_index int which profile to load, 0 if you have not used multiple profiles 0 Returns: Type Description Callable[[Dict[str, torch.Tensor]], torch.Tensor] A function to perform inference Source code in src/transformer_deploy/backends/trt_utils.py def load_engine ( runtime : Runtime , engine_file_path : str , profile_index : int = 0 ) -> Callable [[ Dict [ str , torch . Tensor ]], torch . Tensor ]: \"\"\" Load serialized TensorRT engine. :param runtime: shared variable :param engine_file_path: path to the serialized engine :param profile_index: which profile to load, 0 if you have not used multiple profiles :return: A function to perform inference \"\"\" with open ( file = engine_file_path , mode = \"rb\" ) as f : engine : ICudaEngine = runtime . deserialize_cuda_engine ( f . read ()) stream : int = torch . cuda . current_stream () . cuda_stream context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream ) # retrieve input/output IDs input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] def tensorrt_model ( inputs : Dict [ str , torch . Tensor ]) -> torch . Tensor : return infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , )[ 0 ] return tensorrt_model","title":"load_engine()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.save_engine","text":"Serialize TensorRT engine to file. Parameters: Name Type Description Default engine ICudaEngine TensorRT engine required engine_file_path str output path required Source code in src/transformer_deploy/backends/trt_utils.py def save_engine ( engine : ICudaEngine , engine_file_path : str ) -> None : \"\"\" Serialize TensorRT engine to file. :param engine: TensorRT engine :param engine_file_path: output path \"\"\" with open ( engine_file_path , \"wb\" ) as f : f . write ( engine . serialize ())","title":"save_engine()"},{"location":"reference/benchmarks/utils/","text":"Shared functions related to benchmarks. compare_outputs ( pytorch_output , engine_output ) # Compare 2 model outputs by computing the mean of absolute value difference between them. Parameters: Name Type Description Default pytorch_output ndarray reference output required engine_output ndarray other engine output required Returns: Type Description float difference between outputs as a single float Source code in src/transformer_deploy/benchmarks/utils.py def compare_outputs ( pytorch_output : np . ndarray , engine_output : np . ndarray ) -> float : \"\"\" Compare 2 model outputs by computing the mean of absolute value difference between them. :param pytorch_output: reference output :param engine_output: other engine output :return: difference between outputs as a single float \"\"\" return np . mean ( np . abs ( pytorch_output - engine_output )) generate_input ( seq_len , batch_size , input_names , device = 'cuda' ) # Generate dummy inputs. Parameters: Name Type Description Default seq_len int number of token per input. required batch_size int first dimension of the tensor required input_names List[str] tensor input names to generate required device str where to store tensors (Pytorch only). One of [cpu, cuda] 'cuda' Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, numpy.ndarray]] a tuple of tensors, Pytorch and numpy Source code in src/transformer_deploy/benchmarks/utils.py def generate_input ( seq_len : int , batch_size : int , input_names : List [ str ], device : str = \"cuda\" ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , np . ndarray ]]: \"\"\" Generate dummy inputs. :param seq_len: number of token per input. :param batch_size: first dimension of the tensor :param input_names: tensor input names to generate :param device: where to store tensors (Pytorch only). One of [cpu, cuda] :return: a tuple of tensors, Pytorch and numpy \"\"\" assert device in [ \"cpu\" , \"cuda\" ] shape = ( batch_size , seq_len ) inputs_pytorch : Dict [ str , torch . Tensor ] = dict () for name in input_names : inputs_pytorch [ name ] = torch . ones ( size = shape , dtype = torch . int32 , device = device ) inputs_onnx : Dict [ str , np . ndarray ] = { k : np . ascontiguousarray ( v . detach () . cpu () . numpy ()) for k , v in inputs_pytorch . items () } return inputs_pytorch , inputs_onnx generate_multiple_inputs ( seq_len , batch_size , input_names , nb_inputs_to_gen , device ) # Generate multiple random inputs. Parameters: Name Type Description Default seq_len int sequence length to generate required batch_size int number of sequences per batch to generate required input_names List[str] tensor input names to generate required nb_inputs_to_gen int number of batches of sequences to generate required device str one of [cpu, cuda] required Returns: Type Description Tuple[List[Dict[str, torch.Tensor]], List[Dict[str, numpy.ndarray]]] generated sequences Source code in src/transformer_deploy/benchmarks/utils.py def generate_multiple_inputs ( seq_len : int , batch_size : int , input_names : List [ str ], nb_inputs_to_gen : int , device : str ) -> Tuple [ List [ Dict [ str , torch . Tensor ]], List [ Dict [ str , np . ndarray ]]]: \"\"\" Generate multiple random inputs. :param seq_len: sequence length to generate :param batch_size: number of sequences per batch to generate :param input_names: tensor input names to generate :param nb_inputs_to_gen: number of batches of sequences to generate :param device: one of [cpu, cuda] :return: generated sequences \"\"\" all_inputs_pytorch : List [ Dict [ str , torch . Tensor ]] = list () all_inputs_onnx : List [ Dict [ str , np . ndarray ]] = list () for _ in range ( nb_inputs_to_gen ): inputs_pytorch , inputs_onnx = generate_input ( seq_len = seq_len , batch_size = batch_size , input_names = input_names , device = device ) all_inputs_pytorch . append ( inputs_pytorch ) all_inputs_onnx . append ( inputs_onnx ) return all_inputs_pytorch , all_inputs_onnx print_timings ( name , timings ) # Format and print inference latencies. Parameters: Name Type Description Default name str inference engine name required timings List[float] latencies measured during the inference required Source code in src/transformer_deploy/benchmarks/utils.py def print_timings ( name : str , timings : List [ float ]) -> None : \"\"\" Format and print inference latencies. :param name: inference engine name :param timings: latencies measured during the inference \"\"\" mean_time = 1e3 * np . mean ( timings ) std_time = 1e3 * np . std ( timings ) min_time = 1e3 * np . min ( timings ) max_time = 1e3 * np . max ( timings ) median , percent_95_time , percent_99_time = 1e3 * np . percentile ( timings , [ 50 , 95 , 99 ]) print ( f \"[ { name } ] \" f \"mean= { mean_time : .2f } ms, \" f \"sd= { std_time : .2f } ms, \" f \"min= { min_time : .2f } ms, \" f \"max= { max_time : .2f } ms, \" f \"median= { median : .2f } ms, \" f \"95p= { percent_95_time : .2f } ms, \" f \"99p= { percent_99_time : .2f } ms\" ) setup_logging ( level = 20 ) # Set the generic Python logger Parameters: Name Type Description Default level int logger level 20 Source code in src/transformer_deploy/benchmarks/utils.py def setup_logging ( level : int = logging . INFO ) -> None : \"\"\" Set the generic Python logger :param level: logger level \"\"\" logging . basicConfig ( format = \" %(asctime)s %(levelname)-8s %(message)s \" , datefmt = \"%m/ %d /%Y %H:%M:%S\" , level = level ) to_numpy ( tensors ) # Convert list of torch / numpy tensors to a numpy tensor Parameters: Name Type Description Default tensors List[Union[numpy.ndarray, torch.Tensor]] list of torch / numpy tensors required Returns: Type Description ndarray numpy tensor Source code in src/transformer_deploy/benchmarks/utils.py def to_numpy ( tensors : List [ Union [ np . ndarray , torch . Tensor ]]) -> np . ndarray : \"\"\" Convert list of torch / numpy tensors to a numpy tensor :param tensors: list of torch / numpy tensors :return: numpy tensor \"\"\" if isinstance ( tensors [ 0 ], torch . Tensor ): pytorch_output = [ t . detach () . cpu () . numpy () for t in tensors ] elif isinstance ( tensors [ 0 ], np . ndarray ): pytorch_output = tensors else : raise Exception ( f \"unknown tensor type: { type ( tensors [ 0 ]) } \" ) return np . asarray ( pytorch_output ) track_infer_time ( buffer ) # A context manager to perform latency measures Parameters: Name Type Description Default buffer List[int] a List where to save latencies for each input required Source code in src/transformer_deploy/benchmarks/utils.py @contextmanager def track_infer_time ( buffer : List [ int ]) -> None : \"\"\" A context manager to perform latency measures :param buffer: a List where to save latencies for each input \"\"\" start = time . perf_counter () yield end = time . perf_counter () buffer . append ( end - start )","title":"Utils"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.compare_outputs","text":"Compare 2 model outputs by computing the mean of absolute value difference between them. Parameters: Name Type Description Default pytorch_output ndarray reference output required engine_output ndarray other engine output required Returns: Type Description float difference between outputs as a single float Source code in src/transformer_deploy/benchmarks/utils.py def compare_outputs ( pytorch_output : np . ndarray , engine_output : np . ndarray ) -> float : \"\"\" Compare 2 model outputs by computing the mean of absolute value difference between them. :param pytorch_output: reference output :param engine_output: other engine output :return: difference between outputs as a single float \"\"\" return np . mean ( np . abs ( pytorch_output - engine_output ))","title":"compare_outputs()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.generate_input","text":"Generate dummy inputs. Parameters: Name Type Description Default seq_len int number of token per input. required batch_size int first dimension of the tensor required input_names List[str] tensor input names to generate required device str where to store tensors (Pytorch only). One of [cpu, cuda] 'cuda' Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, numpy.ndarray]] a tuple of tensors, Pytorch and numpy Source code in src/transformer_deploy/benchmarks/utils.py def generate_input ( seq_len : int , batch_size : int , input_names : List [ str ], device : str = \"cuda\" ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , np . ndarray ]]: \"\"\" Generate dummy inputs. :param seq_len: number of token per input. :param batch_size: first dimension of the tensor :param input_names: tensor input names to generate :param device: where to store tensors (Pytorch only). One of [cpu, cuda] :return: a tuple of tensors, Pytorch and numpy \"\"\" assert device in [ \"cpu\" , \"cuda\" ] shape = ( batch_size , seq_len ) inputs_pytorch : Dict [ str , torch . Tensor ] = dict () for name in input_names : inputs_pytorch [ name ] = torch . ones ( size = shape , dtype = torch . int32 , device = device ) inputs_onnx : Dict [ str , np . ndarray ] = { k : np . ascontiguousarray ( v . detach () . cpu () . numpy ()) for k , v in inputs_pytorch . items () } return inputs_pytorch , inputs_onnx","title":"generate_input()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.generate_multiple_inputs","text":"Generate multiple random inputs. Parameters: Name Type Description Default seq_len int sequence length to generate required batch_size int number of sequences per batch to generate required input_names List[str] tensor input names to generate required nb_inputs_to_gen int number of batches of sequences to generate required device str one of [cpu, cuda] required Returns: Type Description Tuple[List[Dict[str, torch.Tensor]], List[Dict[str, numpy.ndarray]]] generated sequences Source code in src/transformer_deploy/benchmarks/utils.py def generate_multiple_inputs ( seq_len : int , batch_size : int , input_names : List [ str ], nb_inputs_to_gen : int , device : str ) -> Tuple [ List [ Dict [ str , torch . Tensor ]], List [ Dict [ str , np . ndarray ]]]: \"\"\" Generate multiple random inputs. :param seq_len: sequence length to generate :param batch_size: number of sequences per batch to generate :param input_names: tensor input names to generate :param nb_inputs_to_gen: number of batches of sequences to generate :param device: one of [cpu, cuda] :return: generated sequences \"\"\" all_inputs_pytorch : List [ Dict [ str , torch . Tensor ]] = list () all_inputs_onnx : List [ Dict [ str , np . ndarray ]] = list () for _ in range ( nb_inputs_to_gen ): inputs_pytorch , inputs_onnx = generate_input ( seq_len = seq_len , batch_size = batch_size , input_names = input_names , device = device ) all_inputs_pytorch . append ( inputs_pytorch ) all_inputs_onnx . append ( inputs_onnx ) return all_inputs_pytorch , all_inputs_onnx","title":"generate_multiple_inputs()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.print_timings","text":"Format and print inference latencies. Parameters: Name Type Description Default name str inference engine name required timings List[float] latencies measured during the inference required Source code in src/transformer_deploy/benchmarks/utils.py def print_timings ( name : str , timings : List [ float ]) -> None : \"\"\" Format and print inference latencies. :param name: inference engine name :param timings: latencies measured during the inference \"\"\" mean_time = 1e3 * np . mean ( timings ) std_time = 1e3 * np . std ( timings ) min_time = 1e3 * np . min ( timings ) max_time = 1e3 * np . max ( timings ) median , percent_95_time , percent_99_time = 1e3 * np . percentile ( timings , [ 50 , 95 , 99 ]) print ( f \"[ { name } ] \" f \"mean= { mean_time : .2f } ms, \" f \"sd= { std_time : .2f } ms, \" f \"min= { min_time : .2f } ms, \" f \"max= { max_time : .2f } ms, \" f \"median= { median : .2f } ms, \" f \"95p= { percent_95_time : .2f } ms, \" f \"99p= { percent_99_time : .2f } ms\" )","title":"print_timings()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.setup_logging","text":"Set the generic Python logger Parameters: Name Type Description Default level int logger level 20 Source code in src/transformer_deploy/benchmarks/utils.py def setup_logging ( level : int = logging . INFO ) -> None : \"\"\" Set the generic Python logger :param level: logger level \"\"\" logging . basicConfig ( format = \" %(asctime)s %(levelname)-8s %(message)s \" , datefmt = \"%m/ %d /%Y %H:%M:%S\" , level = level )","title":"setup_logging()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.to_numpy","text":"Convert list of torch / numpy tensors to a numpy tensor Parameters: Name Type Description Default tensors List[Union[numpy.ndarray, torch.Tensor]] list of torch / numpy tensors required Returns: Type Description ndarray numpy tensor Source code in src/transformer_deploy/benchmarks/utils.py def to_numpy ( tensors : List [ Union [ np . ndarray , torch . Tensor ]]) -> np . ndarray : \"\"\" Convert list of torch / numpy tensors to a numpy tensor :param tensors: list of torch / numpy tensors :return: numpy tensor \"\"\" if isinstance ( tensors [ 0 ], torch . Tensor ): pytorch_output = [ t . detach () . cpu () . numpy () for t in tensors ] elif isinstance ( tensors [ 0 ], np . ndarray ): pytorch_output = tensors else : raise Exception ( f \"unknown tensor type: { type ( tensors [ 0 ]) } \" ) return np . asarray ( pytorch_output )","title":"to_numpy()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.track_infer_time","text":"A context manager to perform latency measures Parameters: Name Type Description Default buffer List[int] a List where to save latencies for each input required Source code in src/transformer_deploy/benchmarks/utils.py @contextmanager def track_infer_time ( buffer : List [ int ]) -> None : \"\"\" A context manager to perform latency measures :param buffer: a List where to save latencies for each input \"\"\" start = time . perf_counter () yield end = time . perf_counter () buffer . append ( end - start )","title":"track_infer_time()"},{"location":"reference/triton/configuration/","text":"Generate Nvidia Triton server configuration files. Configuration ( ABC ) # Source code in src/transformer_deploy/triton/configuration.py class Configuration ( ABC ): engine_type : Optional [ EngineType ] python_code : Optional [ str ] def __init__ ( self , working_directory : str , model_name_base : str , dim_output : List [ int ], nb_instance : int , tensor_input_names : List [ str ], device : str , ): \"\"\" Configuration file setup. :param working_directory: path to the working directory :param model_name_base: model name to use (used to call TensorRT API) :param dim_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param tensor_input_names: input names expected by the model :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name_base = model_name_base self . dim_output = dim_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . tensor_input_names = tensor_input_names self . working_dir : Path = Path ( working_directory ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" @property def python_folder_name ( self ) -> str : raise Exception ( \"to implement\" ) def _get_tokens ( self ) -> str : \"\"\" Generate input tensor configuration :return: input tensor configuration string \"\"\" result : List [ str ] = list () for input_name in self . tensor_input_names : text = f \"\"\" {{ name: \" { input_name } \" data_type: TYPE_INT32 dims: [-1, -1] }} \"\"\" . strip () result . append ( text ) return \", \\n \" . join ( result ) @property def model_name ( self ) -> str : assert self . engine_type is not None return self . model_name_base + ( \"_onnx\" if self . engine_type == EngineType . ONNX else \"_tensorrt\" ) @property def model_folder_name ( self ) -> str : return f \" { self . model_name } _model\" @property def inference_folder_name ( self ) -> str : return f \" { self . model_name } _inference\" @property def inference_platform ( self ) -> str : if self . engine_type == EngineType . ONNX : return \"onnxruntime_onnx\" elif self . engine_type == EngineType . TensorRT : return \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { self . engine_type } \" ) def _instance_group ( self ) -> str : \"\"\" Generate instance configuration. :return: instance configuration \"\"\" return f \"\"\" instance_group [ {{ count: { self . nb_instance } kind: { self . device_kind } }} ] \"\"\" . strip () @staticmethod def _get_header ( name : str , platform : Optional [ str ] = None , backend : Optional [ str ] = None ): assert platform is not None or backend is not None text = f \"\"\" name: \" { name } \" max_batch_size: 0 \"\"\" . strip () if platform is not None : text += f ' \\n platform: \" { platform } \"' if backend is not None : text += f ' \\n backend: \" { backend } \"' return text def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: 0 platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . _get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} { self . _instance_group () } \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : \"\"\" Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. :param tokenizer: tokenizer to use :param config: tranformer model config to use :param model_path: main folder where to save configurations and artefacts :param engine_type: type of inference engine (ONNX or TensorRT) \"\"\" self . engine_type = engine_type target = self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"1\" ) target . mkdir ( parents = True , exist_ok = True ) target . joinpath ( \"model.py\" ) . write_text ( self . python_code ) tokenizer . save_pretrained ( str ( target . absolute ())) config . save_pretrained ( str ( target . absolute ())) model_folder_path = self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) model_folder_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" )) __init__ ( self , working_directory , model_name_base , dim_output , nb_instance , tensor_input_names , device ) special # Configuration file setup. Parameters: Name Type Description Default working_directory str path to the working directory required model_name_base str model name to use (used to call TensorRT API) required dim_output List[int] number of tensor outputs required nb_instance int number of parallel instances to use. Mainly useful to optimize CPU inference. required tensor_input_names List[str] input names expected by the model required device str where perform is done. One of [cpu, cuda] required Source code in src/transformer_deploy/triton/configuration.py def __init__ ( self , working_directory : str , model_name_base : str , dim_output : List [ int ], nb_instance : int , tensor_input_names : List [ str ], device : str , ): \"\"\" Configuration file setup. :param working_directory: path to the working directory :param model_name_base: model name to use (used to call TensorRT API) :param dim_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param tensor_input_names: input names expected by the model :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name_base = model_name_base self . dim_output = dim_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . tensor_input_names = tensor_input_names self . working_dir : Path = Path ( working_directory ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" create_configs ( self , tokenizer , config , model_path , engine_type ) # Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : \"\"\" Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. :param tokenizer: tokenizer to use :param config: tranformer model config to use :param model_path: main folder where to save configurations and artefacts :param engine_type: type of inference engine (ONNX or TensorRT) \"\"\" self . engine_type = engine_type target = self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"1\" ) target . mkdir ( parents = True , exist_ok = True ) target . joinpath ( \"model.py\" ) . write_text ( self . python_code ) tokenizer . save_pretrained ( str ( target . absolute ())) config . save_pretrained ( str ( target . absolute ())) model_folder_path = self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) model_folder_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" )) get_model_conf ( self ) # Generate model configuration. Returns: Type Description str model configuration Source code in src/transformer_deploy/triton/configuration.py def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: 0 platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . _get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} { self . _instance_group () } \"\"\" . strip () EngineType ( Enum ) # Type of model to use Source code in src/transformer_deploy/triton/configuration.py class EngineType ( Enum ): \"\"\" Type of model to use \"\"\" ONNX = 1 TensorRT = 2","title":"Configuration"},{"location":"reference/triton/configuration/#src.transformer_deploy.triton.configuration.Configuration","text":"Source code in src/transformer_deploy/triton/configuration.py class Configuration ( ABC ): engine_type : Optional [ EngineType ] python_code : Optional [ str ] def __init__ ( self , working_directory : str , model_name_base : str , dim_output : List [ int ], nb_instance : int , tensor_input_names : List [ str ], device : str , ): \"\"\" Configuration file setup. :param working_directory: path to the working directory :param model_name_base: model name to use (used to call TensorRT API) :param dim_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param tensor_input_names: input names expected by the model :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name_base = model_name_base self . dim_output = dim_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . tensor_input_names = tensor_input_names self . working_dir : Path = Path ( working_directory ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" @property def python_folder_name ( self ) -> str : raise Exception ( \"to implement\" ) def _get_tokens ( self ) -> str : \"\"\" Generate input tensor configuration :return: input tensor configuration string \"\"\" result : List [ str ] = list () for input_name in self . tensor_input_names : text = f \"\"\" {{ name: \" { input_name } \" data_type: TYPE_INT32 dims: [-1, -1] }} \"\"\" . strip () result . append ( text ) return \", \\n \" . join ( result ) @property def model_name ( self ) -> str : assert self . engine_type is not None return self . model_name_base + ( \"_onnx\" if self . engine_type == EngineType . ONNX else \"_tensorrt\" ) @property def model_folder_name ( self ) -> str : return f \" { self . model_name } _model\" @property def inference_folder_name ( self ) -> str : return f \" { self . model_name } _inference\" @property def inference_platform ( self ) -> str : if self . engine_type == EngineType . ONNX : return \"onnxruntime_onnx\" elif self . engine_type == EngineType . TensorRT : return \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { self . engine_type } \" ) def _instance_group ( self ) -> str : \"\"\" Generate instance configuration. :return: instance configuration \"\"\" return f \"\"\" instance_group [ {{ count: { self . nb_instance } kind: { self . device_kind } }} ] \"\"\" . strip () @staticmethod def _get_header ( name : str , platform : Optional [ str ] = None , backend : Optional [ str ] = None ): assert platform is not None or backend is not None text = f \"\"\" name: \" { name } \" max_batch_size: 0 \"\"\" . strip () if platform is not None : text += f ' \\n platform: \" { platform } \"' if backend is not None : text += f ' \\n backend: \" { backend } \"' return text def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: 0 platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . _get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} { self . _instance_group () } \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : \"\"\" Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. :param tokenizer: tokenizer to use :param config: tranformer model config to use :param model_path: main folder where to save configurations and artefacts :param engine_type: type of inference engine (ONNX or TensorRT) \"\"\" self . engine_type = engine_type target = self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"1\" ) target . mkdir ( parents = True , exist_ok = True ) target . joinpath ( \"model.py\" ) . write_text ( self . python_code ) tokenizer . save_pretrained ( str ( target . absolute ())) config . save_pretrained ( str ( target . absolute ())) model_folder_path = self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) model_folder_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" ))","title":"Configuration"},{"location":"reference/triton/configuration/#src.transformer_deploy.triton.configuration.Configuration.__init__","text":"Configuration file setup. Parameters: Name Type Description Default working_directory str path to the working directory required model_name_base str model name to use (used to call TensorRT API) required dim_output List[int] number of tensor outputs required nb_instance int number of parallel instances to use. Mainly useful to optimize CPU inference. required tensor_input_names List[str] input names expected by the model required device str where perform is done. One of [cpu, cuda] required Source code in src/transformer_deploy/triton/configuration.py def __init__ ( self , working_directory : str , model_name_base : str , dim_output : List [ int ], nb_instance : int , tensor_input_names : List [ str ], device : str , ): \"\"\" Configuration file setup. :param working_directory: path to the working directory :param model_name_base: model name to use (used to call TensorRT API) :param dim_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param tensor_input_names: input names expected by the model :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name_base = model_name_base self . dim_output = dim_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . tensor_input_names = tensor_input_names self . working_dir : Path = Path ( working_directory ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\"","title":"__init__()"},{"location":"reference/triton/configuration/#src.transformer_deploy.triton.configuration.Configuration.create_configs","text":"Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : \"\"\" Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. :param tokenizer: tokenizer to use :param config: tranformer model config to use :param model_path: main folder where to save configurations and artefacts :param engine_type: type of inference engine (ONNX or TensorRT) \"\"\" self . engine_type = engine_type target = self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"1\" ) target . mkdir ( parents = True , exist_ok = True ) target . joinpath ( \"model.py\" ) . write_text ( self . python_code ) tokenizer . save_pretrained ( str ( target . absolute ())) config . save_pretrained ( str ( target . absolute ())) model_folder_path = self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) model_folder_path . mkdir ( parents = True , exist_ok = True ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" ))","title":"create_configs()"},{"location":"reference/triton/configuration/#src.transformer_deploy.triton.configuration.Configuration.get_model_conf","text":"Generate model configuration. Returns: Type Description str model configuration Source code in src/transformer_deploy/triton/configuration.py def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: 0 platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . _get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} { self . _instance_group () } \"\"\" . strip ()","title":"get_model_conf()"},{"location":"reference/triton/configuration/#src.transformer_deploy.triton.configuration.EngineType","text":"Type of model to use Source code in src/transformer_deploy/triton/configuration.py class EngineType ( Enum ): \"\"\" Type of model to use \"\"\" ONNX = 1 TensorRT = 2","title":"EngineType"},{"location":"reference/triton/configuration_decoder/","text":"Generate Nvidia Triton server configuration files for decoder based model (GPT-2). ConfigurationDec ( Configuration ) # Source code in src/transformer_deploy/triton/configuration_decoder.py class ConfigurationDec ( Configuration ): @property def python_code ( self ): return inspect . getsource ( generative_model ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _generate\" def get_generation_conf ( self ) -> str : \"\"\" Generate sequence configuration. :return: Generate sequence configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_generation_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) create_configs ( self , tokenizer , config , model_path , engine_type ) # Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_decoder.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_generation_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) get_generation_conf ( self ) # Generate sequence configuration. Returns: Type Description str Generate sequence configuration Source code in src/transformer_deploy/triton/configuration_decoder.py def get_generation_conf ( self ) -> str : \"\"\" Generate sequence configuration. :return: Generate sequence configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip ()","title":"Configuration decoder"},{"location":"reference/triton/configuration_decoder/#src.transformer_deploy.triton.configuration_decoder.ConfigurationDec","text":"Source code in src/transformer_deploy/triton/configuration_decoder.py class ConfigurationDec ( Configuration ): @property def python_code ( self ): return inspect . getsource ( generative_model ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _generate\" def get_generation_conf ( self ) -> str : \"\"\" Generate sequence configuration. :return: Generate sequence configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_generation_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"ConfigurationDec"},{"location":"reference/triton/configuration_decoder/#src.transformer_deploy.triton.configuration_decoder.ConfigurationDec.create_configs","text":"Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_decoder.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_generation_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"create_configs()"},{"location":"reference/triton/configuration_decoder/#src.transformer_deploy.triton.configuration_decoder.ConfigurationDec.get_generation_conf","text":"Generate sequence configuration. Returns: Type Description str Generate sequence configuration Source code in src/transformer_deploy/triton/configuration_decoder.py def get_generation_conf ( self ) -> str : \"\"\" Generate sequence configuration. :return: Generate sequence configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip ()","title":"get_generation_conf()"},{"location":"reference/triton/configuration_encoder/","text":"Generate Nvidia Triton server configuration files for encoder based models (Bert, Roberta, Electra, etc.). ConfigurationEnc ( Configuration ) # Source code in src/transformer_deploy/triton/configuration_encoder.py class ConfigurationEnc ( Configuration ): @property def python_code ( self ): return inspect . getsource ( python_tokenizer ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _tokenize\" def get_tokenize_conf ( self ) -> str : \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . _get_tokens () } ] { self . _instance_group () } \"\"\" . strip () def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" output_map_blocks = list () for input_name in self . tensor_input_names : output_map_text = f \"\"\" {{ key: \" { input_name } \" value: \" { input_name } \" }} \"\"\" . strip () output_map_blocks . append ( output_map_text ) mapping_keys = \", \\n \" . join ( output_map_blocks ) return f \"\"\" { self . _get_header ( name = self . inference_folder_name , platform = \"ensemble\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} ensemble_scheduling {{ step [ {{ model_name: \" { self . python_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ { mapping_keys } ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ { mapping_keys } ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) for path , conf_content in [ ( self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_tokenize_conf ()), ( self . working_dir . joinpath ( self . inference_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) create_configs ( self , tokenizer , config , model_path , engine_type ) # Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_encoder.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) for path , conf_content in [ ( self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_tokenize_conf ()), ( self . working_dir . joinpath ( self . inference_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) get_inference_conf ( self ) # Generate inference step configuration. Returns: Type Description str inference step configuration Source code in src/transformer_deploy/triton/configuration_encoder.py def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" output_map_blocks = list () for input_name in self . tensor_input_names : output_map_text = f \"\"\" {{ key: \" { input_name } \" value: \" { input_name } \" }} \"\"\" . strip () output_map_blocks . append ( output_map_text ) mapping_keys = \", \\n \" . join ( output_map_blocks ) return f \"\"\" { self . _get_header ( name = self . inference_folder_name , platform = \"ensemble\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} ensemble_scheduling {{ step [ {{ model_name: \" { self . python_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ { mapping_keys } ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ { mapping_keys } ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () get_tokenize_conf ( self ) # Generate tokenization step configuration. Returns: Type Description str tokenization step configuration Source code in src/transformer_deploy/triton/configuration_encoder.py def get_tokenize_conf ( self ) -> str : \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . _get_tokens () } ] { self . _instance_group () } \"\"\" . strip ()","title":"Configuration encoder"},{"location":"reference/triton/configuration_encoder/#src.transformer_deploy.triton.configuration_encoder.ConfigurationEnc","text":"Source code in src/transformer_deploy/triton/configuration_encoder.py class ConfigurationEnc ( Configuration ): @property def python_code ( self ): return inspect . getsource ( python_tokenizer ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _tokenize\" def get_tokenize_conf ( self ) -> str : \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . _get_tokens () } ] { self . _instance_group () } \"\"\" . strip () def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" output_map_blocks = list () for input_name in self . tensor_input_names : output_map_text = f \"\"\" {{ key: \" { input_name } \" value: \" { input_name } \" }} \"\"\" . strip () output_map_blocks . append ( output_map_text ) mapping_keys = \", \\n \" . join ( output_map_blocks ) return f \"\"\" { self . _get_header ( name = self . inference_folder_name , platform = \"ensemble\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} ensemble_scheduling {{ step [ {{ model_name: \" { self . python_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ { mapping_keys } ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ { mapping_keys } ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) for path , conf_content in [ ( self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_tokenize_conf ()), ( self . working_dir . joinpath ( self . inference_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"ConfigurationEnc"},{"location":"reference/triton/configuration_encoder/#src.transformer_deploy.triton.configuration_encoder.ConfigurationEnc.create_configs","text":"Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_encoder.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type ) for path , conf_content in [ ( self . working_dir . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf ()), ( self . working_dir . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_tokenize_conf ()), ( self . working_dir . joinpath ( self . inference_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf ()), ]: # type: Path, str path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"create_configs()"},{"location":"reference/triton/configuration_encoder/#src.transformer_deploy.triton.configuration_encoder.ConfigurationEnc.get_inference_conf","text":"Generate inference step configuration. Returns: Type Description str inference step configuration Source code in src/transformer_deploy/triton/configuration_encoder.py def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" output_map_blocks = list () for input_name in self . tensor_input_names : output_map_text = f \"\"\" {{ key: \" { input_name } \" value: \" { input_name } \" }} \"\"\" . strip () output_map_blocks . append ( output_map_text ) mapping_keys = \", \\n \" . join ( output_map_blocks ) return f \"\"\" { self . _get_header ( name = self . inference_folder_name , platform = \"ensemble\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: { str ( self . dim_output ) } }} ensemble_scheduling {{ step [ {{ model_name: \" { self . python_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ { mapping_keys } ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ { mapping_keys } ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip ()","title":"get_inference_conf()"},{"location":"reference/triton/configuration_encoder/#src.transformer_deploy.triton.configuration_encoder.ConfigurationEnc.get_tokenize_conf","text":"Generate tokenization step configuration. Returns: Type Description str tokenization step configuration Source code in src/transformer_deploy/triton/configuration_encoder.py def get_tokenize_conf ( self ) -> str : \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . _get_tokens () } ] { self . _instance_group () } \"\"\" . strip ()","title":"get_tokenize_conf()"},{"location":"reference/triton/configuration_token_classifier/","text":"Generate Nvidia Triton server configuration files for encoder based models (Bert, Roberta, Electra, etc.). ConfigurationTokenClassifier ( Configuration ) # Source code in src/transformer_deploy/triton/configuration_token_classifier.py class ConfigurationTokenClassifier ( Configuration ): @property def python_code ( self ): return inspect . getsource ( token_classifier ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _inference\" def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType , ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type , ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf (), ), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf (), ), ]: path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) create_configs ( self , tokenizer , config , model_path , engine_type ) # Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_token_classifier.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType , ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type , ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf (), ), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf (), ), ]: path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content ) get_inference_conf ( self ) # Generate inference step configuration. Returns: Type Description str inference step configuration Source code in src/transformer_deploy/triton/configuration_token_classifier.py def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip ()","title":"Configuration token classifier"},{"location":"reference/triton/configuration_token_classifier/#src.transformer_deploy.triton.configuration_token_classifier.ConfigurationTokenClassifier","text":"Source code in src/transformer_deploy/triton/configuration_token_classifier.py class ConfigurationTokenClassifier ( Configuration ): @property def python_code ( self ): return inspect . getsource ( token_classifier ) @property def python_folder_name ( self ) -> str : return f \" { self . model_name } _inference\" def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip () def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType , ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type , ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf (), ), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf (), ), ]: path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"ConfigurationTokenClassifier"},{"location":"reference/triton/configuration_token_classifier/#src.transformer_deploy.triton.configuration_token_classifier.ConfigurationTokenClassifier.create_configs","text":"Create Triton configuration folder layout, generate configuration files, generate/move artefacts, etc. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required config PretrainedConfig tranformer model config to use required model_path str main folder where to save configurations and artefacts required engine_type EngineType type of inference engine (ONNX or TensorRT) required Source code in src/transformer_deploy/triton/configuration_token_classifier.py def create_configs ( self , tokenizer : PreTrainedTokenizer , config : PretrainedConfig , model_path : str , engine_type : EngineType , ) -> None : super () . create_configs ( tokenizer = tokenizer , config = config , model_path = model_path , engine_type = engine_type , ) wd_path = Path ( self . working_dir ) for path , conf_content in [ ( wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_model_conf (), ), ( wd_path . joinpath ( self . python_folder_name ) . joinpath ( \"config.pbtxt\" ), self . get_inference_conf (), ), ]: path . parent . mkdir ( parents = True , exist_ok = True ) path . parent . joinpath ( \"1\" ) . mkdir ( exist_ok = True ) path . write_text ( conf_content )","title":"create_configs()"},{"location":"reference/triton/configuration_token_classifier/#src.transformer_deploy.triton.configuration_token_classifier.ConfigurationTokenClassifier.get_inference_conf","text":"Generate inference step configuration. Returns: Type Description str inference step configuration Source code in src/transformer_deploy/triton/configuration_token_classifier.py def get_inference_conf ( self ) -> str : \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" return f \"\"\" { self . _get_header ( name = self . python_folder_name , backend = \"python\" ) } input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ {{ name: \"output\" data_type: TYPE_STRING dims: [ -1 ] }} ] { self . _instance_group () } parameters: {{ key: \"FORCE_CPU_ONLY_INPUT_TENSORS\" value: {{ string_value:\"no\" }} }} \"\"\" . strip ()","title":"get_inference_conf()"},{"location":"reference/utils/args/","text":"Command line args parser parse_args ( commands = None ) # Parse command line arguments Parameters: Name Type Description Default commands List[str] to provide command line programatically None Returns: Type Description Namespace parsed command line Source code in src/transformer_deploy/utils/args.py def parse_args ( commands : List [ str ] = None ) -> argparse . Namespace : \"\"\" Parse command line arguments :param commands: to provide command line programatically :return: parsed command line \"\"\" parser = argparse . ArgumentParser ( description = \"optimize and deploy transformers\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( \"-m\" , \"--model\" , required = True , help = \"path to model or URL to Hugging Face hub\" ) parser . add_argument ( \"-t\" , \"--tokenizer\" , help = \"path to tokenizer or URL to Hugging Face hub\" ) parser . add_argument ( \"--task\" , default = \"classification\" , choices = [ \"classification\" , \"embedding\" , \"text-generation\" , \"token-classification\" ], help = \"task to manage. embeddings is for sentence-transformers models\" , ) parser . add_argument ( \"--auth-token\" , default = None , help = ( \"Hugging Face Hub auth token. Set to `None` (default) for public models. \" \"For private models, use `True` to use local cached token, or a string of your HF API token\" ), ) parser . add_argument ( \"-b\" , \"--batch-size\" , default = [ 1 , 1 , 1 ], help = \"batch sizes to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-s\" , \"--seq-len\" , default = [ 16 , 16 , 16 ], help = \"sequence lengths to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-q\" , \"--quantization\" , action = \"store_true\" , help = \"INT-8 GPU quantization support\" ) parser . add_argument ( \"-w\" , \"--workspace-size\" , default = 10000 , help = \"workspace size in MiB (TensorRT)\" , type = int ) parser . add_argument ( \"-o\" , \"--output\" , default = \"triton_models\" , help = \"name to be used for \" ) parser . add_argument ( \"-n\" , \"--name\" , default = \"transformer\" , help = \"model name to be used in triton server\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"display detailed information\" ) parser . add_argument ( \"--backend\" , default = [ \"onnx\" ], help = \"backend to use. multiple args accepted.\" , nargs = \"*\" , choices = [ \"onnx\" , \"tensorrt\" ], ) parser . add_argument ( \"-d\" , \"--device\" , default = None , help = \"device to use. If not set, will be cuda if available.\" , choices = [ \"cpu\" , \"cuda\" ], ) parser . add_argument ( \"--nb-threads\" , default = 1 , help = \"# of CPU threads to use for inference\" , type = int ) parser . add_argument ( \"--nb-instances\" , default = 1 , help = \"# of model instances, may improve throughput (Triton)\" , type = int ) parser . add_argument ( \"--warmup\" , default = 10 , help = \"# of inferences to warm each model\" , type = int ) parser . add_argument ( \"--nb-measures\" , default = 1000 , help = \"# of inferences for benchmarks\" , type = int ) parser . add_argument ( \"--seed\" , default = 123 , help = \"seed for random inputs, etc.\" , type = int ) parser . add_argument ( \"--atol\" , default = 3e-1 , help = \"tolerance when comparing outputs to Pytorch ones\" , type = float ) args , _ = parser . parse_known_args ( args = commands ) return args","title":"Args"},{"location":"reference/utils/args/#src.transformer_deploy.utils.args.parse_args","text":"Parse command line arguments Parameters: Name Type Description Default commands List[str] to provide command line programatically None Returns: Type Description Namespace parsed command line Source code in src/transformer_deploy/utils/args.py def parse_args ( commands : List [ str ] = None ) -> argparse . Namespace : \"\"\" Parse command line arguments :param commands: to provide command line programatically :return: parsed command line \"\"\" parser = argparse . ArgumentParser ( description = \"optimize and deploy transformers\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( \"-m\" , \"--model\" , required = True , help = \"path to model or URL to Hugging Face hub\" ) parser . add_argument ( \"-t\" , \"--tokenizer\" , help = \"path to tokenizer or URL to Hugging Face hub\" ) parser . add_argument ( \"--task\" , default = \"classification\" , choices = [ \"classification\" , \"embedding\" , \"text-generation\" , \"token-classification\" ], help = \"task to manage. embeddings is for sentence-transformers models\" , ) parser . add_argument ( \"--auth-token\" , default = None , help = ( \"Hugging Face Hub auth token. Set to `None` (default) for public models. \" \"For private models, use `True` to use local cached token, or a string of your HF API token\" ), ) parser . add_argument ( \"-b\" , \"--batch-size\" , default = [ 1 , 1 , 1 ], help = \"batch sizes to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-s\" , \"--seq-len\" , default = [ 16 , 16 , 16 ], help = \"sequence lengths to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-q\" , \"--quantization\" , action = \"store_true\" , help = \"INT-8 GPU quantization support\" ) parser . add_argument ( \"-w\" , \"--workspace-size\" , default = 10000 , help = \"workspace size in MiB (TensorRT)\" , type = int ) parser . add_argument ( \"-o\" , \"--output\" , default = \"triton_models\" , help = \"name to be used for \" ) parser . add_argument ( \"-n\" , \"--name\" , default = \"transformer\" , help = \"model name to be used in triton server\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"display detailed information\" ) parser . add_argument ( \"--backend\" , default = [ \"onnx\" ], help = \"backend to use. multiple args accepted.\" , nargs = \"*\" , choices = [ \"onnx\" , \"tensorrt\" ], ) parser . add_argument ( \"-d\" , \"--device\" , default = None , help = \"device to use. If not set, will be cuda if available.\" , choices = [ \"cpu\" , \"cuda\" ], ) parser . add_argument ( \"--nb-threads\" , default = 1 , help = \"# of CPU threads to use for inference\" , type = int ) parser . add_argument ( \"--nb-instances\" , default = 1 , help = \"# of model instances, may improve throughput (Triton)\" , type = int ) parser . add_argument ( \"--warmup\" , default = 10 , help = \"# of inferences to warm each model\" , type = int ) parser . add_argument ( \"--nb-measures\" , default = 1000 , help = \"# of inferences for benchmarks\" , type = int ) parser . add_argument ( \"--seed\" , default = 123 , help = \"seed for random inputs, etc.\" , type = int ) parser . add_argument ( \"--atol\" , default = 3e-1 , help = \"tolerance when comparing outputs to Pytorch ones\" , type = float ) args , _ = parser . parse_known_args ( args = commands ) return args","title":"parse_args()"},{"location":"reference/utils/generative_model/","text":"This module is copy-pasted in generated Triton configuration folder to perform the tokenization step. GPTModelWrapper ( Module , GenerationMixin ) # Source code in src/transformer_deploy/utils/generative_model.py class GPTModelWrapper ( Module , GenerationMixin ): def __init__ ( self , config : PretrainedConfig , device : torch . device , inference : Callable [[ torch . Tensor ], torch . Tensor ] ): super () . __init__ () self . config : PretrainedConfig = config self . device : torch . device = device self . inference : Callable [[ torch . Tensor ], torch . Tensor ] = inference self . main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 def prepare_inputs_for_generation ( self , input_ids , ** kwargs ): return { self . main_input_name : input_ids , } def forward ( self , input_ids , ** _ ): logits = self . inference ( input_ids ) return CausalLMOutputWithCrossAttentions ( logits = logits ) forward ( self , input_ids , ** _ ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/utils/generative_model.py def forward ( self , input_ids , ** _ ): logits = self . inference ( input_ids ) return CausalLMOutputWithCrossAttentions ( logits = logits ) prepare_inputs_for_generation ( self , input_ids , ** kwargs ) # Implement in subclasses of [ PreTrainedModel ] for custom behavior to prepare inputs in the generate method. Source code in src/transformer_deploy/utils/generative_model.py def prepare_inputs_for_generation ( self , input_ids , ** kwargs ): return { self . main_input_name : input_ids , } TritonPythonModel # Source code in src/transformer_deploy/utils/generative_model.py class TritonPythonModel : tokenizer : PreTrainedTokenizer device : str def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc model_config = AutoConfig . from_pretrained ( current_path ) target_model = args [ \"model_name\" ] . replace ( \"_generate\" , \"_model\" ) def inference_triton ( input_ids : torch . Tensor ) -> torch . Tensor : input_ids = input_ids . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids ))] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) tensor = tensor . cuda () return tensor self . model = GPTModelWrapper ( config = model_config , device = self . device , inference = inference_triton ) if self . device == \"cuda\" : self . model = self . model . cuda () self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) # to silent a warning during seq generation self . model . config . pad_token_id = self . tokenizer . eos_token_id def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = False ) # tensorrt uses int32 as input type, ort also because we force the format input_ids = tokens . input_ids . type ( dtype = torch . int32 ) if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) output_seq : torch . Tensor = self . model . generate ( input_ids , max_length = 32 ) decoded_texts : List [ str ] = [ self . tokenizer . decode ( seq , skip_special_tokens = True ) for seq in output_seq ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( t , dtype = object )) for t in decoded_texts ] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses execute ( self , requests ) # Parse and tokenize each request Parameters: Name Type Description Default requests 1 or more requests received by Triton server. required Returns: Type Description List[List[pb_utils.Tensor]] text as input tensors Source code in src/transformer_deploy/utils/generative_model.py def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = False ) # tensorrt uses int32 as input type, ort also because we force the format input_ids = tokens . input_ids . type ( dtype = torch . int32 ) if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) output_seq : torch . Tensor = self . model . generate ( input_ids , max_length = 32 ) decoded_texts : List [ str ] = [ self . tokenizer . decode ( seq , skip_special_tokens = True ) for seq in output_seq ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( t , dtype = object )) for t in decoded_texts ] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses initialize ( self , args ) # Initialize the tokenization process Parameters: Name Type Description Default args Dict[str, str] arguments from Triton config file required Source code in src/transformer_deploy/utils/generative_model.py def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc model_config = AutoConfig . from_pretrained ( current_path ) target_model = args [ \"model_name\" ] . replace ( \"_generate\" , \"_model\" ) def inference_triton ( input_ids : torch . Tensor ) -> torch . Tensor : input_ids = input_ids . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids ))] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) tensor = tensor . cuda () return tensor self . model = GPTModelWrapper ( config = model_config , device = self . device , inference = inference_triton ) if self . device == \"cuda\" : self . model = self . model . cuda () self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) # to silent a warning during seq generation self . model . config . pad_token_id = self . tokenizer . eos_token_id","title":"Generative model"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.GPTModelWrapper","text":"Source code in src/transformer_deploy/utils/generative_model.py class GPTModelWrapper ( Module , GenerationMixin ): def __init__ ( self , config : PretrainedConfig , device : torch . device , inference : Callable [[ torch . Tensor ], torch . Tensor ] ): super () . __init__ () self . config : PretrainedConfig = config self . device : torch . device = device self . inference : Callable [[ torch . Tensor ], torch . Tensor ] = inference self . main_input_name = \"input_ids\" # https://github.com/huggingface/transformers/pull/14803 def prepare_inputs_for_generation ( self , input_ids , ** kwargs ): return { self . main_input_name : input_ids , } def forward ( self , input_ids , ** _ ): logits = self . inference ( input_ids ) return CausalLMOutputWithCrossAttentions ( logits = logits )","title":"GPTModelWrapper"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.GPTModelWrapper.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/utils/generative_model.py def forward ( self , input_ids , ** _ ): logits = self . inference ( input_ids ) return CausalLMOutputWithCrossAttentions ( logits = logits )","title":"forward()"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.GPTModelWrapper.prepare_inputs_for_generation","text":"Implement in subclasses of [ PreTrainedModel ] for custom behavior to prepare inputs in the generate method. Source code in src/transformer_deploy/utils/generative_model.py def prepare_inputs_for_generation ( self , input_ids , ** kwargs ): return { self . main_input_name : input_ids , }","title":"prepare_inputs_for_generation()"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.TritonPythonModel","text":"Source code in src/transformer_deploy/utils/generative_model.py class TritonPythonModel : tokenizer : PreTrainedTokenizer device : str def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc model_config = AutoConfig . from_pretrained ( current_path ) target_model = args [ \"model_name\" ] . replace ( \"_generate\" , \"_model\" ) def inference_triton ( input_ids : torch . Tensor ) -> torch . Tensor : input_ids = input_ids . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids ))] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) tensor = tensor . cuda () return tensor self . model = GPTModelWrapper ( config = model_config , device = self . device , inference = inference_triton ) if self . device == \"cuda\" : self . model = self . model . cuda () self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) # to silent a warning during seq generation self . model . config . pad_token_id = self . tokenizer . eos_token_id def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = False ) # tensorrt uses int32 as input type, ort also because we force the format input_ids = tokens . input_ids . type ( dtype = torch . int32 ) if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) output_seq : torch . Tensor = self . model . generate ( input_ids , max_length = 32 ) decoded_texts : List [ str ] = [ self . tokenizer . decode ( seq , skip_special_tokens = True ) for seq in output_seq ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( t , dtype = object )) for t in decoded_texts ] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses","title":"TritonPythonModel"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.TritonPythonModel.execute","text":"Parse and tokenize each request Parameters: Name Type Description Default requests 1 or more requests received by Triton server. required Returns: Type Description List[List[pb_utils.Tensor]] text as input tensors Source code in src/transformer_deploy/utils/generative_model.py def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = False ) # tensorrt uses int32 as input type, ort also because we force the format input_ids = tokens . input_ids . type ( dtype = torch . int32 ) if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) output_seq : torch . Tensor = self . model . generate ( input_ids , max_length = 32 ) decoded_texts : List [ str ] = [ self . tokenizer . decode ( seq , skip_special_tokens = True ) for seq in output_seq ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( t , dtype = object )) for t in decoded_texts ] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses","title":"execute()"},{"location":"reference/utils/generative_model/#src.transformer_deploy.utils.generative_model.TritonPythonModel.initialize","text":"Initialize the tokenization process Parameters: Name Type Description Default args Dict[str, str] arguments from Triton config file required Source code in src/transformer_deploy/utils/generative_model.py def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" # more variables in https://github.com/triton-inference-server/python_backend/blob/main/src/python.cc model_config = AutoConfig . from_pretrained ( current_path ) target_model = args [ \"model_name\" ] . replace ( \"_generate\" , \"_model\" ) def inference_triton ( input_ids : torch . Tensor ) -> torch . Tensor : input_ids = input_ids . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids ))] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) tensor = tensor . cuda () return tensor self . model = GPTModelWrapper ( config = model_config , device = self . device , inference = inference_triton ) if self . device == \"cuda\" : self . model = self . model . cuda () self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) # to silent a warning during seq generation self . model . config . pad_token_id = self . tokenizer . eos_token_id","title":"initialize()"},{"location":"reference/utils/token_classifier/","text":"This module is copy-pasted in generated Triton configuration folder to perform inference. AggregationStrategy ( Enum ) # All the valid aggregation strategies for TokenClassificationPipeline Source code in src/transformer_deploy/utils/token_classifier.py class AggregationStrategy ( Enum ): \"\"\"All the valid aggregation strategies for TokenClassificationPipeline\"\"\" NONE = \"none\" SIMPLE = \"simple\" FIRST = \"first\" AVERAGE = \"average\" MAX = \"max\" TritonPythonModel # Source code in src/transformer_deploy/utils/token_classifier.py class TritonPythonModel : tokenizer : PreTrainedTokenizer device : str # change aggregation strategy here aggregation_strategy = AggregationStrategy . FIRST ignore_labels = [ \"O\" ] def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) target_model = args [ \"model_name\" ] . replace ( \"_inference\" , \"_model\" ) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" def inference_triton ( input_ids : torch . Tensor , token_type_ids : torch . Tensor , attention_mask : torch . Tensor , ) -> np . ndarray : input_ids = input_ids . type ( dtype = torch . int32 ) token_type_ids = token_type_ids . type ( dtype = torch . int32 ) attention_mask = attention_mask . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids )), pb_utils . Tensor . from_dlpack ( \"token_type_ids\" , torch . to_dlpack ( token_type_ids )), pb_utils . Tensor . from_dlpack ( \"attention_mask\" , torch . to_dlpack ( attention_mask )), ] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs , ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) return tensor . detach () . cpu () . numpy () self . model = inference_triton self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) self . config = AutoConfig . from_pretrained ( current_path ) def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = True , return_special_tokens_mask = True , return_offsets_mapping = self . tokenizer . is_fast , ) input_ids = tokens [ \"input_ids\" ] token_type_ids = tokens [ \"token_type_ids\" ] attention_mask = tokens [ \"attention_mask\" ] if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) token_type_ids = token_type_ids . to ( \"cuda\" ) attention_mask = attention_mask . to ( \"cuda\" ) output_seq : np . ndarray = self . model ( input_ids , token_type_ids , attention_mask ) logits = output_seq [ 0 ] sentence = query [ 0 ] input_ids = input_ids . cpu () . numpy ()[ 0 ] offset_mapping = tokens [ \"offset_mapping\" ][ 0 ] if \"offset_mapping\" in tokens else None special_tokens_mask = tokens [ \"special_tokens_mask\" ] . numpy ()[ 0 ] maxes = np . max ( logits , axis =- 1 , keepdims = True ) shifted_exp = np . exp ( logits - maxes ) scores = shifted_exp / shifted_exp . sum ( axis =- 1 , keepdims = True ) pre_entities = self . gather_pre_entities ( sentence , input_ids , scores , offset_mapping , special_tokens_mask , self . aggregation_strategy , ) grouped_entities = self . aggregate ( pre_entities , self . aggregation_strategy ) # Filter anything that is in self.ignore_labels entities = [ entity for entity in grouped_entities if entity . get ( \"entity\" , None ) not in self . ignore_labels and entity . get ( \"entity_group\" , None ) not in self . ignore_labels ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( json . dumps ( entities ), dtype = object ))] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses def gather_pre_entities ( self , sentence : str , input_ids : np . ndarray , scores : np . ndarray , offset_mapping : Optional [ List [ Tuple [ int , int ]]], special_tokens_mask : np . ndarray , aggregation_strategy : AggregationStrategy , ) -> List [ dict ]: \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\" pre_entities = [] for idx , token_scores in enumerate ( scores ): # Filter special_tokens, they should only occur # at the sentence boundaries since we're not encoding pairs of # sentences so we don't have to keep track of those. if special_tokens_mask [ idx ]: continue word = self . tokenizer . convert_ids_to_tokens ( int ( input_ids [ idx ])) if offset_mapping is not None : start_ind , end_ind = offset_mapping [ idx ] if not isinstance ( start_ind , int ): start_ind = int ( start_ind . numpy ()) end_ind = int ( end_ind . numpy ()) word_ref = sentence [ start_ind : end_ind ] if getattr ( self . tokenizer . _tokenizer . model , \"continuing_subword_prefix\" , None ): # This is a BPE, word aware tokenizer, there is a correct way # to fuse tokens is_subword = len ( word ) != len ( word_ref ) else : # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation # mixtures that will be considered \"words\". Non word aware models cannot do better # than this unfortunately. if aggregation_strategy in { AggregationStrategy . FIRST , AggregationStrategy . AVERAGE , AggregationStrategy . MAX , }: warnings . warn ( \"Tokenizer does not support real words, using fallback heuristic\" , UserWarning , ) is_subword = sentence [ start_ind - 1 : start_ind ] != \" \" if start_ind > 0 else False # noqa: E203 if int ( input_ids [ idx ]) == self . tokenizer . unk_token_id : word = word_ref is_subword = False else : start_ind = None end_ind = None is_subword = False pre_entity = { \"word\" : word , \"scores\" : token_scores , \"start\" : start_ind , \"end\" : end_ind , \"index\" : idx , \"is_subword\" : is_subword , } pre_entities . append ( pre_entity ) return pre_entities def aggregate ( self , pre_entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: entities = [] for pre_entity in pre_entities : entity_idx = pre_entity [ \"scores\" ] . argmax () score = pre_entity [ \"scores\" ][ entity_idx ] entity = { \"entity\" : self . config . id2label [ entity_idx ], \"score\" : float ( score ), \"index\" : pre_entity [ \"index\" ], \"word\" : pre_entity [ \"word\" ], \"start\" : pre_entity [ \"start\" ], \"end\" : pre_entity [ \"end\" ], } entities . append ( entity ) else : entities = self . aggregate_words ( pre_entities , aggregation_strategy ) if aggregation_strategy == AggregationStrategy . NONE : return entities return self . group_entities ( entities ) def aggregate_word ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> dict : word = self . tokenizer . convert_tokens_to_string ([ entity [ \"word\" ] for entity in entities ]) if aggregation_strategy == AggregationStrategy . FIRST : scores = entities [ 0 ][ \"scores\" ] idx = scores . argmax () score = scores [ idx ] entity = self . config . id2label [ idx ] elif aggregation_strategy == AggregationStrategy . MAX : max_entity = max ( entities , key = lambda entity : entity [ \"scores\" ] . max ()) scores = max_entity [ \"scores\" ] idx = scores . argmax () score = scores [ idx ] entity = self . config . id2label [ idx ] elif aggregation_strategy == AggregationStrategy . AVERAGE : scores = np . stack ([ entity [ \"scores\" ] for entity in entities ]) average_scores = np . nanmean ( scores , axis = 0 ) entity_idx = average_scores . argmax () entity = self . config . id2label [ entity_idx ] score = average_scores [ entity_idx ] else : raise ValueError ( \"Invalid aggregation_strategy\" ) new_entity = { \"entity\" : entity , \"score\" : float ( score ), \"word\" : word , \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return new_entity def aggregate_words ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: \"\"\" Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT \"\"\" if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: raise ValueError ( \"NONE and SIMPLE strategies are invalid for word aggregation\" ) word_entities = [] word_group = None for entity in entities : if word_group is None : word_group = [ entity ] elif entity [ \"is_subword\" ]: word_group . append ( entity ) else : word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) word_group = [ entity ] # Last item word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) return word_entities def group_sub_entities ( self , entities : List [ dict ]) -> dict : \"\"\" Group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" # Get the first entity in the entity group entity = entities [ 0 ][ \"entity\" ] . split ( \"-\" )[ - 1 ] scores = np . nanmean ([ entity [ \"score\" ] for entity in entities ]) tokens = [ entity [ \"word\" ] for entity in entities ] entity_group = { \"entity_group\" : entity , \"score\" : float ( np . mean ( scores )), \"word\" : self . tokenizer . convert_tokens_to_string ( tokens ), \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return entity_group def get_tag ( self , entity_name : str ) -> Tuple [ str , str ]: if entity_name . startswith ( \"B-\" ): bi = \"B\" tag = entity_name [ 2 :] elif entity_name . startswith ( \"I-\" ): bi = \"I\" tag = entity_name [ 2 :] else : # It's not in B-, I- format # Default to I- for continuation. bi = \"I\" tag = entity_name return bi , tag def group_entities ( self , entities : List [ dict ]) -> List [ dict ]: \"\"\" Find and group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" entity_groups = [] entity_group_disagg = [] for entity in entities : if not entity_group_disagg : entity_group_disagg . append ( entity ) continue # If the current entity is similar and adjacent to the previous entity, # append it to the disaggregated entity group # The split is meant to account for the \"B\" and \"I\" prefixes # Shouldn't merge if both entities are B-type bi , tag = self . get_tag ( entity [ \"entity\" ]) last_bi , last_tag = self . get_tag ( entity_group_disagg [ - 1 ][ \"entity\" ]) if tag == last_tag and bi != \"B\" : # Modify subword type to be previous_type entity_group_disagg . append ( entity ) else : # If the current entity is different from the previous entity # aggregate the disaggregated entity group entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) entity_group_disagg = [ entity ] if entity_group_disagg : # it's the last entity, add it to the entity groups entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) return entity_groups aggregate_words ( self , entities , aggregation_strategy ) # Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT Source code in src/transformer_deploy/utils/token_classifier.py def aggregate_words ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: \"\"\" Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT \"\"\" if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: raise ValueError ( \"NONE and SIMPLE strategies are invalid for word aggregation\" ) word_entities = [] word_group = None for entity in entities : if word_group is None : word_group = [ entity ] elif entity [ \"is_subword\" ]: word_group . append ( entity ) else : word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) word_group = [ entity ] # Last item word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) return word_entities execute ( self , requests ) # Parse and tokenize each request Parameters: Name Type Description Default requests 1 or more requests received by Triton server. required Returns: Type Description List[List[pb_utils.Tensor]] text as input tensors Source code in src/transformer_deploy/utils/token_classifier.py def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = True , return_special_tokens_mask = True , return_offsets_mapping = self . tokenizer . is_fast , ) input_ids = tokens [ \"input_ids\" ] token_type_ids = tokens [ \"token_type_ids\" ] attention_mask = tokens [ \"attention_mask\" ] if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) token_type_ids = token_type_ids . to ( \"cuda\" ) attention_mask = attention_mask . to ( \"cuda\" ) output_seq : np . ndarray = self . model ( input_ids , token_type_ids , attention_mask ) logits = output_seq [ 0 ] sentence = query [ 0 ] input_ids = input_ids . cpu () . numpy ()[ 0 ] offset_mapping = tokens [ \"offset_mapping\" ][ 0 ] if \"offset_mapping\" in tokens else None special_tokens_mask = tokens [ \"special_tokens_mask\" ] . numpy ()[ 0 ] maxes = np . max ( logits , axis =- 1 , keepdims = True ) shifted_exp = np . exp ( logits - maxes ) scores = shifted_exp / shifted_exp . sum ( axis =- 1 , keepdims = True ) pre_entities = self . gather_pre_entities ( sentence , input_ids , scores , offset_mapping , special_tokens_mask , self . aggregation_strategy , ) grouped_entities = self . aggregate ( pre_entities , self . aggregation_strategy ) # Filter anything that is in self.ignore_labels entities = [ entity for entity in grouped_entities if entity . get ( \"entity\" , None ) not in self . ignore_labels and entity . get ( \"entity_group\" , None ) not in self . ignore_labels ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( json . dumps ( entities ), dtype = object ))] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses gather_pre_entities ( self , sentence , input_ids , scores , offset_mapping , special_tokens_mask , aggregation_strategy ) # Fuse various numpy arrays into dicts with all the information needed for aggregation Source code in src/transformer_deploy/utils/token_classifier.py def gather_pre_entities ( self , sentence : str , input_ids : np . ndarray , scores : np . ndarray , offset_mapping : Optional [ List [ Tuple [ int , int ]]], special_tokens_mask : np . ndarray , aggregation_strategy : AggregationStrategy , ) -> List [ dict ]: \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\" pre_entities = [] for idx , token_scores in enumerate ( scores ): # Filter special_tokens, they should only occur # at the sentence boundaries since we're not encoding pairs of # sentences so we don't have to keep track of those. if special_tokens_mask [ idx ]: continue word = self . tokenizer . convert_ids_to_tokens ( int ( input_ids [ idx ])) if offset_mapping is not None : start_ind , end_ind = offset_mapping [ idx ] if not isinstance ( start_ind , int ): start_ind = int ( start_ind . numpy ()) end_ind = int ( end_ind . numpy ()) word_ref = sentence [ start_ind : end_ind ] if getattr ( self . tokenizer . _tokenizer . model , \"continuing_subword_prefix\" , None ): # This is a BPE, word aware tokenizer, there is a correct way # to fuse tokens is_subword = len ( word ) != len ( word_ref ) else : # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation # mixtures that will be considered \"words\". Non word aware models cannot do better # than this unfortunately. if aggregation_strategy in { AggregationStrategy . FIRST , AggregationStrategy . AVERAGE , AggregationStrategy . MAX , }: warnings . warn ( \"Tokenizer does not support real words, using fallback heuristic\" , UserWarning , ) is_subword = sentence [ start_ind - 1 : start_ind ] != \" \" if start_ind > 0 else False # noqa: E203 if int ( input_ids [ idx ]) == self . tokenizer . unk_token_id : word = word_ref is_subword = False else : start_ind = None end_ind = None is_subword = False pre_entity = { \"word\" : word , \"scores\" : token_scores , \"start\" : start_ind , \"end\" : end_ind , \"index\" : idx , \"is_subword\" : is_subword , } pre_entities . append ( pre_entity ) return pre_entities group_entities ( self , entities ) # Find and group together the adjacent tokens with the same entity predicted. Args: entities ( dict ): The entities predicted by the pipeline. Source code in src/transformer_deploy/utils/token_classifier.py def group_entities ( self , entities : List [ dict ]) -> List [ dict ]: \"\"\" Find and group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" entity_groups = [] entity_group_disagg = [] for entity in entities : if not entity_group_disagg : entity_group_disagg . append ( entity ) continue # If the current entity is similar and adjacent to the previous entity, # append it to the disaggregated entity group # The split is meant to account for the \"B\" and \"I\" prefixes # Shouldn't merge if both entities are B-type bi , tag = self . get_tag ( entity [ \"entity\" ]) last_bi , last_tag = self . get_tag ( entity_group_disagg [ - 1 ][ \"entity\" ]) if tag == last_tag and bi != \"B\" : # Modify subword type to be previous_type entity_group_disagg . append ( entity ) else : # If the current entity is different from the previous entity # aggregate the disaggregated entity group entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) entity_group_disagg = [ entity ] if entity_group_disagg : # it's the last entity, add it to the entity groups entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) return entity_groups group_sub_entities ( self , entities ) # Group together the adjacent tokens with the same entity predicted. Args: entities ( dict ): The entities predicted by the pipeline. Source code in src/transformer_deploy/utils/token_classifier.py def group_sub_entities ( self , entities : List [ dict ]) -> dict : \"\"\" Group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" # Get the first entity in the entity group entity = entities [ 0 ][ \"entity\" ] . split ( \"-\" )[ - 1 ] scores = np . nanmean ([ entity [ \"score\" ] for entity in entities ]) tokens = [ entity [ \"word\" ] for entity in entities ] entity_group = { \"entity_group\" : entity , \"score\" : float ( np . mean ( scores )), \"word\" : self . tokenizer . convert_tokens_to_string ( tokens ), \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return entity_group initialize ( self , args ) # Initialize the tokenization process Parameters: Name Type Description Default args Dict[str, str] arguments from Triton config file required Source code in src/transformer_deploy/utils/token_classifier.py def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) target_model = args [ \"model_name\" ] . replace ( \"_inference\" , \"_model\" ) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" def inference_triton ( input_ids : torch . Tensor , token_type_ids : torch . Tensor , attention_mask : torch . Tensor , ) -> np . ndarray : input_ids = input_ids . type ( dtype = torch . int32 ) token_type_ids = token_type_ids . type ( dtype = torch . int32 ) attention_mask = attention_mask . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids )), pb_utils . Tensor . from_dlpack ( \"token_type_ids\" , torch . to_dlpack ( token_type_ids )), pb_utils . Tensor . from_dlpack ( \"attention_mask\" , torch . to_dlpack ( attention_mask )), ] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs , ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) return tensor . detach () . cpu () . numpy () self . model = inference_triton self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) self . config = AutoConfig . from_pretrained ( current_path )","title":"Token classifier"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.AggregationStrategy","text":"All the valid aggregation strategies for TokenClassificationPipeline Source code in src/transformer_deploy/utils/token_classifier.py class AggregationStrategy ( Enum ): \"\"\"All the valid aggregation strategies for TokenClassificationPipeline\"\"\" NONE = \"none\" SIMPLE = \"simple\" FIRST = \"first\" AVERAGE = \"average\" MAX = \"max\"","title":"AggregationStrategy"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel","text":"Source code in src/transformer_deploy/utils/token_classifier.py class TritonPythonModel : tokenizer : PreTrainedTokenizer device : str # change aggregation strategy here aggregation_strategy = AggregationStrategy . FIRST ignore_labels = [ \"O\" ] def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) target_model = args [ \"model_name\" ] . replace ( \"_inference\" , \"_model\" ) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" def inference_triton ( input_ids : torch . Tensor , token_type_ids : torch . Tensor , attention_mask : torch . Tensor , ) -> np . ndarray : input_ids = input_ids . type ( dtype = torch . int32 ) token_type_ids = token_type_ids . type ( dtype = torch . int32 ) attention_mask = attention_mask . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids )), pb_utils . Tensor . from_dlpack ( \"token_type_ids\" , torch . to_dlpack ( token_type_ids )), pb_utils . Tensor . from_dlpack ( \"attention_mask\" , torch . to_dlpack ( attention_mask )), ] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs , ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) return tensor . detach () . cpu () . numpy () self . model = inference_triton self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) self . config = AutoConfig . from_pretrained ( current_path ) def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = True , return_special_tokens_mask = True , return_offsets_mapping = self . tokenizer . is_fast , ) input_ids = tokens [ \"input_ids\" ] token_type_ids = tokens [ \"token_type_ids\" ] attention_mask = tokens [ \"attention_mask\" ] if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) token_type_ids = token_type_ids . to ( \"cuda\" ) attention_mask = attention_mask . to ( \"cuda\" ) output_seq : np . ndarray = self . model ( input_ids , token_type_ids , attention_mask ) logits = output_seq [ 0 ] sentence = query [ 0 ] input_ids = input_ids . cpu () . numpy ()[ 0 ] offset_mapping = tokens [ \"offset_mapping\" ][ 0 ] if \"offset_mapping\" in tokens else None special_tokens_mask = tokens [ \"special_tokens_mask\" ] . numpy ()[ 0 ] maxes = np . max ( logits , axis =- 1 , keepdims = True ) shifted_exp = np . exp ( logits - maxes ) scores = shifted_exp / shifted_exp . sum ( axis =- 1 , keepdims = True ) pre_entities = self . gather_pre_entities ( sentence , input_ids , scores , offset_mapping , special_tokens_mask , self . aggregation_strategy , ) grouped_entities = self . aggregate ( pre_entities , self . aggregation_strategy ) # Filter anything that is in self.ignore_labels entities = [ entity for entity in grouped_entities if entity . get ( \"entity\" , None ) not in self . ignore_labels and entity . get ( \"entity_group\" , None ) not in self . ignore_labels ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( json . dumps ( entities ), dtype = object ))] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses def gather_pre_entities ( self , sentence : str , input_ids : np . ndarray , scores : np . ndarray , offset_mapping : Optional [ List [ Tuple [ int , int ]]], special_tokens_mask : np . ndarray , aggregation_strategy : AggregationStrategy , ) -> List [ dict ]: \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\" pre_entities = [] for idx , token_scores in enumerate ( scores ): # Filter special_tokens, they should only occur # at the sentence boundaries since we're not encoding pairs of # sentences so we don't have to keep track of those. if special_tokens_mask [ idx ]: continue word = self . tokenizer . convert_ids_to_tokens ( int ( input_ids [ idx ])) if offset_mapping is not None : start_ind , end_ind = offset_mapping [ idx ] if not isinstance ( start_ind , int ): start_ind = int ( start_ind . numpy ()) end_ind = int ( end_ind . numpy ()) word_ref = sentence [ start_ind : end_ind ] if getattr ( self . tokenizer . _tokenizer . model , \"continuing_subword_prefix\" , None ): # This is a BPE, word aware tokenizer, there is a correct way # to fuse tokens is_subword = len ( word ) != len ( word_ref ) else : # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation # mixtures that will be considered \"words\". Non word aware models cannot do better # than this unfortunately. if aggregation_strategy in { AggregationStrategy . FIRST , AggregationStrategy . AVERAGE , AggregationStrategy . MAX , }: warnings . warn ( \"Tokenizer does not support real words, using fallback heuristic\" , UserWarning , ) is_subword = sentence [ start_ind - 1 : start_ind ] != \" \" if start_ind > 0 else False # noqa: E203 if int ( input_ids [ idx ]) == self . tokenizer . unk_token_id : word = word_ref is_subword = False else : start_ind = None end_ind = None is_subword = False pre_entity = { \"word\" : word , \"scores\" : token_scores , \"start\" : start_ind , \"end\" : end_ind , \"index\" : idx , \"is_subword\" : is_subword , } pre_entities . append ( pre_entity ) return pre_entities def aggregate ( self , pre_entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: entities = [] for pre_entity in pre_entities : entity_idx = pre_entity [ \"scores\" ] . argmax () score = pre_entity [ \"scores\" ][ entity_idx ] entity = { \"entity\" : self . config . id2label [ entity_idx ], \"score\" : float ( score ), \"index\" : pre_entity [ \"index\" ], \"word\" : pre_entity [ \"word\" ], \"start\" : pre_entity [ \"start\" ], \"end\" : pre_entity [ \"end\" ], } entities . append ( entity ) else : entities = self . aggregate_words ( pre_entities , aggregation_strategy ) if aggregation_strategy == AggregationStrategy . NONE : return entities return self . group_entities ( entities ) def aggregate_word ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> dict : word = self . tokenizer . convert_tokens_to_string ([ entity [ \"word\" ] for entity in entities ]) if aggregation_strategy == AggregationStrategy . FIRST : scores = entities [ 0 ][ \"scores\" ] idx = scores . argmax () score = scores [ idx ] entity = self . config . id2label [ idx ] elif aggregation_strategy == AggregationStrategy . MAX : max_entity = max ( entities , key = lambda entity : entity [ \"scores\" ] . max ()) scores = max_entity [ \"scores\" ] idx = scores . argmax () score = scores [ idx ] entity = self . config . id2label [ idx ] elif aggregation_strategy == AggregationStrategy . AVERAGE : scores = np . stack ([ entity [ \"scores\" ] for entity in entities ]) average_scores = np . nanmean ( scores , axis = 0 ) entity_idx = average_scores . argmax () entity = self . config . id2label [ entity_idx ] score = average_scores [ entity_idx ] else : raise ValueError ( \"Invalid aggregation_strategy\" ) new_entity = { \"entity\" : entity , \"score\" : float ( score ), \"word\" : word , \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return new_entity def aggregate_words ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: \"\"\" Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT \"\"\" if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: raise ValueError ( \"NONE and SIMPLE strategies are invalid for word aggregation\" ) word_entities = [] word_group = None for entity in entities : if word_group is None : word_group = [ entity ] elif entity [ \"is_subword\" ]: word_group . append ( entity ) else : word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) word_group = [ entity ] # Last item word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) return word_entities def group_sub_entities ( self , entities : List [ dict ]) -> dict : \"\"\" Group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" # Get the first entity in the entity group entity = entities [ 0 ][ \"entity\" ] . split ( \"-\" )[ - 1 ] scores = np . nanmean ([ entity [ \"score\" ] for entity in entities ]) tokens = [ entity [ \"word\" ] for entity in entities ] entity_group = { \"entity_group\" : entity , \"score\" : float ( np . mean ( scores )), \"word\" : self . tokenizer . convert_tokens_to_string ( tokens ), \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return entity_group def get_tag ( self , entity_name : str ) -> Tuple [ str , str ]: if entity_name . startswith ( \"B-\" ): bi = \"B\" tag = entity_name [ 2 :] elif entity_name . startswith ( \"I-\" ): bi = \"I\" tag = entity_name [ 2 :] else : # It's not in B-, I- format # Default to I- for continuation. bi = \"I\" tag = entity_name return bi , tag def group_entities ( self , entities : List [ dict ]) -> List [ dict ]: \"\"\" Find and group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" entity_groups = [] entity_group_disagg = [] for entity in entities : if not entity_group_disagg : entity_group_disagg . append ( entity ) continue # If the current entity is similar and adjacent to the previous entity, # append it to the disaggregated entity group # The split is meant to account for the \"B\" and \"I\" prefixes # Shouldn't merge if both entities are B-type bi , tag = self . get_tag ( entity [ \"entity\" ]) last_bi , last_tag = self . get_tag ( entity_group_disagg [ - 1 ][ \"entity\" ]) if tag == last_tag and bi != \"B\" : # Modify subword type to be previous_type entity_group_disagg . append ( entity ) else : # If the current entity is different from the previous entity # aggregate the disaggregated entity group entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) entity_group_disagg = [ entity ] if entity_group_disagg : # it's the last entity, add it to the entity groups entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) return entity_groups","title":"TritonPythonModel"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.aggregate_words","text":"Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT Source code in src/transformer_deploy/utils/token_classifier.py def aggregate_words ( self , entities : List [ dict ], aggregation_strategy : AggregationStrategy ) -> List [ dict ]: \"\"\" Override tokens from a given word that disagree to force agreement on word boundaries. Example: micro|soft| com|pany| B-ENT I-NAME I-ENT I-ENT will be rewritten with first strategy as microsoft| company| B-ENT I-ENT \"\"\" if aggregation_strategy in { AggregationStrategy . NONE , AggregationStrategy . SIMPLE , }: raise ValueError ( \"NONE and SIMPLE strategies are invalid for word aggregation\" ) word_entities = [] word_group = None for entity in entities : if word_group is None : word_group = [ entity ] elif entity [ \"is_subword\" ]: word_group . append ( entity ) else : word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) word_group = [ entity ] # Last item word_entities . append ( self . aggregate_word ( word_group , aggregation_strategy )) return word_entities","title":"aggregate_words()"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.execute","text":"Parse and tokenize each request Parameters: Name Type Description Default requests 1 or more requests received by Triton server. required Returns: Type Description List[List[pb_utils.Tensor]] text as input tensors Source code in src/transformer_deploy/utils/token_classifier.py def execute ( self , requests ) -> \"List[List[pb_utils.Tensor]]\" : \"\"\" Parse and tokenize each request :param requests: 1 or more requests received by Triton server. :return: text as input tensors \"\"\" responses = [] # for loop for batch requests (disabled in our case) for request in requests : # binary data typed back to string query = [ t . decode ( \"UTF-8\" ) for t in pb_utils . get_input_tensor_by_name ( request , \"TEXT\" ) . as_numpy () . tolist ()] tokens : BatchEncoding = self . tokenizer ( text = query [ 0 ], return_tensors = TensorType . PYTORCH , return_attention_mask = True , return_special_tokens_mask = True , return_offsets_mapping = self . tokenizer . is_fast , ) input_ids = tokens [ \"input_ids\" ] token_type_ids = tokens [ \"token_type_ids\" ] attention_mask = tokens [ \"attention_mask\" ] if self . device == \"cuda\" : input_ids = input_ids . to ( \"cuda\" ) token_type_ids = token_type_ids . to ( \"cuda\" ) attention_mask = attention_mask . to ( \"cuda\" ) output_seq : np . ndarray = self . model ( input_ids , token_type_ids , attention_mask ) logits = output_seq [ 0 ] sentence = query [ 0 ] input_ids = input_ids . cpu () . numpy ()[ 0 ] offset_mapping = tokens [ \"offset_mapping\" ][ 0 ] if \"offset_mapping\" in tokens else None special_tokens_mask = tokens [ \"special_tokens_mask\" ] . numpy ()[ 0 ] maxes = np . max ( logits , axis =- 1 , keepdims = True ) shifted_exp = np . exp ( logits - maxes ) scores = shifted_exp / shifted_exp . sum ( axis =- 1 , keepdims = True ) pre_entities = self . gather_pre_entities ( sentence , input_ids , scores , offset_mapping , special_tokens_mask , self . aggregation_strategy , ) grouped_entities = self . aggregate ( pre_entities , self . aggregation_strategy ) # Filter anything that is in self.ignore_labels entities = [ entity for entity in grouped_entities if entity . get ( \"entity\" , None ) not in self . ignore_labels and entity . get ( \"entity_group\" , None ) not in self . ignore_labels ] tensor_output = [ pb_utils . Tensor ( \"output\" , np . array ( json . dumps ( entities ), dtype = object ))] responses . append ( pb_utils . InferenceResponse ( tensor_output )) return responses","title":"execute()"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.gather_pre_entities","text":"Fuse various numpy arrays into dicts with all the information needed for aggregation Source code in src/transformer_deploy/utils/token_classifier.py def gather_pre_entities ( self , sentence : str , input_ids : np . ndarray , scores : np . ndarray , offset_mapping : Optional [ List [ Tuple [ int , int ]]], special_tokens_mask : np . ndarray , aggregation_strategy : AggregationStrategy , ) -> List [ dict ]: \"\"\"Fuse various numpy arrays into dicts with all the information needed for aggregation\"\"\" pre_entities = [] for idx , token_scores in enumerate ( scores ): # Filter special_tokens, they should only occur # at the sentence boundaries since we're not encoding pairs of # sentences so we don't have to keep track of those. if special_tokens_mask [ idx ]: continue word = self . tokenizer . convert_ids_to_tokens ( int ( input_ids [ idx ])) if offset_mapping is not None : start_ind , end_ind = offset_mapping [ idx ] if not isinstance ( start_ind , int ): start_ind = int ( start_ind . numpy ()) end_ind = int ( end_ind . numpy ()) word_ref = sentence [ start_ind : end_ind ] if getattr ( self . tokenizer . _tokenizer . model , \"continuing_subword_prefix\" , None ): # This is a BPE, word aware tokenizer, there is a correct way # to fuse tokens is_subword = len ( word ) != len ( word_ref ) else : # This is a fallback heuristic. This will fail most likely on any kind of text + punctuation # mixtures that will be considered \"words\". Non word aware models cannot do better # than this unfortunately. if aggregation_strategy in { AggregationStrategy . FIRST , AggregationStrategy . AVERAGE , AggregationStrategy . MAX , }: warnings . warn ( \"Tokenizer does not support real words, using fallback heuristic\" , UserWarning , ) is_subword = sentence [ start_ind - 1 : start_ind ] != \" \" if start_ind > 0 else False # noqa: E203 if int ( input_ids [ idx ]) == self . tokenizer . unk_token_id : word = word_ref is_subword = False else : start_ind = None end_ind = None is_subword = False pre_entity = { \"word\" : word , \"scores\" : token_scores , \"start\" : start_ind , \"end\" : end_ind , \"index\" : idx , \"is_subword\" : is_subword , } pre_entities . append ( pre_entity ) return pre_entities","title":"gather_pre_entities()"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.group_entities","text":"Find and group together the adjacent tokens with the same entity predicted. Args: entities ( dict ): The entities predicted by the pipeline. Source code in src/transformer_deploy/utils/token_classifier.py def group_entities ( self , entities : List [ dict ]) -> List [ dict ]: \"\"\" Find and group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" entity_groups = [] entity_group_disagg = [] for entity in entities : if not entity_group_disagg : entity_group_disagg . append ( entity ) continue # If the current entity is similar and adjacent to the previous entity, # append it to the disaggregated entity group # The split is meant to account for the \"B\" and \"I\" prefixes # Shouldn't merge if both entities are B-type bi , tag = self . get_tag ( entity [ \"entity\" ]) last_bi , last_tag = self . get_tag ( entity_group_disagg [ - 1 ][ \"entity\" ]) if tag == last_tag and bi != \"B\" : # Modify subword type to be previous_type entity_group_disagg . append ( entity ) else : # If the current entity is different from the previous entity # aggregate the disaggregated entity group entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) entity_group_disagg = [ entity ] if entity_group_disagg : # it's the last entity, add it to the entity groups entity_groups . append ( self . group_sub_entities ( entity_group_disagg )) return entity_groups","title":"group_entities()"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.group_sub_entities","text":"Group together the adjacent tokens with the same entity predicted. Args: entities ( dict ): The entities predicted by the pipeline. Source code in src/transformer_deploy/utils/token_classifier.py def group_sub_entities ( self , entities : List [ dict ]) -> dict : \"\"\" Group together the adjacent tokens with the same entity predicted. Args: entities (`dict`): The entities predicted by the pipeline. \"\"\" # Get the first entity in the entity group entity = entities [ 0 ][ \"entity\" ] . split ( \"-\" )[ - 1 ] scores = np . nanmean ([ entity [ \"score\" ] for entity in entities ]) tokens = [ entity [ \"word\" ] for entity in entities ] entity_group = { \"entity_group\" : entity , \"score\" : float ( np . mean ( scores )), \"word\" : self . tokenizer . convert_tokens_to_string ( tokens ), \"start\" : entities [ 0 ][ \"start\" ], \"end\" : entities [ - 1 ][ \"end\" ], } return entity_group","title":"group_sub_entities()"},{"location":"reference/utils/token_classifier/#src.transformer_deploy.utils.token_classifier.TritonPythonModel.initialize","text":"Initialize the tokenization process Parameters: Name Type Description Default args Dict[str, str] arguments from Triton config file required Source code in src/transformer_deploy/utils/token_classifier.py def initialize ( self , args : Dict [ str , str ]) -> None : \"\"\" Initialize the tokenization process :param args: arguments from Triton config file \"\"\" current_path : str = os . path . join ( args [ \"model_repository\" ], args [ \"model_version\" ]) target_model = args [ \"model_name\" ] . replace ( \"_inference\" , \"_model\" ) self . device = \"cpu\" if args [ \"model_instance_kind\" ] == \"CPU\" else \"cuda\" def inference_triton ( input_ids : torch . Tensor , token_type_ids : torch . Tensor , attention_mask : torch . Tensor , ) -> np . ndarray : input_ids = input_ids . type ( dtype = torch . int32 ) token_type_ids = token_type_ids . type ( dtype = torch . int32 ) attention_mask = attention_mask . type ( dtype = torch . int32 ) inputs = [ pb_utils . Tensor . from_dlpack ( \"input_ids\" , torch . to_dlpack ( input_ids )), pb_utils . Tensor . from_dlpack ( \"token_type_ids\" , torch . to_dlpack ( token_type_ids )), pb_utils . Tensor . from_dlpack ( \"attention_mask\" , torch . to_dlpack ( attention_mask )), ] inference_request = pb_utils . InferenceRequest ( model_name = target_model , requested_output_names = [ \"output\" ], inputs = inputs , ) inference_response = inference_request . exec () if inference_response . has_error (): raise pb_utils . TritonModelException ( inference_response . error () . message ()) else : output = pb_utils . get_output_tensor_by_name ( inference_response , \"output\" ) tensor : torch . Tensor = torch . from_dlpack ( output . to_dlpack ()) return tensor . detach () . cpu () . numpy () self . model = inference_triton self . tokenizer = AutoTokenizer . from_pretrained ( current_path ) self . config = AutoConfig . from_pretrained ( current_path )","title":"initialize()"}]}