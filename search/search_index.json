{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Efficient, scalable and enterprise-grade CPU / GPU inference server for Hugging Face transformer models At Lefebvre Dalloz we run in production several semantic search engine in the legal domain, in non-marketing language it's a reranker, and we based ours on Transformer . In those setup, latency is key to provide good user experience, and relevancy inference is done online for hundreds of snippets per user query. We have tested many solutions, and below is what we found: Pytorch + FastAPI = \ud83d\udc22 Most tutorials on Transformer deployment in production are built over Pytorch and FastAPI. Both are great tools but not very performant in inference. Microsoft ONNX Runtime + Nvidia Triton inference server = \ufe0f\ud83c\udfc3\ud83d\udca8 Then, if you spend some time, you can build something over ONNX Runtime and Triton inference server. You will usually get from 2X to 4X faster inference compared to vanilla Pytorch. It's cool! Nvidia TensorRT + Nvidia Triton inference server = \u26a1\ufe0f\ud83c\udfc3\ud83d\udca8\ud83d\udca8 However, if you want the best in class performances on GPU , there is only a single possible combination: Nvidia TensorRT and Triton. You will usually get 5X faster inference compared to vanilla Pytorch. Sometimes it can raises up to 10X faster inference . Buuuuttt... TensorRT can ask some efforts to master, it requires tricks not easy to come with, we implemented them for you! Features # heavily optimize transformer models for inference ( CPU and GPU ) -> between 5X and 10X speed-up deploy model on Nvidia Triton inference server (enterprise-grade), 6X faster than FastAPI add quantization support for both CPU and GPU simple to use: optimization done in a single command line! supported model: any model than can be exported to ONNX (-> most of them) supported tasks: classification, feature extraction (aka sentence-transformers dense embeddings)","title":"Why transformer-deploy?"},{"location":"#features","text":"heavily optimize transformer models for inference ( CPU and GPU ) -> between 5X and 10X speed-up deploy model on Nvidia Triton inference server (enterprise-grade), 6X faster than FastAPI add quantization support for both CPU and GPU simple to use: optimization done in a single command line! supported model: any model than can be exported to ONNX (-> most of them) supported tasks: classification, feature extraction (aka sentence-transformers dense embeddings)","title":"Features"},{"location":"benchmarks/","text":"Benchmarks # Most Transformer encoder based models are supported like Bert, Roberta, miniLM, Camembert, Albert, XLM-R, Distilbert, Electra, etc. Best results are obtained with TensorRT 8.2. Below examples are representative of the performance gain to expect from this library. Other improvements not shown here include GPU memory usage decrease, multi stream, etc. Small architecture # batch 1, seq length 16 on T4/RTX 3090 GPUs (up to 10X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 16 16 16 --batch-size 1 1 1 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=0.65ms, sd=0.11ms, min=0.57ms, max=0.96ms, median=0.59ms, 95p=0.93ms, 99p=0.94ms [ONNX Runtime (vanilla)] mean=1.31ms, sd=0.05ms, min=1.27ms, max=1.48ms, median=1.30ms, 95p=1.44ms, 99p=1.45ms [ONNX Runtime (optimized)] mean=0.71ms, sd=0.01ms, min=0.69ms, max=0.74ms, median=0.70ms, 95p=0.73ms, 99p=0.74ms [Pytorch (FP32)] mean=5.01ms, sd=0.06ms, min=4.94ms, max=6.72ms, median=5.01ms, 95p=5.07ms, 99p=5.13ms [Pytorch (FP16)] mean=5.44ms, sd=0.07ms, min=5.36ms, max=6.80ms, median=5.43ms, 95p=5.49ms, 99p=5.55ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=0.45ms, sd=0.05ms, min=0.41ms, max=0.78ms, median=0.45ms, 95p=0.55ms, 99p=0.73ms [ONNX Runtime (vanilla)] mean=1.32ms, sd=0.11ms, min=1.24ms, max=2.36ms, median=1.30ms, 95p=1.50ms, 99p=1.74ms [ONNX Runtime (optimized)] mean=0.84ms, sd=0.11ms, min=0.76ms, max=2.03ms, median=0.81ms, 95p=1.10ms, 99p=1.25ms [Pytorch (FP32)] mean=4.68ms, sd=0.28ms, min=4.38ms, max=7.83ms, median=4.65ms, 95p=4.97ms, 99p=6.16ms [Pytorch (FP16)] mean=5.25ms, sd=0.60ms, min=4.83ms, max=8.54ms, median=5.03ms, 95p=6.54ms, 99p=7.77ms batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=16.38ms, sd=0.30ms, min=15.45ms, max=17.42ms, median=16.42ms, 95p=16.83ms, 99p=17.09ms [ONNX Runtime (vanilla)] mean=65.12ms, sd=1.53ms, min=61.74ms, max=68.51ms, median=65.21ms, 95p=67.46ms, 99p=67.90ms [ONNX Runtime (optimized)] mean=26.75ms, sd=0.30ms, min=25.96ms, max=27.71ms, median=26.73ms, 95p=27.23ms, 99p=27.52ms [Pytorch (FP32)] mean=82.22ms, sd=1.02ms, min=78.83ms, max=85.02ms, median=82.28ms, 95p=83.80ms, 99p=84.43ms [Pytorch (FP16)] mean=46.29ms, sd=0.41ms, min=45.23ms, max=47.56ms, median=46.30ms, 95p=46.98ms, 99p=47.37ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=5.44ms, sd=0.45ms, min=5.03ms, max=8.91ms, median=5.20ms, 95p=6.11ms, 99p=7.39ms [ONNX Runtime (vanilla)] mean=16.87ms, sd=2.15ms, min=15.38ms, max=26.03ms, median=15.82ms, 95p=22.63ms, 99p=24.20ms [ONNX Runtime (optimized)] mean=8.07ms, sd=0.58ms, min=7.59ms, max=13.63ms, median=7.93ms, 95p=8.71ms, 99p=11.45ms [Pytorch (FP32)] mean=17.09ms, sd=0.21ms, min=16.87ms, max=18.99ms, median=17.04ms, 95p=17.49ms, 99p=18.08ms [Pytorch (FP16)] mean=14.77ms, sd=1.83ms, min=13.50ms, max=20.97ms, median=13.87ms, 95p=19.15ms, 99p=20.01ms Base architecture # batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m cardiffnlp/twitter-roberta-base-sentiment --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=80.57ms, sd=1.00ms, min=76.23ms, max=83.16ms, median=80.53ms, 95p=82.14ms, 99p=82.53ms [ONNX Runtime (vanilla)] mean=353.81ms, sd=14.79ms, min=335.54ms, max=390.86ms, median=348.41ms, 95p=382.09ms, 99p=386.84ms [ONNX Runtime (optimized)] mean=97.94ms, sd=1.66ms, min=93.83ms, max=102.11ms, median=97.84ms, 95p=100.73ms, 99p=101.57ms [Pytorch (FP32)] mean=398.49ms, sd=25.76ms, min=369.81ms, max=454.55ms, median=387.17ms, 95p=445.52ms, 99p=450.81ms [Pytorch (FP16)] mean=134.18ms, sd=1.16ms, min=131.60ms, max=138.48ms, median=133.80ms, 95p=136.57ms, 99p=137.39ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=27.52ms, sd=1.61ms, min=24.49ms, max=33.78ms, median=28.01ms, 95p=30.33ms, 99p=31.22ms [ONNX Runtime (vanilla)] mean=65.95ms, sd=6.18ms, min=60.84ms, max=99.75ms, median=62.97ms, 95p=81.02ms, 99p=89.10ms [ONNX Runtime (optimized)] mean=32.73ms, sd=4.80ms, min=28.84ms, max=48.84ms, median=30.15ms, 95p=43.03ms, 99p=44.78ms [Pytorch (FP32)] mean=69.18ms, sd=4.79ms, min=65.97ms, max=97.74ms, median=67.16ms, 95p=77.88ms, 99p=92.43ms [Pytorch (FP16)] mean=48.78ms, sd=2.02ms, min=47.02ms, max=61.37ms, median=47.67ms, 95p=52.34ms, 99p=55.56ms Large architecture # batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m roberta-large-mnli --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=240.39ms, sd=11.01ms, min=217.59ms, max=259.57ms, median=242.68ms, 95p=255.03ms, 99p=257.04ms [ONNX Runtime (vanilla)] mean=1176.73ms, sd=63.51ms, min=1020.00ms, max=1225.03ms, median=1210.08ms, 95p=1217.54ms, 99p=1220.25ms [ONNX Runtime (optimized)] mean=295.03ms, sd=19.69ms, min=255.74ms, max=314.78ms, median=307.07ms, 95p=311.20ms, 99p=312.47ms [Pytorch (FP32)] mean=1220.41ms, sd=75.93ms, min=1119.93ms, max=1342.10ms, median=1216.23ms, 95p=1329.08ms, 99p=1336.47ms [Pytorch (FP16)] mean=438.26ms, sd=13.71ms, min=398.29ms, max=459.97ms, median=442.36ms, 95p=453.96ms, 99p=457.57ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=79.54ms, sd=5.99ms, min=74.47ms, max=113.25ms, median=76.87ms, 95p=88.02ms, 99p=104.48ms [ONNX Runtime (vanilla)] mean=202.88ms, sd=16.21ms, min=187.91ms, max=277.85ms, median=194.80ms, 95p=239.58ms, 99p=261.44ms [ONNX Runtime (optimized)] mean=97.04ms, sd=5.55ms, min=90.83ms, max=121.88ms, median=94.04ms, 95p=104.81ms, 99p=107.75ms [Pytorch (FP32)] mean=202.80ms, sd=11.16ms, min=194.47ms, max=284.70ms, median=198.46ms, 95p=221.72ms, 99p=257.31ms [Pytorch (FP16)] mean=142.63ms, sd=6.35ms, min=136.24ms, max=189.95ms, median=139.90ms, 95p=154.10ms, 99p=160.16ms","title":"AWS benchmarks"},{"location":"benchmarks/#benchmarks","text":"Most Transformer encoder based models are supported like Bert, Roberta, miniLM, Camembert, Albert, XLM-R, Distilbert, Electra, etc. Best results are obtained with TensorRT 8.2. Below examples are representative of the performance gain to expect from this library. Other improvements not shown here include GPU memory usage decrease, multi stream, etc.","title":"Benchmarks"},{"location":"benchmarks/#small-architecture","text":"batch 1, seq length 16 on T4/RTX 3090 GPUs (up to 10X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 16 16 16 --batch-size 1 1 1 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=0.65ms, sd=0.11ms, min=0.57ms, max=0.96ms, median=0.59ms, 95p=0.93ms, 99p=0.94ms [ONNX Runtime (vanilla)] mean=1.31ms, sd=0.05ms, min=1.27ms, max=1.48ms, median=1.30ms, 95p=1.44ms, 99p=1.45ms [ONNX Runtime (optimized)] mean=0.71ms, sd=0.01ms, min=0.69ms, max=0.74ms, median=0.70ms, 95p=0.73ms, 99p=0.74ms [Pytorch (FP32)] mean=5.01ms, sd=0.06ms, min=4.94ms, max=6.72ms, median=5.01ms, 95p=5.07ms, 99p=5.13ms [Pytorch (FP16)] mean=5.44ms, sd=0.07ms, min=5.36ms, max=6.80ms, median=5.43ms, 95p=5.49ms, 99p=5.55ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=0.45ms, sd=0.05ms, min=0.41ms, max=0.78ms, median=0.45ms, 95p=0.55ms, 99p=0.73ms [ONNX Runtime (vanilla)] mean=1.32ms, sd=0.11ms, min=1.24ms, max=2.36ms, median=1.30ms, 95p=1.50ms, 99p=1.74ms [ONNX Runtime (optimized)] mean=0.84ms, sd=0.11ms, min=0.76ms, max=2.03ms, median=0.81ms, 95p=1.10ms, 99p=1.25ms [Pytorch (FP32)] mean=4.68ms, sd=0.28ms, min=4.38ms, max=7.83ms, median=4.65ms, 95p=4.97ms, 99p=6.16ms [Pytorch (FP16)] mean=5.25ms, sd=0.60ms, min=4.83ms, max=8.54ms, median=5.03ms, 95p=6.54ms, 99p=7.77ms batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=16.38ms, sd=0.30ms, min=15.45ms, max=17.42ms, median=16.42ms, 95p=16.83ms, 99p=17.09ms [ONNX Runtime (vanilla)] mean=65.12ms, sd=1.53ms, min=61.74ms, max=68.51ms, median=65.21ms, 95p=67.46ms, 99p=67.90ms [ONNX Runtime (optimized)] mean=26.75ms, sd=0.30ms, min=25.96ms, max=27.71ms, median=26.73ms, 95p=27.23ms, 99p=27.52ms [Pytorch (FP32)] mean=82.22ms, sd=1.02ms, min=78.83ms, max=85.02ms, median=82.28ms, 95p=83.80ms, 99p=84.43ms [Pytorch (FP16)] mean=46.29ms, sd=0.41ms, min=45.23ms, max=47.56ms, median=46.30ms, 95p=46.98ms, 99p=47.37ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=5.44ms, sd=0.45ms, min=5.03ms, max=8.91ms, median=5.20ms, 95p=6.11ms, 99p=7.39ms [ONNX Runtime (vanilla)] mean=16.87ms, sd=2.15ms, min=15.38ms, max=26.03ms, median=15.82ms, 95p=22.63ms, 99p=24.20ms [ONNX Runtime (optimized)] mean=8.07ms, sd=0.58ms, min=7.59ms, max=13.63ms, median=7.93ms, 95p=8.71ms, 99p=11.45ms [Pytorch (FP32)] mean=17.09ms, sd=0.21ms, min=16.87ms, max=18.99ms, median=17.04ms, 95p=17.49ms, 99p=18.08ms [Pytorch (FP16)] mean=14.77ms, sd=1.83ms, min=13.50ms, max=20.97ms, median=13.87ms, 95p=19.15ms, 99p=20.01ms","title":"Small architecture"},{"location":"benchmarks/#base-architecture","text":"batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m cardiffnlp/twitter-roberta-base-sentiment --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=80.57ms, sd=1.00ms, min=76.23ms, max=83.16ms, median=80.53ms, 95p=82.14ms, 99p=82.53ms [ONNX Runtime (vanilla)] mean=353.81ms, sd=14.79ms, min=335.54ms, max=390.86ms, median=348.41ms, 95p=382.09ms, 99p=386.84ms [ONNX Runtime (optimized)] mean=97.94ms, sd=1.66ms, min=93.83ms, max=102.11ms, median=97.84ms, 95p=100.73ms, 99p=101.57ms [Pytorch (FP32)] mean=398.49ms, sd=25.76ms, min=369.81ms, max=454.55ms, median=387.17ms, 95p=445.52ms, 99p=450.81ms [Pytorch (FP16)] mean=134.18ms, sd=1.16ms, min=131.60ms, max=138.48ms, median=133.80ms, 95p=136.57ms, 99p=137.39ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=27.52ms, sd=1.61ms, min=24.49ms, max=33.78ms, median=28.01ms, 95p=30.33ms, 99p=31.22ms [ONNX Runtime (vanilla)] mean=65.95ms, sd=6.18ms, min=60.84ms, max=99.75ms, median=62.97ms, 95p=81.02ms, 99p=89.10ms [ONNX Runtime (optimized)] mean=32.73ms, sd=4.80ms, min=28.84ms, max=48.84ms, median=30.15ms, 95p=43.03ms, 99p=44.78ms [Pytorch (FP32)] mean=69.18ms, sd=4.79ms, min=65.97ms, max=97.74ms, median=67.16ms, 95p=77.88ms, 99p=92.43ms [Pytorch (FP16)] mean=48.78ms, sd=2.02ms, min=47.02ms, max=61.37ms, median=47.67ms, 95p=52.34ms, 99p=55.56ms","title":"Base architecture"},{"location":"benchmarks/#large-architecture","text":"batch 16, seq length 384 on T4/RTX 3090 GPUs (up to 5X faster with TensorRT vs Pytorch) command: convert_model -m roberta-large-mnli --backend tensorrt onnx pytorch --seq-len 384 384 384 --batch-size 16 16 16 ### GPU Nvidia T4 Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=240.39ms, sd=11.01ms, min=217.59ms, max=259.57ms, median=242.68ms, 95p=255.03ms, 99p=257.04ms [ONNX Runtime (vanilla)] mean=1176.73ms, sd=63.51ms, min=1020.00ms, max=1225.03ms, median=1210.08ms, 95p=1217.54ms, 99p=1220.25ms [ONNX Runtime (optimized)] mean=295.03ms, sd=19.69ms, min=255.74ms, max=314.78ms, median=307.07ms, 95p=311.20ms, 99p=312.47ms [Pytorch (FP32)] mean=1220.41ms, sd=75.93ms, min=1119.93ms, max=1342.10ms, median=1216.23ms, 95p=1329.08ms, 99p=1336.47ms [Pytorch (FP16)] mean=438.26ms, sd=13.71ms, min=398.29ms, max=459.97ms, median=442.36ms, 95p=453.96ms, 99p=457.57ms ### GPU Nvidia RTX 3090 Inference done on NVIDIA GeForce RTX 3090 latencies: [TensorRT (FP16)] mean=79.54ms, sd=5.99ms, min=74.47ms, max=113.25ms, median=76.87ms, 95p=88.02ms, 99p=104.48ms [ONNX Runtime (vanilla)] mean=202.88ms, sd=16.21ms, min=187.91ms, max=277.85ms, median=194.80ms, 95p=239.58ms, 99p=261.44ms [ONNX Runtime (optimized)] mean=97.04ms, sd=5.55ms, min=90.83ms, max=121.88ms, median=94.04ms, 95p=104.81ms, 99p=107.75ms [Pytorch (FP32)] mean=202.80ms, sd=11.16ms, min=194.47ms, max=284.70ms, median=198.46ms, 95p=221.72ms, 99p=257.31ms [Pytorch (FP16)] mean=142.63ms, sd=6.35ms, min=136.24ms, max=189.95ms, median=139.90ms, 95p=154.10ms, 99p=160.16ms","title":"Large architecture"},{"location":"compare/","text":"High level comparaison # Inference engine # The inference engine performs the computation only, it doesn't manage the communication part (HTTP/GRPC API, etc.). Summary don't use Pytorch in production for inference ONNX Runtime is your good enough API for most inference jobs if you need best performances, use TensorRT Nvidia TensorRT Microsoft ONNX Runtime Meta Pytorch comments transformer-deploy support Licence Apache 2, optimization engine is closed source MIT Modified BSD ease of use (API) Nvidia has chosen to not hide technical details + model is specific to a single hardware + model + data shapes association ease of use (documentation) (spread out, incomplete) (improving) (strong community) Hardware support GPU + Jetson CPU + GPU + IoT + Edge + Mobile CPU + GPU Performance TensorRT is usually 5 to 10X faster than Pytorch when you use quantization, etc. Accuracy TensorRT optimizations may be a bit too aggressive and decrease model accuracy. It requires manual modification to retrieve it. Inference HTTP/GRPC server # Nvidia Triton Meta TorchServe FastAPI comments transformer-deploy support Licence Modified BSD Apache 2 MIT ease of use (API) As a classic HTTP server, FastAPI may appear easier to use ease of use (documentation) FastAPI has one of the most beautiful documentation ever! Performance FastAPI is 6-10X slower to manage user query than Triton Support CPU GPU dynamic batching combine individual inference requests together to improve inference throughput concurrent model execution run multiple models (or multiple instances of the same model) pipeline one or more models and the connection of input and output tensors between those models native multiple backends* support *backends: Microsoft ONNX Runtime, Nvidia Triton, Meta Pytorch REST API GRPC API Inference metrics GPU utilization, server throughput, and server latency","title":"Which tool to choose?"},{"location":"compare/#high-level-comparaison","text":"","title":"High level comparaison"},{"location":"compare/#inference-engine","text":"The inference engine performs the computation only, it doesn't manage the communication part (HTTP/GRPC API, etc.). Summary don't use Pytorch in production for inference ONNX Runtime is your good enough API for most inference jobs if you need best performances, use TensorRT Nvidia TensorRT Microsoft ONNX Runtime Meta Pytorch comments transformer-deploy support Licence Apache 2, optimization engine is closed source MIT Modified BSD ease of use (API) Nvidia has chosen to not hide technical details + model is specific to a single hardware + model + data shapes association ease of use (documentation) (spread out, incomplete) (improving) (strong community) Hardware support GPU + Jetson CPU + GPU + IoT + Edge + Mobile CPU + GPU Performance TensorRT is usually 5 to 10X faster than Pytorch when you use quantization, etc. Accuracy TensorRT optimizations may be a bit too aggressive and decrease model accuracy. It requires manual modification to retrieve it.","title":"Inference engine"},{"location":"compare/#inference-httpgrpc-server","text":"Nvidia Triton Meta TorchServe FastAPI comments transformer-deploy support Licence Modified BSD Apache 2 MIT ease of use (API) As a classic HTTP server, FastAPI may appear easier to use ease of use (documentation) FastAPI has one of the most beautiful documentation ever! Performance FastAPI is 6-10X slower to manage user query than Triton Support CPU GPU dynamic batching combine individual inference requests together to improve inference throughput concurrent model execution run multiple models (or multiple instances of the same model) pipeline one or more models and the connection of input and output tensors between those models native multiple backends* support *backends: Microsoft ONNX Runtime, Nvidia Triton, Meta Pytorch REST API GRPC API Inference metrics GPU utilization, server throughput, and server latency","title":"Inference HTTP/GRPC server"},{"location":"demo/","text":"Optimize and deploy model on Nvidia Triton server # To better understand the context of this demo, check Hugging Face Transformer inference UNDER 1 millisecond latency This folder contains scripts to run different benchmarks: triton_client.py : query the model with a string triton_client_model.py : query the model directly (without using the tokenizer) with numpy arrays triton_client_requests.py : query the model directly (without using the tokenizer) with numpy arrays using only requests library triton_client_tokenizer.py : query the tokenizer only fast_api_server_onnx.py : FastAPI inference server to compare to Nvidia Triton Infinity demo information # In sept 2021, \ud83e\udd17 Hugging Face released a new product called Infinity . It\u2019s described as a server to perform inference at enterprise scale . The communication is around the promise that the product can perform Transformer inference at 1 millisecond latency on the GPU . There are very few information about its performances outside this YouTube video: demo video (Youtube) According to the demo presenter, Hugging Face Infinity server costs at least \ud83d\udcb020 000$/year for a single model deployed on a single machine (no information is publicly available on price scalability). In the next parts we will try to compare this open source library with the commercial solution from Hugging Face. Setup they used for their own demo: AWS instance GPU model seq len batch size latency g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 16 1 1.7ms g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 128 1 2.5ms The purpose of this tutorial is to explain how to heavily optimize a Transformer from Hugging Face and deploy it on a production-ready inference server, end to end. The performance improvement brought by this process applies to all scenarios, from short sequences to long ones, from a batch of size 1 to large batches. When the architecture is compliant with the expectations of the tools, the process always brings a significant performance boost compared to vanilla PyTorch. The process is in 3 steps: convert Pytorch model to a graph optimize the graph deploy the graph on a performant inference server At the end we will compare the performance of our inference server to the numbers shown by Hugging Face during the demo and will see that we are faster for both 16 and 128 tokens input sequences with batch size 1 (as far as I know, Hugging Face has not publicly shared information on other scenarios). Model optimization # We will optimize philschmid/MiniLM-L6-H384-uncased-sst2 model from the Hugging Face hub. We will use the 3 backends for that: ONNX Runtime, TensorRT and Pytorch. Usually, ONNX Runtime provide a good trade-off between simplicity and performance, TensorRT the best performances and Pytorch the simplest approach (at least it's the most well known tool). # add -v $PWD/src:/opt/tritonserver/src to apply source code modification to the container docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.3.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" 16 128 128 means that the TensorRT model will optimize for a sequence length between 16 and 128 tokens. Most of the time it's a bad idea to use dynamic axis on sequence length, it makes TensorRT slower. ONNX Runtime don't use this information and it has no impact on it. After a few minutes, it should display something like this: Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=1.00ms, sd=0.13ms, min=0.92ms, max=1.34ms, median=0.95ms, 95p=1.31ms, 99p=1.33ms [ONNX Runtime (vanilla)] mean=1.67ms, sd=0.08ms, min=1.59ms, max=3.48ms, median=1.65ms, 95p=1.85ms, 99p=1.87ms [ONNX Runtime (optimized)] mean=0.73ms, sd=0.01ms, min=0.71ms, max=0.87ms, median=0.73ms, 95p=0.75ms, 99p=0.76ms [Pytorch (FP32)] mean=5.13ms, sd=0.06ms, min=5.06ms, max=6.85ms, median=5.13ms, 95p=5.18ms, 99p=5.22ms [Pytorch (FP16)] mean=5.39ms, sd=0.10ms, min=5.31ms, max=8.39ms, median=5.39ms, 95p=5.45ms, 99p=5.48ms interesting to note that ONNX Runtime provide better performances than TensorRT for this setup, it's quite rare... Models are stored in newly generated ./triton_models/ folder. Subfolders contain templates for Nvidia Triton server. Launch Nvidia Triton inference server # \u26a0\ufe0f WARNING \u26a0\ufe0f: if you have generated the models outside the Docker container, check that your TensorRT version is the same than the Triton backend one. Launch Nvidia Triton inference server : # add --shm-size 256m -> to have up to 4 Python backends (tokenizer) at the same time (64Mb per instance) docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:21.12-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside. Performance analysis # Measures: 16 tokens + TensorRT: # need a local installation of the package # pip install .[GPU] ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/triton_client.py --length 16 --model tensorrt 10 /31/2021 12 :09:34 INFO timing [ triton transformers ] : mean = 1 .53ms, sd = 0 .06ms, min = 1 .48ms, max = 1 .78ms, median = 1 .51ms, 95p = 1 .66ms, 99p = 1 .74ms [[ -3.4355469 3 .2753906 ]] 128 tokens + TensorRT: ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/triton_client.py --length 128 --model tensorrt 10 /31/2021 12 :12:00 INFO timing [ triton transformers ] : mean = 1 .96ms, sd = 0 .08ms, min = 1 .88ms, max = 2 .24ms, median = 1 .93ms, 95p = 2 .17ms, 99p = 2 .23ms [[ -3.4589844 3 .3027344 ]] There is also a performance analysis tool provided by Nvidia called perf_analyzer # perf_analyzer needs this dependency sudo apt install libb64-dev # add -a for async measures, and -i grpc to use that protocol instead of http ~/.local/bin/perf_analyzer -m transformer_tensorrt_inference \\ --percentile = 95 \\ --string-data \"This live event is great. I will sign-up for Infinity.\" \\ --shape TEXT:1 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv # just test the model part (easier to get random input) ~/.local/bin/perf_analyzer --input-data zero -m transformer_tensorrt_model \\ --shape input_ids:1,128 \\ --shape attention_mask:1,128 \\ --shape token_type_ids:1,128 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv The tool need to be run on Ubuntu >= 20.04 (and won't work on Ubuntu 18.04 used for the AWS official Ubuntu deep learning image) Model analyzer # Model analyzer is a powerful tool to adjust the Triton server configuration. To run it: docker run -it --rm --gpus all -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.1.1 \\ bash -c \"model-analyzer profile -f /project/demo/config_analyzer.yaml\" FastAPI server baseline # This is our baseline, easy to run, but not very performant. # launch server, disable logging for best performances python3 -m uvicorn --log-level warning demo.fast_api_server_onnx:app --port 8000 --host 0 .0.0.0 # other variation, 1 worker per CPU for best latency (plus not a good idea to have several times the same model on a single GPU): python3 -m gunicorn -w 1 -k uvicorn.workers.UvicornWorker --log-level warning demo.fast_api_server_onnx --bind 0 .0.0.0:8000 # simple inference timing time curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict # slightly more serious measure sudo apt-get install linux-tools-common linux-tools-generic linux-tools- ` uname -r ` sudo perf stat -r 50 -d curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict -s > /dev/null It should produce: Performance counter stats for 'curl -G --data-urlencode query=This live event is great. I will sign-up for Infinity. localhost:8000/predict' ( 50 runs ) : 6 .14 msec task-clock # 0.494 CPUs utilized ( +- 0.59% ) 3 context-switches # 0.462 K/sec ( +- 1.84% ) 0 cpu-migrations # 0.000 K/sec 577 page-faults # 0.094 M/sec ( +- 0.06% ) <not supported> cycles <not supported> instructions <not supported> branches <not supported> branch-misses <not supported> L1-dcache-loads <not supported> L1-dcache-load-misses <not supported> LLC-loads <not supported> LLC-load-misses 0 .0124429 +- 0 .0000547 seconds time elapsed ( +- 0 .44% )","title":"From optimization to deployment: end to end demo"},{"location":"demo/#optimize-and-deploy-model-on-nvidia-triton-server","text":"To better understand the context of this demo, check Hugging Face Transformer inference UNDER 1 millisecond latency This folder contains scripts to run different benchmarks: triton_client.py : query the model with a string triton_client_model.py : query the model directly (without using the tokenizer) with numpy arrays triton_client_requests.py : query the model directly (without using the tokenizer) with numpy arrays using only requests library triton_client_tokenizer.py : query the tokenizer only fast_api_server_onnx.py : FastAPI inference server to compare to Nvidia Triton","title":"Optimize and deploy model on Nvidia Triton server"},{"location":"demo/#infinity-demo-information","text":"In sept 2021, \ud83e\udd17 Hugging Face released a new product called Infinity . It\u2019s described as a server to perform inference at enterprise scale . The communication is around the promise that the product can perform Transformer inference at 1 millisecond latency on the GPU . There are very few information about its performances outside this YouTube video: demo video (Youtube) According to the demo presenter, Hugging Face Infinity server costs at least \ud83d\udcb020 000$/year for a single model deployed on a single machine (no information is publicly available on price scalability). In the next parts we will try to compare this open source library with the commercial solution from Hugging Face. Setup they used for their own demo: AWS instance GPU model seq len batch size latency g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 16 1 1.7ms g4dn x.large Nvidia T4 philschmid/MiniLM-L6-H384-uncased-sst2 128 1 2.5ms The purpose of this tutorial is to explain how to heavily optimize a Transformer from Hugging Face and deploy it on a production-ready inference server, end to end. The performance improvement brought by this process applies to all scenarios, from short sequences to long ones, from a batch of size 1 to large batches. When the architecture is compliant with the expectations of the tools, the process always brings a significant performance boost compared to vanilla PyTorch. The process is in 3 steps: convert Pytorch model to a graph optimize the graph deploy the graph on a performant inference server At the end we will compare the performance of our inference server to the numbers shown by Hugging Face during the demo and will see that we are faster for both 16 and 128 tokens input sequences with batch size 1 (as far as I know, Hugging Face has not publicly shared information on other scenarios).","title":"Infinity demo information"},{"location":"demo/#model-optimization","text":"We will optimize philschmid/MiniLM-L6-H384-uncased-sst2 model from the Hugging Face hub. We will use the 3 backends for that: ONNX Runtime, TensorRT and Pytorch. Usually, ONNX Runtime provide a good trade-off between simplicity and performance, TensorRT the best performances and Pytorch the simplest approach (at least it's the most well known tool). # add -v $PWD/src:/opt/tritonserver/src to apply source code modification to the container docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.3.0 \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend tensorrt onnx \\ --seq-len 16 128 128\" 16 128 128 means that the TensorRT model will optimize for a sequence length between 16 and 128 tokens. Most of the time it's a bad idea to use dynamic axis on sequence length, it makes TensorRT slower. ONNX Runtime don't use this information and it has no impact on it. After a few minutes, it should display something like this: Inference done on Tesla T4 latencies: [TensorRT (FP16)] mean=1.00ms, sd=0.13ms, min=0.92ms, max=1.34ms, median=0.95ms, 95p=1.31ms, 99p=1.33ms [ONNX Runtime (vanilla)] mean=1.67ms, sd=0.08ms, min=1.59ms, max=3.48ms, median=1.65ms, 95p=1.85ms, 99p=1.87ms [ONNX Runtime (optimized)] mean=0.73ms, sd=0.01ms, min=0.71ms, max=0.87ms, median=0.73ms, 95p=0.75ms, 99p=0.76ms [Pytorch (FP32)] mean=5.13ms, sd=0.06ms, min=5.06ms, max=6.85ms, median=5.13ms, 95p=5.18ms, 99p=5.22ms [Pytorch (FP16)] mean=5.39ms, sd=0.10ms, min=5.31ms, max=8.39ms, median=5.39ms, 95p=5.45ms, 99p=5.48ms interesting to note that ONNX Runtime provide better performances than TensorRT for this setup, it's quite rare... Models are stored in newly generated ./triton_models/ folder. Subfolders contain templates for Nvidia Triton server.","title":"Model optimization"},{"location":"demo/#launch-nvidia-triton-inference-server","text":"\u26a0\ufe0f WARNING \u26a0\ufe0f: if you have generated the models outside the Docker container, check that your TensorRT version is the same than the Triton backend one. Launch Nvidia Triton inference server : # add --shm-size 256m -> to have up to 4 Python backends (tokenizer) at the same time (64Mb per instance) docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:21.12-py3 \\ bash -c \"pip install transformers && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside.","title":"Launch Nvidia Triton inference server"},{"location":"demo/#performance-analysis","text":"Measures: 16 tokens + TensorRT: # need a local installation of the package # pip install .[GPU] ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/triton_client.py --length 16 --model tensorrt 10 /31/2021 12 :09:34 INFO timing [ triton transformers ] : mean = 1 .53ms, sd = 0 .06ms, min = 1 .48ms, max = 1 .78ms, median = 1 .51ms, 95p = 1 .66ms, 99p = 1 .74ms [[ -3.4355469 3 .2753906 ]] 128 tokens + TensorRT: ubuntu@ip-XXX:~/transformer-deploy$ python3 demo/triton_client.py --length 128 --model tensorrt 10 /31/2021 12 :12:00 INFO timing [ triton transformers ] : mean = 1 .96ms, sd = 0 .08ms, min = 1 .88ms, max = 2 .24ms, median = 1 .93ms, 95p = 2 .17ms, 99p = 2 .23ms [[ -3.4589844 3 .3027344 ]] There is also a performance analysis tool provided by Nvidia called perf_analyzer # perf_analyzer needs this dependency sudo apt install libb64-dev # add -a for async measures, and -i grpc to use that protocol instead of http ~/.local/bin/perf_analyzer -m transformer_tensorrt_inference \\ --percentile = 95 \\ --string-data \"This live event is great. I will sign-up for Infinity.\" \\ --shape TEXT:1 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv # just test the model part (easier to get random input) ~/.local/bin/perf_analyzer --input-data zero -m transformer_tensorrt_model \\ --shape input_ids:1,128 \\ --shape attention_mask:1,128 \\ --shape token_type_ids:1,128 \\ --concurrency-range 1 :4 \\ -i grpc \\ -a \\ -f perf.csv The tool need to be run on Ubuntu >= 20.04 (and won't work on Ubuntu 18.04 used for the AWS official Ubuntu deep learning image)","title":"Performance analysis"},{"location":"demo/#model-analyzer","text":"Model analyzer is a powerful tool to adjust the Triton server configuration. To run it: docker run -it --rm --gpus all -v $PWD :/project ghcr.io/els-rd/transformer-deploy:0.1.1 \\ bash -c \"model-analyzer profile -f /project/demo/config_analyzer.yaml\"","title":"Model analyzer"},{"location":"demo/#fastapi-server-baseline","text":"This is our baseline, easy to run, but not very performant. # launch server, disable logging for best performances python3 -m uvicorn --log-level warning demo.fast_api_server_onnx:app --port 8000 --host 0 .0.0.0 # other variation, 1 worker per CPU for best latency (plus not a good idea to have several times the same model on a single GPU): python3 -m gunicorn -w 1 -k uvicorn.workers.UvicornWorker --log-level warning demo.fast_api_server_onnx --bind 0 .0.0.0:8000 # simple inference timing time curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict # slightly more serious measure sudo apt-get install linux-tools-common linux-tools-generic linux-tools- ` uname -r ` sudo perf stat -r 50 -d curl -G --data-urlencode query = \"This live event is great. I will sign-up for Infinity.\" localhost:8000/predict -s > /dev/null It should produce: Performance counter stats for 'curl -G --data-urlencode query=This live event is great. I will sign-up for Infinity. localhost:8000/predict' ( 50 runs ) : 6 .14 msec task-clock # 0.494 CPUs utilized ( +- 0.59% ) 3 context-switches # 0.462 K/sec ( +- 1.84% ) 0 cpu-migrations # 0.000 K/sec 577 page-faults # 0.094 M/sec ( +- 0.06% ) <not supported> cycles <not supported> instructions <not supported> branches <not supported> branch-misses <not supported> L1-dcache-loads <not supported> L1-dcache-load-misses <not supported> LLC-loads <not supported> LLC-load-misses 0 .0124429 +- 0 .0000547 seconds time elapsed ( +- 0 .44% )","title":"FastAPI server baseline"},{"location":"faq/","text":"FAQ # Is CPU deployment a viable option? # Let's start with the usual \"it depends\" It's a viable option if: your sequences are short or very short (<= 128 tokens) you use a distilled/small flavor model (like miniLM or XtremeDistil ) you don't use batching, as CPU are not very good at it, and using a batch size of 1 you can avoid padding you tune number of threads and number of inference engine instances: using all threads available won't provide best results, in many cases, you will get a better throughput by using multiple instances of the inference engine. Triton server has an option for that. you use dynamic quantization In most cases, Nvidia T4 GPU (the cheapest GPU option available on all cloud) will offer you the best perf / cost trade-off by a large margin. Compared to Intel Xeon of 2nd/3rd generation, they are cheaper for better results. What should I use to optimize GPU deployment? # on the hardware side, 1 or more T4 GPU TensorRT for the optimization INT-8 quantization ( QAT ) fixed sequence length and dynamic batch axis Can I use sparse tensors if my GPU supports it? # At the time of writing, sparse tensors can only be used if you implement your model with TensorRT operators manually. If you import your model from ONNX , you won't see any acceleration, it should improve in Q3 2022. Should I use quantization or distillation or both? # ... it depends, but usually quantization will bring a X2 speed up compared to an already optimized model with little cost in accuracy. Distillation can bring you more speed, but in many cases, will cost you more in accuracy (at least on hard NLP tasks). How this compares to the TensorFlow ecosystem? # Vanilla TensorFlow has a good ecosystem, it even has a basic integration of TensorRT (basic -> not all feature/optimization). If you need really good inference optimization, Nvidia advices in its official documentation to export the model on onnx and then follow the same optimization process than any PyTorch model. So, on a perf only side, there is no difference. For sure, the idea written everywhere that for production TensorFlow is a better choice is just wrong. For instance, Amazon and Microsoft use Nvidia Triton inference server on most of their products using ML (like Microsoft Office , or advertising on Amazon ), in 2021 at least. And Microsoft Bing is built over TensorRT . Should I use Torch-TensorRT? # Torch-TensorRT is not yet mature (few ATen -> trt op support), and has not yet Triton backend (they are working on it). It seems that the new Pytorch Fx interface is the right direction to go from PyTorch to TensorRT. What do I do if I have accuracy issues with mixed precision? # Most of the time, it's an operator which overflows (too big value for FP16 numbers). Polygraphy should help you to find the operator(s) to fix. Then, during engine building, you can fix the precision to FP32 for some operators. Check in the convert source code how we did it. What do I do if ONNX export fails? # Most of the time ONNX export fail because of unsupported operations. One way to workaround is to reimplement that part or override the module with symbolic() static function. More info on https://pytorch.org/docs/stable/onnx.html#static-symbolic-method Why don't you support GPU quantization on ONNX Runtime instead of TensorRT? # There are few reasons why ONNX Runtime GPU quantization support is not supported: it doesn\u2019t support QAT , it \"just\" patches ONNX file and run them on TensorRT provider. Because ONNX file can\u2019t be retrained, you can't do a fine tuning after quantization. Concretely, it means that if post training quantization doesn't provide an accuracy good enough for your use case, you won't use quantization at all. QAT library from Nvidia (the one used in this project) let you easily enable and disable each quantizer (on Python). You can see how useful it is in the quantization demo notebook, we used that feature to disable quantization on layernorm on specific layers to retrieve 1 point of accuracy without sacrificing perf at the post training quantization step. However, the way it currently works on ONNX Runtime is all or nothing (for each operator). Of course, if you are well verse if ONNX things, you can manually parse your graph with a graph surgery tools and make your change but it would take a lot of time compared to just for loop in your Pytorch modules. and more important, in our own experiment, models that contain unsupported operators by TensorRT just crashed on ONNX Runtime\u2026 It was unexpected as ONNX Runtime is supposed to split graph and leverage several providers when one doesn't support an operation. I suppose in my case that the issue is that the operator exists but not with the right type. At the end, to make it work, I needed to patch the source code. So not a better user experience.","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#is-cpu-deployment-a-viable-option","text":"Let's start with the usual \"it depends\" It's a viable option if: your sequences are short or very short (<= 128 tokens) you use a distilled/small flavor model (like miniLM or XtremeDistil ) you don't use batching, as CPU are not very good at it, and using a batch size of 1 you can avoid padding you tune number of threads and number of inference engine instances: using all threads available won't provide best results, in many cases, you will get a better throughput by using multiple instances of the inference engine. Triton server has an option for that. you use dynamic quantization In most cases, Nvidia T4 GPU (the cheapest GPU option available on all cloud) will offer you the best perf / cost trade-off by a large margin. Compared to Intel Xeon of 2nd/3rd generation, they are cheaper for better results.","title":"Is CPU deployment a viable option?"},{"location":"faq/#what-should-i-use-to-optimize-gpu-deployment","text":"on the hardware side, 1 or more T4 GPU TensorRT for the optimization INT-8 quantization ( QAT ) fixed sequence length and dynamic batch axis","title":"What should I use to optimize GPU deployment?"},{"location":"faq/#can-i-use-sparse-tensors-if-my-gpu-supports-it","text":"At the time of writing, sparse tensors can only be used if you implement your model with TensorRT operators manually. If you import your model from ONNX , you won't see any acceleration, it should improve in Q3 2022.","title":"Can I use sparse tensors if my GPU supports it?"},{"location":"faq/#should-i-use-quantization-or-distillation-or-both","text":"... it depends, but usually quantization will bring a X2 speed up compared to an already optimized model with little cost in accuracy. Distillation can bring you more speed, but in many cases, will cost you more in accuracy (at least on hard NLP tasks).","title":"Should I use quantization or distillation or both?"},{"location":"faq/#how-this-compares-to-the-tensorflow-ecosystem","text":"Vanilla TensorFlow has a good ecosystem, it even has a basic integration of TensorRT (basic -> not all feature/optimization). If you need really good inference optimization, Nvidia advices in its official documentation to export the model on onnx and then follow the same optimization process than any PyTorch model. So, on a perf only side, there is no difference. For sure, the idea written everywhere that for production TensorFlow is a better choice is just wrong. For instance, Amazon and Microsoft use Nvidia Triton inference server on most of their products using ML (like Microsoft Office , or advertising on Amazon ), in 2021 at least. And Microsoft Bing is built over TensorRT .","title":"How this compares to the TensorFlow ecosystem?"},{"location":"faq/#should-i-use-torch-tensorrt","text":"Torch-TensorRT is not yet mature (few ATen -> trt op support), and has not yet Triton backend (they are working on it). It seems that the new Pytorch Fx interface is the right direction to go from PyTorch to TensorRT.","title":"Should I use Torch-TensorRT?"},{"location":"faq/#what-do-i-do-if-i-have-accuracy-issues-with-mixed-precision","text":"Most of the time, it's an operator which overflows (too big value for FP16 numbers). Polygraphy should help you to find the operator(s) to fix. Then, during engine building, you can fix the precision to FP32 for some operators. Check in the convert source code how we did it.","title":"What do I do if I have accuracy issues with mixed precision?"},{"location":"faq/#what-do-i-do-if-onnx-export-fails","text":"Most of the time ONNX export fail because of unsupported operations. One way to workaround is to reimplement that part or override the module with symbolic() static function. More info on https://pytorch.org/docs/stable/onnx.html#static-symbolic-method","title":"What do I do if ONNX export fails?"},{"location":"faq/#why-dont-you-support-gpu-quantization-on-onnx-runtime-instead-of-tensorrt","text":"There are few reasons why ONNX Runtime GPU quantization support is not supported: it doesn\u2019t support QAT , it \"just\" patches ONNX file and run them on TensorRT provider. Because ONNX file can\u2019t be retrained, you can't do a fine tuning after quantization. Concretely, it means that if post training quantization doesn't provide an accuracy good enough for your use case, you won't use quantization at all. QAT library from Nvidia (the one used in this project) let you easily enable and disable each quantizer (on Python). You can see how useful it is in the quantization demo notebook, we used that feature to disable quantization on layernorm on specific layers to retrieve 1 point of accuracy without sacrificing perf at the post training quantization step. However, the way it currently works on ONNX Runtime is all or nothing (for each operator). Of course, if you are well verse if ONNX things, you can manually parse your graph with a graph surgery tools and make your change but it would take a lot of time compared to just for loop in your Pytorch modules. and more important, in our own experiment, models that contain unsupported operators by TensorRT just crashed on ONNX Runtime\u2026 It was unexpected as ONNX Runtime is supposed to split graph and leverage several providers when one doesn't support an operation. I suppose in my case that the issue is that the operator exists but not with the right type. At the end, to make it work, I needed to patch the source code. So not a better user experience.","title":"Why don't you support GPU quantization on ONNX Runtime instead of TensorRT?"},{"location":"onnx_convert/","text":"Convert Pytorch model to ONNX # To ease optimization we need to convert our Pytorch model written in imperative code in a mostly static graph. Therefore, optimization tooling will be able to run static analysis and search for some pattern to optimize. The target graph format is ONNX . from https://onnx.ai/ ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators \u2014 the building blocks of machine learning and deep learning models \u2014 and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\u201d (https://onnx.ai/). The format has initially been created by Facebook and Microsoft to have a bridge between Pytorch (research) and Caffee2 (production). There are 2 ways to perform an export from Pytorch: tracing mode : send some (dummy) data to the model, and the tool will trace them inside the model, that way it will guess what the graph looks like; scripting : requires the models to be written in a certain way to work, its main advantage is that the dynamic logic is kept intact but adds many constraints in the way models are written. Attention Tracing mode is not magic, for instance it can\u2019t see operations you are doing in numpy (if any), the graph will be static, some if/else code is fixed forever, for loop will be unrolled, etc. Hugging Face and model authors took care that main/most models are tracing mode compatible. Following commented code performs the ONNX conversion: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from collections import OrderedDict import torch from torch.onnx import TrainingMode def convert_to_onnx ( model_pytorch , output_path : str , inputs_pytorch , opset : int = 12 ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. :param model_pytorch: Pytorch model :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param opset: version of ONNX protocol to use, usually 12, or 13 if you use per channel quantized model \"\"\" # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = opset , # the ONNX version to use, 13 if quantized model, 12 for not quantized ones do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) Note One particular point is that we declare some axis as dynamic. If we were not doing that, the graph would only accept tensors with the exact same shape that the ones we are using to build it (the dummy data), so sequence length or batch size would be fixed. The name we have given to input and output fields will be reused in other tools.","title":"How ONNX conversion works?"},{"location":"onnx_convert/#convert-pytorch-model-to-onnx","text":"To ease optimization we need to convert our Pytorch model written in imperative code in a mostly static graph. Therefore, optimization tooling will be able to run static analysis and search for some pattern to optimize. The target graph format is ONNX . from https://onnx.ai/ ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators \u2014 the building blocks of machine learning and deep learning models \u2014 and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\u201d (https://onnx.ai/). The format has initially been created by Facebook and Microsoft to have a bridge between Pytorch (research) and Caffee2 (production). There are 2 ways to perform an export from Pytorch: tracing mode : send some (dummy) data to the model, and the tool will trace them inside the model, that way it will guess what the graph looks like; scripting : requires the models to be written in a certain way to work, its main advantage is that the dynamic logic is kept intact but adds many constraints in the way models are written. Attention Tracing mode is not magic, for instance it can\u2019t see operations you are doing in numpy (if any), the graph will be static, some if/else code is fixed forever, for loop will be unrolled, etc. Hugging Face and model authors took care that main/most models are tracing mode compatible. Following commented code performs the ONNX conversion: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 from collections import OrderedDict import torch from torch.onnx import TrainingMode def convert_to_onnx ( model_pytorch , output_path : str , inputs_pytorch , opset : int = 12 ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. :param model_pytorch: Pytorch model :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param opset: version of ONNX protocol to use, usually 12, or 13 if you use per channel quantized model \"\"\" # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = opset , # the ONNX version to use, 13 if quantized model, 12 for not quantized ones do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) Note One particular point is that we declare some axis as dynamic. If we were not doing that, the graph would only accept tensors with the exact same shape that the ones we are using to build it (the dummy data), so sequence length or batch size would be fixed. The name we have given to input and output fields will be reused in other tools.","title":"Convert Pytorch model to ONNX"},{"location":"optimizations/","text":"Optimizing a model for inference # There are few ways to optimize a model for inference, some of them are basically a simplification of its graph: find and remove redundant operations : for instance dropout has no use outside the training loop, it can be removed without any impact on inference; perform constant folding : meaning find some parts of the graph made of constant expressions, and compute the results at compile time instead of runtime (similar to most programming language compiler); kernel fusion : to avoid 1/ loading time, and 2/ share memory to avoid back and forth transfers with the global memory. Obviously, it will mainly benefit to memory bound operations (like multiply and add operations, a very common pattern in deep learning), it\u2019s called \u201ckernel fusion\u201d; Another orthogonal approach is to use lower precision tensors, it may be FP16 float number or INT-8 quantization. Attention Mixed precision and INT-8 quantization may have an accuracy cost. The reason is that you can't code as many information in FP16 or INT-8 tensor that you can in FP32 tensor. Sometimes you have not enough granularity, some other times the range is not big enough. When it happens, you need to modify the graph to keep some operators in full precision. This library does it for mixed precision and provide you with a simple way to do it for INT-8 quantization","title":"What is done during inference optimization?"},{"location":"optimizations/#optimizing-a-model-for-inference","text":"There are few ways to optimize a model for inference, some of them are basically a simplification of its graph: find and remove redundant operations : for instance dropout has no use outside the training loop, it can be removed without any impact on inference; perform constant folding : meaning find some parts of the graph made of constant expressions, and compute the results at compile time instead of runtime (similar to most programming language compiler); kernel fusion : to avoid 1/ loading time, and 2/ share memory to avoid back and forth transfers with the global memory. Obviously, it will mainly benefit to memory bound operations (like multiply and add operations, a very common pattern in deep learning), it\u2019s called \u201ckernel fusion\u201d; Another orthogonal approach is to use lower precision tensors, it may be FP16 float number or INT-8 quantization. Attention Mixed precision and INT-8 quantization may have an accuracy cost. The reason is that you can't code as many information in FP16 or INT-8 tensor that you can in FP32 tensor. Sometimes you have not enough granularity, some other times the range is not big enough. When it happens, you need to modify the graph to keep some operators in full precision. This library does it for mixed precision and provide you with a simple way to do it for INT-8 quantization","title":"Optimizing a model for inference"},{"location":"python/","text":"TensorRT usage in Python script # There are 2 ways to use a TensorRT optimized model: deploy it on Triton server use it directly in Python This document is about the second option. High level explanations # call load_engine() to parse an existing TensorRT engine or build_engine() to convert an ONNX file setup a CUDA stream (for async call), a TensorRT runtime and a context load your profile (s) call infer_tensorrt() Build engine # We assume that you have already prepared your ONNX file. Now we need to convert to TensorRT: import tensorrt as trt from tensorrt.tensorrt import Logger , Runtime from transformer_deploy.backends.trt_utils import build_engine trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 max_seq_len = 256 batch_size = 32 engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) Prepare inference # Now the engine is ready, we can prepare the inference: import pycuda.autoinit from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext from transformer_deploy.backends.trt_utils import get_binding_idxs stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] Inference # from transformer_deploy.backends.trt_utils import infer_tensorrt input_np = ... tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print ( tensorrt_output ) ... and you are done! \ud83c\udf89 Tip To go deeper, check in the API: Convert Backends/Trt utils ... and if you are looking for inspiration, check onnx-tensorrt","title":"Use TensorRT in Python script"},{"location":"python/#tensorrt-usage-in-python-script","text":"There are 2 ways to use a TensorRT optimized model: deploy it on Triton server use it directly in Python This document is about the second option.","title":"TensorRT usage in Python script"},{"location":"python/#high-level-explanations","text":"call load_engine() to parse an existing TensorRT engine or build_engine() to convert an ONNX file setup a CUDA stream (for async call), a TensorRT runtime and a context load your profile (s) call infer_tensorrt()","title":"High level explanations"},{"location":"python/#build-engine","text":"We assume that you have already prepared your ONNX file. Now we need to convert to TensorRT: import tensorrt as trt from tensorrt.tensorrt import Logger , Runtime from transformer_deploy.backends.trt_utils import build_engine trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 max_seq_len = 256 batch_size = 32 engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , )","title":"Build engine"},{"location":"python/#prepare-inference","text":"Now the engine is ready, we can prepare the inference: import pycuda.autoinit from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext from transformer_deploy.backends.trt_utils import get_binding_idxs stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int]","title":"Prepare inference"},{"location":"python/#inference","text":"from transformer_deploy.backends.trt_utils import infer_tensorrt input_np = ... tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print ( tensorrt_output ) ... and you are done! \ud83c\udf89 Tip To go deeper, check in the API: Convert Backends/Trt utils ... and if you are looking for inspiration, check onnx-tensorrt","title":"Inference"},{"location":"quantization/","text":"(function (global, factory) { typeof exports === 'object' && typeof module !== 'undefined' ? module.exports = factory() : typeof define === 'function' && define.amd ? define(factory) : (global = global || self, global.ClipboardCopyElement = factory()); }(this, function () { 'use strict'; function createNode(text) { const node = document.createElement('pre'); node.style.width = '1px'; node.style.height = '1px'; node.style.position = 'fixed'; node.style.top = '5px'; node.textContent = text; return node; } function copyNode(node) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(node.textContent); } const selection = getSelection(); if (selection == null) { return Promise.reject(new Error()); } selection.removeAllRanges(); const range = document.createRange(); range.selectNodeContents(node); selection.addRange(range); document.execCommand('copy'); selection.removeAllRanges(); return Promise.resolve(); } function copyText(text) { if ('clipboard' in navigator) { // eslint-disable-next-line flowtype/no-flow-fix-me-comments // $FlowFixMe Clipboard is not defined in Flow yet. return navigator.clipboard.writeText(text); } const body = document.body; if (!body) { return Promise.reject(new Error()); } const node = createNode(text); body.appendChild(node); copyNode(node); body.removeChild(node); return Promise.resolve(); } function copy(button) { const id = button.getAttribute('for'); const text = button.getAttribute('value'); function trigger() { button.dispatchEvent(new CustomEvent('clipboard-copy', { bubbles: true })); } if (text) { copyText(text).then(trigger); } else if (id) { const root = 'getRootNode' in Element.prototype ? button.getRootNode() : button.ownerDocument; if (!(root instanceof Document || 'ShadowRoot' in window && root instanceof ShadowRoot)) return; const node = root.getElementById(id); if (node) copyTarget(node).then(trigger); } } function copyTarget(content) { if (content instanceof HTMLInputElement || content instanceof HTMLTextAreaElement) { return copyText(content.value); } else if (content instanceof HTMLAnchorElement && content.hasAttribute('href')) { return copyText(content.href); } else { return copyNode(content); } } function clicked(event) { const button = event.currentTarget; if (button instanceof HTMLElement) { copy(button); } } function keydown(event) { if (event.key === ' ' || event.key === 'Enter') { const button = event.currentTarget; if (button instanceof HTMLElement) { event.preventDefault(); copy(button); } } } function focused(event) { event.currentTarget.addEventListener('keydown', keydown); } function blurred(event) { event.currentTarget.removeEventListener('keydown', keydown); } class ClipboardCopyElement extends HTMLElement { constructor() { super(); this.addEventListener('click', clicked); this.addEventListener('focus', focused); this.addEventListener('blur', blurred); } connectedCallback() { if (!this.hasAttribute('tabindex')) { this.setAttribute('tabindex', '0'); } if (!this.hasAttribute('role')) { this.setAttribute('role', 'button'); } } get value() { return this.getAttribute('value') || ''; } set value(text) { this.setAttribute('value', text); } } if (!window.customElements.get('clipboard-copy')) { window.ClipboardCopyElement = ClipboardCopyElement; window.customElements.define('clipboard-copy', ClipboardCopyElement); } return ClipboardCopyElement; })); document.addEventListener('clipboard-copy', function(event) { const notice = event.target.querySelector('.notice') notice.hidden = false setTimeout(function() { notice.hidden = true }, 1000) }) (function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { var widgetRendererSrc = 'https://unpkg.com/@jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); pre { line-height: 125%; } td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; } td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; } .highlight-ipynb .hll { background-color: var(--jp-cell-editor-active-background) } .highlight-ipynb { background: var(--jp-cell-editor-background); color: var(--jp-mirror-editor-variable-color) } .highlight-ipynb .c { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment */ .highlight-ipynb .err { color: var(--jp-mirror-editor-error-color) } /* Error */ .highlight-ipynb .k { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword */ .highlight-ipynb .o { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator */ .highlight-ipynb .p { color: var(--jp-mirror-editor-punctuation-color) } /* Punctuation */ .highlight-ipynb .ch { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Hashbang */ .highlight-ipynb .cm { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Multiline */ .highlight-ipynb .cp { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Preproc */ .highlight-ipynb .cpf { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.PreprocFile */ .highlight-ipynb .c1 { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Single */ .highlight-ipynb .cs { color: var(--jp-mirror-editor-comment-color); font-style: italic } /* Comment.Special */ .highlight-ipynb .kc { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Constant */ .highlight-ipynb .kd { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Declaration */ .highlight-ipynb .kn { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Namespace */ .highlight-ipynb .kp { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Pseudo */ .highlight-ipynb .kr { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Reserved */ .highlight-ipynb .kt { color: var(--jp-mirror-editor-keyword-color); font-weight: bold } /* Keyword.Type */ .highlight-ipynb .m { color: var(--jp-mirror-editor-number-color) } /* Literal.Number */ .highlight-ipynb .s { color: var(--jp-mirror-editor-string-color) } /* Literal.String */ .highlight-ipynb .ow { color: var(--jp-mirror-editor-operator-color); font-weight: bold } /* Operator.Word */ .highlight-ipynb .w { color: var(--jp-mirror-editor-variable-color) } /* Text.Whitespace */ .highlight-ipynb .mb { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Bin */ .highlight-ipynb .mf { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Float */ .highlight-ipynb .mh { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Hex */ .highlight-ipynb .mi { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer */ .highlight-ipynb .mo { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Oct */ .highlight-ipynb .sa { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Affix */ .highlight-ipynb .sb { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Backtick */ .highlight-ipynb .sc { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Char */ .highlight-ipynb .dl { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Delimiter */ .highlight-ipynb .sd { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Doc */ .highlight-ipynb .s2 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Double */ .highlight-ipynb .se { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Escape */ .highlight-ipynb .sh { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Heredoc */ .highlight-ipynb .si { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Interpol */ .highlight-ipynb .sx { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Other */ .highlight-ipynb .sr { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Regex */ .highlight-ipynb .s1 { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Single */ .highlight-ipynb .ss { color: var(--jp-mirror-editor-string-color) } /* Literal.String.Symbol */ .highlight-ipynb .il { color: var(--jp-mirror-editor-number-color) } /* Literal.Number.Integer.Long */ :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper{/*! Copyright 2015-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. *//*! Copyright 2017-present Palantir Technologies, Inc. All rights reserved. Licensed under the Apache License, Version 2.0. */}.jupyter-wrapper [data-jp-theme-scrollbars=true]{scrollbar-color:rgb(var(--jp-scrollbar-thumb-color)) var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar{scrollbar-color:rgba(var(--jp-scrollbar-thumb-color), 0.5) transparent}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-corner{background:var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-thumb{background:rgb(var(--jp-scrollbar-thumb-color));border:var(--jp-scrollbar-thumb-margin) solid transparent;background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-right:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] ::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color);border-bottom:var(--jp-scrollbar-endpad) solid var(--jp-scrollbar-background-color)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-corner,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-corner{background-color:transparent}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-thumb,.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-thumb{background:rgba(var(--jp-scrollbar-thumb-color), 0.5);border:var(--jp-scrollbar-thumb-margin) solid transparent;background-clip:content-box;border-radius:var(--jp-scrollbar-thumb-radius)}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-hscrollbar::-webkit-scrollbar-track:horizontal{border-left:var(--jp-scrollbar-endpad) solid transparent;border-right:var(--jp-scrollbar-endpad) solid transparent}.jupyter-wrapper [data-jp-theme-scrollbars=true] .CodeMirror-vscrollbar::-webkit-scrollbar-track:vertical{border-top:var(--jp-scrollbar-endpad) solid transparent;border-bottom:var(--jp-scrollbar-endpad) solid transparent}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{min-height:16px;max-height:16px;min-width:45px;border-top:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{min-width:16px;max-width:16px;min-height:45px;border-left:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar-button{background-color:#f0f0f0;background-position:center center;min-height:15px;max-height:15px;min-width:15px;max-width:15px}.jupyter-wrapper .lm-ScrollBar-button:hover{background-color:#dadada}.jupyter-wrapper .lm-ScrollBar-button.lm-mod-active{background-color:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-track{background:#f0f0f0}.jupyter-wrapper .lm-ScrollBar-thumb{background:#cdcdcd}.jupyter-wrapper .lm-ScrollBar-thumb:hover{background:#bababa}.jupyter-wrapper .lm-ScrollBar-thumb.lm-mod-active{background:#a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-thumb{height:100%;min-width:15px;border-left:1px solid #a0a0a0;border-right:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-thumb{width:100%;min-height:15px;border-top:1px solid #a0a0a0;border-bottom:1px solid #a0a0a0}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-left);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-right);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=decrement]{background-image:var(--jp-icon-caret-up);background-size:17px}.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical] .lm-ScrollBar-button[data-action=increment]{background-image:var(--jp-icon-caret-down);background-size:17px}.jupyter-wrapper .p-Widget,.jupyter-wrapper .lm-Widget{box-sizing:border-box;position:relative;overflow:hidden;cursor:default}.jupyter-wrapper .p-Widget.p-mod-hidden,.jupyter-wrapper .lm-Widget.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-CommandPalette,.jupyter-wrapper .lm-CommandPalette{display:flex;flex-direction:column;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-CommandPalette-search,.jupyter-wrapper .lm-CommandPalette-search{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-content,.jupyter-wrapper .lm-CommandPalette-content{flex:1 1 auto;margin:0;padding:0;min-height:0;overflow:auto;list-style-type:none}.jupyter-wrapper .p-CommandPalette-header,.jupyter-wrapper .lm-CommandPalette-header{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-CommandPalette-item,.jupyter-wrapper .lm-CommandPalette-item{display:flex;flex-direction:row}.jupyter-wrapper .p-CommandPalette-itemIcon,.jupyter-wrapper .lm-CommandPalette-itemIcon{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemContent,.jupyter-wrapper .lm-CommandPalette-itemContent{flex:1 1 auto;overflow:hidden}.jupyter-wrapper .p-CommandPalette-itemShortcut,.jupyter-wrapper .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .p-CommandPalette-itemLabel,.jupyter-wrapper .lm-CommandPalette-itemLabel{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .p-DockPanel,.jupyter-wrapper .lm-DockPanel{z-index:0}.jupyter-wrapper .p-DockPanel-widget,.jupyter-wrapper .lm-DockPanel-widget{z-index:0}.jupyter-wrapper .p-DockPanel-tabBar,.jupyter-wrapper .lm-DockPanel-tabBar{z-index:1}.jupyter-wrapper .p-DockPanel-handle,.jupyter-wrapper .lm-DockPanel-handle{z-index:2}.jupyter-wrapper .p-DockPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-DockPanel-handle:after,.jupyter-wrapper .lm-DockPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]{cursor:ew-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical],.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]{cursor:ns-resize}.jupyter-wrapper .p-DockPanel-handle[data-orientation=horizontal]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=horizontal]:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-DockPanel-handle[data-orientation=vertical]:after,.jupyter-wrapper .lm-DockPanel-handle[data-orientation=vertical]:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-DockPanel-overlay,.jupyter-wrapper .lm-DockPanel-overlay{z-index:3;box-sizing:border-box;pointer-events:none}.jupyter-wrapper .p-DockPanel-overlay.p-mod-hidden,.jupyter-wrapper .lm-DockPanel-overlay.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-Menu,.jupyter-wrapper .lm-Menu{z-index:10000;position:absolute;white-space:nowrap;overflow-x:hidden;overflow-y:auto;outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-Menu-content,.jupyter-wrapper .lm-Menu-content{margin:0;padding:0;display:table;list-style-type:none}.jupyter-wrapper .p-Menu-item,.jupyter-wrapper .lm-Menu-item{display:table-row}.jupyter-wrapper .p-Menu-item.p-mod-hidden,.jupyter-wrapper .p-Menu-item.p-mod-collapsed,.jupyter-wrapper .lm-Menu-item.lm-mod-hidden,.jupyter-wrapper .lm-Menu-item.lm-mod-collapsed{display:none !important}.jupyter-wrapper .p-Menu-itemIcon,.jupyter-wrapper .p-Menu-itemSubmenuIcon,.jupyter-wrapper .lm-Menu-itemIcon,.jupyter-wrapper .lm-Menu-itemSubmenuIcon{display:table-cell;text-align:center}.jupyter-wrapper .p-Menu-itemLabel,.jupyter-wrapper .lm-Menu-itemLabel{display:table-cell;text-align:left}.jupyter-wrapper .p-Menu-itemShortcut,.jupyter-wrapper .lm-Menu-itemShortcut{display:table-cell;text-align:right}.jupyter-wrapper .p-MenuBar,.jupyter-wrapper .lm-MenuBar{outline:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-MenuBar-content,.jupyter-wrapper .lm-MenuBar-content{margin:0;padding:0;display:flex;flex-direction:row;list-style-type:none}.jupyter-wrapper .p--MenuBar-item,.jupyter-wrapper .lm-MenuBar-item{box-sizing:border-box}.jupyter-wrapper .p-MenuBar-itemIcon,.jupyter-wrapper .p-MenuBar-itemLabel,.jupyter-wrapper .lm-MenuBar-itemIcon,.jupyter-wrapper .lm-MenuBar-itemLabel{display:inline-block}.jupyter-wrapper .p-ScrollBar,.jupyter-wrapper .lm-ScrollBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-ScrollBar[data-orientation=horizontal],.jupyter-wrapper .lm-ScrollBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-ScrollBar[data-orientation=vertical],.jupyter-wrapper .lm-ScrollBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-ScrollBar-button,.jupyter-wrapper .lm-ScrollBar-button{box-sizing:border-box;flex:0 0 auto}.jupyter-wrapper .p-ScrollBar-track,.jupyter-wrapper .lm-ScrollBar-track{box-sizing:border-box;position:relative;overflow:hidden;flex:1 1 auto}.jupyter-wrapper .p-ScrollBar-thumb,.jupyter-wrapper .lm-ScrollBar-thumb{box-sizing:border-box;position:absolute}.jupyter-wrapper .p-SplitPanel-child,.jupyter-wrapper .lm-SplitPanel-child{z-index:0}.jupyter-wrapper .p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel-handle{z-index:1}.jupyter-wrapper .p-SplitPanel-handle.p-mod-hidden,.jupyter-wrapper .lm-SplitPanel-handle.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel-handle:after{position:absolute;top:0;left:0;width:100%;height:100%;content:\"\"}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle{cursor:ew-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle{cursor:ns-resize}.jupyter-wrapper .p-SplitPanel[data-orientation=horizontal]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=horizontal]>.lm-SplitPanel-handle:after{left:50%;min-width:8px;transform:translateX(-50%)}.jupyter-wrapper .p-SplitPanel[data-orientation=vertical]>.p-SplitPanel-handle:after,.jupyter-wrapper .lm-SplitPanel[data-orientation=vertical]>.lm-SplitPanel-handle:after{top:50%;min-height:8px;transform:translateY(-50%)}.jupyter-wrapper .p-TabBar,.jupyter-wrapper .lm-TabBar{display:flex;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal],.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical],.jupyter-wrapper .lm-TabBar[data-orientation=vertical]{flex-direction:column}.jupyter-wrapper .p-TabBar-content,.jupyter-wrapper .lm-TabBar-content{margin:0;padding:0;display:flex;flex:1 1 auto;list-style-type:none}.jupyter-wrapper .p-TabBar[data-orientation=horizontal]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=horizontal]>.lm-TabBar-content{flex-direction:row}.jupyter-wrapper .p-TabBar[data-orientation=vertical]>.p-TabBar-content,.jupyter-wrapper .lm-TabBar[data-orientation=vertical]>.lm-TabBar-content{flex-direction:column}.jupyter-wrapper .p-TabBar-tab,.jupyter-wrapper .lm-TabBar-tab{display:flex;flex-direction:row;box-sizing:border-box;overflow:hidden}.jupyter-wrapper .p-TabBar-tabIcon,.jupyter-wrapper .p-TabBar-tabCloseIcon,.jupyter-wrapper .lm-TabBar-tabIcon,.jupyter-wrapper .lm-TabBar-tabCloseIcon{flex:0 0 auto}.jupyter-wrapper .p-TabBar-tabLabel,.jupyter-wrapper .lm-TabBar-tabLabel{flex:1 1 auto;overflow:hidden;white-space:nowrap}.jupyter-wrapper .p-TabBar-tab.p-mod-hidden,.jupyter-wrapper .lm-TabBar-tab.lm-mod-hidden{display:none !important}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging .lm-TabBar-tab{position:relative}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=horizontal] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=horizontal] .lm-TabBar-tab{left:0;transition:left 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging[data-orientation=vertical] .p-TabBar-tab,.jupyter-wrapper .lm-TabBar.lm-mod-dragging[data-orientation=vertical] .lm-TabBar-tab{top:0;transition:top 150ms ease}.jupyter-wrapper .p-TabBar.p-mod-dragging .p-TabBar-tab.p-mod-dragging .lm-TabBar.lm-mod-dragging .lm-TabBar-tab.lm-mod-dragging{transition:none}.jupyter-wrapper .p-TabPanel-tabBar,.jupyter-wrapper .lm-TabPanel-tabBar{z-index:1}.jupyter-wrapper .p-TabPanel-stackedPanel,.jupyter-wrapper .lm-TabPanel-stackedPanel{z-index:0}.jupyter-wrapper ::-moz-selection{background:rgba(125,188,255,.6)}.jupyter-wrapper ::selection{background:rgba(125,188,255,.6)}.jupyter-wrapper .bp3-heading{color:#182026;font-weight:600;margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-dark .bp3-heading{color:#f5f8fa}.jupyter-wrapper h1.bp3-heading,.jupyter-wrapper .bp3-running-text h1{line-height:40px;font-size:36px}.jupyter-wrapper h2.bp3-heading,.jupyter-wrapper .bp3-running-text h2{line-height:32px;font-size:28px}.jupyter-wrapper h3.bp3-heading,.jupyter-wrapper .bp3-running-text h3{line-height:25px;font-size:22px}.jupyter-wrapper h4.bp3-heading,.jupyter-wrapper .bp3-running-text h4{line-height:21px;font-size:18px}.jupyter-wrapper h5.bp3-heading,.jupyter-wrapper .bp3-running-text h5{line-height:19px;font-size:16px}.jupyter-wrapper h6.bp3-heading,.jupyter-wrapper .bp3-running-text h6{line-height:16px;font-size:14px}.jupyter-wrapper .bp3-ui-text{text-transform:none;line-height:1.28581;letter-spacing:0;font-size:14px;font-weight:400}.jupyter-wrapper .bp3-monospace-text{text-transform:none;font-family:monospace}.jupyter-wrapper .bp3-text-muted{color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-text-muted{color:#a7b6c2}.jupyter-wrapper .bp3-text-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-text-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-text-overflow-ellipsis{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal}.jupyter-wrapper .bp3-running-text{line-height:1.5;font-size:14px}.jupyter-wrapper .bp3-running-text h1{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h1{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h2{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h2{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h3{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h3{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h4{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h4{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h5{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h5{color:#f5f8fa}.jupyter-wrapper .bp3-running-text h6{color:#182026;font-weight:600;margin-top:40px;margin-bottom:20px}.jupyter-wrapper .bp3-dark .bp3-running-text h6{color:#f5f8fa}.jupyter-wrapper .bp3-running-text hr{margin:20px 0;border:none;border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text hr{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-running-text p{margin:0 0 10px;padding:0}.jupyter-wrapper .bp3-text-large{font-size:16px}.jupyter-wrapper .bp3-text-small{font-size:12px}.jupyter-wrapper a{text-decoration:none;color:#106ba3}.jupyter-wrapper a:hover{cursor:pointer;text-decoration:underline;color:#106ba3}.jupyter-wrapper a .bp3-icon,.jupyter-wrapper a .bp3-icon-standard,.jupyter-wrapper a .bp3-icon-large{color:inherit}.jupyter-wrapper a code,.jupyter-wrapper .bp3-dark a code{color:inherit}.jupyter-wrapper .bp3-dark a,.jupyter-wrapper .bp3-dark a:hover{color:#48aff0}.jupyter-wrapper .bp3-dark a .bp3-icon,.jupyter-wrapper .bp3-dark a .bp3-icon-standard,.jupyter-wrapper .bp3-dark a .bp3-icon-large,.jupyter-wrapper .bp3-dark a:hover .bp3-icon,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-standard,.jupyter-wrapper .bp3-dark a:hover .bp3-icon-large{color:inherit}.jupyter-wrapper .bp3-running-text code,.jupyter-wrapper .bp3-code{text-transform:none;font-family:monospace;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2);background:rgba(255,255,255,.7);padding:2px 5px;color:#5c7080;font-size:smaller}.jupyter-wrapper .bp3-dark .bp3-running-text code,.jupyter-wrapper .bp3-running-text .bp3-dark code,.jupyter-wrapper .bp3-dark .bp3-code{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#a7b6c2}.jupyter-wrapper .bp3-running-text a>code,.jupyter-wrapper a>.bp3-code{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-running-text a>code,.jupyter-wrapper .bp3-running-text .bp3-dark a>code,.jupyter-wrapper .bp3-dark a>.bp3-code{color:inherit}.jupyter-wrapper .bp3-running-text pre,.jupyter-wrapper .bp3-code-block{text-transform:none;font-family:monospace;display:block;margin:10px 0;border-radius:3px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);background:rgba(255,255,255,.7);padding:13px 15px 12px;line-height:1.4;color:#182026;font-size:13px;word-break:break-all;word-wrap:break-word}.jupyter-wrapper .bp3-dark .bp3-running-text pre,.jupyter-wrapper .bp3-running-text .bp3-dark pre,.jupyter-wrapper .bp3-dark .bp3-code-block{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-running-text pre>code,.jupyter-wrapper .bp3-code-block>code{-webkit-box-shadow:none;box-shadow:none;background:none;padding:0;color:inherit;font-size:inherit}.jupyter-wrapper .bp3-running-text kbd,.jupyter-wrapper .bp3-key{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background:#fff;min-width:24px;height:24px;padding:3px 6px;vertical-align:middle;line-height:24px;color:#5c7080;font-family:inherit;font-size:12px}.jupyter-wrapper .bp3-running-text kbd .bp3-icon,.jupyter-wrapper .bp3-key .bp3-icon,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-standard,.jupyter-wrapper .bp3-key .bp3-icon-standard,.jupyter-wrapper .bp3-running-text kbd .bp3-icon-large,.jupyter-wrapper .bp3-key .bp3-icon-large{margin-right:5px}.jupyter-wrapper .bp3-dark .bp3-running-text kbd,.jupyter-wrapper .bp3-running-text .bp3-dark kbd,.jupyter-wrapper .bp3-dark .bp3-key{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);background:#394b59;color:#a7b6c2}.jupyter-wrapper .bp3-running-text blockquote,.jupyter-wrapper .bp3-blockquote{margin:0 0 10px;border-left:solid 4px rgba(167,182,194,.5);padding:0 20px}.jupyter-wrapper .bp3-dark .bp3-running-text blockquote,.jupyter-wrapper .bp3-running-text .bp3-dark blockquote,.jupyter-wrapper .bp3-dark .bp3-blockquote{border-color:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-running-text ul,.jupyter-wrapper .bp3-running-text ol,.jupyter-wrapper .bp3-list{margin:10px 0;padding-left:30px}.jupyter-wrapper .bp3-running-text ul li:not(:last-child),.jupyter-wrapper .bp3-running-text ol li:not(:last-child),.jupyter-wrapper .bp3-list li:not(:last-child){margin-bottom:5px}.jupyter-wrapper .bp3-running-text ul ol,.jupyter-wrapper .bp3-running-text ol ol,.jupyter-wrapper .bp3-list ol,.jupyter-wrapper .bp3-running-text ul ul,.jupyter-wrapper .bp3-running-text ol ul,.jupyter-wrapper .bp3-list ul{margin-top:5px}.jupyter-wrapper .bp3-list-unstyled{margin:0;padding:0;list-style:none}.jupyter-wrapper .bp3-list-unstyled li{padding:0}.jupyter-wrapper .bp3-rtl{text-align:right}.jupyter-wrapper .bp3-dark{color:#f5f8fa}.jupyter-wrapper :focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-focus-disabled :focus{outline:none !important}.jupyter-wrapper .bp3-focus-disabled :focus~.bp3-control-indicator{outline:none !important}.jupyter-wrapper .bp3-alert{max-width:400px;padding:20px}.jupyter-wrapper .bp3-alert-body{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-alert-body .bp3-icon{margin-top:0;margin-right:20px;font-size:40px}.jupyter-wrapper .bp3-alert-footer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:reverse;-ms-flex-direction:row-reverse;flex-direction:row-reverse;margin-top:10px}.jupyter-wrapper .bp3-alert-footer .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-breadcrumbs{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0;cursor:default;height:30px;padding:0;list-style:none}.jupyter-wrapper .bp3-breadcrumbs>li{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-breadcrumbs>li::after{display:block;margin:0 5px;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M10.71 7.29l-4-4a1.003 1.003 0 0 0-1.42 1.42L8.59 8 5.3 11.29c-.19.18-.3.43-.3.71a1.003 1.003 0 0 0 1.71.71l4-4c.18-.18.29-.43.29-.71 0-.28-.11-.53-.29-.71z' fill='%235C7080'/%3e%3c/svg%3e\");width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs>li:last-of-type::after{display:none}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumb-current,.jupyter-wrapper .bp3-breadcrumbs-collapsed{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px}.jupyter-wrapper .bp3-breadcrumb,.jupyter-wrapper .bp3-breadcrumbs-collapsed{color:#5c7080}.jupyter-wrapper .bp3-breadcrumb:hover{text-decoration:none}.jupyter-wrapper .bp3-breadcrumb.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-breadcrumb .bp3-icon{margin-right:5px}.jupyter-wrapper .bp3-breadcrumb-current{color:inherit;font-weight:600}.jupyter-wrapper .bp3-breadcrumb-current .bp3-input{vertical-align:baseline;font-size:inherit;font-weight:inherit}.jupyter-wrapper .bp3-breadcrumbs-collapsed{margin-right:2px;border:none;border-radius:3px;background:#ced9e0;cursor:pointer;padding:1px 5px;vertical-align:text-bottom}.jupyter-wrapper .bp3-breadcrumbs-collapsed::before{display:block;background:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cg fill='%235C7080'%3e%3ccircle cx='2' cy='8.03' r='2'/%3e%3ccircle cx='14' cy='8.03' r='2'/%3e%3ccircle cx='8' cy='8.03' r='2'/%3e%3c/g%3e%3c/svg%3e\") center no-repeat;width:16px;height:16px;content:\"\"}.jupyter-wrapper .bp3-breadcrumbs-collapsed:hover{background:#bfccd6;text-decoration:none;color:#182026}.jupyter-wrapper .bp3-dark .bp3-breadcrumb,.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs>li::after{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-breadcrumb.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-breadcrumb-current{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-breadcrumbs-collapsed:hover{background:rgba(16,22,26,.6);color:#f5f8fa}.jupyter-wrapper .bp3-button{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;min-width:30px;min-height:30px}.jupyter-wrapper .bp3-button>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-button>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-button::before,.jupyter-wrapper .bp3-button>*{margin-right:7px}.jupyter-wrapper .bp3-button:empty::before,.jupyter-wrapper .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button:empty{padding:0 !important}.jupyter-wrapper .bp3-button:disabled,.jupyter-wrapper .bp3-button.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-button.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button.bp3-align-right,.jupyter-wrapper .bp3-align-right .bp3-button{text-align:right}.jupyter-wrapper .bp3-button.bp3-align-left,.jupyter-wrapper .bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active:hover,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-button.bp3-intent-primary{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-primary:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-intent-primary.bp3-disabled{border-color:transparent;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-success{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0f9960;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-success:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#0d8050}.jupyter-wrapper .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0a6640;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-intent-success.bp3-disabled{border-color:transparent;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(15,153,96,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-warning{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#d9822b;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-warning:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#bf7326}.jupyter-wrapper .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a66321;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-intent-warning.bp3-disabled{border-color:transparent;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(217,130,43,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button.bp3-intent-danger{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#db3737;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{color:#fff}.jupyter-wrapper .bp3-button.bp3-intent-danger:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#c23030}.jupyter-wrapper .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#a82a2a;background-image:none}.jupyter-wrapper .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-intent-danger.bp3-disabled{border-color:transparent;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(219,55,55,.5);background-image:none;color:rgba(255,255,255,.6)}.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#fff}.jupyter-wrapper .bp3-button.bp3-large,.jupyter-wrapper .bp3-large .bp3-button{min-width:40px;min-height:40px;padding:5px 15px;font-size:16px}.jupyter-wrapper .bp3-button.bp3-large::before,.jupyter-wrapper .bp3-button.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-button::before,.jupyter-wrapper .bp3-large .bp3-button>*{margin-right:10px}.jupyter-wrapper .bp3-button.bp3-large:empty::before,.jupyter-wrapper .bp3-button.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-button:empty::before,.jupyter-wrapper .bp3-large .bp3-button>:last-child{margin-right:0}.jupyter-wrapper .bp3-button.bp3-small,.jupyter-wrapper .bp3-small .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-button.bp3-loading{position:relative}.jupyter-wrapper .bp3-button.bp3-loading[class*=bp3-icon-]::before{visibility:hidden}.jupyter-wrapper .bp3-button.bp3-loading .bp3-button-spinner{position:absolute;margin:0}.jupyter-wrapper .bp3-button.bp3-loading>:not(.bp3-button-spinner){visibility:hidden}.jupyter-wrapper .bp3-button[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon,.jupyter-wrapper .bp3-button .bp3-icon-standard,.jupyter-wrapper .bp3-button .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-button .bp3-icon.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-standard.bp3-align-right,.jupyter-wrapper .bp3-button .bp3-icon-large.bp3-align-right{margin-left:7px}.jupyter-wrapper .bp3-button .bp3-icon:first-child:last-child,.jupyter-wrapper .bp3-button .bp3-spinner+.bp3-icon:last-child{margin:0 -7px}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]){-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]):disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]).bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-])[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-button:not([class*=bp3-intent-]) .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-].bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-image:none;color:rgba(255,255,255,.3)}.jupyter-wrapper .bp3-dark .bp3-button[class*=bp3-intent-] .bp3-button-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-button:disabled::before,.jupyter-wrapper .bp3-button:disabled .bp3-icon,.jupyter-wrapper .bp3-button:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button:disabled .bp3-icon-large,.jupyter-wrapper .bp3-button.bp3-disabled::before,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-standard,.jupyter-wrapper .bp3-button.bp3-disabled .bp3-icon-large,.jupyter-wrapper .bp3-button[class*=bp3-intent-]::before,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-standard,.jupyter-wrapper .bp3-button[class*=bp3-intent-] .bp3-icon-large{color:inherit !important}.jupyter-wrapper .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button.bp3-minimal:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button.bp3-minimal.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button.bp3-minimal.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper a.bp3-button{text-align:center;text-decoration:none;-webkit-transition:none;transition:none}.jupyter-wrapper a.bp3-button,.jupyter-wrapper a.bp3-button:hover,.jupyter-wrapper a.bp3-button:active{color:#182026}.jupyter-wrapper a.bp3-button.bp3-disabled{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-text{-webkit-box-flex:0;-ms-flex:0 1 auto;flex:0 1 auto}.jupyter-wrapper .bp3-button.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button.bp3-align-right .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button-text,.jupyter-wrapper .bp3-button-group.bp3-align-right .bp3-button-text{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex}.jupyter-wrapper .bp3-button-group .bp3-button{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;z-index:4}.jupyter-wrapper .bp3-button-group .bp3-button:focus{z-index:5}.jupyter-wrapper .bp3-button-group .bp3-button:hover{z-index:6}.jupyter-wrapper .bp3-button-group .bp3-button:active,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-active{z-index:7}.jupyter-wrapper .bp3-button-group .bp3-button:disabled,.jupyter-wrapper .bp3-button-group .bp3-button.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]{z-index:9}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:focus{z-index:10}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:hover{z-index:11}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:active,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-active{z-index:12}.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-]:disabled,.jupyter-wrapper .bp3-button-group .bp3-button[class*=bp3-intent-].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:first-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:first-child){border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:-1px;border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-minimal .bp3-button.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-button-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-button-group .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-button-group .bp3-button.bp3-fill,.jupyter-wrapper .bp3-button-group.bp3-fill .bp3-button:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-button-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;vertical-align:top}.jupyter-wrapper .bp3-button-group.bp3-vertical.bp3-fill{width:unset;height:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical .bp3-button{margin-right:0 !important;width:100%}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:first-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:first-child{border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:last-child .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-button-group.bp3-vertical:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-bottom:-1px}.jupyter-wrapper .bp3-button-group.bp3-align-left .bp3-button{text-align:left}.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group:not(.bp3-minimal)>.bp3-button:not(:last-child){margin-right:1px}.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-popover-wrapper:not(:last-child) .bp3-button,.jupyter-wrapper .bp3-dark .bp3-button-group.bp3-vertical>.bp3-button:not(:last-child){margin-bottom:1px}.jupyter-wrapper .bp3-callout{line-height:1.5;font-size:14px;position:relative;border-radius:3px;background-color:rgba(138,155,168,.15);width:100%;padding:10px 12px 9px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]{padding-left:40px}.jupyter-wrapper .bp3-callout[class*=bp3-icon-]::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout.bp3-callout-icon{padding-left:40px}.jupyter-wrapper .bp3-callout.bp3-callout-icon>.bp3-icon:first-child{position:absolute;top:10px;left:10px;color:#5c7080}.jupyter-wrapper .bp3-callout .bp3-heading{margin-top:0;margin-bottom:5px;line-height:20px}.jupyter-wrapper .bp3-callout .bp3-heading:last-child{margin-bottom:0}.jupyter-wrapper .bp3-dark .bp3-callout{background-color:rgba(138,155,168,.2)}.jupyter-wrapper .bp3-dark .bp3-callout[class*=bp3-icon-]::before{color:#a7b6c2}.jupyter-wrapper .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-primary .bp3-heading{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-primary .bp3-heading{color:#48aff0}.jupyter-wrapper .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-success .bp3-heading{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-success .bp3-heading{color:#3dcc91}.jupyter-wrapper .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-warning .bp3-heading{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-warning .bp3-heading{color:#ffb366}.jupyter-wrapper .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.15)}.jupyter-wrapper .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-callout.bp3-intent-danger .bp3-heading{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger[class*=bp3-icon-]::before,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger>.bp3-icon:first-child,.jupyter-wrapper .bp3-dark .bp3-callout.bp3-intent-danger .bp3-heading{color:#ff7373}.jupyter-wrapper .bp3-running-text .bp3-callout{margin:20px 0}.jupyter-wrapper .bp3-card{border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#fff;padding:20px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-card.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);background-color:#30404d}.jupyter-wrapper .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.15),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-0.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-0{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0);box-shadow:0 0 0 1px rgba(16,22,26,.4),0 0 0 rgba(16,22,26,0),0 0 0 rgba(16,22,26,0)}.jupyter-wrapper .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-1.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-1{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 1px 1px rgba(16,22,26,.2),0 2px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-2.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-2{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.4),0 2px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-3.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-3{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-elevation-4.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-elevation-4{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);cursor:pointer}.jupyter-wrapper .bp3-card.bp3-interactive:hover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-card.bp3-interactive:active{opacity:.9;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);-webkit-transition-duration:0;transition-duration:0}.jupyter-wrapper .bp3-card.bp3-interactive:active.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-card.bp3-interactive:active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-collapse{height:0;overflow-y:hidden;-webkit-transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:height 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body{-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-collapse .bp3-collapse-body[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-context-menu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-context-menu-popover-target{position:fixed}.jupyter-wrapper .bp3-divider{margin:5px;border-right:1px solid rgba(16,22,26,.15);border-bottom:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-divider{border-color:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dialog-container{opacity:1;-webkit-transform:scale(1);transform:scale(1);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;min-height:100%;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-enter-active>.bp3-dialog,.jupyter-wrapper .bp3-dialog-container.bp3-overlay-appear-active>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit>.bp3-dialog{opacity:1;-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-dialog-container.bp3-overlay-exit-active>.bp3-dialog{opacity:0;-webkit-transform:scale(0.5);transform:scale(0.5);-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:opacity,transform;transition-property:opacity,transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-dialog{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:30px 0;border-radius:6px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#ebf1f5;width:500px;padding-bottom:20px;pointer-events:all;-webkit-user-select:text;-moz-user-select:text;-ms-user-select:text;user-select:text}.jupyter-wrapper .bp3-dialog:focus{outline:0}.jupyter-wrapper .bp3-dialog.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-dialog{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#293742;color:#f5f8fa}.jupyter-wrapper .bp3-dialog-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:6px 6px 0 0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);background:#fff;min-height:40px;padding-right:5px;padding-left:20px}.jupyter-wrapper .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dialog-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-dialog-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-dialog-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-dialog-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4);background:#30404d}.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-dialog-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dialog-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:20px;line-height:18px}.jupyter-wrapper .bp3-dialog-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin:0 20px}.jupyter-wrapper .bp3-dialog-footer-actions{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-ms-flex-pack:end;justify-content:flex-end}.jupyter-wrapper .bp3-dialog-footer-actions .bp3-button{margin-left:10px}.jupyter-wrapper .bp3-drawer{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background:#fff;padding:0}.jupyter-wrapper .bp3-drawer:focus{outline:0}.jupyter-wrapper .bp3-drawer.bp3-position-top{top:0;right:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear{-webkit-transform:translateY(-100%);transform:translateY(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-top.bp3-overlay-exit-active{-webkit-transform:translateY(-100%);transform:translateY(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer.bp3-position-bottom.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left{top:0;bottom:0;left:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear{-webkit-transform:translateX(-100%);transform:translateX(-100%)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-left.bp3-overlay-exit-active{-webkit-transform:translateX(-100%);transform:translateX(-100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right{top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer.bp3-position-right.bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical){top:0;right:0;bottom:0;width:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear{-webkit-transform:translateX(100%);transform:translateX(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-appear-active{-webkit-transform:translateX(0);transform:translateX(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit{-webkit-transform:translateX(0);transform:translateX(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right):not(.bp3-vertical).bp3-overlay-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical{right:0;bottom:0;left:0;height:50%}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear{-webkit-transform:translateY(100%);transform:translateY(100%)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-enter-active,.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-drawer:not(.bp3-position-top):not(.bp3-position-bottom):not(.bp3-position-left):not(.bp3-position-right).bp3-vertical.bp3-overlay-exit-active{-webkit-transform:translateY(100%);transform:translateY(100%);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-drawer.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-drawer{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-drawer-header{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border-radius:0;-webkit-box-shadow:0 1px 0 rgba(16,22,26,.15);box-shadow:0 1px 0 rgba(16,22,26,.15);min-height:40px;padding:5px;padding-left:20px}.jupyter-wrapper .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-drawer-header .bp3-icon{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;margin-right:10px;color:#5c7080}.jupyter-wrapper .bp3-drawer-header .bp3-heading{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;margin:0;line-height:inherit}.jupyter-wrapper .bp3-drawer-header .bp3-heading:last-child{margin-right:20px}.jupyter-wrapper .bp3-dark .bp3-drawer-header{-webkit-box-shadow:0 1px 0 rgba(16,22,26,.4);box-shadow:0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon-large,.jupyter-wrapper .bp3-dark .bp3-drawer-header .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-drawer-body{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;overflow:auto;line-height:18px}.jupyter-wrapper .bp3-drawer-footer{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);padding:10px 20px}.jupyter-wrapper .bp3-dark .bp3-drawer-footer{-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.4);box-shadow:inset 0 1px 0 rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text{display:inline-block;position:relative;cursor:text;max-width:100%;vertical-align:top;white-space:nowrap}.jupyter-wrapper .bp3-editable-text::before{position:absolute;top:-3px;right:-3px;bottom:-3px;left:-3px;border-radius:3px;content:\"\";-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9),box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#137cbd}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(19,124,189,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#0f9960}.jupyter-wrapper .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px rgba(15,153,96,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#d9822b}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px rgba(217,130,43,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#db3737}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px rgba(219,55,55,.4)}.jupyter-wrapper .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-editable-text:hover::before{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-disabled::before{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary .bp3-editable-text-content{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary:hover::before{-webkit-box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4);box-shadow:0 0 0 0 rgba(72,175,240,0),0 0 0 0 rgba(72,175,240,0),inset 0 0 0 1px rgba(72,175,240,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-primary.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #48aff0,0 0 0 3px rgba(72,175,240,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success .bp3-editable-text-content{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success:hover::before{-webkit-box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4);box-shadow:0 0 0 0 rgba(61,204,145,0),0 0 0 0 rgba(61,204,145,0),inset 0 0 0 1px rgba(61,204,145,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-success.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #3dcc91,0 0 0 3px rgba(61,204,145,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning .bp3-editable-text-content{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4);box-shadow:0 0 0 0 rgba(255,179,102,0),0 0 0 0 rgba(255,179,102,0),inset 0 0 0 1px rgba(255,179,102,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-warning.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ffb366,0 0 0 3px rgba(255,179,102,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger .bp3-editable-text-content{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger:hover::before{-webkit-box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4);box-shadow:0 0 0 0 rgba(255,115,115,0),0 0 0 0 rgba(255,115,115,0),inset 0 0 0 1px rgba(255,115,115,.4)}.jupyter-wrapper .bp3-dark .bp3-editable-text.bp3-intent-danger.bp3-editable-text-editing::before{-webkit-box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #ff7373,0 0 0 3px rgba(255,115,115,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-editable-text-input,.jupyter-wrapper .bp3-editable-text-content{display:inherit;position:relative;min-width:inherit;max-width:inherit;vertical-align:top;text-transform:inherit;letter-spacing:inherit;color:inherit;font:inherit;resize:none}.jupyter-wrapper .bp3-editable-text-input{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;width:100%;padding:0;white-space:pre-wrap}.jupyter-wrapper .bp3-editable-text-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-editable-text-input:focus{outline:none}.jupyter-wrapper .bp3-editable-text-input::-ms-clear{display:none}.jupyter-wrapper .bp3-editable-text-content{overflow:hidden;padding-right:2px;text-overflow:ellipsis;white-space:pre}.jupyter-wrapper .bp3-editable-text-editing>.bp3-editable-text-content{position:absolute;left:0;visibility:hidden}.jupyter-wrapper .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-editable-text-placeholder>.bp3-editable-text-content{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-editable-text.bp3-multiline{display:block}.jupyter-wrapper .bp3-editable-text.bp3-multiline .bp3-editable-text-content{overflow:auto;white-space:pre-wrap;word-wrap:break-word}.jupyter-wrapper .bp3-control-group{-webkit-transform:translateZ(0);transform:translateZ(0);display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-control-group>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select,.jupyter-wrapper .bp3-control-group .bp3-input,.jupyter-wrapper .bp3-control-group .bp3-select{position:relative}.jupyter-wrapper .bp3-control-group .bp3-input{z-index:2;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-input:focus{z-index:14;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input[class*=bp3-intent]:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-input[readonly],.jupyter-wrapper .bp3-control-group .bp3-input:disabled,.jupyter-wrapper .bp3-control-group .bp3-input.bp3-disabled{z-index:1}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input{z-index:13}.jupyter-wrapper .bp3-control-group .bp3-input-group[class*=bp3-intent] .bp3-input:focus{z-index:15}.jupyter-wrapper .bp3-control-group .bp3-button,.jupyter-wrapper .bp3-control-group .bp3-html-select select,.jupyter-wrapper .bp3-control-group .bp3-select select{-webkit-transform:translateZ(0);transform:translateZ(0);z-index:4;border-radius:inherit}.jupyter-wrapper .bp3-control-group .bp3-button:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select:focus,.jupyter-wrapper .bp3-control-group .bp3-select select:focus{z-index:5}.jupyter-wrapper .bp3-control-group .bp3-button:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select:hover,.jupyter-wrapper .bp3-control-group .bp3-select select:hover{z-index:6}.jupyter-wrapper .bp3-control-group .bp3-button:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select:active,.jupyter-wrapper .bp3-control-group .bp3-select select:active{z-index:7}.jupyter-wrapper .bp3-control-group .bp3-button[readonly],.jupyter-wrapper .bp3-control-group .bp3-button:disabled,.jupyter-wrapper .bp3-control-group .bp3-button.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[readonly],.jupyter-wrapper .bp3-control-group .bp3-select select:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select.bp3-disabled{z-index:3}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]{z-index:9}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:focus,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:focus{z-index:10}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:hover,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:hover{z-index:11}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:active,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:active{z-index:12}.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-button[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-html-select select[class*=bp3-intent].bp3-disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent][readonly],.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent]:disabled,.jupyter-wrapper .bp3-control-group .bp3-select select[class*=bp3-intent].bp3-disabled{z-index:8}.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-control-group .bp3-input-group>.bp3-input-action{z-index:16}.jupyter-wrapper .bp3-control-group .bp3-select::after,.jupyter-wrapper .bp3-control-group .bp3-html-select::after,.jupyter-wrapper .bp3-control-group .bp3-select>.bp3-icon,.jupyter-wrapper .bp3-control-group .bp3-html-select>.bp3-icon{z-index:17}.jupyter-wrapper .bp3-control-group:not(.bp3-vertical)>*{margin-right:-1px}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>*{margin-right:0}.jupyter-wrapper .bp3-dark .bp3-control-group:not(.bp3-vertical)>.bp3-button+.bp3-button{margin-left:1px}.jupyter-wrapper .bp3-control-group .bp3-popover-wrapper,.jupyter-wrapper .bp3-control-group .bp3-popover-target{border-radius:inherit}.jupyter-wrapper .bp3-control-group>:first-child{border-radius:3px 0 0 3px}.jupyter-wrapper .bp3-control-group>:last-child{margin-right:0;border-radius:0 3px 3px 0}.jupyter-wrapper .bp3-control-group>:only-child{margin-right:0;border-radius:3px}.jupyter-wrapper .bp3-control-group .bp3-input-group .bp3-button{border-radius:3px}.jupyter-wrapper .bp3-control-group>.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-fill>*:not(.bp3-fixed){-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto}.jupyter-wrapper .bp3-control-group.bp3-vertical{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column}.jupyter-wrapper .bp3-control-group.bp3-vertical>*{margin-top:-1px}.jupyter-wrapper .bp3-control-group.bp3-vertical>:first-child{margin-top:0;border-radius:3px 3px 0 0}.jupyter-wrapper .bp3-control-group.bp3-vertical>:last-child{border-radius:0 0 3px 3px}.jupyter-wrapper .bp3-control{display:block;position:relative;margin-bottom:10px;cursor:pointer;text-transform:none}.jupyter-wrapper .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control:hover input:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active:checked~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control input:disabled:checked~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control:not(.bp3-align-right){padding-left:26px}.jupyter-wrapper .bp3-control:not(.bp3-align-right) .bp3-control-indicator{margin-left:-26px}.jupyter-wrapper .bp3-control.bp3-align-right{padding-right:26px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{margin-right:-26px}.jupyter-wrapper .bp3-control.bp3-disabled{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-control.bp3-inline{display:inline-block;margin-right:20px}.jupyter-wrapper .bp3-control input{position:absolute;top:0;left:0;opacity:0;z-index:-1}.jupyter-wrapper .bp3-control .bp3-control-indicator{display:inline-block;position:relative;margin-top:-3px;margin-right:10px;border:none;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));cursor:pointer;width:1em;height:1em;vertical-align:middle;font-size:16px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-control .bp3-control-indicator::before{display:block;width:1em;height:1em;content:\"\"}.jupyter-wrapper .bp3-control:hover .bp3-control-indicator{background-color:#ebf1f5}.jupyter-wrapper .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background:#d8e1e8}.jupyter-wrapper .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed}.jupyter-wrapper .bp3-control input:focus~.bp3-control-indicator{outline:rgba(19,124,189,.6) auto 2px;outline-offset:2px;-moz-outline-radius:6px}.jupyter-wrapper .bp3-control.bp3-align-right .bp3-control-indicator{float:right;margin-top:1px;margin-left:10px}.jupyter-wrapper .bp3-control.bp3-large{font-size:16px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right){padding-left:30px}.jupyter-wrapper .bp3-control.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right{padding-right:30px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-30px}.jupyter-wrapper .bp3-control.bp3-large .bp3-control-indicator{font-size:20px}.jupyter-wrapper .bp3-control.bp3-large.bp3-align-right .bp3-control-indicator{margin-top:0}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#137cbd;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.1)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.1), rgba(255, 255, 255, 0));color:#fff}.jupyter-wrapper .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 -1px 0 rgba(16,22,26,.2);background-color:#106ba3}.jupyter-wrapper .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox:hover input:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:not(:disabled):active:indeterminate~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.4),inset 0 1px 2px rgba(16,22,26,.2);background-color:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-control.bp3-checkbox .bp3-control-indicator{border-radius:3px}.jupyter-wrapper .bp3-control.bp3-checkbox input:checked~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M12 5c-.28 0-.53.11-.71.29L7 9.59l-2.29-2.3a1.003 1.003 0 0 0-1.42 1.42l3 3c.18.18.43.29.71.29s.53-.11.71-.29l5-5A1.003 1.003 0 0 0 12 5z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-checkbox input:indeterminate~.bp3-control-indicator::before{background-image:url(\"data:image/svg+xml,%3csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 16 16'%3e%3cpath fill-rule='evenodd' clip-rule='evenodd' d='M11 7H5c-.55 0-1 .45-1 1s.45 1 1 1h6c.55 0 1-.45 1-1s-.45-1-1-1z' fill='white'/%3e%3c/svg%3e\")}.jupyter-wrapper .bp3-control.bp3-radio .bp3-control-indicator{border-radius:50%}.jupyter-wrapper .bp3-control.bp3-radio input:checked~.bp3-control-indicator::before{background-image:radial-gradient(#ffffff, #ffffff 28%, transparent 32%)}.jupyter-wrapper .bp3-control.bp3-radio input:checked:disabled~.bp3-control-indicator::before{opacity:.5}.jupyter-wrapper .bp3-control.bp3-radio input:focus~.bp3-control-indicator{-moz-outline-radius:16px}.jupyter-wrapper .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(167,182,194,.5)}.jupyter-wrapper .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(115,134,148,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(92,112,128,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(206,217,224,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(19,124,189,.5)}.jupyter-wrapper .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(255,255,255,.8)}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right){padding-left:38px}.jupyter-wrapper .bp3-control.bp3-switch:not(.bp3-align-right) .bp3-control-indicator{margin-left:-38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right{padding-right:38px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-align-right .bp3-control-indicator{margin-right:-38px}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator{border:none;border-radius:1.75em;-webkit-box-shadow:none !important;box-shadow:none !important;width:auto;min-width:1.75em;-webkit-transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:background-color 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator::before{position:absolute;left:0;margin:2px;border-radius:50%;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);background:#fff;width:calc(1em - 4px);height:calc(1em - 4px);-webkit-transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:left 100ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{left:calc(100% - 1em)}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right){padding-left:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large:not(.bp3-align-right) .bp3-control-indicator{margin-left:-45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right{padding-right:45px}.jupyter-wrapper .bp3-control.bp3-switch.bp3-large.bp3-align-right .bp3-control-indicator{margin-right:-45px}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input~.bp3-control-indicator{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input~.bp3-control-indicator{background:rgba(16,22,26,.7)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:not(:disabled):active~.bp3-control-indicator{background:rgba(16,22,26,.9)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator{background:rgba(57,75,89,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator{background:#137cbd}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch:hover input:checked~.bp3-control-indicator{background:#106ba3}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:not(:disabled):active~.bp3-control-indicator{background:#0e5a8a}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator{background:rgba(14,90,138,.5)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked:disabled~.bp3-control-indicator::before{background:rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch .bp3-control-indicator::before{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background:#394b59}.jupyter-wrapper .bp3-dark .bp3-control.bp3-switch input:checked~.bp3-control-indicator::before{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-control.bp3-switch .bp3-switch-inner-text{text-align:center;font-size:.7em}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:first-child{visibility:hidden;margin-right:1.2em;margin-left:.5em;line-height:0}.jupyter-wrapper .bp3-control.bp3-switch .bp3-control-indicator-child:last-child{visibility:visible;margin-right:.5em;margin-left:1.2em;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:first-child{visibility:visible;line-height:1em}.jupyter-wrapper .bp3-control.bp3-switch input:checked~.bp3-control-indicator .bp3-control-indicator-child:last-child{visibility:hidden;line-height:0}.jupyter-wrapper .bp3-dark .bp3-control{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-control.bp3-disabled{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-control .bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0))}.jupyter-wrapper .bp3-dark .bp3-control:hover .bp3-control-indicator{background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-control input:not(:disabled):active~.bp3-control-indicator{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background:#202b33}.jupyter-wrapper .bp3-dark .bp3-control input:disabled~.bp3-control-indicator{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);cursor:not-allowed}.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:checked~.bp3-control-indicator,.jupyter-wrapper .bp3-dark .bp3-control.bp3-checkbox input:disabled:indeterminate~.bp3-control-indicator{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-file-input{display:inline-block;position:relative;cursor:pointer;height:30px}.jupyter-wrapper .bp3-file-input input{opacity:0;margin:0;min-width:200px}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active:hover,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-input input:disabled+.bp3-file-upload-input::after.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-input input.bp3-disabled+.bp3-file-upload-input::after.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#182026}.jupyter-wrapper .bp3-dark .bp3-file-input.bp3-file-input-has-selection .bp3-file-upload-input{color:#f5f8fa}.jupyter-wrapper .bp3-file-input.bp3-fill{width:100%}.jupyter-wrapper .bp3-file-input.bp3-large,.jupyter-wrapper .bp3-large .bp3-file-input{height:40px}.jupyter-wrapper .bp3-file-input .bp3-file-upload-input-custom-text::after{content:attr(bp3-button-text)}.jupyter-wrapper .bp3-file-upload-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;left:0;padding-right:80px;color:rgba(92,112,128,.6);-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-file-upload-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input:focus,.jupyter-wrapper .bp3-file-upload-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-file-upload-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;min-width:24px;min-height:24px;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;position:absolute;top:0;right:0;margin:3px;border-radius:3px;width:70px;text-align:center;line-height:24px;content:\"Browse\"}.jupyter-wrapper .bp3-file-upload-input::after:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after:disabled.bp3-active:hover,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-file-upload-input::after.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-file-upload-input:hover::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-file-upload-input:active::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-large .bp3-file-upload-input{height:40px;line-height:40px;font-size:16px;padding-right:95px}.jupyter-wrapper .bp3-large .bp3-file-upload-input[type=search],.jupyter-wrapper .bp3-large .bp3-file-upload-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-large .bp3-file-upload-input::after{min-width:30px;min-height:30px;margin:5px;width:85px;line-height:30px}.jupyter-wrapper .bp3-dark .bp3-file-upload-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-file-upload-input::after .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:hover::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-file-upload-input:active::after{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-file-upload-input::after{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1)}.jupyter-wrapper .bp3-form-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin:0 0 15px}.jupyter-wrapper .bp3-form-group label.bp3-label{margin-bottom:5px}.jupyter-wrapper .bp3-form-group .bp3-control{margin-top:7px}.jupyter-wrapper .bp3-form-group .bp3-form-helper-text{margin-top:5px;color:#5c7080;font-size:12px}.jupyter-wrapper .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#106ba3}.jupyter-wrapper .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#0d8050}.jupyter-wrapper .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#bf7326}.jupyter-wrapper .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#c23030}.jupyter-wrapper .bp3-form-group.bp3-inline{-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-form-group.bp3-inline.bp3-large label.bp3-label{margin:0 10px 0 0;line-height:40px}.jupyter-wrapper .bp3-form-group.bp3-inline label.bp3-label{margin:0 10px 0 0;line-height:30px}.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-primary .bp3-form-helper-text{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-success .bp3-form-helper-text{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-warning .bp3-form-helper-text{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-intent-danger .bp3-form-helper-text{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-form-group .bp3-form-helper-text{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-label,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-text-muted,.jupyter-wrapper .bp3-dark .bp3-form-group.bp3-disabled .bp3-form-helper-text{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-input-group{display:block;position:relative}.jupyter-wrapper .bp3-input-group .bp3-input{position:relative;width:100%}.jupyter-wrapper .bp3-input-group .bp3-input:not(:first-child){padding-left:30px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:last-child){padding-right:30px}.jupyter-wrapper .bp3-input-group .bp3-input-action,.jupyter-wrapper .bp3-input-group>.bp3-button,.jupyter-wrapper .bp3-input-group>.bp3-icon{position:absolute;top:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:first-child,.jupyter-wrapper .bp3-input-group>.bp3-button:first-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:first-child{left:0}.jupyter-wrapper .bp3-input-group .bp3-input-action:last-child,.jupyter-wrapper .bp3-input-group>.bp3-button:last-child,.jupyter-wrapper .bp3-input-group>.bp3-icon:last-child{right:0}.jupyter-wrapper .bp3-input-group .bp3-button{min-width:24px;min-height:24px;margin:3px;padding:0 7px}.jupyter-wrapper .bp3-input-group .bp3-button:empty{padding:0}.jupyter-wrapper .bp3-input-group>.bp3-icon{z-index:1;color:#5c7080}.jupyter-wrapper .bp3-input-group>.bp3-icon:empty{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-input-group>.bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input-action>.bp3-spinner{margin:7px}.jupyter-wrapper .bp3-input-group .bp3-tag{margin:5px}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#5c7080}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus),.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus){color:#a7b6c2}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:not(:hover):not(:focus) .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-button.bp3-minimal:disabled .bp3-icon-large,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-standard,.jupyter-wrapper .bp3-input-group .bp3-input:not(:focus)+.bp3-input-action .bp3-button.bp3-minimal:disabled .bp3-icon-large{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-input-group.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-button{min-width:30px;min-height:30px;margin:5px}.jupyter-wrapper .bp3-input-group.bp3-large>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input-action>.bp3-spinner{margin:12px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:first-child){padding-left:40px}.jupyter-wrapper .bp3-input-group.bp3-large .bp3-input:not(:last-child){padding-right:40px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-button{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-tag{min-width:20px;min-height:20px;margin:2px}.jupyter-wrapper .bp3-input-group.bp3-small>.bp3-icon,.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input-action>.bp3-spinner{margin:4px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input[type=search],.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:first-child){padding-left:24px}.jupyter-wrapper .bp3-input-group.bp3-small .bp3-input:not(:last-child){padding-right:24px}.jupyter-wrapper .bp3-input-group.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-input-group.bp3-round .bp3-button,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-input,.jupyter-wrapper .bp3-input-group.bp3-round .bp3-tag{border-radius:30px}.jupyter-wrapper .bp3-dark .bp3-input-group .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-disabled .bp3-icon{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-primary .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-primary>.bp3-icon{color:#48aff0}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-success .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-success>.bp3-icon{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-success>.bp3-icon{color:#3dcc91}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-warning .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-warning>.bp3-icon{color:#ffb366}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input:disabled,.jupyter-wrapper .bp3-input-group.bp3-intent-danger .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-input-group.bp3-intent-danger>.bp3-icon{color:#ff7373}.jupyter-wrapper .bp3-input{outline:none;border:none;border-radius:3px;-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);background:#fff;height:30px;padding:0 10px;vertical-align:middle;line-height:30px;color:#182026;font-size:14px;font-weight:400;-webkit-transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-box-shadow 100ms cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-appearance:none;-moz-appearance:none;appearance:none}.jupyter-wrapper .bp3-input::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input:focus,.jupyter-wrapper .bp3-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input[type=search],.jupyter-wrapper .bp3-input.bp3-round{border-radius:30px;-webkit-box-sizing:border-box;box-sizing:border-box;padding-left:10px}.jupyter-wrapper .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.15);box-shadow:inset 0 0 0 1px rgba(16,22,26,.15)}.jupyter-wrapper .bp3-input:disabled,.jupyter-wrapper .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6);resize:none}.jupyter-wrapper .bp3-input.bp3-large{height:40px;line-height:40px;font-size:16px}.jupyter-wrapper .bp3-input.bp3-large[type=search],.jupyter-wrapper .bp3-input.bp3-large.bp3-round{padding:0 15px}.jupyter-wrapper .bp3-input.bp3-small{height:24px;padding-right:8px;padding-left:8px;line-height:24px;font-size:12px}.jupyter-wrapper .bp3-input.bp3-small[type=search],.jupyter-wrapper .bp3-input.bp3-small.bp3-round{padding:0 12px}.jupyter-wrapper .bp3-input.bp3-fill{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:100%}.jupyter-wrapper .bp3-dark .bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px #137cbd,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary[readonly]{-webkit-box-shadow:inset 0 0 0 1px #137cbd;box-shadow:inset 0 0 0 1px #137cbd}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-primary.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success{-webkit-box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),0 0 0 0 rgba(15,153,96,0),inset 0 0 0 1px #0f9960,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:focus{-webkit-box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0f9960,0 0 0 1px #0f9960,0 0 0 3px rgba(15,153,96,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success[readonly]{-webkit-box-shadow:inset 0 0 0 1px #0f9960;box-shadow:inset 0 0 0 1px #0f9960}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-success.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning{-webkit-box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),0 0 0 0 rgba(217,130,43,0),inset 0 0 0 1px #d9822b,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:focus{-webkit-box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #d9822b,0 0 0 1px #d9822b,0 0 0 3px rgba(217,130,43,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning[readonly]{-webkit-box-shadow:inset 0 0 0 1px #d9822b;box-shadow:inset 0 0 0 1px #d9822b}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-warning.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.15),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger{-webkit-box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),0 0 0 0 rgba(219,55,55,0),inset 0 0 0 1px #db3737,inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:focus{-webkit-box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #db3737,0 0 0 1px #db3737,0 0 0 3px rgba(219,55,55,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger[readonly]{-webkit-box-shadow:inset 0 0 0 1px #db3737;box-shadow:inset 0 0 0 1px #db3737}.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-input.bp3-intent-danger.bp3-disabled{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-input::-ms-clear{display:none}.jupyter-wrapper textarea.bp3-input{max-width:100%;padding:10px}.jupyter-wrapper textarea.bp3-input,.jupyter-wrapper textarea.bp3-input.bp3-large,.jupyter-wrapper textarea.bp3-input.bp3-small{height:auto;line-height:inherit}.jupyter-wrapper textarea.bp3-input.bp3-small{padding:8px}.jupyter-wrapper .bp3-dark textarea.bp3-input{-webkit-box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),0 0 0 0 rgba(19,124,189,0),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background:rgba(16,22,26,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark textarea.bp3-input::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark textarea.bp3-input:focus{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input[readonly]{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark textarea.bp3-input:disabled,.jupyter-wrapper .bp3-dark textarea.bp3-input.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background:rgba(57,75,89,.5);color:rgba(167,182,194,.6)}.jupyter-wrapper label.bp3-label{display:block;margin-top:0;margin-bottom:15px}.jupyter-wrapper label.bp3-label .bp3-html-select,.jupyter-wrapper label.bp3-label .bp3-input,.jupyter-wrapper label.bp3-label .bp3-select,.jupyter-wrapper label.bp3-label .bp3-slider,.jupyter-wrapper label.bp3-label .bp3-popover-wrapper{display:block;margin-top:5px;text-transform:none}.jupyter-wrapper label.bp3-label .bp3-button-group{margin-top:5px}.jupyter-wrapper label.bp3-label .bp3-select select,.jupyter-wrapper label.bp3-label .bp3-html-select select{width:100%;vertical-align:top;font-weight:400}.jupyter-wrapper label.bp3-label.bp3-disabled,.jupyter-wrapper label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(92,112,128,.6)}.jupyter-wrapper label.bp3-label.bp3-inline{line-height:30px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-html-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-select,.jupyter-wrapper label.bp3-label.bp3-inline .bp3-popover-wrapper{display:inline-block;margin:0 0 0 5px;vertical-align:top}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-button-group{margin:0 0 0 5px}.jupyter-wrapper label.bp3-label.bp3-inline .bp3-input-group .bp3-input{margin-left:0}.jupyter-wrapper label.bp3-label.bp3-inline.bp3-large{line-height:40px}.jupyter-wrapper label.bp3-label:not(.bp3-inline) .bp3-popover-target{display:block}.jupyter-wrapper .bp3-dark label.bp3-label{color:#f5f8fa}.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled,.jupyter-wrapper .bp3-dark label.bp3-label.bp3-disabled .bp3-text-muted{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button{-webkit-box-flex:1;-ms-flex:1 1 14px;flex:1 1 14px;width:30px;min-height:0;padding:0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:first-child{border-radius:0 3px 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical>.bp3-button:last-child{border-radius:0 0 3px 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:first-child{border-radius:3px 0 0 0}.jupyter-wrapper .bp3-numeric-input .bp3-button-group.bp3-vertical:first-child>.bp3-button:last-child{border-radius:0 0 0 3px}.jupyter-wrapper .bp3-numeric-input.bp3-large .bp3-button-group.bp3-vertical>.bp3-button{width:40px}.jupyter-wrapper form{display:block}.jupyter-wrapper .bp3-html-select select,.jupyter-wrapper .bp3-select select{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;border:none;border-radius:3px;cursor:pointer;padding:5px 10px;vertical-align:middle;text-align:left;font-size:14px;-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;border-radius:3px;width:100%;height:30px;padding:0 25px 0 10px;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-html-select select>.bp3-fill,.jupyter-wrapper .bp3-select select>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-html-select select::before,.jupyter-wrapper .bp3-select select::before,.jupyter-wrapper .bp3-html-select select>*,.jupyter-wrapper .bp3-select select>*{margin-right:7px}.jupyter-wrapper .bp3-html-select select:empty::before,.jupyter-wrapper .bp3-select select:empty::before,.jupyter-wrapper .bp3-html-select select>:last-child,.jupyter-wrapper .bp3-select select>:last-child{margin-right:0}.jupyter-wrapper .bp3-html-select select:hover,.jupyter-wrapper .bp3-select select:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-html-select select:active,.jupyter-wrapper .bp3-select select:active,.jupyter-wrapper .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-select select.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled,.jupyter-wrapper .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-select select.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select:disabled.bp3-active:hover,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select select.bp3-disabled.bp3-active:hover,.jupyter-wrapper .bp3-select select.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal select{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:hover{-webkit-box-shadow:none;box-shadow:none;background:rgba(167,182,194,.3);text-decoration:none;color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:rgba(115,134,148,.3);color:#182026}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active{background:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select{-webkit-box-shadow:none;box-shadow:none;background:none;color:inherit}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:hover{background:rgba(138,155,168,.15)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-active{background:rgba(138,155,168,.3);color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover{background:none;cursor:not-allowed;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select:disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-disabled:hover.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-disabled:hover.bp3-active{background:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:hover{background:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#106ba3}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(16,107,163,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-primary .bp3-button-spinner .bp3-spinner-head{stroke:#106ba3}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:hover{background:rgba(19,124,189,.2);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-active{background:rgba(19,124,189,.3);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled{background:none;color:rgba(72,175,240,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-primary.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-primary.bp3-disabled.bp3-active{background:rgba(19,124,189,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:hover{background:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#0d8050}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled{background:none;color:rgba(13,128,80,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-success .bp3-button-spinner .bp3-spinner-head{stroke:#0d8050}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:hover{background:rgba(15,153,96,.2);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-active{background:rgba(15,153,96,.3);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled{background:none;color:rgba(61,204,145,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-success.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-success.bp3-disabled.bp3-active{background:rgba(15,153,96,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:hover{background:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#bf7326}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(191,115,38,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-warning .bp3-button-spinner .bp3-spinner-head{stroke:#bf7326}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:hover{background:rgba(217,130,43,.2);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-active{background:rgba(217,130,43,.3);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled{background:none;color:rgba(255,179,102,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-warning.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-warning.bp3-disabled.bp3-active{background:rgba(217,130,43,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{-webkit-box-shadow:none;box-shadow:none;background:none;color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:hover{background:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#c23030}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(194,48,48,.5)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-select.bp3-minimal select.bp3-intent-danger .bp3-button-spinner .bp3-spinner-head{stroke:#c23030}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:hover,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:hover{background:rgba(219,55,55,.2);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-active{background:rgba(219,55,55,.3);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled{background:none;color:rgba(255,115,115,.5)}.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-html-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select.bp3-minimal select.bp3-intent-danger.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-select.bp3-minimal .bp3-dark select.bp3-intent-danger.bp3-disabled.bp3-active{background:rgba(219,55,55,.3)}.jupyter-wrapper .bp3-html-select.bp3-large select,.jupyter-wrapper .bp3-select.bp3-large select{height:40px;padding-right:35px;font-size:16px}.jupyter-wrapper .bp3-dark .bp3-html-select select,.jupyter-wrapper .bp3-dark .bp3-select select{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover,.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select select:hover,.jupyter-wrapper .bp3-dark .bp3-select select:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-html-select select:active,.jupyter-wrapper .bp3-dark .bp3-select select:active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-select select:disabled,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-html-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-html-select select.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-select select.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-html-select select .bp3-button-spinner .bp3-spinner-head,.jupyter-wrapper .bp3-dark .bp3-select select .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-html-select select:disabled,.jupyter-wrapper .bp3-select select:disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon,.jupyter-wrapper .bp3-select::after{position:absolute;top:7px;right:7px;color:#5c7080;pointer-events:none}.jupyter-wrapper .bp3-html-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-select .bp3-disabled.bp3-icon,.jupyter-wrapper .bp3-disabled.bp3-select::after{color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-html-select,.jupyter-wrapper .bp3-select{display:inline-block;position:relative;vertical-align:middle;letter-spacing:normal}.jupyter-wrapper .bp3-html-select select::-ms-expand,.jupyter-wrapper .bp3-select select::-ms-expand{display:none}.jupyter-wrapper .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-select .bp3-icon{color:#5c7080}.jupyter-wrapper .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-select .bp3-icon:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-html-select .bp3-icon:hover,.jupyter-wrapper .bp3-dark .bp3-select .bp3-icon:hover{color:#f5f8fa}.jupyter-wrapper .bp3-html-select.bp3-large::after,.jupyter-wrapper .bp3-html-select.bp3-large .bp3-icon,.jupyter-wrapper .bp3-select.bp3-large::after,.jupyter-wrapper .bp3-select.bp3-large .bp3-icon{top:12px;right:12px}.jupyter-wrapper .bp3-html-select.bp3-fill,.jupyter-wrapper .bp3-html-select.bp3-fill select,.jupyter-wrapper .bp3-select.bp3-fill,.jupyter-wrapper .bp3-select.bp3-fill select{width:100%}.jupyter-wrapper .bp3-dark .bp3-html-select option,.jupyter-wrapper .bp3-dark .bp3-select option{background-color:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-html-select::after,.jupyter-wrapper .bp3-dark .bp3-select::after{color:#a7b6c2}.jupyter-wrapper .bp3-select::after{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6c6\"}.jupyter-wrapper .bp3-running-text table,.jupyter-wrapper table.bp3-html-table{border-spacing:0;font-size:14px}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th,.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{padding:11px;vertical-align:top;text-align:left}.jupyter-wrapper .bp3-running-text table th,.jupyter-wrapper table.bp3-html-table th{color:#182026;font-weight:600}.jupyter-wrapper .bp3-running-text table td,.jupyter-wrapper table.bp3-html-table td{color:#182026}.jupyter-wrapper .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-running-text table th,.jupyter-wrapper .bp3-running-text .bp3-dark table th,.jupyter-wrapper .bp3-dark table.bp3-html-table th{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table td,.jupyter-wrapper .bp3-running-text .bp3-dark table td,.jupyter-wrapper .bp3-dark table.bp3-html-table td{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child th,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child th,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child th,.jupyter-wrapper .bp3-dark .bp3-running-text table tbody tr:first-child td,.jupyter-wrapper .bp3-running-text .bp3-dark table tbody tr:first-child td,.jupyter-wrapper .bp3-dark table.bp3-html-table tbody tr:first-child td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed th,.jupyter-wrapper table.bp3-html-table.bp3-html-table-condensed td,.jupyter-wrapper table.bp3-html-table.bp3-small th,.jupyter-wrapper table.bp3-html-table.bp3-small td{padding-top:6px;padding-bottom:6px}.jupyter-wrapper table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(191,204,214,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 1px 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15);box-shadow:inset 1px 0 0 0 rgba(16,22,26,.15)}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(191,204,214,.3);cursor:pointer}.jupyter-wrapper table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-striped tbody tr:nth-child(odd) td{background:rgba(92,112,128,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered th:not(:first-child){-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td{-webkit-box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 0 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered tbody tr td:not(:first-child){-webkit-box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 1px 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td{-webkit-box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15);box-shadow:inset 1px 0 0 0 rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-html-table-bordered.bp3-html-table-striped tbody tr:not(:first-child) td:first-child{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:hover td{background-color:rgba(92,112,128,.3);cursor:pointer}.jupyter-wrapper .bp3-dark table.bp3-html-table.bp3-interactive tbody tr:active td{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-key-combo{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-key-combo>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-key-combo>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-key-combo::before,.jupyter-wrapper .bp3-key-combo>*{margin-right:5px}.jupyter-wrapper .bp3-key-combo:empty::before,.jupyter-wrapper .bp3-key-combo>:last-child{margin-right:0}.jupyter-wrapper .bp3-hotkey-dialog{top:40px;padding-bottom:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-dialog-body{margin:0;padding:0}.jupyter-wrapper .bp3-hotkey-dialog .bp3-hotkey-label{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1}.jupyter-wrapper .bp3-hotkey-column{margin:auto;max-height:80vh;overflow-y:auto;padding:30px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading{margin-bottom:20px}.jupyter-wrapper .bp3-hotkey-column .bp3-heading:not(:first-child){margin-top:40px}.jupyter-wrapper .bp3-hotkey{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:justify;-ms-flex-pack:justify;justify-content:space-between;margin-right:0;margin-left:0}.jupyter-wrapper .bp3-hotkey:not(:last-child){margin-bottom:10px}.jupyter-wrapper .bp3-icon{display:inline-block;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;vertical-align:text-bottom}.jupyter-wrapper .bp3-icon:not(:empty)::before{content:\"\" !important;content:unset !important}.jupyter-wrapper .bp3-icon>svg{display:block}.jupyter-wrapper .bp3-icon>svg:not([fill]){fill:currentColor}.jupyter-wrapper .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-icon-large.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-icon-large.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-icon-large.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-icon-large.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-dark .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-icon-large.bp3-intent-danger{color:#ff7373}.jupyter-wrapper span.bp3-icon-standard{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon-large{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;display:inline-block}.jupyter-wrapper span.bp3-icon:empty{line-height:1;font-family:\"Icons20\";font-size:inherit;font-weight:400;font-style:normal}.jupyter-wrapper span.bp3-icon:empty::before{-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}.jupyter-wrapper .bp3-icon-add::before{content:\"\ue63e\"}.jupyter-wrapper .bp3-icon-add-column-left::before{content:\"\ue6f9\"}.jupyter-wrapper .bp3-icon-add-column-right::before{content:\"\ue6fa\"}.jupyter-wrapper .bp3-icon-add-row-bottom::before{content:\"\ue6f8\"}.jupyter-wrapper .bp3-icon-add-row-top::before{content:\"\ue6f7\"}.jupyter-wrapper .bp3-icon-add-to-artifact::before{content:\"\ue67c\"}.jupyter-wrapper .bp3-icon-add-to-folder::before{content:\"\ue6d2\"}.jupyter-wrapper .bp3-icon-airplane::before{content:\"\ue74b\"}.jupyter-wrapper .bp3-icon-align-center::before{content:\"\ue603\"}.jupyter-wrapper .bp3-icon-align-justify::before{content:\"\ue605\"}.jupyter-wrapper .bp3-icon-align-left::before{content:\"\ue602\"}.jupyter-wrapper .bp3-icon-align-right::before{content:\"\ue604\"}.jupyter-wrapper .bp3-icon-alignment-bottom::before{content:\"\ue727\"}.jupyter-wrapper .bp3-icon-alignment-horizontal-center::before{content:\"\ue726\"}.jupyter-wrapper .bp3-icon-alignment-left::before{content:\"\ue722\"}.jupyter-wrapper .bp3-icon-alignment-right::before{content:\"\ue724\"}.jupyter-wrapper .bp3-icon-alignment-top::before{content:\"\ue725\"}.jupyter-wrapper .bp3-icon-alignment-vertical-center::before{content:\"\ue723\"}.jupyter-wrapper .bp3-icon-annotation::before{content:\"\ue6f0\"}.jupyter-wrapper .bp3-icon-application::before{content:\"\ue735\"}.jupyter-wrapper .bp3-icon-applications::before{content:\"\ue621\"}.jupyter-wrapper .bp3-icon-archive::before{content:\"\ue907\"}.jupyter-wrapper .bp3-icon-arrow-bottom-left::before{content:\"\u2199\"}.jupyter-wrapper .bp3-icon-arrow-bottom-right::before{content:\"\u2198\"}.jupyter-wrapper .bp3-icon-arrow-down::before{content:\"\u2193\"}.jupyter-wrapper .bp3-icon-arrow-left::before{content:\"\u2190\"}.jupyter-wrapper .bp3-icon-arrow-right::before{content:\"\u2192\"}.jupyter-wrapper .bp3-icon-arrow-top-left::before{content:\"\u2196\"}.jupyter-wrapper .bp3-icon-arrow-top-right::before{content:\"\u2197\"}.jupyter-wrapper .bp3-icon-arrow-up::before{content:\"\u2191\"}.jupyter-wrapper .bp3-icon-arrows-horizontal::before{content:\"\u2194\"}.jupyter-wrapper .bp3-icon-arrows-vertical::before{content:\"\u2195\"}.jupyter-wrapper .bp3-icon-asterisk::before{content:\"*\"}.jupyter-wrapper .bp3-icon-automatic-updates::before{content:\"\ue65f\"}.jupyter-wrapper .bp3-icon-badge::before{content:\"\ue6e3\"}.jupyter-wrapper .bp3-icon-ban-circle::before{content:\"\ue69d\"}.jupyter-wrapper .bp3-icon-bank-account::before{content:\"\ue76f\"}.jupyter-wrapper .bp3-icon-barcode::before{content:\"\ue676\"}.jupyter-wrapper .bp3-icon-blank::before{content:\"\ue900\"}.jupyter-wrapper .bp3-icon-blocked-person::before{content:\"\ue768\"}.jupyter-wrapper .bp3-icon-bold::before{content:\"\ue606\"}.jupyter-wrapper .bp3-icon-book::before{content:\"\ue6b8\"}.jupyter-wrapper .bp3-icon-bookmark::before{content:\"\ue61a\"}.jupyter-wrapper .bp3-icon-box::before{content:\"\ue6bf\"}.jupyter-wrapper .bp3-icon-briefcase::before{content:\"\ue674\"}.jupyter-wrapper .bp3-icon-bring-data::before{content:\"\ue90a\"}.jupyter-wrapper .bp3-icon-build::before{content:\"\ue72d\"}.jupyter-wrapper .bp3-icon-calculator::before{content:\"\ue70b\"}.jupyter-wrapper .bp3-icon-calendar::before{content:\"\ue62b\"}.jupyter-wrapper .bp3-icon-camera::before{content:\"\ue69e\"}.jupyter-wrapper .bp3-icon-caret-down::before{content:\"\u2304\"}.jupyter-wrapper .bp3-icon-caret-left::before{content:\"\u2329\"}.jupyter-wrapper .bp3-icon-caret-right::before{content:\"\u232a\"}.jupyter-wrapper .bp3-icon-caret-up::before{content:\"\u2303\"}.jupyter-wrapper .bp3-icon-cell-tower::before{content:\"\ue770\"}.jupyter-wrapper .bp3-icon-changes::before{content:\"\ue623\"}.jupyter-wrapper .bp3-icon-chart::before{content:\"\ue67e\"}.jupyter-wrapper .bp3-icon-chat::before{content:\"\ue689\"}.jupyter-wrapper .bp3-icon-chevron-backward::before{content:\"\ue6df\"}.jupyter-wrapper .bp3-icon-chevron-down::before{content:\"\ue697\"}.jupyter-wrapper .bp3-icon-chevron-forward::before{content:\"\ue6e0\"}.jupyter-wrapper .bp3-icon-chevron-left::before{content:\"\ue694\"}.jupyter-wrapper .bp3-icon-chevron-right::before{content:\"\ue695\"}.jupyter-wrapper .bp3-icon-chevron-up::before{content:\"\ue696\"}.jupyter-wrapper .bp3-icon-circle::before{content:\"\ue66a\"}.jupyter-wrapper .bp3-icon-circle-arrow-down::before{content:\"\ue68e\"}.jupyter-wrapper .bp3-icon-circle-arrow-left::before{content:\"\ue68c\"}.jupyter-wrapper .bp3-icon-circle-arrow-right::before{content:\"\ue68b\"}.jupyter-wrapper .bp3-icon-circle-arrow-up::before{content:\"\ue68d\"}.jupyter-wrapper .bp3-icon-citation::before{content:\"\ue61b\"}.jupyter-wrapper .bp3-icon-clean::before{content:\"\ue7c5\"}.jupyter-wrapper .bp3-icon-clipboard::before{content:\"\ue61d\"}.jupyter-wrapper .bp3-icon-cloud::before{content:\"\u2601\"}.jupyter-wrapper .bp3-icon-cloud-download::before{content:\"\ue690\"}.jupyter-wrapper .bp3-icon-cloud-upload::before{content:\"\ue691\"}.jupyter-wrapper .bp3-icon-code::before{content:\"\ue661\"}.jupyter-wrapper .bp3-icon-code-block::before{content:\"\ue6c5\"}.jupyter-wrapper .bp3-icon-cog::before{content:\"\ue645\"}.jupyter-wrapper .bp3-icon-collapse-all::before{content:\"\ue763\"}.jupyter-wrapper .bp3-icon-column-layout::before{content:\"\ue6da\"}.jupyter-wrapper .bp3-icon-comment::before{content:\"\ue68a\"}.jupyter-wrapper .bp3-icon-comparison::before{content:\"\ue637\"}.jupyter-wrapper .bp3-icon-compass::before{content:\"\ue79c\"}.jupyter-wrapper .bp3-icon-compressed::before{content:\"\ue6c0\"}.jupyter-wrapper .bp3-icon-confirm::before{content:\"\ue639\"}.jupyter-wrapper .bp3-icon-console::before{content:\"\ue79b\"}.jupyter-wrapper .bp3-icon-contrast::before{content:\"\ue6cb\"}.jupyter-wrapper .bp3-icon-control::before{content:\"\ue67f\"}.jupyter-wrapper .bp3-icon-credit-card::before{content:\"\ue649\"}.jupyter-wrapper .bp3-icon-cross::before{content:\"\u2717\"}.jupyter-wrapper .bp3-icon-crown::before{content:\"\ue7b4\"}.jupyter-wrapper .bp3-icon-cube::before{content:\"\ue7c8\"}.jupyter-wrapper .bp3-icon-cube-add::before{content:\"\ue7c9\"}.jupyter-wrapper .bp3-icon-cube-remove::before{content:\"\ue7d0\"}.jupyter-wrapper .bp3-icon-curved-range-chart::before{content:\"\ue71b\"}.jupyter-wrapper .bp3-icon-cut::before{content:\"\ue6ef\"}.jupyter-wrapper .bp3-icon-dashboard::before{content:\"\ue751\"}.jupyter-wrapper .bp3-icon-data-lineage::before{content:\"\ue908\"}.jupyter-wrapper .bp3-icon-database::before{content:\"\ue683\"}.jupyter-wrapper .bp3-icon-delete::before{content:\"\ue644\"}.jupyter-wrapper .bp3-icon-delta::before{content:\"\u0394\"}.jupyter-wrapper .bp3-icon-derive-column::before{content:\"\ue739\"}.jupyter-wrapper .bp3-icon-desktop::before{content:\"\ue6af\"}.jupyter-wrapper .bp3-icon-diagram-tree::before{content:\"\ue7b3\"}.jupyter-wrapper .bp3-icon-direction-left::before{content:\"\ue681\"}.jupyter-wrapper .bp3-icon-direction-right::before{content:\"\ue682\"}.jupyter-wrapper .bp3-icon-disable::before{content:\"\ue600\"}.jupyter-wrapper .bp3-icon-document::before{content:\"\ue630\"}.jupyter-wrapper .bp3-icon-document-open::before{content:\"\ue71e\"}.jupyter-wrapper .bp3-icon-document-share::before{content:\"\ue71f\"}.jupyter-wrapper .bp3-icon-dollar::before{content:\"$\"}.jupyter-wrapper .bp3-icon-dot::before{content:\"\u2022\"}.jupyter-wrapper .bp3-icon-double-caret-horizontal::before{content:\"\ue6c7\"}.jupyter-wrapper .bp3-icon-double-caret-vertical::before{content:\"\ue6c6\"}.jupyter-wrapper .bp3-icon-double-chevron-down::before{content:\"\ue703\"}.jupyter-wrapper .bp3-icon-double-chevron-left::before{content:\"\ue6ff\"}.jupyter-wrapper .bp3-icon-double-chevron-right::before{content:\"\ue701\"}.jupyter-wrapper .bp3-icon-double-chevron-up::before{content:\"\ue702\"}.jupyter-wrapper .bp3-icon-doughnut-chart::before{content:\"\ue6ce\"}.jupyter-wrapper .bp3-icon-download::before{content:\"\ue62f\"}.jupyter-wrapper .bp3-icon-drag-handle-horizontal::before{content:\"\ue716\"}.jupyter-wrapper .bp3-icon-drag-handle-vertical::before{content:\"\ue715\"}.jupyter-wrapper .bp3-icon-draw::before{content:\"\ue66b\"}.jupyter-wrapper .bp3-icon-drive-time::before{content:\"\ue615\"}.jupyter-wrapper .bp3-icon-duplicate::before{content:\"\ue69c\"}.jupyter-wrapper .bp3-icon-edit::before{content:\"\u270e\"}.jupyter-wrapper .bp3-icon-eject::before{content:\"\u23cf\"}.jupyter-wrapper .bp3-icon-endorsed::before{content:\"\ue75f\"}.jupyter-wrapper .bp3-icon-envelope::before{content:\"\u2709\"}.jupyter-wrapper .bp3-icon-equals::before{content:\"\ue7d9\"}.jupyter-wrapper .bp3-icon-eraser::before{content:\"\ue773\"}.jupyter-wrapper .bp3-icon-error::before{content:\"\ue648\"}.jupyter-wrapper .bp3-icon-euro::before{content:\"\u20ac\"}.jupyter-wrapper .bp3-icon-exchange::before{content:\"\ue636\"}.jupyter-wrapper .bp3-icon-exclude-row::before{content:\"\ue6ea\"}.jupyter-wrapper .bp3-icon-expand-all::before{content:\"\ue764\"}.jupyter-wrapper .bp3-icon-export::before{content:\"\ue633\"}.jupyter-wrapper .bp3-icon-eye-off::before{content:\"\ue6cc\"}.jupyter-wrapper .bp3-icon-eye-on::before{content:\"\ue75a\"}.jupyter-wrapper .bp3-icon-eye-open::before{content:\"\ue66f\"}.jupyter-wrapper .bp3-icon-fast-backward::before{content:\"\ue6a8\"}.jupyter-wrapper .bp3-icon-fast-forward::before{content:\"\ue6ac\"}.jupyter-wrapper .bp3-icon-feed::before{content:\"\ue656\"}.jupyter-wrapper .bp3-icon-feed-subscribed::before{content:\"\ue78f\"}.jupyter-wrapper .bp3-icon-film::before{content:\"\ue6a1\"}.jupyter-wrapper .bp3-icon-filter::before{content:\"\ue638\"}.jupyter-wrapper .bp3-icon-filter-keep::before{content:\"\ue78c\"}.jupyter-wrapper .bp3-icon-filter-list::before{content:\"\ue6ee\"}.jupyter-wrapper .bp3-icon-filter-open::before{content:\"\ue7d7\"}.jupyter-wrapper .bp3-icon-filter-remove::before{content:\"\ue78d\"}.jupyter-wrapper .bp3-icon-flag::before{content:\"\u2691\"}.jupyter-wrapper .bp3-icon-flame::before{content:\"\ue7a9\"}.jupyter-wrapper .bp3-icon-flash::before{content:\"\ue6b3\"}.jupyter-wrapper .bp3-icon-floppy-disk::before{content:\"\ue6b7\"}.jupyter-wrapper .bp3-icon-flow-branch::before{content:\"\ue7c1\"}.jupyter-wrapper .bp3-icon-flow-end::before{content:\"\ue7c4\"}.jupyter-wrapper .bp3-icon-flow-linear::before{content:\"\ue7c0\"}.jupyter-wrapper .bp3-icon-flow-review::before{content:\"\ue7c2\"}.jupyter-wrapper .bp3-icon-flow-review-branch::before{content:\"\ue7c3\"}.jupyter-wrapper .bp3-icon-flows::before{content:\"\ue659\"}.jupyter-wrapper .bp3-icon-folder-close::before{content:\"\ue652\"}.jupyter-wrapper .bp3-icon-folder-new::before{content:\"\ue7b0\"}.jupyter-wrapper .bp3-icon-folder-open::before{content:\"\ue651\"}.jupyter-wrapper .bp3-icon-folder-shared::before{content:\"\ue653\"}.jupyter-wrapper .bp3-icon-folder-shared-open::before{content:\"\ue670\"}.jupyter-wrapper .bp3-icon-follower::before{content:\"\ue760\"}.jupyter-wrapper .bp3-icon-following::before{content:\"\ue761\"}.jupyter-wrapper .bp3-icon-font::before{content:\"\ue6b4\"}.jupyter-wrapper .bp3-icon-fork::before{content:\"\ue63a\"}.jupyter-wrapper .bp3-icon-form::before{content:\"\ue795\"}.jupyter-wrapper .bp3-icon-full-circle::before{content:\"\ue685\"}.jupyter-wrapper .bp3-icon-full-stacked-chart::before{content:\"\ue75e\"}.jupyter-wrapper .bp3-icon-fullscreen::before{content:\"\ue699\"}.jupyter-wrapper .bp3-icon-function::before{content:\"\ue6e5\"}.jupyter-wrapper .bp3-icon-gantt-chart::before{content:\"\ue6f4\"}.jupyter-wrapper .bp3-icon-geolocation::before{content:\"\ue640\"}.jupyter-wrapper .bp3-icon-geosearch::before{content:\"\ue613\"}.jupyter-wrapper .bp3-icon-git-branch::before{content:\"\ue72a\"}.jupyter-wrapper .bp3-icon-git-commit::before{content:\"\ue72b\"}.jupyter-wrapper .bp3-icon-git-merge::before{content:\"\ue729\"}.jupyter-wrapper .bp3-icon-git-new-branch::before{content:\"\ue749\"}.jupyter-wrapper .bp3-icon-git-pull::before{content:\"\ue728\"}.jupyter-wrapper .bp3-icon-git-push::before{content:\"\ue72c\"}.jupyter-wrapper .bp3-icon-git-repo::before{content:\"\ue748\"}.jupyter-wrapper .bp3-icon-glass::before{content:\"\ue6b1\"}.jupyter-wrapper .bp3-icon-globe::before{content:\"\ue666\"}.jupyter-wrapper .bp3-icon-globe-network::before{content:\"\ue7b5\"}.jupyter-wrapper .bp3-icon-graph::before{content:\"\ue673\"}.jupyter-wrapper .bp3-icon-graph-remove::before{content:\"\ue609\"}.jupyter-wrapper .bp3-icon-greater-than::before{content:\"\ue7e1\"}.jupyter-wrapper .bp3-icon-greater-than-or-equal-to::before{content:\"\ue7e2\"}.jupyter-wrapper .bp3-icon-grid::before{content:\"\ue6d0\"}.jupyter-wrapper .bp3-icon-grid-view::before{content:\"\ue6e4\"}.jupyter-wrapper .bp3-icon-group-objects::before{content:\"\ue60a\"}.jupyter-wrapper .bp3-icon-grouped-bar-chart::before{content:\"\ue75d\"}.jupyter-wrapper .bp3-icon-hand::before{content:\"\ue6de\"}.jupyter-wrapper .bp3-icon-hand-down::before{content:\"\ue6bb\"}.jupyter-wrapper .bp3-icon-hand-left::before{content:\"\ue6bc\"}.jupyter-wrapper .bp3-icon-hand-right::before{content:\"\ue6b9\"}.jupyter-wrapper .bp3-icon-hand-up::before{content:\"\ue6ba\"}.jupyter-wrapper .bp3-icon-header::before{content:\"\ue6b5\"}.jupyter-wrapper .bp3-icon-header-one::before{content:\"\ue793\"}.jupyter-wrapper .bp3-icon-header-two::before{content:\"\ue794\"}.jupyter-wrapper .bp3-icon-headset::before{content:\"\ue6dc\"}.jupyter-wrapper .bp3-icon-heart::before{content:\"\u2665\"}.jupyter-wrapper .bp3-icon-heart-broken::before{content:\"\ue7a2\"}.jupyter-wrapper .bp3-icon-heat-grid::before{content:\"\ue6f3\"}.jupyter-wrapper .bp3-icon-heatmap::before{content:\"\ue614\"}.jupyter-wrapper .bp3-icon-help::before{content:\"?\"}.jupyter-wrapper .bp3-icon-helper-management::before{content:\"\ue66d\"}.jupyter-wrapper .bp3-icon-highlight::before{content:\"\ue6ed\"}.jupyter-wrapper .bp3-icon-history::before{content:\"\ue64a\"}.jupyter-wrapper .bp3-icon-home::before{content:\"\u2302\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart::before{content:\"\ue70c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-asc::before{content:\"\ue75c\"}.jupyter-wrapper .bp3-icon-horizontal-bar-chart-desc::before{content:\"\ue71d\"}.jupyter-wrapper .bp3-icon-horizontal-distribution::before{content:\"\ue720\"}.jupyter-wrapper .bp3-icon-id-number::before{content:\"\ue771\"}.jupyter-wrapper .bp3-icon-image-rotate-left::before{content:\"\ue73a\"}.jupyter-wrapper .bp3-icon-image-rotate-right::before{content:\"\ue73b\"}.jupyter-wrapper .bp3-icon-import::before{content:\"\ue632\"}.jupyter-wrapper .bp3-icon-inbox::before{content:\"\ue629\"}.jupyter-wrapper .bp3-icon-inbox-filtered::before{content:\"\ue7d1\"}.jupyter-wrapper .bp3-icon-inbox-geo::before{content:\"\ue7d2\"}.jupyter-wrapper .bp3-icon-inbox-search::before{content:\"\ue7d3\"}.jupyter-wrapper .bp3-icon-inbox-update::before{content:\"\ue7d4\"}.jupyter-wrapper .bp3-icon-info-sign::before{content:\"\u2139\"}.jupyter-wrapper .bp3-icon-inheritance::before{content:\"\ue7d5\"}.jupyter-wrapper .bp3-icon-inner-join::before{content:\"\ue7a3\"}.jupyter-wrapper .bp3-icon-insert::before{content:\"\ue66c\"}.jupyter-wrapper .bp3-icon-intersection::before{content:\"\ue765\"}.jupyter-wrapper .bp3-icon-ip-address::before{content:\"\ue772\"}.jupyter-wrapper .bp3-icon-issue::before{content:\"\ue774\"}.jupyter-wrapper .bp3-icon-issue-closed::before{content:\"\ue776\"}.jupyter-wrapper .bp3-icon-issue-new::before{content:\"\ue775\"}.jupyter-wrapper .bp3-icon-italic::before{content:\"\ue607\"}.jupyter-wrapper .bp3-icon-join-table::before{content:\"\ue738\"}.jupyter-wrapper .bp3-icon-key::before{content:\"\ue78e\"}.jupyter-wrapper .bp3-icon-key-backspace::before{content:\"\ue707\"}.jupyter-wrapper .bp3-icon-key-command::before{content:\"\ue705\"}.jupyter-wrapper .bp3-icon-key-control::before{content:\"\ue704\"}.jupyter-wrapper .bp3-icon-key-delete::before{content:\"\ue708\"}.jupyter-wrapper .bp3-icon-key-enter::before{content:\"\ue70a\"}.jupyter-wrapper .bp3-icon-key-escape::before{content:\"\ue709\"}.jupyter-wrapper .bp3-icon-key-option::before{content:\"\ue742\"}.jupyter-wrapper .bp3-icon-key-shift::before{content:\"\ue706\"}.jupyter-wrapper .bp3-icon-key-tab::before{content:\"\ue757\"}.jupyter-wrapper .bp3-icon-known-vehicle::before{content:\"\ue73c\"}.jupyter-wrapper .bp3-icon-label::before{content:\"\ue665\"}.jupyter-wrapper .bp3-icon-layer::before{content:\"\ue6cf\"}.jupyter-wrapper .bp3-icon-layers::before{content:\"\ue618\"}.jupyter-wrapper .bp3-icon-layout::before{content:\"\ue60c\"}.jupyter-wrapper .bp3-icon-layout-auto::before{content:\"\ue60d\"}.jupyter-wrapper .bp3-icon-layout-balloon::before{content:\"\ue6d3\"}.jupyter-wrapper .bp3-icon-layout-circle::before{content:\"\ue60e\"}.jupyter-wrapper .bp3-icon-layout-grid::before{content:\"\ue610\"}.jupyter-wrapper .bp3-icon-layout-group-by::before{content:\"\ue611\"}.jupyter-wrapper .bp3-icon-layout-hierarchy::before{content:\"\ue60f\"}.jupyter-wrapper .bp3-icon-layout-linear::before{content:\"\ue6c3\"}.jupyter-wrapper .bp3-icon-layout-skew-grid::before{content:\"\ue612\"}.jupyter-wrapper .bp3-icon-layout-sorted-clusters::before{content:\"\ue6d4\"}.jupyter-wrapper .bp3-icon-learning::before{content:\"\ue904\"}.jupyter-wrapper .bp3-icon-left-join::before{content:\"\ue7a4\"}.jupyter-wrapper .bp3-icon-less-than::before{content:\"\ue7e3\"}.jupyter-wrapper .bp3-icon-less-than-or-equal-to::before{content:\"\ue7e4\"}.jupyter-wrapper .bp3-icon-lifesaver::before{content:\"\ue7c7\"}.jupyter-wrapper .bp3-icon-lightbulb::before{content:\"\ue6b0\"}.jupyter-wrapper .bp3-icon-link::before{content:\"\ue62d\"}.jupyter-wrapper .bp3-icon-list::before{content:\"\u2630\"}.jupyter-wrapper .bp3-icon-list-columns::before{content:\"\ue7b9\"}.jupyter-wrapper .bp3-icon-list-detail-view::before{content:\"\ue743\"}.jupyter-wrapper .bp3-icon-locate::before{content:\"\ue619\"}.jupyter-wrapper .bp3-icon-lock::before{content:\"\ue625\"}.jupyter-wrapper .bp3-icon-log-in::before{content:\"\ue69a\"}.jupyter-wrapper .bp3-icon-log-out::before{content:\"\ue64c\"}.jupyter-wrapper .bp3-icon-manual::before{content:\"\ue6f6\"}.jupyter-wrapper .bp3-icon-manually-entered-data::before{content:\"\ue74a\"}.jupyter-wrapper .bp3-icon-map::before{content:\"\ue662\"}.jupyter-wrapper .bp3-icon-map-create::before{content:\"\ue741\"}.jupyter-wrapper .bp3-icon-map-marker::before{content:\"\ue67d\"}.jupyter-wrapper .bp3-icon-maximize::before{content:\"\ue635\"}.jupyter-wrapper .bp3-icon-media::before{content:\"\ue62c\"}.jupyter-wrapper .bp3-icon-menu::before{content:\"\ue762\"}.jupyter-wrapper .bp3-icon-menu-closed::before{content:\"\ue655\"}.jupyter-wrapper .bp3-icon-menu-open::before{content:\"\ue654\"}.jupyter-wrapper .bp3-icon-merge-columns::before{content:\"\ue74f\"}.jupyter-wrapper .bp3-icon-merge-links::before{content:\"\ue60b\"}.jupyter-wrapper .bp3-icon-minimize::before{content:\"\ue634\"}.jupyter-wrapper .bp3-icon-minus::before{content:\"\u2212\"}.jupyter-wrapper .bp3-icon-mobile-phone::before{content:\"\ue717\"}.jupyter-wrapper .bp3-icon-mobile-video::before{content:\"\ue69f\"}.jupyter-wrapper .bp3-icon-moon::before{content:\"\ue754\"}.jupyter-wrapper .bp3-icon-more::before{content:\"\ue62a\"}.jupyter-wrapper .bp3-icon-mountain::before{content:\"\ue7b1\"}.jupyter-wrapper .bp3-icon-move::before{content:\"\ue693\"}.jupyter-wrapper .bp3-icon-mugshot::before{content:\"\ue6db\"}.jupyter-wrapper .bp3-icon-multi-select::before{content:\"\ue680\"}.jupyter-wrapper .bp3-icon-music::before{content:\"\ue6a6\"}.jupyter-wrapper .bp3-icon-new-drawing::before{content:\"\ue905\"}.jupyter-wrapper .bp3-icon-new-grid-item::before{content:\"\ue747\"}.jupyter-wrapper .bp3-icon-new-layer::before{content:\"\ue902\"}.jupyter-wrapper .bp3-icon-new-layers::before{content:\"\ue903\"}.jupyter-wrapper .bp3-icon-new-link::before{content:\"\ue65c\"}.jupyter-wrapper .bp3-icon-new-object::before{content:\"\ue65d\"}.jupyter-wrapper .bp3-icon-new-person::before{content:\"\ue6e9\"}.jupyter-wrapper .bp3-icon-new-prescription::before{content:\"\ue78b\"}.jupyter-wrapper .bp3-icon-new-text-box::before{content:\"\ue65b\"}.jupyter-wrapper .bp3-icon-ninja::before{content:\"\ue675\"}.jupyter-wrapper .bp3-icon-not-equal-to::before{content:\"\ue7e0\"}.jupyter-wrapper .bp3-icon-notifications::before{content:\"\ue624\"}.jupyter-wrapper .bp3-icon-notifications-updated::before{content:\"\ue7b8\"}.jupyter-wrapper .bp3-icon-numbered-list::before{content:\"\ue746\"}.jupyter-wrapper .bp3-icon-numerical::before{content:\"\ue756\"}.jupyter-wrapper .bp3-icon-office::before{content:\"\ue69b\"}.jupyter-wrapper .bp3-icon-offline::before{content:\"\ue67a\"}.jupyter-wrapper .bp3-icon-oil-field::before{content:\"\ue73f\"}.jupyter-wrapper .bp3-icon-one-column::before{content:\"\ue658\"}.jupyter-wrapper .bp3-icon-outdated::before{content:\"\ue7a8\"}.jupyter-wrapper .bp3-icon-page-layout::before{content:\"\ue660\"}.jupyter-wrapper .bp3-icon-panel-stats::before{content:\"\ue777\"}.jupyter-wrapper .bp3-icon-panel-table::before{content:\"\ue778\"}.jupyter-wrapper .bp3-icon-paperclip::before{content:\"\ue664\"}.jupyter-wrapper .bp3-icon-paragraph::before{content:\"\ue76c\"}.jupyter-wrapper .bp3-icon-path::before{content:\"\ue753\"}.jupyter-wrapper .bp3-icon-path-search::before{content:\"\ue65e\"}.jupyter-wrapper .bp3-icon-pause::before{content:\"\ue6a9\"}.jupyter-wrapper .bp3-icon-people::before{content:\"\ue63d\"}.jupyter-wrapper .bp3-icon-percentage::before{content:\"\ue76a\"}.jupyter-wrapper .bp3-icon-person::before{content:\"\ue63c\"}.jupyter-wrapper .bp3-icon-phone::before{content:\"\u260e\"}.jupyter-wrapper .bp3-icon-pie-chart::before{content:\"\ue684\"}.jupyter-wrapper .bp3-icon-pin::before{content:\"\ue646\"}.jupyter-wrapper .bp3-icon-pivot::before{content:\"\ue6f1\"}.jupyter-wrapper .bp3-icon-pivot-table::before{content:\"\ue6eb\"}.jupyter-wrapper .bp3-icon-play::before{content:\"\ue6ab\"}.jupyter-wrapper .bp3-icon-plus::before{content:\"+\"}.jupyter-wrapper .bp3-icon-polygon-filter::before{content:\"\ue6d1\"}.jupyter-wrapper .bp3-icon-power::before{content:\"\ue6d9\"}.jupyter-wrapper .bp3-icon-predictive-analysis::before{content:\"\ue617\"}.jupyter-wrapper .bp3-icon-prescription::before{content:\"\ue78a\"}.jupyter-wrapper .bp3-icon-presentation::before{content:\"\ue687\"}.jupyter-wrapper .bp3-icon-print::before{content:\"\u2399\"}.jupyter-wrapper .bp3-icon-projects::before{content:\"\ue622\"}.jupyter-wrapper .bp3-icon-properties::before{content:\"\ue631\"}.jupyter-wrapper .bp3-icon-property::before{content:\"\ue65a\"}.jupyter-wrapper .bp3-icon-publish-function::before{content:\"\ue752\"}.jupyter-wrapper .bp3-icon-pulse::before{content:\"\ue6e8\"}.jupyter-wrapper .bp3-icon-random::before{content:\"\ue698\"}.jupyter-wrapper .bp3-icon-record::before{content:\"\ue6ae\"}.jupyter-wrapper .bp3-icon-redo::before{content:\"\ue6c4\"}.jupyter-wrapper .bp3-icon-refresh::before{content:\"\ue643\"}.jupyter-wrapper .bp3-icon-regression-chart::before{content:\"\ue758\"}.jupyter-wrapper .bp3-icon-remove::before{content:\"\ue63f\"}.jupyter-wrapper .bp3-icon-remove-column::before{content:\"\ue755\"}.jupyter-wrapper .bp3-icon-remove-column-left::before{content:\"\ue6fd\"}.jupyter-wrapper .bp3-icon-remove-column-right::before{content:\"\ue6fe\"}.jupyter-wrapper .bp3-icon-remove-row-bottom::before{content:\"\ue6fc\"}.jupyter-wrapper .bp3-icon-remove-row-top::before{content:\"\ue6fb\"}.jupyter-wrapper .bp3-icon-repeat::before{content:\"\ue692\"}.jupyter-wrapper .bp3-icon-reset::before{content:\"\ue7d6\"}.jupyter-wrapper .bp3-icon-resolve::before{content:\"\ue672\"}.jupyter-wrapper .bp3-icon-rig::before{content:\"\ue740\"}.jupyter-wrapper .bp3-icon-right-join::before{content:\"\ue7a5\"}.jupyter-wrapper .bp3-icon-ring::before{content:\"\ue6f2\"}.jupyter-wrapper .bp3-icon-rotate-document::before{content:\"\ue6e1\"}.jupyter-wrapper .bp3-icon-rotate-page::before{content:\"\ue6e2\"}.jupyter-wrapper .bp3-icon-satellite::before{content:\"\ue76b\"}.jupyter-wrapper .bp3-icon-saved::before{content:\"\ue6b6\"}.jupyter-wrapper .bp3-icon-scatter-plot::before{content:\"\ue73e\"}.jupyter-wrapper .bp3-icon-search::before{content:\"\ue64b\"}.jupyter-wrapper .bp3-icon-search-around::before{content:\"\ue608\"}.jupyter-wrapper .bp3-icon-search-template::before{content:\"\ue628\"}.jupyter-wrapper .bp3-icon-search-text::before{content:\"\ue663\"}.jupyter-wrapper .bp3-icon-segmented-control::before{content:\"\ue6ec\"}.jupyter-wrapper .bp3-icon-select::before{content:\"\ue616\"}.jupyter-wrapper .bp3-icon-selection::before{content:\"\u29bf\"}.jupyter-wrapper .bp3-icon-send-to::before{content:\"\ue66e\"}.jupyter-wrapper .bp3-icon-send-to-graph::before{content:\"\ue736\"}.jupyter-wrapper .bp3-icon-send-to-map::before{content:\"\ue737\"}.jupyter-wrapper .bp3-icon-series-add::before{content:\"\ue796\"}.jupyter-wrapper .bp3-icon-series-configuration::before{content:\"\ue79a\"}.jupyter-wrapper .bp3-icon-series-derived::before{content:\"\ue799\"}.jupyter-wrapper .bp3-icon-series-filtered::before{content:\"\ue798\"}.jupyter-wrapper .bp3-icon-series-search::before{content:\"\ue797\"}.jupyter-wrapper .bp3-icon-settings::before{content:\"\ue6a2\"}.jupyter-wrapper .bp3-icon-share::before{content:\"\ue62e\"}.jupyter-wrapper .bp3-icon-shield::before{content:\"\ue7b2\"}.jupyter-wrapper .bp3-icon-shop::before{content:\"\ue6c2\"}.jupyter-wrapper .bp3-icon-shopping-cart::before{content:\"\ue6c1\"}.jupyter-wrapper .bp3-icon-signal-search::before{content:\"\ue909\"}.jupyter-wrapper .bp3-icon-sim-card::before{content:\"\ue718\"}.jupyter-wrapper .bp3-icon-slash::before{content:\"\ue769\"}.jupyter-wrapper .bp3-icon-small-cross::before{content:\"\ue6d7\"}.jupyter-wrapper .bp3-icon-small-minus::before{content:\"\ue70e\"}.jupyter-wrapper .bp3-icon-small-plus::before{content:\"\ue70d\"}.jupyter-wrapper .bp3-icon-small-tick::before{content:\"\ue6d8\"}.jupyter-wrapper .bp3-icon-snowflake::before{content:\"\ue7b6\"}.jupyter-wrapper .bp3-icon-social-media::before{content:\"\ue671\"}.jupyter-wrapper .bp3-icon-sort::before{content:\"\ue64f\"}.jupyter-wrapper .bp3-icon-sort-alphabetical::before{content:\"\ue64d\"}.jupyter-wrapper .bp3-icon-sort-alphabetical-desc::before{content:\"\ue6c8\"}.jupyter-wrapper .bp3-icon-sort-asc::before{content:\"\ue6d5\"}.jupyter-wrapper .bp3-icon-sort-desc::before{content:\"\ue6d6\"}.jupyter-wrapper .bp3-icon-sort-numerical::before{content:\"\ue64e\"}.jupyter-wrapper .bp3-icon-sort-numerical-desc::before{content:\"\ue6c9\"}.jupyter-wrapper .bp3-icon-split-columns::before{content:\"\ue750\"}.jupyter-wrapper .bp3-icon-square::before{content:\"\ue686\"}.jupyter-wrapper .bp3-icon-stacked-chart::before{content:\"\ue6e7\"}.jupyter-wrapper .bp3-icon-star::before{content:\"\u2605\"}.jupyter-wrapper .bp3-icon-star-empty::before{content:\"\u2606\"}.jupyter-wrapper .bp3-icon-step-backward::before{content:\"\ue6a7\"}.jupyter-wrapper .bp3-icon-step-chart::before{content:\"\ue70f\"}.jupyter-wrapper .bp3-icon-step-forward::before{content:\"\ue6ad\"}.jupyter-wrapper .bp3-icon-stop::before{content:\"\ue6aa\"}.jupyter-wrapper .bp3-icon-stopwatch::before{content:\"\ue901\"}.jupyter-wrapper .bp3-icon-strikethrough::before{content:\"\ue7a6\"}.jupyter-wrapper .bp3-icon-style::before{content:\"\ue601\"}.jupyter-wrapper .bp3-icon-swap-horizontal::before{content:\"\ue745\"}.jupyter-wrapper .bp3-icon-swap-vertical::before{content:\"\ue744\"}.jupyter-wrapper .bp3-icon-symbol-circle::before{content:\"\ue72e\"}.jupyter-wrapper .bp3-icon-symbol-cross::before{content:\"\ue731\"}.jupyter-wrapper .bp3-icon-symbol-diamond::before{content:\"\ue730\"}.jupyter-wrapper .bp3-icon-symbol-square::before{content:\"\ue72f\"}.jupyter-wrapper .bp3-icon-symbol-triangle-down::before{content:\"\ue733\"}.jupyter-wrapper .bp3-icon-symbol-triangle-up::before{content:\"\ue732\"}.jupyter-wrapper .bp3-icon-tag::before{content:\"\ue61c\"}.jupyter-wrapper .bp3-icon-take-action::before{content:\"\ue6ca\"}.jupyter-wrapper .bp3-icon-taxi::before{content:\"\ue79e\"}.jupyter-wrapper .bp3-icon-text-highlight::before{content:\"\ue6dd\"}.jupyter-wrapper .bp3-icon-th::before{content:\"\ue667\"}.jupyter-wrapper .bp3-icon-th-derived::before{content:\"\ue669\"}.jupyter-wrapper .bp3-icon-th-disconnect::before{content:\"\ue7d8\"}.jupyter-wrapper .bp3-icon-th-filtered::before{content:\"\ue7c6\"}.jupyter-wrapper .bp3-icon-th-list::before{content:\"\ue668\"}.jupyter-wrapper .bp3-icon-thumbs-down::before{content:\"\ue6be\"}.jupyter-wrapper .bp3-icon-thumbs-up::before{content:\"\ue6bd\"}.jupyter-wrapper .bp3-icon-tick::before{content:\"\u2713\"}.jupyter-wrapper .bp3-icon-tick-circle::before{content:\"\ue779\"}.jupyter-wrapper .bp3-icon-time::before{content:\"\u23f2\"}.jupyter-wrapper .bp3-icon-timeline-area-chart::before{content:\"\ue6cd\"}.jupyter-wrapper .bp3-icon-timeline-bar-chart::before{content:\"\ue620\"}.jupyter-wrapper .bp3-icon-timeline-events::before{content:\"\ue61e\"}.jupyter-wrapper .bp3-icon-timeline-line-chart::before{content:\"\ue61f\"}.jupyter-wrapper .bp3-icon-tint::before{content:\"\ue6b2\"}.jupyter-wrapper .bp3-icon-torch::before{content:\"\ue677\"}.jupyter-wrapper .bp3-icon-tractor::before{content:\"\ue90c\"}.jupyter-wrapper .bp3-icon-train::before{content:\"\ue79f\"}.jupyter-wrapper .bp3-icon-translate::before{content:\"\ue759\"}.jupyter-wrapper .bp3-icon-trash::before{content:\"\ue63b\"}.jupyter-wrapper .bp3-icon-tree::before{content:\"\ue7b7\"}.jupyter-wrapper .bp3-icon-trending-down::before{content:\"\ue71a\"}.jupyter-wrapper .bp3-icon-trending-up::before{content:\"\ue719\"}.jupyter-wrapper .bp3-icon-truck::before{content:\"\ue90b\"}.jupyter-wrapper .bp3-icon-two-columns::before{content:\"\ue657\"}.jupyter-wrapper .bp3-icon-unarchive::before{content:\"\ue906\"}.jupyter-wrapper .bp3-icon-underline::before{content:\"\u2381\"}.jupyter-wrapper .bp3-icon-undo::before{content:\"\u238c\"}.jupyter-wrapper .bp3-icon-ungroup-objects::before{content:\"\ue688\"}.jupyter-wrapper .bp3-icon-unknown-vehicle::before{content:\"\ue73d\"}.jupyter-wrapper .bp3-icon-unlock::before{content:\"\ue626\"}.jupyter-wrapper .bp3-icon-unpin::before{content:\"\ue650\"}.jupyter-wrapper .bp3-icon-unresolve::before{content:\"\ue679\"}.jupyter-wrapper .bp3-icon-updated::before{content:\"\ue7a7\"}.jupyter-wrapper .bp3-icon-upload::before{content:\"\ue68f\"}.jupyter-wrapper .bp3-icon-user::before{content:\"\ue627\"}.jupyter-wrapper .bp3-icon-variable::before{content:\"\ue6f5\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-asc::before{content:\"\ue75b\"}.jupyter-wrapper .bp3-icon-vertical-bar-chart-desc::before{content:\"\ue71c\"}.jupyter-wrapper .bp3-icon-vertical-distribution::before{content:\"\ue721\"}.jupyter-wrapper .bp3-icon-video::before{content:\"\ue6a0\"}.jupyter-wrapper .bp3-icon-volume-down::before{content:\"\ue6a4\"}.jupyter-wrapper .bp3-icon-volume-off::before{content:\"\ue6a3\"}.jupyter-wrapper .bp3-icon-volume-up::before{content:\"\ue6a5\"}.jupyter-wrapper .bp3-icon-walk::before{content:\"\ue79d\"}.jupyter-wrapper .bp3-icon-warning-sign::before{content:\"\ue647\"}.jupyter-wrapper .bp3-icon-waterfall-chart::before{content:\"\ue6e6\"}.jupyter-wrapper .bp3-icon-widget::before{content:\"\ue678\"}.jupyter-wrapper .bp3-icon-widget-button::before{content:\"\ue790\"}.jupyter-wrapper .bp3-icon-widget-footer::before{content:\"\ue792\"}.jupyter-wrapper .bp3-icon-widget-header::before{content:\"\ue791\"}.jupyter-wrapper .bp3-icon-wrench::before{content:\"\ue734\"}.jupyter-wrapper .bp3-icon-zoom-in::before{content:\"\ue641\"}.jupyter-wrapper .bp3-icon-zoom-out::before{content:\"\ue642\"}.jupyter-wrapper .bp3-icon-zoom-to-fit::before{content:\"\ue67b\"}.jupyter-wrapper .bp3-submenu>.bp3-popover-wrapper{display:block}.jupyter-wrapper .bp3-submenu .bp3-popover-target{display:block}.jupyter-wrapper .bp3-submenu.bp3-popover{-webkit-box-shadow:none;box-shadow:none;padding:0 5px}.jupyter-wrapper .bp3-submenu.bp3-popover>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-dark .bp3-submenu.bp3-popover>.bp3-popover-content,.jupyter-wrapper .bp3-submenu.bp3-popover.bp3-dark>.bp3-popover-content{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-menu{margin:0;border-radius:3px;background:#fff;min-width:180px;padding:5px;list-style:none;text-align:left;color:#182026}.jupyter-wrapper .bp3-menu-divider{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-divider{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-item{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;border-radius:2px;padding:5px 7px;text-decoration:none;line-height:20px;color:inherit;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-menu-item>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>*{margin-right:7px}.jupyter-wrapper .bp3-menu-item:empty::before,.jupyter-wrapper .bp3-menu-item>:last-child{margin-right:0}.jupyter-wrapper .bp3-menu-item>.bp3-fill{word-break:break-word}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(167,182,194,.3);cursor:pointer;text-decoration:none}.jupyter-wrapper .bp3-menu-item.bp3-disabled{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-dark .bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-menu-item{background-color:rgba(138,155,168,.15);color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{background-color:inherit;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-success{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-menu-item::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-right:7px}.jupyter-wrapper .bp3-menu-item::before,.jupyter-wrapper .bp3-menu-item>.bp3-icon{margin-top:2px;color:#5c7080}.jupyter-wrapper .bp3-menu-item .bp3-menu-item-label{color:#5c7080}.jupyter-wrapper .bp3-menu-item:hover,.jupyter-wrapper .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-menu-item{color:inherit}.jupyter-wrapper .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-menu-item:active{background-color:rgba(115,134,148,.3)}.jupyter-wrapper .bp3-menu-item.bp3-disabled{outline:none !important;background-color:inherit !important;cursor:not-allowed !important;color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(92,112,128,.6) !important}.jupyter-wrapper .bp3-large .bp3-menu-item{padding:9px 7px;line-height:22px;font-size:16px}.jupyter-wrapper .bp3-large .bp3-menu-item .bp3-icon{margin-top:3px}.jupyter-wrapper .bp3-large .bp3-menu-item::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;margin-top:1px;margin-right:10px}.jupyter-wrapper button.bp3-menu-item{border:none;background:none;width:100%;text-align:left}.jupyter-wrapper .bp3-menu-header{display:block;margin:5px;border-top:1px solid rgba(16,22,26,.15);cursor:default;padding-left:2px}.jupyter-wrapper .bp3-dark .bp3-menu-header{border-top-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-menu-header:first-of-type{border-top:none}.jupyter-wrapper .bp3-menu-header>h6{color:#182026;font-weight:600;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;margin:0;padding:10px 7px 0 1px;line-height:17px}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-large .bp3-menu-header>h6{padding-top:15px;padding-bottom:5px;font-size:18px}.jupyter-wrapper .bp3-large .bp3-menu-header:first-of-type>h6{padding-top:0}.jupyter-wrapper .bp3-dark .bp3-menu{background:#30404d;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary .bp3-menu-item-label{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active{background-color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active{background-color:#106ba3}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-primary.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-primary.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success .bp3-menu-item-label{color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active{background-color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active{background-color:#0d8050}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-success.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-success.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning .bp3-menu-item-label{color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active{background-color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active{background-color:#bf7326}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-warning.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-warning.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-icon{color:inherit}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger .bp3-menu-item-label{color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active{background-color:#db3737}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active{background-color:#c23030}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::before,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover::after,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:hover .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-submenu .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-submenu .bp3-dark .bp3-popover-target.bp3-popover-open>.bp3-intent-danger.bp3-menu-item .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger:active .bp3-menu-item-label,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active::after,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-intent-danger.bp3-active .bp3-menu-item-label{color:#fff}.jupyter-wrapper .bp3-dark .bp3-menu-item::before,.jupyter-wrapper .bp3-dark .bp3-menu-item>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item .bp3-menu-item-label{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-active,.jupyter-wrapper .bp3-dark .bp3-menu-item:active{background-color:rgba(138,155,168,.3)}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled::before,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-menu-item.bp3-disabled .bp3-menu-item-label{color:rgba(167,182,194,.6) !important}.jupyter-wrapper .bp3-dark .bp3-menu-divider,.jupyter-wrapper .bp3-dark .bp3-menu-header{border-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-dark .bp3-menu-header>h6{color:#f5f8fa}.jupyter-wrapper .bp3-label .bp3-menu{margin-top:5px}.jupyter-wrapper .bp3-navbar{position:relative;z-index:10;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.2);background-color:#fff;width:100%;height:50px;padding:0 15px}.jupyter-wrapper .bp3-navbar.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-navbar{background-color:#394b59}.jupyter-wrapper .bp3-navbar.bp3-dark{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-navbar{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 0 0 rgba(16,22,26,0),0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-navbar.bp3-fixed-top{position:fixed;top:0;right:0;left:0}.jupyter-wrapper .bp3-navbar-heading{margin-right:15px;font-size:16px}.jupyter-wrapper .bp3-navbar-group{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:50px}.jupyter-wrapper .bp3-navbar-group.bp3-align-left{float:left}.jupyter-wrapper .bp3-navbar-group.bp3-align-right{float:right}.jupyter-wrapper .bp3-navbar-divider{margin:0 10px;border-left:1px solid rgba(16,22,26,.15);height:20px}.jupyter-wrapper .bp3-dark .bp3-navbar-divider{border-left-color:rgba(255,255,255,.15)}.jupyter-wrapper .bp3-non-ideal-state{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;width:100%;height:100%;text-align:center}.jupyter-wrapper .bp3-non-ideal-state>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-non-ideal-state>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-non-ideal-state::before,.jupyter-wrapper .bp3-non-ideal-state>*{margin-bottom:20px}.jupyter-wrapper .bp3-non-ideal-state:empty::before,.jupyter-wrapper .bp3-non-ideal-state>:last-child{margin-bottom:0}.jupyter-wrapper .bp3-non-ideal-state>*{max-width:400px}.jupyter-wrapper .bp3-non-ideal-state-visual{color:rgba(92,112,128,.6);font-size:60px}.jupyter-wrapper .bp3-dark .bp3-non-ideal-state-visual{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-overflow-list{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-wrap:nowrap;flex-wrap:nowrap;min-width:0}.jupyter-wrapper .bp3-overflow-list-spacer{-ms-flex-negative:1;flex-shrink:1;width:1px}.jupyter-wrapper body.bp3-overlay-open{overflow:hidden}.jupyter-wrapper .bp3-overlay{position:static;top:0;right:0;bottom:0;left:0;z-index:20}.jupyter-wrapper .bp3-overlay:not(.bp3-overlay-open){pointer-events:none}.jupyter-wrapper .bp3-overlay.bp3-overlay-container{position:fixed;overflow:hidden}.jupyter-wrapper .bp3-overlay.bp3-overlay-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container{position:fixed;overflow:auto}.jupyter-wrapper .bp3-overlay.bp3-overlay-scroll-container.bp3-overlay-inline{position:absolute}.jupyter-wrapper .bp3-overlay.bp3-overlay-inline{display:inline;overflow:visible}.jupyter-wrapper .bp3-overlay-content{position:fixed;z-index:20}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-content,.jupyter-wrapper .bp3-overlay-scroll-container .bp3-overlay-content{position:absolute}.jupyter-wrapper .bp3-overlay-backdrop{position:fixed;top:0;right:0;bottom:0;left:0;opacity:1;z-index:20;background-color:rgba(16,22,26,.7);overflow:auto;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear{opacity:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-enter-active,.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit{opacity:1}.jupyter-wrapper .bp3-overlay-backdrop.bp3-overlay-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-overlay-backdrop:focus{outline:none}.jupyter-wrapper .bp3-overlay-inline .bp3-overlay-backdrop{position:absolute}.jupyter-wrapper .bp3-panel-stack{position:relative;overflow:hidden}.jupyter-wrapper .bp3-panel-stack-header{display:-webkit-box;display:-ms-flexbox;display:flex;-ms-flex-negative:0;flex-shrink:0;-webkit-box-align:center;-ms-flex-align:center;align-items:center;z-index:1;-webkit-box-shadow:0 1px rgba(16,22,26,.15);box-shadow:0 1px rgba(16,22,26,.15);height:30px}.jupyter-wrapper .bp3-dark .bp3-panel-stack-header{-webkit-box-shadow:0 1px rgba(255,255,255,.15);box-shadow:0 1px rgba(255,255,255,.15)}.jupyter-wrapper .bp3-panel-stack-header>span{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:1;-ms-flex:1;flex:1;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch}.jupyter-wrapper .bp3-panel-stack-header .bp3-heading{margin:0 5px}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back{margin-left:5px;padding-left:0;white-space:nowrap}.jupyter-wrapper .bp3-button.bp3-panel-stack-header-back .bp3-icon{margin:0 2px}.jupyter-wrapper .bp3-panel-stack-view{position:absolute;top:0;right:0;bottom:0;left:0;display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;margin-right:-1px;border-right:1px solid rgba(16,22,26,.15);background-color:#fff;overflow-y:auto}.jupyter-wrapper .bp3-dark .bp3-panel-stack-view{background-color:#30404d}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-push .bp3-panel-stack-exit-active{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear{-webkit-transform:translateX(-50%);transform:translateX(-50%);opacity:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-enter-active,.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-appear-active{-webkit-transform:translate(0%);transform:translate(0%);opacity:1;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit{-webkit-transform:translate(0%);transform:translate(0%);opacity:1}.jupyter-wrapper .bp3-panel-stack-pop .bp3-panel-stack-exit-active{-webkit-transform:translateX(100%);transform:translateX(100%);opacity:0;-webkit-transition-property:opacity,-webkit-transform;transition-property:opacity,-webkit-transform;transition-property:transform,opacity;transition-property:transform,opacity,-webkit-transform;-webkit-transition-duration:400ms;transition-duration:400ms;-webkit-transition-timing-function:ease;transition-timing-function:ease;-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1);display:inline-block;z-index:20;border-radius:3px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow{position:absolute;width:30px;height:30px}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{margin:5px;width:20px;height:20px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover{margin-top:-17px;margin-bottom:17px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{bottom:-11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover{margin-left:17px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{left:-11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover{margin-top:17px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{top:-11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover{margin-right:17px;margin-left:-17px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{right:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-popover>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-popover>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-popover>.bp3-popover-arrow{top:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-popover>.bp3-popover-arrow{right:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-popover>.bp3-popover-arrow{left:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-popover>.bp3-popover-arrow{bottom:-0.3934px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-popover{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-popover{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-popover{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-popover .bp3-popover-content{background:#fff;color:inherit}.jupyter-wrapper .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-popover .bp3-popover-arrow-fill{fill:#fff}.jupyter-wrapper .bp3-popover-enter>.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover{-webkit-transform:scale(0.3);transform:scale(0.3);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover .bp3-popover-content{position:relative;border-radius:3px}.jupyter-wrapper .bp3-popover.bp3-popover-content-sizing .bp3-popover-content{max-width:350px;padding:20px}.jupyter-wrapper .bp3-popover-target+.bp3-overlay .bp3-popover.bp3-popover-content-sizing{width:350px}.jupyter-wrapper .bp3-popover.bp3-minimal{margin:0 !important}.jupyter-wrapper .bp3-popover.bp3-minimal .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-popover.bp3-minimal.bp3-popover,.jupyter-wrapper .bp3-popover-appear-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-popover.bp3-minimal.bp3-popover{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-popover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-content{background:#30404d;color:inherit}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-popover.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-popover .bp3-popover-arrow-fill{fill:#30404d}.jupyter-wrapper .bp3-popover-arrow::before{display:block;position:absolute;-webkit-transform:rotate(45deg);transform:rotate(45deg);border-radius:2px;content:\"\"}.jupyter-wrapper .bp3-tether-pinned .bp3-popover-arrow{display:none}.jupyter-wrapper .bp3-popover-backdrop{background:rgba(255,255,255,0)}.jupyter-wrapper .bp3-transition-container{opacity:1;display:-webkit-box;display:-ms-flexbox;display:flex;z-index:20}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear{opacity:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-enter-active,.jupyter-wrapper .bp3-transition-container.bp3-popover-appear-active{opacity:1;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit{opacity:1}.jupyter-wrapper .bp3-transition-container.bp3-popover-exit-active{opacity:0;-webkit-transition-property:opacity;transition-property:opacity;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-transition-container:focus{outline:none}.jupyter-wrapper .bp3-transition-container.bp3-popover-leave .bp3-popover-content{pointer-events:none}.jupyter-wrapper .bp3-transition-container[data-x-out-of-boundaries]{display:none}.jupyter-wrapper span.bp3-popover-target{display:inline-block}.jupyter-wrapper .bp3-popover-wrapper.bp3-fill{width:100%}.jupyter-wrapper .bp3-portal{position:absolute;top:0;right:0;left:0}@-webkit-keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}@keyframes linear-progress-bar-stripes{from{background-position:0 0}to{background-position:30px 0}}.jupyter-wrapper .bp3-progress-bar{display:block;position:relative;border-radius:40px;background:rgba(92,112,128,.2);width:100%;height:8px;overflow:hidden}.jupyter-wrapper .bp3-progress-bar .bp3-progress-meter{position:absolute;border-radius:40px;background:linear-gradient(-45deg, rgba(255, 255, 255, 0.2) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.2) 50%, rgba(255, 255, 255, 0.2) 75%, transparent 75%);background-color:rgba(92,112,128,.8);background-size:30px 30px;width:100%;height:100%;-webkit-transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:width 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-progress-bar:not(.bp3-no-animation):not(.bp3-no-stripes) .bp3-progress-meter{animation:linear-progress-bar-stripes 300ms linear infinite reverse}.jupyter-wrapper .bp3-progress-bar.bp3-no-stripes .bp3-progress-meter{background-image:none}.jupyter-wrapper .bp3-dark .bp3-progress-bar{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-dark .bp3-progress-bar .bp3-progress-meter{background-color:#8a9ba8}.jupyter-wrapper .bp3-progress-bar.bp3-intent-primary .bp3-progress-meter{background-color:#137cbd}.jupyter-wrapper .bp3-progress-bar.bp3-intent-success .bp3-progress-meter{background-color:#0f9960}.jupyter-wrapper .bp3-progress-bar.bp3-intent-warning .bp3-progress-meter{background-color:#d9822b}.jupyter-wrapper .bp3-progress-bar.bp3-intent-danger .bp3-progress-meter{background-color:#db3737}@-webkit-keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}@keyframes skeleton-glow{from{border-color:rgba(206,217,224,.2);background:rgba(206,217,224,.2)}to{border-color:rgba(92,112,128,.2);background:rgba(92,112,128,.2)}}.jupyter-wrapper .bp3-skeleton{border-color:rgba(206,217,224,.2) !important;border-radius:2px;-webkit-box-shadow:none !important;box-shadow:none !important;background:rgba(206,217,224,.2);background-clip:padding-box !important;cursor:default;color:transparent !important;-webkit-animation:1000ms linear infinite alternate skeleton-glow;animation:1000ms linear infinite alternate skeleton-glow;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-skeleton::before,.jupyter-wrapper .bp3-skeleton::after,.jupyter-wrapper .bp3-skeleton *{visibility:hidden !important}.jupyter-wrapper .bp3-slider{width:100%;min-width:150px;height:40px;position:relative;outline:none;cursor:default;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-slider:hover{cursor:pointer}.jupyter-wrapper .bp3-slider:active{cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-slider.bp3-disabled{opacity:.5;cursor:not-allowed}.jupyter-wrapper .bp3-slider.bp3-slider-unlabeled{height:16px}.jupyter-wrapper .bp3-slider-track,.jupyter-wrapper .bp3-slider-progress{top:5px;right:0;left:0;height:6px;position:absolute}.jupyter-wrapper .bp3-slider-track{border-radius:3px;overflow:hidden}.jupyter-wrapper .bp3-slider-progress{background:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-dark .bp3-slider-progress{background:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-slider-progress.bp3-intent-primary{background-color:#137cbd}.jupyter-wrapper .bp3-slider-progress.bp3-intent-success{background-color:#0f9960}.jupyter-wrapper .bp3-slider-progress.bp3-intent-warning{background-color:#d9822b}.jupyter-wrapper .bp3-slider-progress.bp3-intent-danger{background-color:#db3737}.jupyter-wrapper .bp3-slider-handle{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-color:#f5f8fa;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.8)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.8), rgba(255, 255, 255, 0));color:#182026;position:absolute;top:0;left:0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:pointer;width:16px;height:16px}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5}.jupyter-wrapper .bp3-slider-handle:active,.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none}.jupyter-wrapper .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-slider-handle.bp3-disabled{outline:none;-webkit-box-shadow:none;box-shadow:none;background-color:rgba(206,217,224,.5);background-image:none;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle:disabled.bp3-active:hover,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active,.jupyter-wrapper .bp3-slider-handle.bp3-disabled.bp3-active:hover{background:rgba(206,217,224,.7)}.jupyter-wrapper .bp3-slider-handle:focus{z-index:1}.jupyter-wrapper .bp3-slider-handle:hover{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 -1px 0 rgba(16,22,26,.1);background-clip:padding-box;background-color:#ebf1f5;z-index:2;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 1px 1px rgba(16,22,26,.2);cursor:-webkit-grab;cursor:grab}.jupyter-wrapper .bp3-slider-handle.bp3-active{-webkit-box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:inset 0 0 0 1px rgba(16,22,26,.2),inset 0 1px 2px rgba(16,22,26,.2);background-color:#d8e1e8;background-image:none;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);box-shadow:0 0 0 1px rgba(16,22,26,.2),inset 0 1px 1px rgba(16,22,26,.1);cursor:-webkit-grabbing;cursor:grabbing}.jupyter-wrapper .bp3-disabled .bp3-slider-handle{-webkit-box-shadow:none;box-shadow:none;background:#bfccd6;pointer-events:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#394b59;background-image:-webkit-gradient(linear, left top, left bottom, from(rgba(255, 255, 255, 0.05)), to(rgba(255, 255, 255, 0)));background-image:linear-gradient(to bottom, rgba(255, 255, 255, 0.05), rgba(255, 255, 255, 0));color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover,.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-dark .bp3-slider-handle:active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.6),inset 0 1px 2px rgba(16,22,26,.2);background-color:#202b33;background-image:none}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(57,75,89,.5);background-image:none;color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-slider-handle:disabled.bp3-active,.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-disabled.bp3-active{background:rgba(57,75,89,.7)}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-button-spinner .bp3-spinner-head{background:rgba(16,22,26,.5);stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-slider-handle,.jupyter-wrapper .bp3-dark .bp3-slider-handle:hover{background-color:#394b59}.jupyter-wrapper .bp3-dark .bp3-slider-handle.bp3-active{background-color:#293742}.jupyter-wrapper .bp3-dark .bp3-disabled .bp3-slider-handle{border-color:#5c7080;-webkit-box-shadow:none;box-shadow:none;background:#5c7080}.jupyter-wrapper .bp3-slider-handle .bp3-slider-label{margin-left:8px;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-disabled .bp3-slider-handle .bp3-slider-label{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-slider-handle.bp3-start,.jupyter-wrapper .bp3-slider-handle.bp3-end{width:8px}.jupyter-wrapper .bp3-slider-handle.bp3-start{border-top-right-radius:0;border-bottom-right-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end{margin-left:8px;border-top-left-radius:0;border-bottom-left-radius:0}.jupyter-wrapper .bp3-slider-handle.bp3-end .bp3-slider-label{margin-left:0}.jupyter-wrapper .bp3-slider-label{-webkit-transform:translate(-50%, 20px);transform:translate(-50%, 20px);display:inline-block;position:absolute;padding:2px 5px;vertical-align:top;line-height:1;font-size:12px}.jupyter-wrapper .bp3-slider.bp3-vertical{width:40px;min-width:40px;height:150px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-track,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:0;bottom:0;left:5px;width:6px;height:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-progress{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-label{-webkit-transform:translate(20px, 50%);transform:translate(20px, 50%)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle{top:auto}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle .bp3-slider-label{margin-top:-8px;margin-left:0}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end,.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{margin-left:0;width:16px;height:8px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start{border-top-left-radius:0;border-bottom-right-radius:3px}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-start .bp3-slider-label{-webkit-transform:translate(20px);transform:translate(20px)}.jupyter-wrapper .bp3-slider.bp3-vertical .bp3-slider-handle.bp3-end{margin-bottom:8px;border-top-left-radius:3px;border-bottom-left-radius:0;border-bottom-right-radius:0}@-webkit-keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes pt-spinner-animation{from{-webkit-transform:rotate(0deg);transform:rotate(0deg)}to{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.jupyter-wrapper .bp3-spinner{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;justify-content:center;overflow:visible;vertical-align:middle}.jupyter-wrapper .bp3-spinner svg{display:block}.jupyter-wrapper .bp3-spinner path{fill-opacity:0}.jupyter-wrapper .bp3-spinner .bp3-spinner-head{-webkit-transform-origin:center;transform-origin:center;-webkit-transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:stroke-dashoffset 200ms cubic-bezier(0.4, 1, 0.75, 0.9);stroke:rgba(92,112,128,.8);stroke-linecap:round}.jupyter-wrapper .bp3-spinner .bp3-spinner-track{stroke:rgba(92,112,128,.2)}.jupyter-wrapper .bp3-spinner-animation{-webkit-animation:pt-spinner-animation 500ms linear infinite;animation:pt-spinner-animation 500ms linear infinite}.jupyter-wrapper .bp3-no-spin>.bp3-spinner-animation{-webkit-animation:none;animation:none}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-head{stroke:#8a9ba8}.jupyter-wrapper .bp3-dark .bp3-spinner .bp3-spinner-track{stroke:rgba(16,22,26,.5)}.jupyter-wrapper .bp3-spinner.bp3-intent-primary .bp3-spinner-head{stroke:#137cbd}.jupyter-wrapper .bp3-spinner.bp3-intent-success .bp3-spinner-head{stroke:#0f9960}.jupyter-wrapper .bp3-spinner.bp3-intent-warning .bp3-spinner-head{stroke:#d9822b}.jupyter-wrapper .bp3-spinner.bp3-intent-danger .bp3-spinner-head{stroke:#db3737}.jupyter-wrapper .bp3-tabs.bp3-vertical{display:-webkit-box;display:-ms-flexbox;display:flex}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list{-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab{border-radius:3px;width:100%;padding:0 10px}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab[aria-selected=true]{-webkit-box-shadow:none;box-shadow:none;background-color:rgba(19,124,189,.2)}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-list .bp3-tab-indicator-wrapper .bp3-tab-indicator{top:0;right:0;bottom:0;left:0;border-radius:3px;background-color:rgba(19,124,189,.2);height:auto}.jupyter-wrapper .bp3-tabs.bp3-vertical>.bp3-tab-panel{margin-top:0;padding-left:20px}.jupyter-wrapper .bp3-tab-list{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end;position:relative;margin:0;border:none;padding:0;list-style:none}.jupyter-wrapper .bp3-tab-list>*:not(:last-child){margin-right:20px}.jupyter-wrapper .bp3-tab{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;position:relative;cursor:pointer;max-width:100%;vertical-align:top;line-height:30px;color:#182026;font-size:14px}.jupyter-wrapper .bp3-tab a{display:block;text-decoration:none;color:inherit}.jupyter-wrapper .bp3-tab-indicator-wrapper~.bp3-tab{-webkit-box-shadow:none !important;box-shadow:none !important;background-color:transparent !important}.jupyter-wrapper .bp3-tab[aria-disabled=true]{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tab[aria-selected=true]{border-radius:0;-webkit-box-shadow:inset 0 -3px 0 #106ba3;box-shadow:inset 0 -3px 0 #106ba3}.jupyter-wrapper .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-tab:not([aria-disabled=true]):hover{color:#106ba3}.jupyter-wrapper .bp3-tab:focus{-moz-outline-radius:0}.jupyter-wrapper .bp3-large>.bp3-tab{line-height:40px;font-size:16px}.jupyter-wrapper .bp3-tab-panel{margin-top:20px}.jupyter-wrapper .bp3-tab-panel[aria-hidden=true]{display:none}.jupyter-wrapper .bp3-tab-indicator-wrapper{position:absolute;top:0;left:0;-webkit-transform:translateX(0),translateY(0);transform:translateX(0),translateY(0);-webkit-transition:height,width,-webkit-transform;transition:height,width,-webkit-transform;transition:height,transform,width;transition:height,transform,width,-webkit-transform;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);pointer-events:none}.jupyter-wrapper .bp3-tab-indicator-wrapper .bp3-tab-indicator{position:absolute;right:0;bottom:0;left:0;background-color:#106ba3;height:3px}.jupyter-wrapper .bp3-tab-indicator-wrapper.bp3-no-animation{-webkit-transition:none;transition:none}.jupyter-wrapper .bp3-dark .bp3-tab{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tab[aria-disabled=true]{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true]{-webkit-box-shadow:inset 0 -3px 0 #48aff0;box-shadow:inset 0 -3px 0 #48aff0}.jupyter-wrapper .bp3-dark .bp3-tab[aria-selected=true],.jupyter-wrapper .bp3-dark .bp3-tab:not([aria-disabled=true]):hover{color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tab-indicator{background-color:#48aff0}.jupyter-wrapper .bp3-flex-expander{-webkit-box-flex:1;-ms-flex:1 1;flex:1 1}.jupyter-wrapper .bp3-tag{display:-webkit-inline-box;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:relative;border:none;border-radius:3px;-webkit-box-shadow:none;box-shadow:none;background-color:#5c7080;min-width:20px;max-width:100%;min-height:20px;padding:2px 6px;line-height:16px;color:#f5f8fa;font-size:12px}.jupyter-wrapper .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-interactive:hover{background-color:rgba(92,112,128,.85)}.jupyter-wrapper .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-interactive:active{background-color:rgba(92,112,128,.7)}.jupyter-wrapper .bp3-tag>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag::before,.jupyter-wrapper .bp3-tag>*{margin-right:4px}.jupyter-wrapper .bp3-tag:empty::before,.jupyter-wrapper .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag:focus{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag.bp3-round{border-radius:30px;padding-right:8px;padding-left:8px}.jupyter-wrapper .bp3-dark .bp3-tag{background-color:#bfccd6;color:#182026}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:hover{background-color:rgba(191,204,214,.85)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-interactive:active{background-color:rgba(191,204,214,.7)}.jupyter-wrapper .bp3-dark .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag .bp3-icon-large{fill:currentColor}.jupyter-wrapper .bp3-tag>.bp3-icon,.jupyter-wrapper .bp3-tag .bp3-icon-standard,.jupyter-wrapper .bp3-tag .bp3-icon-large{fill:#fff}.jupyter-wrapper .bp3-tag.bp3-large,.jupyter-wrapper .bp3-large .bp3-tag{min-width:30px;min-height:30px;padding:0 10px;line-height:20px;font-size:14px}.jupyter-wrapper .bp3-tag.bp3-large::before,.jupyter-wrapper .bp3-tag.bp3-large>*,.jupyter-wrapper .bp3-large .bp3-tag::before,.jupyter-wrapper .bp3-large .bp3-tag>*{margin-right:7px}.jupyter-wrapper .bp3-tag.bp3-large:empty::before,.jupyter-wrapper .bp3-tag.bp3-large>:last-child,.jupyter-wrapper .bp3-large .bp3-tag:empty::before,.jupyter-wrapper .bp3-large .bp3-tag>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag.bp3-large.bp3-round,.jupyter-wrapper .bp3-large .bp3-tag.bp3-round{padding-right:12px;padding-left:12px}.jupyter-wrapper .bp3-tag.bp3-intent-primary{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-success{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-warning{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.7)}.jupyter-wrapper .bp3-tag.bp3-intent-danger{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.85)}.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.7)}.jupyter-wrapper .bp3-tag.bp3-fill{display:-webkit-box;display:-ms-flexbox;display:flex;width:100%}.jupyter-wrapper .bp3-tag.bp3-minimal>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal .bp3-icon-large{fill:#5c7080}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){background-color:rgba(138,155,168,.2);color:#182026}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(92,112,128,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]){color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:hover{background-color:rgba(191,204,214,.3)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]).bp3-interactive:active{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-])>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal:not([class*=bp3-intent-]) .bp3-icon-large{fill:#a7b6c2}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.15);color:#106ba3}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-primary .bp3-icon-large{fill:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary{background-color:rgba(19,124,189,.25);color:#48aff0}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:hover{background-color:rgba(19,124,189,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-primary.bp3-interactive:active{background-color:rgba(19,124,189,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.15);color:#0d8050}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-success .bp3-icon-large{fill:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success{background-color:rgba(15,153,96,.25);color:#3dcc91}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:hover{background-color:rgba(15,153,96,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-success.bp3-interactive:active{background-color:rgba(15,153,96,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.15);color:#bf7326}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-warning .bp3-icon-large{fill:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning{background-color:rgba(217,130,43,.25);color:#ffb366}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:hover{background-color:rgba(217,130,43,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-warning.bp3-interactive:active{background-color:rgba(217,130,43,.45)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.15);color:#c23030}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.25)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger>.bp3-icon,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-standard,.jupyter-wrapper .bp3-tag.bp3-minimal.bp3-intent-danger .bp3-icon-large{fill:#db3737}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger{background-color:rgba(219,55,55,.25);color:#ff7373}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive{cursor:pointer}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:hover{background-color:rgba(219,55,55,.35)}.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive.bp3-active,.jupyter-wrapper .bp3-dark .bp3-tag.bp3-minimal.bp3-intent-danger.bp3-interactive:active{background-color:rgba(219,55,55,.45)}.jupyter-wrapper .bp3-tag-remove{display:-webkit-box;display:-ms-flexbox;display:flex;opacity:.5;margin-top:-2px;margin-right:-6px !important;margin-bottom:-2px;border:none;background:none;cursor:pointer;padding:2px;padding-left:0;color:inherit}.jupyter-wrapper .bp3-tag-remove:hover{opacity:.8;background:none;text-decoration:none}.jupyter-wrapper .bp3-tag-remove:active{opacity:1}.jupyter-wrapper .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons16\",sans-serif;font-size:16px;font-weight:400;font-style:normal;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;content:\"\ue6d7\"}.jupyter-wrapper .bp3-large .bp3-tag-remove{margin-right:-10px !important;padding:5px;padding-left:0}.jupyter-wrapper .bp3-large .bp3-tag-remove:empty::before{line-height:1;font-family:\"Icons20\",sans-serif;font-size:20px;font-weight:400;font-style:normal}.jupyter-wrapper .bp3-tag-input{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;cursor:text;height:auto;min-height:30px;padding-right:0;padding-left:5px;line-height:inherit}.jupyter-wrapper .bp3-tag-input>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input>.bp3-tag-input-values{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-icon{margin-top:7px;margin-right:7px;margin-left:2px;color:#5c7080}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-orient:horizontal;-webkit-box-direction:normal;-ms-flex-direction:row;flex-direction:row;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-ms-flex-item-align:stretch;align-self:stretch;margin-top:5px;margin-right:7px;min-width:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{-webkit-box-flex:0;-ms-flex-positive:0;flex-grow:0;-ms-flex-negative:0;flex-shrink:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>.bp3-fill{-webkit-box-flex:1;-ms-flex-positive:1;flex-grow:1;-ms-flex-negative:1;flex-shrink:1}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-right:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:empty::before,.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values:first-child .bp3-input-ghost:first-child{padding-left:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag-input-values>*{margin-bottom:5px}.jupyter-wrapper .bp3-tag-input .bp3-tag{overflow-wrap:break-word}.jupyter-wrapper .bp3-tag-input .bp3-tag.bp3-active{outline:rgba(19,124,189,.6) auto 2px;outline-offset:0;-moz-outline-radius:6px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;width:80px;line-height:20px}.jupyter-wrapper .bp3-tag-input .bp3-input-ghost:disabled,.jupyter-wrapper .bp3-tag-input .bp3-input-ghost.bp3-disabled{cursor:not-allowed}.jupyter-wrapper .bp3-tag-input .bp3-button,.jupyter-wrapper .bp3-tag-input .bp3-spinner{margin:3px;margin-left:0}.jupyter-wrapper .bp3-tag-input .bp3-button{min-width:24px;min-height:24px;padding:0 7px}.jupyter-wrapper .bp3-tag-input.bp3-large{height:auto;min-height:40px}.jupyter-wrapper .bp3-tag-input.bp3-large::before,.jupyter-wrapper .bp3-tag-input.bp3-large>*{margin-right:10px}.jupyter-wrapper .bp3-tag-input.bp3-large:empty::before,.jupyter-wrapper .bp3-tag-input.bp3-large>:last-child{margin-right:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-tag-input-icon{margin-top:10px;margin-left:5px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-input-ghost{line-height:30px}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-button{min-width:30px;min-height:30px;padding:5px 10px;margin:5px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-large .bp3-spinner{margin:8px;margin-left:0}.jupyter-wrapper .bp3-tag-input.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 1px 1px rgba(16,22,26,.2);background-color:#fff}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tag-input.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 1px 1px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-tag-input-icon,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-tag-input-icon{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost{color:#f5f8fa}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-webkit-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-webkit-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-moz-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-moz-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost:-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost:-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::-ms-input-placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::-ms-input-placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input .bp3-input-ghost::placeholder,.jupyter-wrapper .bp3-tag-input.bp3-dark .bp3-input-ghost::placeholder{color:rgba(167,182,194,.6)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active{-webkit-box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #137cbd,0 0 0 1px #137cbd,0 0 0 3px rgba(19,124,189,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);background-color:rgba(16,22,26,.3)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-primary,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-primary{-webkit-box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #106ba3,0 0 0 3px rgba(16,107,163,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-success,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-success{-webkit-box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #0d8050,0 0 0 3px rgba(13,128,80,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-warning,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-warning{-webkit-box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #bf7326,0 0 0 3px rgba(191,115,38,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-dark .bp3-tag-input.bp3-active.bp3-intent-danger,.jupyter-wrapper .bp3-tag-input.bp3-dark.bp3-active.bp3-intent-danger{-webkit-box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4);box-shadow:0 0 0 1px #c23030,0 0 0 3px rgba(194,48,48,.3),inset 0 0 0 1px rgba(16,22,26,.3),inset 0 1px 1px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-input-ghost{border:none;-webkit-box-shadow:none;box-shadow:none;background:none;padding:0}.jupyter-wrapper .bp3-input-ghost::-webkit-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-moz-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::-ms-input-placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost::placeholder{opacity:1;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-input-ghost:focus{outline:none !important}.jupyter-wrapper .bp3-toast{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start;position:relative !important;margin:20px 0 0;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);background-color:#fff;min-width:300px;max-width:500px;pointer-events:all}.jupyter-wrapper .bp3-toast.bp3-toast-enter,.jupyter-wrapper .bp3-toast.bp3-toast-appear{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-enter~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px)}.jupyter-wrapper .bp3-toast.bp3-toast-enter-active~.bp3-toast,.jupyter-wrapper .bp3-toast.bp3-toast-appear-active~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);transition-timing-function:cubic-bezier(0.54, 1.12, 0.38, 1.11);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit{opacity:1;-webkit-filter:blur(0);filter:blur(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active{opacity:0;-webkit-filter:blur(10px);filter:blur(10px);-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:opacity,filter;transition-property:opacity,filter,-webkit-filter;-webkit-transition-duration:300ms;transition-duration:300ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-toast.bp3-toast-exit~.bp3-toast{-webkit-transform:translateY(0);transform:translateY(0)}.jupyter-wrapper .bp3-toast.bp3-toast-exit-active~.bp3-toast{-webkit-transform:translateY(-40px);transform:translateY(-40px);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:50ms;transition-delay:50ms}.jupyter-wrapper .bp3-toast .bp3-button-group{-webkit-box-flex:0;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;padding-left:0}.jupyter-wrapper .bp3-toast>.bp3-icon{margin:12px;margin-right:0;color:#5c7080}.jupyter-wrapper .bp3-toast.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-toast{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);background-color:#394b59}.jupyter-wrapper .bp3-toast.bp3-dark>.bp3-icon,.jupyter-wrapper .bp3-dark .bp3-toast>.bp3-icon{color:#a7b6c2}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] a:hover{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-]>.bp3-icon{color:#fff}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::before,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button .bp3-icon,.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{color:rgba(255,255,255,.7) !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:focus{outline-color:rgba(255,255,255,.5)}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:hover{background-color:rgba(255,255,255,.15) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button:active{background-color:rgba(255,255,255,.3) !important;color:#fff !important}.jupyter-wrapper .bp3-toast[class*=bp3-intent-] .bp3-button::after{background:rgba(255,255,255,.3) !important}.jupyter-wrapper .bp3-toast.bp3-intent-primary{background-color:#137cbd;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-success{background-color:#0f9960;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-warning{background-color:#d9822b;color:#fff}.jupyter-wrapper .bp3-toast.bp3-intent-danger{background-color:#db3737;color:#fff}.jupyter-wrapper .bp3-toast-message{-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;padding:11px;word-break:break-word}.jupyter-wrapper .bp3-toast-container{display:-webkit-box !important;display:-ms-flexbox !important;display:flex !important;-webkit-box-orient:vertical;-webkit-box-direction:normal;-ms-flex-direction:column;flex-direction:column;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;right:0;left:0;z-index:40;overflow:hidden;padding:0 20px 20px;pointer-events:none}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-top{top:0;bottom:auto}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-bottom{-webkit-box-orient:vertical;-webkit-box-direction:reverse;-ms-flex-direction:column-reverse;flex-direction:column-reverse;top:auto;bottom:0}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-left{-webkit-box-align:start;-ms-flex-align:start;align-items:flex-start}.jupyter-wrapper .bp3-toast-container.bp3-toast-container-right{-webkit-box-align:end;-ms-flex-align:end;align-items:flex-end}.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-enter:not(.bp3-toast-enter-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active),.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-appear:not(.bp3-toast-appear-active)~.bp3-toast,.jupyter-wrapper .bp3-toast-container-bottom .bp3-toast.bp3-toast-leave-active~.bp3-toast{-webkit-transform:translateY(60px);transform:translateY(60px)}.jupyter-wrapper .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 2px 4px rgba(16,22,26,.2),0 8px 24px rgba(16,22,26,.2);-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow{position:absolute;width:22px;height:22px}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{margin:4px;width:14px;height:14px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip{margin-top:-11px;margin-bottom:11px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{bottom:-8px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(-90deg);transform:rotate(-90deg)}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip{margin-left:11px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{left:-8px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(0);transform:rotate(0)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip{margin-top:11px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{top:-8px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip{margin-right:11px;margin-left:-11px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{right:-8px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow svg{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.jupyter-wrapper .bp3-tether-element-attached-middle>.bp3-tooltip>.bp3-popover-arrow{top:50%;-webkit-transform:translateY(-50%);transform:translateY(-50%)}.jupyter-wrapper .bp3-tether-element-attached-center>.bp3-tooltip>.bp3-popover-arrow{right:50%;-webkit-transform:translateX(50%);transform:translateX(50%)}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-target-attached-top>.bp3-tooltip>.bp3-popover-arrow{top:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-right.bp3-tether-target-attached-right>.bp3-tooltip>.bp3-popover-arrow{right:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-left.bp3-tether-target-attached-left>.bp3-tooltip>.bp3-popover-arrow{left:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-target-attached-bottom>.bp3-tooltip>.bp3-popover-arrow{bottom:-0.22183px}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:top left;transform-origin:top left}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:top center;transform-origin:top center}.jupyter-wrapper .bp3-tether-element-attached-top.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:top right;transform-origin:top right}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:center left;transform-origin:center left}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:center center;transform-origin:center center}.jupyter-wrapper .bp3-tether-element-attached-middle.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:center right;transform-origin:center right}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-left>.bp3-tooltip{-webkit-transform-origin:bottom left;transform-origin:bottom left}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-center>.bp3-tooltip{-webkit-transform-origin:bottom center;transform-origin:bottom center}.jupyter-wrapper .bp3-tether-element-attached-bottom.bp3-tether-element-attached-right>.bp3-tooltip{-webkit-transform-origin:bottom right;transform-origin:bottom right}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{background:#394b59;color:#f5f8fa}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.2);box-shadow:1px 1px 6px rgba(16,22,26,.2)}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.1}.jupyter-wrapper .bp3-tooltip .bp3-popover-arrow-fill{fill:#394b59}.jupyter-wrapper .bp3-popover-enter>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8)}.jupyter-wrapper .bp3-popover-enter-active>.bp3-tooltip,.jupyter-wrapper .bp3-popover-appear-active>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-popover-exit>.bp3-tooltip{-webkit-transform:scale(1);transform:scale(1)}.jupyter-wrapper .bp3-popover-exit-active>.bp3-tooltip{-webkit-transform:scale(0.8);transform:scale(0.8);-webkit-transition-property:-webkit-transform;transition-property:-webkit-transform;transition-property:transform;transition-property:transform,-webkit-transform;-webkit-transition-duration:100ms;transition-duration:100ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-tooltip .bp3-popover-content{padding:10px 12px}.jupyter-wrapper .bp3-tooltip.bp3-dark,.jupyter-wrapper .bp3-dark .bp3-tooltip{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 2px 4px rgba(16,22,26,.4),0 8px 24px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-content,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-content{background:#e1e8ed;color:#394b59}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow::before,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow::before{-webkit-box-shadow:1px 1px 6px rgba(16,22,26,.4);box-shadow:1px 1px 6px rgba(16,22,26,.4)}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-border,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-border{fill:#10161a;fill-opacity:.2}.jupyter-wrapper .bp3-tooltip.bp3-dark .bp3-popover-arrow-fill,.jupyter-wrapper .bp3-dark .bp3-tooltip .bp3-popover-arrow-fill{fill:#e1e8ed}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-content{background:#137cbd;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-primary .bp3-popover-arrow-fill{fill:#137cbd}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-content{background:#0f9960;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-success .bp3-popover-arrow-fill{fill:#0f9960}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-content{background:#d9822b;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-warning .bp3-popover-arrow-fill{fill:#d9822b}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-content{background:#db3737;color:#fff}.jupyter-wrapper .bp3-tooltip.bp3-intent-danger .bp3-popover-arrow-fill{fill:#db3737}.jupyter-wrapper .bp3-tooltip-indicator{border-bottom:dotted 1px;cursor:help}.jupyter-wrapper .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-tree .bp3-icon-large{color:#5c7080}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-tree-node-list{margin:0;padding-left:0;list-style:none}.jupyter-wrapper .bp3-tree-root{position:relative;background-color:transparent;cursor:default;padding-left:0}.jupyter-wrapper .bp3-tree-node-content-0{padding-left:0px}.jupyter-wrapper .bp3-tree-node-content-1{padding-left:23px}.jupyter-wrapper .bp3-tree-node-content-2{padding-left:46px}.jupyter-wrapper .bp3-tree-node-content-3{padding-left:69px}.jupyter-wrapper .bp3-tree-node-content-4{padding-left:92px}.jupyter-wrapper .bp3-tree-node-content-5{padding-left:115px}.jupyter-wrapper .bp3-tree-node-content-6{padding-left:138px}.jupyter-wrapper .bp3-tree-node-content-7{padding-left:161px}.jupyter-wrapper .bp3-tree-node-content-8{padding-left:184px}.jupyter-wrapper .bp3-tree-node-content-9{padding-left:207px}.jupyter-wrapper .bp3-tree-node-content-10{padding-left:230px}.jupyter-wrapper .bp3-tree-node-content-11{padding-left:253px}.jupyter-wrapper .bp3-tree-node-content-12{padding-left:276px}.jupyter-wrapper .bp3-tree-node-content-13{padding-left:299px}.jupyter-wrapper .bp3-tree-node-content-14{padding-left:322px}.jupyter-wrapper .bp3-tree-node-content-15{padding-left:345px}.jupyter-wrapper .bp3-tree-node-content-16{padding-left:368px}.jupyter-wrapper .bp3-tree-node-content-17{padding-left:391px}.jupyter-wrapper .bp3-tree-node-content-18{padding-left:414px}.jupyter-wrapper .bp3-tree-node-content-19{padding-left:437px}.jupyter-wrapper .bp3-tree-node-content-20{padding-left:460px}.jupyter-wrapper .bp3-tree-node-content{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:30px;padding-right:5px}.jupyter-wrapper .bp3-tree-node-content:hover{background-color:rgba(191,204,214,.4)}.jupyter-wrapper .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node-caret-none{min-width:30px}.jupyter-wrapper .bp3-tree-node-caret{color:#5c7080;-webkit-transform:rotate(0deg);transform:rotate(0deg);cursor:pointer;padding:7px;-webkit-transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9);transition:transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9),-webkit-transform 200ms cubic-bezier(0.4, 1, 0.75, 0.9)}.jupyter-wrapper .bp3-tree-node-caret:hover{color:#182026}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree-node-caret:hover{color:#f5f8fa}.jupyter-wrapper .bp3-tree-node-caret.bp3-tree-node-caret-open{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.jupyter-wrapper .bp3-tree-node-caret.bp3-icon-standard::before{content:\"\ue695\"}.jupyter-wrapper .bp3-tree-node-icon{position:relative;margin-right:7px}.jupyter-wrapper .bp3-tree-node-label{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;word-wrap:normal;-webkit-box-flex:1;-ms-flex:1 1 auto;flex:1 1 auto;position:relative;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-label span{display:inline}.jupyter-wrapper .bp3-tree-node-secondary-label{padding:0 5px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-wrapper,.jupyter-wrapper .bp3-tree-node-secondary-label .bp3-popover-target{display:-webkit-box;display:-ms-flexbox;display:flex;-webkit-box-align:center;-ms-flex-align:center;align-items:center}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-content{background-color:inherit;cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-caret,.jupyter-wrapper .bp3-tree-node.bp3-disabled .bp3-tree-node-icon{cursor:not-allowed;color:rgba(92,112,128,.6)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-standard,.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-icon-large{color:#fff}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret::before{color:rgba(255,255,255,.7)}.jupyter-wrapper .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content .bp3-tree-node-caret:hover::before{color:#fff}.jupyter-wrapper .bp3-dark .bp3-tree-node-content:hover{background-color:rgba(92,112,128,.3)}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large{color:#a7b6c2}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-primary,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-primary{color:#137cbd}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-success,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-success{color:#0f9960}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-warning,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-warning{color:#d9822b}.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-standard.bp3-intent-danger,.jupyter-wrapper .bp3-dark .bp3-tree .bp3-icon-large.bp3-intent-danger{color:#db3737}.jupyter-wrapper .bp3-dark .bp3-tree-node.bp3-tree-node-selected>.bp3-tree-node-content{background-color:#137cbd}.jupyter-wrapper .bp3-omnibar{-webkit-filter:blur(0);filter:blur(0);opacity:1;top:20vh;left:calc(50% - 250px);z-index:21;border-radius:3px;-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);box-shadow:0 0 0 1px rgba(16,22,26,.1),0 4px 8px rgba(16,22,26,.2),0 18px 46px 6px rgba(16,22,26,.2);background-color:#fff;width:500px}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2}.jupyter-wrapper .bp3-omnibar.bp3-overlay-enter-active,.jupyter-wrapper .bp3-omnibar.bp3-overlay-appear-active{-webkit-filter:blur(0);filter:blur(0);opacity:1;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit{-webkit-filter:blur(0);filter:blur(0);opacity:1}.jupyter-wrapper .bp3-omnibar.bp3-overlay-exit-active{-webkit-filter:blur(20px);filter:blur(20px);opacity:.2;-webkit-transition-property:opacity,-webkit-filter;transition-property:opacity,-webkit-filter;transition-property:filter,opacity;transition-property:filter,opacity,-webkit-filter;-webkit-transition-duration:200ms;transition-duration:200ms;-webkit-transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);transition-timing-function:cubic-bezier(0.4, 1, 0.75, 0.9);-webkit-transition-delay:0;transition-delay:0}.jupyter-wrapper .bp3-omnibar .bp3-input{border-radius:0;background-color:transparent}.jupyter-wrapper .bp3-omnibar .bp3-input,.jupyter-wrapper .bp3-omnibar .bp3-input:focus{-webkit-box-shadow:none;box-shadow:none}.jupyter-wrapper .bp3-omnibar .bp3-menu{border-radius:0;-webkit-box-shadow:inset 0 1px 0 rgba(16,22,26,.15);box-shadow:inset 0 1px 0 rgba(16,22,26,.15);background-color:transparent;max-height:calc(60vh - 40px);overflow:auto}.jupyter-wrapper .bp3-omnibar .bp3-menu:empty{display:none}.jupyter-wrapper .bp3-dark .bp3-omnibar,.jupyter-wrapper .bp3-omnibar.bp3-dark{-webkit-box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);box-shadow:0 0 0 1px rgba(16,22,26,.2),0 4px 8px rgba(16,22,26,.4),0 18px 46px 6px rgba(16,22,26,.4);background-color:#30404d}.jupyter-wrapper .bp3-omnibar-overlay .bp3-overlay-backdrop{background-color:rgba(16,22,26,.2)}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper .bp3-multi-select{min-width:150px}.jupyter-wrapper .bp3-multi-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto}.jupyter-wrapper .bp3-select-popover .bp3-popover-content{padding:5px}.jupyter-wrapper .bp3-select-popover .bp3-input-group{margin-bottom:0}.jupyter-wrapper .bp3-select-popover .bp3-menu{max-width:400px;max-height:300px;overflow:auto;padding:0}.jupyter-wrapper .bp3-select-popover .bp3-menu:not(:first-child){padding-top:5px}.jupyter-wrapper :root{--jp-icon-add: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDEzaC02djZoLTJ2LTZINXYtMmg2VjVoMnY2aDZ2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-bug: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDhoLTIuODFjLS40NS0uNzgtMS4wNy0xLjQ1LTEuODItMS45NkwxNyA0LjQxIDE1LjU5IDNsLTIuMTcgMi4xN0MxMi45NiA1LjA2IDEyLjQ5IDUgMTIgNWMtLjQ5IDAtLjk2LjA2LTEuNDEuMTdMOC40MSAzIDcgNC40MWwxLjYyIDEuNjNDNy44OCA2LjU1IDcuMjYgNy4yMiA2LjgxIDhINHYyaDIuMDljLS4wNS4zMy0uMDkuNjYtLjA5IDF2MUg0djJoMnYxYzAgLjM0LjA0LjY3LjA5IDFINHYyaDIuODFjMS4wNCAxLjc5IDIuOTcgMyA1LjE5IDNzNC4xNS0xLjIxIDUuMTktM0gyMHYtMmgtMi4wOWMuMDUtLjMzLjA5LS42Ni4wOS0xdi0xaDJ2LTJoLTJ2LTFjMC0uMzQtLjA0LS42Ny0uMDktMUgyMFY4em0tNiA4aC00di0yaDR2MnptMC00aC00di0yaDR2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-build: url(data:image/svg+xml;base64,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);--jp-icon-caret-down-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iOS45LDEzLjYgMy42LDcuNCA0LjQsNi42IDkuOSwxMi4yIDE1LjQsNi43IDE2LjEsNy40ICIvPgoJPC9nPgo8L3N2Zz4K);--jp-icon-caret-down-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNS45TDksOS43bDMuOC0zLjhsMS4yLDEuMmwtNC45LDVsLTQuOS01TDUuMiw1Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-caret-down: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik01LjIsNy41TDksMTEuMmwzLjgtMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-left: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik0xMC44LDEyLjhMNy4xLDlsMy44LTMuOGwwLDcuNkgxMC44eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-right: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiIHNoYXBlLXJlbmRlcmluZz0iZ2VvbWV0cmljUHJlY2lzaW9uIj4KICAgIDxwYXRoIGQ9Ik03LjIsNS4yTDEwLjksOWwtMy44LDMuOFY1LjJINy4yeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-caret-up-empty-thin: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwb2x5Z29uIGNsYXNzPSJzdDEiIHBvaW50cz0iMTUuNCwxMy4zIDkuOSw3LjcgNC40LDEzLjIgMy42LDEyLjUgOS45LDYuMyAxNi4xLDEyLjYgIi8+Cgk8L2c+Cjwvc3ZnPgo=);--jp-icon-caret-up: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KCTxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSIgc2hhcGUtcmVuZGVyaW5nPSJnZW9tZXRyaWNQcmVjaXNpb24iPgoJCTxwYXRoIGQ9Ik01LjIsMTAuNUw5LDYuOGwzLjgsMy44SDUuMnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-case-sensitive: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgogIDxnIGNsYXNzPSJqcC1pY29uLWFjY2VudDIiIGZpbGw9IiNGRkYiPgogICAgPHBhdGggZD0iTTcuNiw4aDAuOWwzLjUsOGgtMS4xTDEwLDE0SDZsLTAuOSwySDRMNy42LDh6IE04LDkuMUw2LjQsMTNoMy4yTDgsOS4xeiIvPgogICAgPHBhdGggZD0iTTE2LjYsOS44Yy0wLjIsMC4xLTAuNCwwLjEtMC43LDAuMWMtMC4yLDAtMC40LTAuMS0wLjYtMC4yYy0wLjEtMC4xLTAuMi0wLjQtMC4yLTAuNyBjLTAuMywwLjMtMC42LDAuNS0wLjksMC43Yy0wLjMsMC4xLTAuNywwLjItMS4xLDAuMmMtMC4zLDAtMC41LDAtMC43LTAuMWMtMC4yLTAuMS0wLjQtMC4yLTAuNi0wLjNjLTAuMi0wLjEtMC4zLTAuMy0wLjQtMC41IGMtMC4xLTAuMi0wLjEtMC40LTAuMS0wLjdjMC0wLjMsMC4xLTAuNiwwLjItMC44YzAuMS0wLjIsMC4zLTAuNCwwLjQtMC41QzEyLDcsMTIuMiw2LjksMTIuNSw2LjhjMC4yLTAuMSwwLjUtMC4xLDAuNy0wLjIgYzAuMy0wLjEsMC41LTAuMSwwLjctMC4xYzAuMiwwLDAuNC0wLjEsMC42LTAuMWMwLjIsMCwwLjMtMC4xLDAuNC0wLjJjMC4xLTAuMSwwLjItMC4yLDAuMi0wLjRjMC0xLTEuMS0xLTEuMy0xIGMtMC40LDAtMS40LDAtMS40LDEuMmgtMC45YzAtMC40LDAuMS0wLjcsMC4yLTFjMC4xLTAuMiwwLjMtMC40LDAuNS0wLjZjMC4yLTAuMiwwLjUtMC4zLDAuOC0wLjNDMTMuMyw0LDEzLjYsNCwxMy45LDQgYzAuMywwLDAuNSwwLDAuOCwwLjFjMC4zLDAsMC41LDAuMSwwLjcsMC4yYzAuMiwwLjEsMC40LDAuMywwLjUsMC41QzE2LDUsMTYsNS4yLDE2LDUuNnYyLjljMCwwLjIsMCwwLjQsMCwwLjUgYzAsMC4xLDAuMSwwLjIsMC4zLDAuMmMwLjEsMCwwLjIsMCwwLjMsMFY5Ljh6IE0xNS4yLDYuOWMtMS4yLDAuNi0zLjEsMC4yLTMuMSwxLjRjMCwxLjQsMy4xLDEsMy4xLTAuNVY2Ljl6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-check: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTYuMTdMNC44MyAxMmwtMS40MiAxLjQxTDkgMTkgMjEgN2wtMS40MS0xLjQxeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle-empty: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyIDJDNi40NyAyIDIgNi40NyAyIDEyczQuNDcgMTAgMTAgMTAgMTAtNC40NyAxMC0xMFMxNy41MyAyIDEyIDJ6bTAgMThjLTQuNDEgMC04LTMuNTktOC04czMuNTktOCA4LTggOCAzLjU5IDggOC0zLjU5IDgtOCA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-circle: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iOSIgY3k9IjkiIHI9IjgiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-clear: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8bWFzayBpZD0iZG9udXRIb2xlIj4KICAgIDxyZWN0IHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgZmlsbD0id2hpdGUiIC8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSI4IiBmaWxsPSJibGFjayIvPgogIDwvbWFzaz4KCiAgPGcgY2xhc3M9ImpwLWljb24zIiBmaWxsPSIjNjE2MTYxIj4KICAgIDxyZWN0IGhlaWdodD0iMTgiIHdpZHRoPSIyIiB4PSIxMSIgeT0iMyIgdHJhbnNmb3JtPSJyb3RhdGUoMzE1LCAxMiwgMTIpIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIxMCIgbWFzaz0idXJsKCNkb251dEhvbGUpIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-close: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1ub25lIGpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIGpwLWljb24zLWhvdmVyIiBmaWxsPSJub25lIj4KICAgIDxjaXJjbGUgY3g9IjEyIiBjeT0iMTIiIHI9IjExIi8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIGpwLWljb24tYWNjZW50Mi1ob3ZlciIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMTkgNi40MUwxNy41OSA1IDEyIDEwLjU5IDYuNDEgNSA1IDYuNDEgMTAuNTkgMTIgNSAxNy41OSA2LjQxIDE5IDEyIDEzLjQxIDE3LjU5IDE5IDE5IDE3LjU5IDEzLjQxIDEyeiIvPgogIDwvZz4KCiAgPGcgY2xhc3M9ImpwLWljb24tbm9uZSBqcC1pY29uLWJ1c3kiIGZpbGw9Im5vbmUiPgogICAgPGNpcmNsZSBjeD0iMTIiIGN5PSIxMiIgcj0iNyIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-console: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwMCAyMDAiPgogIDxnIGNsYXNzPSJqcC1pY29uLWJyYW5kMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMjg4RDEiPgogICAgPHBhdGggZD0iTTIwIDE5LjhoMTYwdjE1OS45SDIweiIvPgogIDwvZz4KICA8ZyBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNmZmYiPgogICAgPHBhdGggZD0iTTEwNSAxMjcuM2g0MHYxMi44aC00MHpNNTEuMSA3N0w3NCA5OS45bC0yMy4zIDIzLjMgMTAuNSAxMC41IDIzLjMtMjMuM0w5NSA5OS45IDg0LjUgODkuNCA2MS42IDY2LjV6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-copy: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTExLjksMUgzLjJDMi40LDEsMS43LDEuNywxLjcsMi41djEwLjJoMS41VjIuNWg4LjdWMXogTTE0LjEsMy45aC04Yy0wLjgsMC0xLjUsMC43LTEuNSwxLjV2MTAuMmMwLDAuOCwwLjcsMS41LDEuNSwxLjVoOCBjMC44LDAsMS41LTAuNywxLjUtMS41VjUuNEMxNS41LDQuNiwxNC45LDMuOSwxNC4xLDMuOXogTTE0LjEsMTUuNWgtOFY1LjRoOFYxNS41eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-cut: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkuNjQgNy42NGMuMjMtLjUuMzYtMS4wNS4zNi0xLjY0IDAtMi4yMS0xLjc5LTQtNC00UzIgMy43OSAyIDZzMS43OSA0IDQgNGMuNTkgMCAxLjE0LS4xMyAxLjY0LS4zNkwxMCAxMmwtMi4zNiAyLjM2QzcuMTQgMTQuMTMgNi41OSAxNCA2IDE0Yy0yLjIxIDAtNCAxLjc5LTQgNHMxLjc5IDQgNCA0IDQtMS43OSA0LTRjMC0uNTktLjEzLTEuMTQtLjM2LTEuNjRMMTIgMTRsNyA3aDN2LTFMOS42NCA3LjY0ek02IDhjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTAgMTJjLTEuMSAwLTItLjg5LTItMnMuOS0yIDItMiAyIC44OSAyIDItLjkgMi0yIDJ6bTYtNy41Yy0uMjggMC0uNS0uMjItLjUtLjVzLjIyLS41LjUtLjUuNS4yMi41LjUtLjIyLjUtLjUuNXpNMTkgM2wtNiA2IDIgMiA3LTdWM3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-download: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE5IDloLTRWM0g5djZINWw3IDcgNy03ek01IDE4djJoMTR2LTJINXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-edit: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMgMTcuMjVWMjFoMy43NUwxNy44MSA5Ljk0bC0zLjc1LTMuNzVMMyAxNy4yNXpNMjAuNzEgNy4wNGMuMzktLjM5LjM5LTEuMDIgMC0xLjQxbC0yLjM0LTIuMzRjLS4zOS0uMzktMS4wMi0uMzktMS40MSAwbC0xLjgzIDEuODMgMy43NSAzLjc1IDEuODMtMS44M3oiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-ellipses: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPGNpcmNsZSBjeD0iNSIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxMiIgY3k9IjEyIiByPSIyIi8+CiAgICA8Y2lyY2xlIGN4PSIxOSIgY3k9IjEyIiByPSIyIi8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-extension: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwLjUgMTFIMTlWN2MwLTEuMS0uOS0yLTItMmgtNFYzLjVDMTMgMi4xMiAxMS44OCAxIDEwLjUgMVM4IDIuMTIgOCAzLjVWNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAydjMuOEgzLjVjMS40OSAwIDIuNyAxLjIxIDIuNyAyLjdzLTEuMjEgMi43LTIuNyAyLjdIMlYyMGMwIDEuMS45IDIgMiAyaDMuOHYtMS41YzAtMS40OSAxLjIxLTIuNyAyLjctMi43IDEuNDkgMCAyLjcgMS4yMSAyLjcgMi43VjIySDE3YzEuMSAwIDItLjkgMi0ydi00aDEuNWMxLjM4IDAgMi41LTEuMTIgMi41LTIuNVMyMS44OCAxMSAyMC41IDExeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-fast-forward: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTQgMThsOC41LTZMNCA2djEyem05LTEydjEybDguNS02TDEzIDZ6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-file-upload: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTkgMTZoNnYtNmg0bC03LTctNyA3aDR6bS00IDJoMTR2Mkg1eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-file: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuMyA4LjJsLTUuNS01LjVjLS4zLS4zLS43LS41LTEuMi0uNUgzLjljLS44LjEtMS42LjktMS42IDEuOHYxNC4xYzAgLjkuNyAxLjYgMS42IDEuNmgxNC4yYy45IDAgMS42LS43IDEuNi0xLjZWOS40Yy4xLS41LS4xLS45LS40LTEuMnptLTUuOC0zLjNsMy40IDMuNmgtMy40VjQuOXptMy45IDEyLjdINC43Yy0uMSAwLS4yIDAtLjItLjJWNC43YzAtLjIuMS0uMy4yLS4zaDcuMnY0LjRzMCAuOC4zIDEuMWMuMy4zIDEuMS4zIDEuMS4zaDQuM3Y3LjJzLS4xLjItLjIuMnoiLz4KPC9zdmc+Cg==);--jp-icon-filter-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEwIDE4aDR2LTJoLTR2MnpNMyA2djJoMThWNkgzem0zIDdoMTJ2LTJINnYyeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY4YzAtMS4xLS45LTItMi0yaC04bC0yLTJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-html5: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiMwMDAiIGQ9Ik0xMDguNCAwaDIzdjIyLjhoMjEuMlYwaDIzdjY5aC0yM1Y0NmgtMjF2MjNoLTIzLjJNMjA2IDIzaC0yMC4zVjBoNjMuN3YyM0gyMjl2NDZoLTIzbTUzLjUtNjloMjQuMWwxNC44IDI0LjNMMzEzLjIgMGgyNC4xdjY5aC0yM1YzNC44bC0xNi4xIDI0LjgtMTYuMS0yNC44VjY5aC0yMi42bTg5LjItNjloMjN2NDYuMmgzMi42VjY5aC01NS42Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iI2U0NGQyNiIgZD0iTTEwNy42IDQ3MWwtMzMtMzcwLjRoMzYyLjhsLTMzIDM3MC4yTDI1NS43IDUxMiIvPgogIDxwYXRoIGNsYXNzPSJqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNmMTY1MjkiIGQ9Ik0yNTYgNDgwLjVWMTMxaDE0OC4zTDM3NiA0NDciLz4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNlYmViZWIiIGQ9Ik0xNDIgMTc2LjNoMTE0djQ1LjRoLTY0LjJsNC4yIDQ2LjVoNjB2NDUuM0gxNTQuNG0yIDIyLjhIMjAybDMuMiAzNi4zIDUwLjggMTMuNnY0Ny40bC05My4yLTI2Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tc2VsZWN0YWJsZS1pbnZlcnNlIiBmaWxsPSIjZmZmIiBkPSJNMzY5LjYgMTc2LjNIMjU1Ljh2NDUuNGgxMDkuNm0tNC4xIDQ2LjVIMjU1Ljh2NDUuNGg1NmwtNS4zIDU5LTUwLjcgMTMuNnY0Ny4ybDkzLTI1LjgiLz4KPC9zdmc+Cg==);--jp-icon-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1icmFuZDQganAtaWNvbi1zZWxlY3RhYmxlLWludmVyc2UiIGZpbGw9IiNGRkYiIGQ9Ik0yLjIgMi4yaDE3LjV2MTcuNUgyLjJ6Ii8+CiAgPHBhdGggY2xhc3M9ImpwLWljb24tYnJhbmQwIGpwLWljb24tc2VsZWN0YWJsZSIgZmlsbD0iIzNGNTFCNSIgZD0iTTIuMiAyLjJ2MTcuNWgxNy41bC4xLTE3LjVIMi4yem0xMi4xIDIuMmMxLjIgMCAyLjIgMSAyLjIgMi4ycy0xIDIuMi0yLjIgMi4yLTIuMi0xLTIuMi0yLjIgMS0yLjIgMi4yLTIuMnpNNC40IDE3LjZsMy4zLTguOCAzLjMgNi42IDIuMi0zLjIgNC40IDUuNEg0LjR6Ii8+Cjwvc3ZnPgo=);--jp-icon-inspector: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNEg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMThjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY2YzAtMS4xLS45LTItMi0yem0tNSAxNEg0di00aDExdjR6bTAtNUg0VjloMTF2NHptNSA1aC00VjloNHY5eiIvPgo8L3N2Zz4K);--jp-icon-json: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMSBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNGOUE4MjUiPgogICAgPHBhdGggZD0iTTIwLjIgMTEuOGMtMS42IDAtMS43LjUtMS43IDEgMCAuNC4xLjkuMSAxLjMuMS41LjEuOS4xIDEuMyAwIDEuNy0xLjQgMi4zLTMuNSAyLjNoLS45di0xLjloLjVjMS4xIDAgMS40IDAgMS40LS44IDAtLjMgMC0uNi0uMS0xIDAtLjQtLjEtLjgtLjEtMS4yIDAtMS4zIDAtMS44IDEuMy0yLTEuMy0uMi0xLjMtLjctMS4zLTIgMC0uNC4xLS44LjEtMS4yLjEtLjQuMS0uNy4xLTEgMC0uOC0uNC0uNy0xLjQtLjhoLS41VjQuMWguOWMyLjIgMCAzLjUuNyAzLjUgMi4zIDAgLjQtLjEuOS0uMSAxLjMtLjEuNS0uMS45LS4xIDEuMyAwIC41LjIgMSAxLjcgMXYxLjh6TTEuOCAxMC4xYzEuNiAwIDEuNy0uNSAxLjctMSAwLS40LS4xLS45LS4xLTEuMy0uMS0uNS0uMS0uOS0uMS0xLjMgMC0xLjYgMS40LTIuMyAzLjUtMi4zaC45djEuOWgtLjVjLTEgMC0xLjQgMC0xLjQuOCAwIC4zIDAgLjYuMSAxIDAgLjIuMS42LjEgMSAwIDEuMyAwIDEuOC0xLjMgMkM2IDExLjIgNiAxMS43IDYgMTNjMCAuNC0uMS44LS4xIDEuMi0uMS4zLS4xLjctLjEgMSAwIC44LjMuOCAxLjQuOGguNXYxLjloLS45Yy0yLjEgMC0zLjUtLjYtMy41LTIuMyAwLS40LjEtLjkuMS0xLjMuMS0uNS4xLS45LjEtMS4zIDAtLjUtLjItMS0xLjctMXYtMS45eiIvPgogICAgPGNpcmNsZSBjeD0iMTEiIGN5PSIxMy44IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY3g9IjExIiBjeT0iOC4yIiByPSIyLjEiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter-favicon: url(data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTUyIiBoZWlnaHQ9IjE2NSIgdmlld0JveD0iMCAwIDE1MiAxNjUiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCIgZmlsbD0iI0YzNzcyNiI+CiAgICA8cGF0aCB0cmFuc2Zvcm09InRyYW5zbGF0ZSgwLjA3ODk0NywgMTEwLjU4MjkyNykiIGQ9Ik03NS45NDIyODQyLDI5LjU4MDQ1NjEgQzQzLjMwMjM5NDcsMjkuNTgwNDU2MSAxNC43OTY3ODMyLDE3LjY1MzQ2MzQgMCwwIEM1LjUxMDgzMjExLDE1Ljg0MDY4MjkgMTUuNzgxNTM4OSwyOS41NjY3NzMyIDI5LjM5MDQ5NDcsMzkuMjc4NDE3MSBDNDIuOTk5Nyw0OC45ODk4NTM3IDU5LjI3MzcsNTQuMjA2NzgwNSA3NS45NjA1Nzg5LDU0LjIwNjc4MDUgQzkyLjY0NzQ1NzksNTQuMjA2NzgwNSAxMDguOTIxNDU4LDQ4Ljk4OTg1MzcgMTIyLjUzMDY2MywzOS4yNzg0MTcxIEMxMzYuMTM5NDUzLDI5LjU2Njc3MzIgMTQ2LjQxMDI4NCwxNS44NDA2ODI5IDE1MS45MjExNTgsMCBDMTM3LjA4Nzg2OCwxNy42NTM0NjM0IDEwOC41ODI1ODksMjkuNTgwNDU2MSA3NS45NDIyODQyLDI5LjU4MDQ1NjEgTDc1Ljk0MjI4NDIsMjkuNTgwNDU2MSBaIiAvPgogICAgPHBhdGggdHJhbnNmb3JtPSJ0cmFuc2xhdGUoMC4wMzczNjgsIDAuNzA0ODc4KSIgZD0iTTc1Ljk3ODQ1NzksMjQuNjI2NDA3MyBDMTA4LjYxODc2MywyNC42MjY0MDczIDEzNy4xMjQ0NTgsMzYuNTUzNDQxNSAxNTEuOTIxMTU4LDU0LjIwNjc4MDUgQzE0Ni40MTAyODQsMzguMzY2MjIyIDEzNi4xMzk0NTMsMjQuNjQwMTMxNyAxMjIuNTMwNjYzLDE0LjkyODQ4NzggQzEwOC45MjE0NTgsNS4yMTY4NDM5IDkyLjY0NzQ1NzksMCA3NS45NjA1Nzg5LDAgQzU5LjI3MzcsMCA0Mi45OTk3LDUuMjE2ODQzOSAyOS4zOTA0OTQ3LDE0LjkyODQ4NzggQzE1Ljc4MTUzODksMjQuNjQwMTMxNyA1LjUxMDgzMjExLDM4LjM2NjIyMiAwLDU0LjIwNjc4MDUgQzE0LjgzMzA4MTYsMzYuNTg5OTI5MyA0My4zMzg1Njg0LDI0LjYyNjQwNzMgNzUuOTc4NDU3OSwyNC42MjY0MDczIEw3NS45Nzg0NTc5LDI0LjYyNjQwNzMgWiIgLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-jupyter: url(data:image/svg+xml;base64,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);--jp-icon-jupyterlab-wordmark: url(data:image/svg+xml;base64,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);--jp-icon-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgZmlsbD0iIzYxNjE2MSIgZD0iTTE1IDlIOXY2aDZWOXptLTIgNGgtMnYtMmgydjJ6bTgtMlY5aC0yVjdjMC0xLjEtLjktMi0yLTJoLTJWM2gtMnYyaC0yVjNIOXYySDdjLTEuMSAwLTIgLjktMiAydjJIM3YyaDJ2MkgzdjJoMnYyYzAgMS4xLjkgMiAyIDJoMnYyaDJ2LTJoMnYyaDJ2LTJoMmMxLjEgMCAyLS45IDItMnYtMmgydi0yaC0ydi0yaDJ6bS00IDZIN1Y3aDEwdjEweiIvPgo8L3N2Zz4K);--jp-icon-keyboard: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMjAgNUg0Yy0xLjEgMC0xLjk5LjktMS45OSAyTDIgMTdjMCAxLjEuOSAyIDIgMmgxNmMxLjEgMCAyLS45IDItMlY3YzAtMS4xLS45LTItMi0yem0tOSAzaDJ2MmgtMlY4em0wIDNoMnYyaC0ydi0yek04IDhoMnYySDhWOHptMCAzaDJ2Mkg4di0yem0tMSAySDV2LTJoMnYyem0wLTNINVY4aDJ2MnptOSA3SDh2LTJoOHYyem0wLTRoLTJ2LTJoMnYyem0wLTNoLTJWOGgydjJ6bTMgM2gtMnYtMmgydjJ6bTAtM2gtMlY4aDJ2MnoiLz4KPC9zdmc+Cg==);--jp-icon-launcher: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkgMTlINVY1aDdWM0g1YTIgMiAwIDAwLTIgMnYxNGEyIDIgMCAwMDIgMmgxNGMxLjEgMCAyLS45IDItMnYtN2gtMnY3ek0xNCAzdjJoMy41OWwtOS44MyA5LjgzIDEuNDEgMS40MUwxOSA2LjQxVjEwaDJWM2gtN3oiLz4KPC9zdmc+Cg==);--jp-icon-line-form: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGZpbGw9IndoaXRlIiBkPSJNNS44OCA0LjEyTDEzLjc2IDEybC03Ljg4IDcuODhMOCAyMmwxMC0xMEw4IDJ6Ii8+Cjwvc3ZnPgo=);--jp-icon-link: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTMuOSAxMmMwLTEuNzEgMS4zOS0zLjEgMy4xLTMuMWg0VjdIN2MtMi43NiAwLTUgMi4yNC01IDVzMi4yNCA1IDUgNWg0di0xLjlIN2MtMS43MSAwLTMuMS0xLjM5LTMuMS0zLjF6TTggMTNoOHYtMkg4djJ6bTktNmgtNHYxLjloNGMxLjcxIDAgMy4xIDEuMzkgMy4xIDMuMXMtMS4zOSAzLjEtMy4xIDMuMWgtNFYxN2g0YzIuNzYgMCA1LTIuMjQgNS01cy0yLjI0LTUtNS01eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-list: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiM2MTYxNjEiIGQ9Ik0xOSA1djE0SDVWNWgxNG0xLjEtMkgzLjljLS41IDAtLjkuNC0uOS45djE2LjJjMCAuNC40LjkuOS45aDE2LjJjLjQgMCAuOS0uNS45LS45VjMuOWMwLS41LS41LS45LS45LS45ek0xMSA3aDZ2MmgtNlY3em0wIDRoNnYyaC02di0yem0wIDRoNnYyaC02ek03IDdoMnYySDd6bTAgNGgydjJIN3ptMCA0aDJ2Mkg3eiIvPgo8L3N2Zz4=);--jp-icon-listings-info: url(data:image/svg+xml;base64,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);--jp-icon-markdown: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjN0IxRkEyIiBkPSJNNSAxNC45aDEybC02LjEgNnptOS40LTYuOGMwLTEuMy0uMS0yLjktLjEtNC41LS40IDEuNC0uOSAyLjktMS4zIDQuM2wtMS4zIDQuM2gtMkw4LjUgNy45Yy0uNC0xLjMtLjctMi45LTEtNC4zLS4xIDEuNi0uMSAzLjItLjIgNC42TDcgMTIuNEg0LjhsLjctMTFoMy4zTDEwIDVjLjQgMS4yLjcgMi43IDEgMy45LjMtMS4yLjctMi42IDEtMy45bDEuMi0zLjdoMy4zbC42IDExaC0yLjRsLS4zLTQuMnoiLz4KPC9zdmc+Cg==);--jp-icon-new-folder: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIwIDZoLThsLTItMkg0Yy0xLjExIDAtMS45OS44OS0xLjk5IDJMMiAxOGMwIDEuMTEuODkgMiAyIDJoMTZjMS4xMSAwIDItLjg5IDItMlY4YzAtMS4xMS0uODktMi0yLTJ6bS0xIDhoLTN2M2gtMnYtM2gtM3YtMmgzVjloMnYzaDN2MnoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-not-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI1IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDMgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMTkgMTcuMTg0NCAyLjk2OTY4IDE0LjMwMzIgMS44NjA5NCAxMS40NDA5WiIvPgogICAgPHBhdGggY2xhc3M9ImpwLWljb24yIiBzdHJva2U9IiMzMzMzMzMiIHN0cm9rZS13aWR0aD0iMiIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOS4zMTU5MiA5LjMyMDMxKSIgZD0iTTcuMzY4NDIgMEwwIDcuMzY0NzkiLz4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDkuMzE1OTIgMTYuNjgzNikgc2NhbGUoMSAtMSkiIGQ9Ik03LjM2ODQyIDBMMCA3LjM2NDc5Ii8+Cjwvc3ZnPgo=);--jp-icon-notebook: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi13YXJuMCBqcC1pY29uLXNlbGVjdGFibGUiIGZpbGw9IiNFRjZDMDAiPgogICAgPHBhdGggZD0iTTE4LjcgMy4zdjE1LjRIMy4zVjMuM2gxNS40bTEuNS0xLjVIMS44djE4LjNoMTguM2wuMS0xOC4zeiIvPgogICAgPHBhdGggZD0iTTE2LjUgMTYuNWwtNS40LTQuMy01LjYgNC4zdi0xMWgxMXoiLz4KICA8L2c+Cjwvc3ZnPgo=);--jp-icon-palette: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTE4IDEzVjIwSDRWNkg5LjAyQzkuMDcgNS4yOSA5LjI0IDQuNjIgOS41IDRINEMyLjkgNCAyIDQuOSAyIDZWMjBDMiAyMS4xIDIuOSAyMiA0IDIySDE4QzE5LjEgMjIgMjAgMjEuMSAyMCAyMFYxNUwxOCAxM1pNMTkuMyA4Ljg5QzE5Ljc0IDguMTkgMjAgNy4zOCAyMCA2LjVDMjAgNC4wMSAxNy45OSAyIDE1LjUgMkMxMy4wMSAyIDExIDQuMDEgMTEgNi41QzExIDguOTkgMTMuMDEgMTEgMTUuNDkgMTFDMTYuMzcgMTEgMTcuMTkgMTAuNzQgMTcuODggMTAuM0wyMSAxMy40MkwyMi40MiAxMkwxOS4zIDguODlaTTE1LjUgOUMxNC4xMiA5IDEzIDcuODggMTMgNi41QzEzIDUuMTIgMTQuMTIgNCAxNS41IDRDMTYuODggNCAxOCA1LjEyIDE4IDYuNUMxOCA3Ljg4IDE2Ljg4IDkgMTUuNSA5WiIvPgogICAgPHBhdGggZmlsbC1ydWxlPSJldmVub2RkIiBjbGlwLXJ1bGU9ImV2ZW5vZGQiIGQ9Ik00IDZIOS4wMTg5NEM5LjAwNjM5IDYuMTY1MDIgOSA2LjMzMTc2IDkgNi41QzkgOC44MTU3NyAxMC4yMTEgMTAuODQ4NyAxMi4wMzQzIDEySDlWMTRIMTZWMTIuOTgxMUMxNi41NzAzIDEyLjkzNzcgMTcuMTIgMTIuODIwNyAxNy42Mzk2IDEyLjYzOTZMMTggMTNWMjBINFY2Wk04IDhINlYxMEg4VjhaTTYgMTJIOFYxNEg2VjEyWk04IDE2SDZWMThIOFYxNlpNOSAxNkgxNlYxOEg5VjE2WiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-paste: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE5IDJoLTQuMThDMTQuNC44NCAxMy4zIDAgMTIgMGMtMS4zIDAtMi40Ljg0LTIuODIgMkg1Yy0xLjEgMC0yIC45LTIgMnYxNmMwIDEuMS45IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjRjMC0xLjEtLjktMi0yLTJ6bS03IDBjLjU1IDAgMSAuNDUgMSAxcy0uNDUgMS0xIDEtMS0uNDUtMS0xIC40NS0xIDEtMXptNyAxOEg1VjRoMnYzaDEwVjRoMnYxNnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-python: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1icmFuZDAganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMEQ0N0ExIj4KICAgIDxwYXRoIGQ9Ik0xMS4xIDYuOVY1LjhINi45YzAtLjUgMC0xLjMuMi0xLjYuNC0uNy44LTEuMSAxLjctMS40IDEuNy0uMyAyLjUtLjMgMy45LS4xIDEgLjEgMS45LjkgMS45IDEuOXY0LjJjMCAuNS0uOSAxLjYtMiAxLjZIOC44Yy0xLjUgMC0yLjQgMS40LTIuNCAyLjh2Mi4ySDQuN0MzLjUgMTUuMSAzIDE0IDMgMTMuMVY5Yy0uMS0xIC42LTIgMS44LTIgMS41LS4xIDYuMy0uMSA2LjMtLjF6Ii8+CiAgICA8cGF0aCBkPSJNMTAuOSAxNS4xdjEuMWg0LjJjMCAuNSAwIDEuMy0uMiAxLjYtLjQuNy0uOCAxLjEtMS43IDEuNC0xLjcuMy0yLjUuMy0zLjkuMS0xLS4xLTEuOS0uOS0xLjktMS45di00LjJjMC0uNS45LTEuNiAyLTEuNmgzLjhjMS41IDAgMi40LTEuNCAyLjQtMi44VjYuNmgxLjdDMTguNSA2LjkgMTkgOCAxOSA4LjlWMTNjMCAxLS43IDIuMS0xLjkgMi4xaC02LjJ6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-r-kernel: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjE5NkYzIiBkPSJNNC40IDIuNWMxLjItLjEgMi45LS4zIDQuOS0uMyAyLjUgMCA0LjEuNCA1LjIgMS4zIDEgLjcgMS41IDEuOSAxLjUgMy41IDAgMi0xLjQgMy41LTIuOSA0LjEgMS4yLjQgMS43IDEuNiAyLjIgMyAuNiAxLjkgMSAzLjkgMS4zIDQuNmgtMy44Yy0uMy0uNC0uOC0xLjctMS4yLTMuN3MtMS4yLTIuNi0yLjYtMi42aC0uOXY2LjRINC40VjIuNXptMy43IDYuOWgxLjRjMS45IDAgMi45LS45IDIuOS0yLjNzLTEtMi4zLTIuOC0yLjNjLS43IDAtMS4zIDAtMS42LjJ2NC41aC4xdi0uMXoiLz4KPC9zdmc+Cg==);--jp-icon-react: url(data:image/svg+xml;base64,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);--jp-icon-refresh: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDE4IDE4Ij4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTkgMTMuNWMtMi40OSAwLTQuNS0yLjAxLTQuNS00LjVTNi41MSA0LjUgOSA0LjVjMS4yNCAwIDIuMzYuNTIgMy4xNyAxLjMzTDEwIDhoNVYzbC0xLjc2IDEuNzZDMTIuMTUgMy42OCAxMC42NiAzIDkgMyA1LjY5IDMgMy4wMSA1LjY5IDMuMDEgOVM1LjY5IDE1IDkgMTVjMi45NyAwIDUuNDMtMi4xNiA1LjktNWgtMS41MmMtLjQ2IDItMi4yNCAzLjUtNC4zOCAzLjV6Ii8+CiAgICA8L2c+Cjwvc3ZnPgo=);--jp-icon-regex: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIwIDIwIj4KICA8ZyBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiM0MTQxNDEiPgogICAgPHJlY3QgeD0iMiIgeT0iMiIgd2lkdGg9IjE2IiBoZWlnaHQ9IjE2Ii8+CiAgPC9nPgoKICA8ZyBjbGFzcz0ianAtaWNvbi1hY2NlbnQyIiBmaWxsPSIjRkZGIj4KICAgIDxjaXJjbGUgY2xhc3M9InN0MiIgY3g9IjUuNSIgY3k9IjE0LjUiIHI9IjEuNSIvPgogICAgPHJlY3QgeD0iMTIiIHk9IjQiIGNsYXNzPSJzdDIiIHdpZHRoPSIxIiBoZWlnaHQ9IjgiLz4KICAgIDxyZWN0IHg9IjguNSIgeT0iNy41IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjg2NiAtMC41IDAuNSAwLjg2NiAtMi4zMjU1IDcuMzIxOSkiIGNsYXNzPSJzdDIiIHdpZHRoPSI4IiBoZWlnaHQ9IjEiLz4KICAgIDxyZWN0IHg9IjEyIiB5PSI0IiB0cmFuc2Zvcm09Im1hdHJpeCgwLjUgLTAuODY2IDAuODY2IDAuNSAtMC42Nzc5IDE0LjgyNTIpIiBjbGFzcz0ic3QyIiB3aWR0aD0iMSIgaGVpZ2h0PSI4Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-run: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTggNXYxNGwxMS03eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-running: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDUxMiA1MTIiPgogIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICA8cGF0aCBkPSJNMjU2IDhDMTE5IDggOCAxMTkgOCAyNTZzMTExIDI0OCAyNDggMjQ4IDI0OC0xMTEgMjQ4LTI0OFMzOTMgOCAyNTYgOHptOTYgMzI4YzAgOC44LTcuMiAxNi0xNiAxNkgxNzZjLTguOCAwLTE2LTcuMi0xNi0xNlYxNzZjMC04LjggNy4yLTE2IDE2LTE2aDE2MGM4LjggMCAxNiA3LjIgMTYgMTZ2MTYweiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-save: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTE3IDNINWMtMS4xMSAwLTIgLjktMiAydjE0YzAgMS4xLjg5IDIgMiAyaDE0YzEuMSAwIDItLjkgMi0yVjdsLTQtNHptLTUgMTZjLTEuNjYgMC0zLTEuMzQtMy0zczEuMzQtMyAzLTMgMyAxLjM0IDMgMy0xLjM0IDMtMyAzem0zLTEwSDVWNWgxMHY0eiIvPgogICAgPC9nPgo8L3N2Zz4K);--jp-icon-search: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-settings: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTkuNDMgMTIuOThjLjA0LS4zMi4wNy0uNjQuMDctLjk4cy0uMDMtLjY2LS4wNy0uOThsMi4xMS0xLjY1Yy4xOS0uMTUuMjQtLjQyLjEyLS42NGwtMi0zLjQ2Yy0uMTItLjIyLS4zOS0uMy0uNjEtLjIybC0yLjQ5IDFjLS41Mi0uNC0xLjA4LS43My0xLjY5LS45OGwtLjM4LTIuNjVBLjQ4OC40ODggMCAwMDE0IDJoLTRjLS4yNSAwLS40Ni4xOC0uNDkuNDJsLS4zOCAyLjY1Yy0uNjEuMjUtMS4xNy41OS0xLjY5Ljk4bC0yLjQ5LTFjLS4yMy0uMDktLjQ5IDAtLjYxLjIybC0yIDMuNDZjLS4xMy4yMi0uMDcuNDkuMTIuNjRsMi4xMSAxLjY1Yy0uMDQuMzItLjA3LjY1LS4wNy45OHMuMDMuNjYuMDcuOThsLTIuMTEgMS42NWMtLjE5LjE1LS4yNC40Mi0uMTIuNjRsMiAzLjQ2Yy4xMi4yMi4zOS4zLjYxLjIybDIuNDktMWMuNTIuNCAxLjA4LjczIDEuNjkuOThsLjM4IDIuNjVjLjAzLjI0LjI0LjQyLjQ5LjQyaDRjLjI1IDAgLjQ2LS4xOC40OS0uNDJsLjM4LTIuNjVjLjYxLS4yNSAxLjE3LS41OSAxLjY5LS45OGwyLjQ5IDFjLjIzLjA5LjQ5IDAgLjYxLS4yMmwyLTMuNDZjLjEyLS4yMi4wNy0uNDktLjEyLS42NGwtMi4xMS0xLjY1ek0xMiAxNS41Yy0xLjkzIDAtMy41LTEuNTctMy41LTMuNXMxLjU3LTMuNSAzLjUtMy41IDMuNSAxLjU3IDMuNSAzLjUtMS41NyAzLjUtMy41IDMuNXoiLz4KPC9zdmc+Cg==);--jp-icon-spreadsheet: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8cGF0aCBjbGFzcz0ianAtaWNvbi1jb250cmFzdDEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNENBRjUwIiBkPSJNMi4yIDIuMnYxNy42aDE3LjZWMi4ySDIuMnptMTUuNCA3LjdoLTUuNVY0LjRoNS41djUuNXpNOS45IDQuNHY1LjVINC40VjQuNGg1LjV6bS01LjUgNy43aDUuNXY1LjVINC40di01LjV6bTcuNyA1LjV2LTUuNWg1LjV2NS41aC01LjV6Ii8+Cjwvc3ZnPgo=);--jp-icon-stop: url(data:image/svg+xml;base64,PHN2ZyBoZWlnaHQ9IjI0IiB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIyNCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxnIGNsYXNzPSJqcC1pY29uMyIgZmlsbD0iIzYxNjE2MSI+CiAgICAgICAgPHBhdGggZD0iTTAgMGgyNHYyNEgweiIgZmlsbD0ibm9uZSIvPgogICAgICAgIDxwYXRoIGQ9Ik02IDZoMTJ2MTJINnoiLz4KICAgIDwvZz4KPC9zdmc+Cg==);--jp-icon-tab: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTIxIDNIM2MtMS4xIDAtMiAuOS0yIDJ2MTRjMCAxLjEuOSAyIDIgMmgxOGMxLjEgMCAyLS45IDItMlY1YzAtMS4xLS45LTItMi0yem0wIDE2SDNWNWgxMHY0aDh2MTB6Ii8+CiAgPC9nPgo8L3N2Zz4K);--jp-icon-terminal: url(data:image/svg+xml;base64,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);--jp-icon-text-editor: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI0Ij4KICA8cGF0aCBjbGFzcz0ianAtaWNvbjMganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjNjE2MTYxIiBkPSJNMTUgMTVIM3YyaDEydi0yem0wLThIM3YyaDEyVjd6TTMgMTNoMTh2LTJIM3Yyem0wIDhoMTh2LTJIM3Yyek0zIDN2MmgxOFYzSDN6Ii8+Cjwvc3ZnPgo=);--jp-icon-trusted: url(data:image/svg+xml;base64,PHN2ZyBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDI0IDI1Ij4KICAgIDxwYXRoIGNsYXNzPSJqcC1pY29uMiIgc3Ryb2tlPSIjMzMzMzMzIiBzdHJva2Utd2lkdGg9IjIiIHRyYW5zZm9ybT0idHJhbnNsYXRlKDIgMykiIGQ9Ik0xLjg2MDk0IDExLjQ0MDlDMC44MjY0NDggOC43NzAyNyAwLjg2Mzc3OSA2LjA1NzY0IDEuMjQ5MDcgNC4xOTkzMkMyLjQ4MjA2IDMuOTMzNDcgNC4wODA2OCAzLjQwMzQ3IDUuNjAxMDIgMi44NDQ5QzcuMjM1NDkgMi4yNDQ0IDguODU2NjYgMS41ODE1IDkuOTg3NiAxLjA5NTM5QzExLjA1OTcgMS41ODM0MSAxMi42MDk0IDIuMjQ0NCAxNC4yMTggMi44NDMzOUMxNS43NTAzIDMuNDEzOTQgMTcuMzk5NSAzLjk1MjU4IDE4Ljc1MzkgNC4yMTM4NUMxOS4xMzY0IDYuMDcxNzcgMTkuMTcwOSA4Ljc3NzIyIDE4LjEzOSAxMS40NDA5QzE3LjAzMDMgMTQuMzAzMiAxNC42NjY4IDE3LjE4NDQgOS45OTk5OSAxOC45MzU0QzUuMzMzMiAxNy4xODQ0IDIuOTY5NjggMTQuMzAzMiAxLjg2MDk0IDExLjQ0MDlaIi8+CiAgICA8cGF0aCBjbGFzcz0ianAtaWNvbjIiIGZpbGw9IiMzMzMzMzMiIHN0cm9rZT0iIzMzMzMzMyIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoOCA5Ljg2NzE5KSIgZD0iTTIuODYwMTUgNC44NjUzNUwwLjcyNjU0OSAyLjk5OTU5TDAgMy42MzA0NUwyLjg2MDE1IDYuMTMxNTdMOCAwLjYzMDg3Mkw3LjI3ODU3IDBMMi44NjAxNSA0Ljg2NTM1WiIvPgo8L3N2Zz4K);--jp-icon-undo: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMjQgMjQiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjUgOGMtMi42NSAwLTUuMDUuOTktNi45IDIuNkwyIDd2OWg5bC0zLjYyLTMuNjJjMS4zOS0xLjE2IDMuMTYtMS44OCA1LjEyLTEuODggMy41NCAwIDYuNTUgMi4zMSA3LjYgNS41bDIuMzctLjc4QzIxLjA4IDExLjAzIDE3LjE1IDggMTIuNSA4eiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-vega: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbjEganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjMjEyMTIxIj4KICAgIDxwYXRoIGQ9Ik0xMC42IDUuNGwyLjItMy4ySDIuMnY3LjNsNC02LjZ6Ii8+CiAgICA8cGF0aCBkPSJNMTUuOCAyLjJsLTQuNCA2LjZMNyA2LjNsLTQuOCA4djUuNWgxNy42VjIuMmgtNHptLTcgMTUuNEg1LjV2LTQuNGgzLjN2NC40em00LjQgMEg5LjhWOS44aDMuNHY3Ljh6bTQuNCAwaC0zLjRWNi41aDMuNHYxMS4xeiIvPgogIDwvZz4KPC9zdmc+Cg==);--jp-icon-yaml: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxNiIgdmlld0JveD0iMCAwIDIyIDIyIj4KICA8ZyBjbGFzcz0ianAtaWNvbi1jb250cmFzdDIganAtaWNvbi1zZWxlY3RhYmxlIiBmaWxsPSIjRDgxQjYwIj4KICAgIDxwYXRoIGQ9Ik03LjIgMTguNnYtNS40TDMgNS42aDMuM2wxLjQgMy4xYy4zLjkuNiAxLjYgMSAyLjUuMy0uOC42LTEuNiAxLTIuNWwxLjQtMy4xaDMuNGwtNC40IDcuNnY1LjVsLTIuOS0uMXoiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxNi41IiByPSIyLjEiLz4KICAgIDxjaXJjbGUgY2xhc3M9InN0MCIgY3g9IjE3LjYiIGN5PSIxMSIgcj0iMi4xIi8+CiAgPC9nPgo8L3N2Zz4K)}.jupyter-wrapper .jp-AddIcon{background-image:var(--jp-icon-add)}.jupyter-wrapper .jp-BugIcon{background-image:var(--jp-icon-bug)}.jupyter-wrapper .jp-BuildIcon{background-image:var(--jp-icon-build)}.jupyter-wrapper .jp-CaretDownEmptyIcon{background-image:var(--jp-icon-caret-down-empty)}.jupyter-wrapper .jp-CaretDownEmptyThinIcon{background-image:var(--jp-icon-caret-down-empty-thin)}.jupyter-wrapper .jp-CaretDownIcon{background-image:var(--jp-icon-caret-down)}.jupyter-wrapper .jp-CaretLeftIcon{background-image:var(--jp-icon-caret-left)}.jupyter-wrapper .jp-CaretRightIcon{background-image:var(--jp-icon-caret-right)}.jupyter-wrapper .jp-CaretUpEmptyThinIcon{background-image:var(--jp-icon-caret-up-empty-thin)}.jupyter-wrapper .jp-CaretUpIcon{background-image:var(--jp-icon-caret-up)}.jupyter-wrapper .jp-CaseSensitiveIcon{background-image:var(--jp-icon-case-sensitive)}.jupyter-wrapper .jp-CheckIcon{background-image:var(--jp-icon-check)}.jupyter-wrapper .jp-CircleEmptyIcon{background-image:var(--jp-icon-circle-empty)}.jupyter-wrapper .jp-CircleIcon{background-image:var(--jp-icon-circle)}.jupyter-wrapper .jp-ClearIcon{background-image:var(--jp-icon-clear)}.jupyter-wrapper .jp-CloseIcon{background-image:var(--jp-icon-close)}.jupyter-wrapper .jp-ConsoleIcon{background-image:var(--jp-icon-console)}.jupyter-wrapper .jp-CopyIcon{background-image:var(--jp-icon-copy)}.jupyter-wrapper .jp-CutIcon{background-image:var(--jp-icon-cut)}.jupyter-wrapper .jp-DownloadIcon{background-image:var(--jp-icon-download)}.jupyter-wrapper .jp-EditIcon{background-image:var(--jp-icon-edit)}.jupyter-wrapper .jp-EllipsesIcon{background-image:var(--jp-icon-ellipses)}.jupyter-wrapper .jp-ExtensionIcon{background-image:var(--jp-icon-extension)}.jupyter-wrapper .jp-FastForwardIcon{background-image:var(--jp-icon-fast-forward)}.jupyter-wrapper .jp-FileIcon{background-image:var(--jp-icon-file)}.jupyter-wrapper .jp-FileUploadIcon{background-image:var(--jp-icon-file-upload)}.jupyter-wrapper .jp-FilterListIcon{background-image:var(--jp-icon-filter-list)}.jupyter-wrapper .jp-FolderIcon{background-image:var(--jp-icon-folder)}.jupyter-wrapper .jp-Html5Icon{background-image:var(--jp-icon-html5)}.jupyter-wrapper .jp-ImageIcon{background-image:var(--jp-icon-image)}.jupyter-wrapper .jp-InspectorIcon{background-image:var(--jp-icon-inspector)}.jupyter-wrapper .jp-JsonIcon{background-image:var(--jp-icon-json)}.jupyter-wrapper .jp-JupyterFaviconIcon{background-image:var(--jp-icon-jupyter-favicon)}.jupyter-wrapper .jp-JupyterIcon{background-image:var(--jp-icon-jupyter)}.jupyter-wrapper .jp-JupyterlabWordmarkIcon{background-image:var(--jp-icon-jupyterlab-wordmark)}.jupyter-wrapper .jp-KernelIcon{background-image:var(--jp-icon-kernel)}.jupyter-wrapper .jp-KeyboardIcon{background-image:var(--jp-icon-keyboard)}.jupyter-wrapper .jp-LauncherIcon{background-image:var(--jp-icon-launcher)}.jupyter-wrapper .jp-LineFormIcon{background-image:var(--jp-icon-line-form)}.jupyter-wrapper .jp-LinkIcon{background-image:var(--jp-icon-link)}.jupyter-wrapper .jp-ListIcon{background-image:var(--jp-icon-list)}.jupyter-wrapper .jp-ListingsInfoIcon{background-image:var(--jp-icon-listings-info)}.jupyter-wrapper .jp-MarkdownIcon{background-image:var(--jp-icon-markdown)}.jupyter-wrapper .jp-NewFolderIcon{background-image:var(--jp-icon-new-folder)}.jupyter-wrapper .jp-NotTrustedIcon{background-image:var(--jp-icon-not-trusted)}.jupyter-wrapper .jp-NotebookIcon{background-image:var(--jp-icon-notebook)}.jupyter-wrapper .jp-PaletteIcon{background-image:var(--jp-icon-palette)}.jupyter-wrapper .jp-PasteIcon{background-image:var(--jp-icon-paste)}.jupyter-wrapper .jp-PythonIcon{background-image:var(--jp-icon-python)}.jupyter-wrapper .jp-RKernelIcon{background-image:var(--jp-icon-r-kernel)}.jupyter-wrapper .jp-ReactIcon{background-image:var(--jp-icon-react)}.jupyter-wrapper .jp-RefreshIcon{background-image:var(--jp-icon-refresh)}.jupyter-wrapper .jp-RegexIcon{background-image:var(--jp-icon-regex)}.jupyter-wrapper .jp-RunIcon{background-image:var(--jp-icon-run)}.jupyter-wrapper .jp-RunningIcon{background-image:var(--jp-icon-running)}.jupyter-wrapper .jp-SaveIcon{background-image:var(--jp-icon-save)}.jupyter-wrapper .jp-SearchIcon{background-image:var(--jp-icon-search)}.jupyter-wrapper .jp-SettingsIcon{background-image:var(--jp-icon-settings)}.jupyter-wrapper .jp-SpreadsheetIcon{background-image:var(--jp-icon-spreadsheet)}.jupyter-wrapper .jp-StopIcon{background-image:var(--jp-icon-stop)}.jupyter-wrapper .jp-TabIcon{background-image:var(--jp-icon-tab)}.jupyter-wrapper .jp-TerminalIcon{background-image:var(--jp-icon-terminal)}.jupyter-wrapper .jp-TextEditorIcon{background-image:var(--jp-icon-text-editor)}.jupyter-wrapper .jp-TrustedIcon{background-image:var(--jp-icon-trusted)}.jupyter-wrapper .jp-UndoIcon{background-image:var(--jp-icon-undo)}.jupyter-wrapper .jp-VegaIcon{background-image:var(--jp-icon-vega)}.jupyter-wrapper .jp-YamlIcon{background-image:var(--jp-icon-yaml)}.jupyter-wrapper :root{--jp-icon-search-white: url(data:image/svg+xml;base64,PHN2ZyB2aWV3Qm94PSIwIDAgMTggMTgiIHdpZHRoPSIxNiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICA8ZyBjbGFzcz0ianAtaWNvbjMiIGZpbGw9IiM2MTYxNjEiPgogICAgPHBhdGggZD0iTTEyLjEsMTAuOWgtMC43bC0wLjItMC4yYzAuOC0wLjksMS4zLTIuMiwxLjMtMy41YzAtMy0yLjQtNS40LTUuNC01LjRTMS44LDQuMiwxLjgsNy4xczIuNCw1LjQsNS40LDUuNCBjMS4zLDAsMi41LTAuNSwzLjUtMS4zbDAuMiwwLjJ2MC43bDQuMSw0LjFsMS4yLTEuMkwxMi4xLDEwLjl6IE03LjEsMTAuOWMtMi4xLDAtMy43LTEuNy0zLjctMy43czEuNy0zLjcsMy43LTMuN3MzLjcsMS43LDMuNywzLjcgUzkuMiwxMC45LDcuMSwxMC45eiIvPgogIDwvZz4KPC9zdmc+Cg==)}.jupyter-wrapper .jp-Icon,.jupyter-wrapper .jp-MaterialIcon{background-position:center;background-repeat:no-repeat;background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-cover{background-position:center;background-repeat:no-repeat;background-size:cover}.jupyter-wrapper .jp-Icon-16{background-size:16px;min-width:16px;min-height:16px}.jupyter-wrapper .jp-Icon-18{background-size:18px;min-width:18px;min-height:18px}.jupyter-wrapper .jp-Icon-20{background-size:20px;min-width:20px;min-height:20px}.jupyter-wrapper .jp-icon0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-accent0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-accent0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-accent1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-accent2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-accent3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-accent4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-none[fill]{fill:none}.jupyter-wrapper .jp-icon-none[stroke]{stroke:none}.jupyter-wrapper .jp-icon-brand0[fill]{fill:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[fill]{fill:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[fill]{fill:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[fill]{fill:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-brand0[stroke]{stroke:var(--jp-brand-color0)}.jupyter-wrapper .jp-icon-brand1[stroke]{stroke:var(--jp-brand-color1)}.jupyter-wrapper .jp-icon-brand2[stroke]{stroke:var(--jp-brand-color2)}.jupyter-wrapper .jp-icon-brand3[stroke]{stroke:var(--jp-brand-color3)}.jupyter-wrapper .jp-icon-brand4[stroke]{stroke:var(--jp-brand-color4)}.jupyter-wrapper .jp-icon-warn0[fill]{fill:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[fill]{fill:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[fill]{fill:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[fill]{fill:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-warn0[stroke]{stroke:var(--jp-warn-color0)}.jupyter-wrapper .jp-icon-warn1[stroke]{stroke:var(--jp-warn-color1)}.jupyter-wrapper .jp-icon-warn2[stroke]{stroke:var(--jp-warn-color2)}.jupyter-wrapper .jp-icon-warn3[stroke]{stroke:var(--jp-warn-color3)}.jupyter-wrapper .jp-icon-contrast0[fill]{fill:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[fill]{fill:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[fill]{fill:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[fill]{fill:var(--jp-icon-contrast-color3)}.jupyter-wrapper .jp-icon-contrast0[stroke]{stroke:var(--jp-icon-contrast-color0)}.jupyter-wrapper .jp-icon-contrast1[stroke]{stroke:var(--jp-icon-contrast-color1)}.jupyter-wrapper .jp-icon-contrast2[stroke]{stroke:var(--jp-icon-contrast-color2)}.jupyter-wrapper .jp-icon-contrast3[stroke]{stroke:var(--jp-icon-contrast-color3)}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #setting-editor .jp-PluginList .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-active .jp-icon-hover :hover .jp-icon-selectable-inverse[fill]{fill:#fff}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #tab-manager .lm-TabBar-tab.jp-mod-dirty.jp-mod-active>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:#fff}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon3[fill]{fill:none}.jupyter-wrapper .lm-DockPanel-tabBar .lm-TabBar-tab.lm-mod-closable.jp-mod-dirty>.lm-TabBar-tabCloseIcon>:not(:hover)>.jp-icon-busy[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable[fill]{fill:#fff}.jupyter-wrapper #jp-main-statusbar .jp-mod-selected .jp-icon-selectable-inverse[fill]{fill:var(--jp-brand-color1)}.jupyter-wrapper :root{--jp-warn-color0: var(--md-orange-700)}.jupyter-wrapper .jp-DragIcon{margin-right:4px}.jupyter-wrapper .jp-icon-alt .jp-icon0[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon0[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon1[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon2[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon3[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon4[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent0[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent1[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent2[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent3[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-alt .jp-icon-accent4[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hoverShow:not(:hover) svg{display:none !important}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[fill]{fill:none}.jupyter-wrapper .jp-icon-hover :hover .jp-icon-none-hover[stroke]{stroke:none}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[fill]{fill:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[fill]{fill:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[fill]{fill:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[fill]{fill:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[fill]{fill:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon0-hover[stroke]{stroke:var(--jp-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon1-hover[stroke]{stroke:var(--jp-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon2-hover[stroke]{stroke:var(--jp-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon3-hover[stroke]{stroke:var(--jp-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon4-hover[stroke]{stroke:var(--jp-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[fill]{fill:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[fill]{fill:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[fill]{fill:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[fill]{fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[fill]{fill:var(--jp-inverse-layout-color4)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent0-hover[stroke]{stroke:var(--jp-inverse-layout-color0)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent1-hover[stroke]{stroke:var(--jp-inverse-layout-color1)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent2-hover[stroke]{stroke:var(--jp-inverse-layout-color2)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent3-hover[stroke]{stroke:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-icon-hover.jp-icon-alt :hover .jp-icon-accent4-hover[stroke]{stroke:var(--jp-inverse-layout-color4)}.jupyter-wrapper :focus{outline:unset;outline-offset:unset;-moz-outline-radius:unset}.jupyter-wrapper .jp-Button{border-radius:var(--jp-border-radius);padding:0px 12px;font-size:var(--jp-ui-font-size1)}.jupyter-wrapper button.jp-Button.bp3-button.bp3-minimal:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Button.minimal{color:unset !important}.jupyter-wrapper .jp-Button.jp-ToolbarButtonComponent{text-transform:none}.jupyter-wrapper .jp-InputGroup input{box-sizing:border-box;border-radius:0;background-color:transparent;color:var(--jp-ui-font-color0);box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .jp-InputGroup input:focus{box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .jp-InputGroup input::placeholder,.jupyter-wrapper input::placeholder{color:var(--jp-ui-font-color3)}.jupyter-wrapper .jp-BPIcon{display:inline-block;vertical-align:middle;margin:auto}.jupyter-wrapper .bp3-icon.jp-BPIcon>svg:not([fill]){fill:var(--jp-inverse-layout-color3)}.jupyter-wrapper .jp-InputGroupAction{padding:6px}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select{background-color:initial;border:none;border-radius:0;box-shadow:none;color:var(--jp-ui-font-color0);display:block;font-size:var(--jp-ui-font-size1);height:24px;line-height:14px;padding:0 25px 0 10px;text-align:left;-moz-appearance:none;-webkit-appearance:none}.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select:hover,.jupyter-wrapper .jp-HTMLSelect.jp-DefaultStyle select>option{background-color:var(--jp-layout-color2);color:var(--jp-ui-font-color0)}.jupyter-wrapper select{box-sizing:border-box}.jupyter-wrapper .jp-Collapse{display:flex;flex-direction:column;align-items:stretch;border-top:1px solid var(--jp-border-color2);border-bottom:1px solid var(--jp-border-color2)}.jupyter-wrapper .jp-Collapse-header{padding:1px 12px;color:var(--jp-ui-font-color1);background-color:var(--jp-layout-color1);font-size:var(--jp-ui-font-size2)}.jupyter-wrapper .jp-Collapse-header:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-Collapse-contents{padding:0px 12px 0px 12px;background-color:var(--jp-layout-color1);color:var(--jp-ui-font-color1);overflow:auto}.jupyter-wrapper :root{--jp-private-commandpalette-search-height: 28px}.jupyter-wrapper .lm-CommandPalette{padding-bottom:0px;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-search{padding:4px;background-color:var(--jp-layout-color1);z-index:2}.jupyter-wrapper .lm-CommandPalette-wrapper{overflow:overlay;padding:0px 9px;background-color:var(--jp-input-active-background);height:30px;box-shadow:inset 0 0 0 var(--jp-border-width) var(--jp-input-border-color)}.jupyter-wrapper .lm-CommandPalette.lm-mod-focused .lm-CommandPalette-wrapper{box-shadow:inset 0 0 0 1px var(--jp-input-active-box-shadow-color),inset 0 0 0 3px var(--jp-input-active-box-shadow-color)}.jupyter-wrapper .lm-CommandPalette-wrapper::after{content:\" \";color:#fff;background-color:var(--jp-brand-color1);position:absolute;top:4px;right:4px;height:30px;width:10px;padding:0px 10px;background-image:var(--jp-icon-search-white);background-size:20px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .lm-CommandPalette-input{background:transparent;width:calc(100% - 18px);float:left;border:none;outline:none;font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);line-height:var(--jp-private-commandpalette-search-height)}.jupyter-wrapper .lm-CommandPalette-input::-webkit-input-placeholder,.jupyter-wrapper .lm-CommandPalette-input::-moz-placeholder,.jupyter-wrapper .lm-CommandPalette-input:-ms-input-placeholder{color:var(--jp-ui-font-color3);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .lm-CommandPalette-header:first-child{margin-top:0px}.jupyter-wrapper .lm-CommandPalette-header{border-bottom:solid var(--jp-border-width) var(--jp-border-color2);color:var(--jp-ui-font-color1);cursor:pointer;display:flex;font-size:var(--jp-ui-font-size0);font-weight:600;letter-spacing:1px;margin-top:8px;padding:8px 0 8px 12px;text-transform:uppercase}.jupyter-wrapper .lm-CommandPalette-header.lm-mod-active{background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-header>mark{background-color:transparent;font-weight:bold;color:var(--jp-ui-font-color1)}.jupyter-wrapper .lm-CommandPalette-item{padding:4px 12px 4px 4px;color:var(--jp-ui-font-color1);font-size:var(--jp-ui-font-size1);font-weight:400;display:flex}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active{background:var(--jp-layout-color3)}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-active:hover:not(.lm-mod-disabled){background:var(--jp-layout-color4)}.jupyter-wrapper .lm-CommandPalette-item:hover:not(.lm-mod-active):not(.lm-mod-disabled){background:var(--jp-layout-color2)}.jupyter-wrapper .lm-CommandPalette-itemContent{overflow:hidden}.jupyter-wrapper .lm-CommandPalette-itemLabel>mark{color:var(--jp-ui-font-color0);background-color:transparent;font-weight:bold}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled mark{color:var(--jp-ui-font-color3)}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemIcon{margin:0 4px 0 0;position:relative;width:16px;top:2px;flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-item.lm-mod-disabled .lm-CommandPalette-itemIcon{opacity:.4}.jupyter-wrapper .lm-CommandPalette-item .lm-CommandPalette-itemShortcut{flex:0 0 auto}.jupyter-wrapper .lm-CommandPalette-itemCaption{display:none}.jupyter-wrapper .lm-CommandPalette-content{background-color:var(--jp-layout-color1)}.jupyter-wrapper .lm-CommandPalette-content:empty:after{content:\"No results\";margin:auto;margin-top:20px;width:100px;display:block;font-size:var(--jp-ui-font-size2);font-family:var(--jp-ui-font-family);font-weight:lighter}.jupyter-wrapper .lm-CommandPalette-emptyMessage{text-align:center;margin-top:24px;line-height:1.32;padding:0px 8px;color:var(--jp-content-font-color3)}.jupyter-wrapper .jp-Dialog{position:absolute;z-index:10000;display:flex;flex-direction:column;align-items:center;justify-content:center;top:0px;left:0px;margin:0;padding:0;width:100%;height:100%;background:var(--jp-dialog-background)}.jupyter-wrapper .jp-Dialog-content{display:flex;flex-direction:column;margin-left:auto;margin-right:auto;background:var(--jp-layout-color1);padding:24px;padding-bottom:12px;min-width:300px;min-height:150px;max-width:1000px;max-height:500px;box-sizing:border-box;box-shadow:var(--jp-elevation-z20);word-wrap:break-word;border-radius:var(--jp-border-radius);font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color1)}.jupyter-wrapper .jp-Dialog-button{overflow:visible}.jupyter-wrapper button.jp-Dialog-button:focus{outline:1px solid var(--jp-brand-color1);outline-offset:4px;-moz-outline-radius:0px}.jupyter-wrapper button.jp-Dialog-button:focus::-moz-focus-inner{border:0}.jupyter-wrapper .jp-Dialog-header{flex:0 0 auto;padding-bottom:12px;font-size:var(--jp-ui-font-size3);font-weight:400;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-body{display:flex;flex-direction:column;flex:1 1 auto;font-size:var(--jp-ui-font-size1);background:var(--jp-layout-color1);overflow:auto}.jupyter-wrapper .jp-Dialog-footer{display:flex;flex-direction:row;justify-content:flex-end;flex:0 0 auto;margin-left:-12px;margin-right:-12px;padding:12px}.jupyter-wrapper .jp-Dialog-title{overflow:hidden;white-space:nowrap;text-overflow:ellipsis}.jupyter-wrapper .jp-Dialog-body>.jp-select-wrapper{width:100%}.jupyter-wrapper .jp-Dialog-body>button{padding:0px 16px}.jupyter-wrapper .jp-Dialog-body>label{line-height:1.4;color:var(--jp-ui-font-color0)}.jupyter-wrapper .jp-Dialog-button.jp-mod-styled:not(:last-child){margin-right:12px}.jupyter-wrapper .jp-HoverBox{position:fixed}.jupyter-wrapper .jp-HoverBox.jp-mod-outofview{display:none}.jupyter-wrapper .jp-IFrame{width:100%;height:100%}.jupyter-wrapper .jp-IFrame>iframe{border:none}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-IFrame:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:transparent}.jupyter-wrapper .jp-MainAreaWidget>:focus{outline:none}.jupyter-wrapper :root{--md-red-50: #ffebee;--md-red-100: #ffcdd2;--md-red-200: #ef9a9a;--md-red-300: #e57373;--md-red-400: #ef5350;--md-red-500: #f44336;--md-red-600: #e53935;--md-red-700: #d32f2f;--md-red-800: #c62828;--md-red-900: #b71c1c;--md-red-A100: #ff8a80;--md-red-A200: #ff5252;--md-red-A400: #ff1744;--md-red-A700: #d50000;--md-pink-50: #fce4ec;--md-pink-100: #f8bbd0;--md-pink-200: #f48fb1;--md-pink-300: #f06292;--md-pink-400: #ec407a;--md-pink-500: #e91e63;--md-pink-600: #d81b60;--md-pink-700: #c2185b;--md-pink-800: #ad1457;--md-pink-900: #880e4f;--md-pink-A100: #ff80ab;--md-pink-A200: #ff4081;--md-pink-A400: #f50057;--md-pink-A700: #c51162;--md-purple-50: #f3e5f5;--md-purple-100: #e1bee7;--md-purple-200: #ce93d8;--md-purple-300: #ba68c8;--md-purple-400: #ab47bc;--md-purple-500: #9c27b0;--md-purple-600: #8e24aa;--md-purple-700: #7b1fa2;--md-purple-800: #6a1b9a;--md-purple-900: #4a148c;--md-purple-A100: #ea80fc;--md-purple-A200: #e040fb;--md-purple-A400: #d500f9;--md-purple-A700: #aa00ff;--md-deep-purple-50: #ede7f6;--md-deep-purple-100: #d1c4e9;--md-deep-purple-200: #b39ddb;--md-deep-purple-300: #9575cd;--md-deep-purple-400: #7e57c2;--md-deep-purple-500: #673ab7;--md-deep-purple-600: #5e35b1;--md-deep-purple-700: #512da8;--md-deep-purple-800: #4527a0;--md-deep-purple-900: #311b92;--md-deep-purple-A100: #b388ff;--md-deep-purple-A200: #7c4dff;--md-deep-purple-A400: #651fff;--md-deep-purple-A700: #6200ea;--md-indigo-50: #e8eaf6;--md-indigo-100: #c5cae9;--md-indigo-200: #9fa8da;--md-indigo-300: #7986cb;--md-indigo-400: #5c6bc0;--md-indigo-500: #3f51b5;--md-indigo-600: #3949ab;--md-indigo-700: #303f9f;--md-indigo-800: #283593;--md-indigo-900: #1a237e;--md-indigo-A100: #8c9eff;--md-indigo-A200: #536dfe;--md-indigo-A400: #3d5afe;--md-indigo-A700: #304ffe;--md-blue-50: #e3f2fd;--md-blue-100: #bbdefb;--md-blue-200: #90caf9;--md-blue-300: #64b5f6;--md-blue-400: #42a5f5;--md-blue-500: #2196f3;--md-blue-600: #1e88e5;--md-blue-700: #1976d2;--md-blue-800: #1565c0;--md-blue-900: #0d47a1;--md-blue-A100: #82b1ff;--md-blue-A200: #448aff;--md-blue-A400: #2979ff;--md-blue-A700: #2962ff;--md-light-blue-50: #e1f5fe;--md-light-blue-100: #b3e5fc;--md-light-blue-200: #81d4fa;--md-light-blue-300: #4fc3f7;--md-light-blue-400: #29b6f6;--md-light-blue-500: #03a9f4;--md-light-blue-600: #039be5;--md-light-blue-700: #0288d1;--md-light-blue-800: #0277bd;--md-light-blue-900: #01579b;--md-light-blue-A100: #80d8ff;--md-light-blue-A200: #40c4ff;--md-light-blue-A400: #00b0ff;--md-light-blue-A700: #0091ea;--md-cyan-50: #e0f7fa;--md-cyan-100: #b2ebf2;--md-cyan-200: #80deea;--md-cyan-300: #4dd0e1;--md-cyan-400: #26c6da;--md-cyan-500: #00bcd4;--md-cyan-600: #00acc1;--md-cyan-700: #0097a7;--md-cyan-800: #00838f;--md-cyan-900: #006064;--md-cyan-A100: #84ffff;--md-cyan-A200: #18ffff;--md-cyan-A400: #00e5ff;--md-cyan-A700: #00b8d4;--md-teal-50: #e0f2f1;--md-teal-100: #b2dfdb;--md-teal-200: #80cbc4;--md-teal-300: #4db6ac;--md-teal-400: #26a69a;--md-teal-500: #009688;--md-teal-600: #00897b;--md-teal-700: #00796b;--md-teal-800: #00695c;--md-teal-900: #004d40;--md-teal-A100: #a7ffeb;--md-teal-A200: #64ffda;--md-teal-A400: #1de9b6;--md-teal-A700: #00bfa5;--md-green-50: #e8f5e9;--md-green-100: #c8e6c9;--md-green-200: #a5d6a7;--md-green-300: #81c784;--md-green-400: #66bb6a;--md-green-500: #4caf50;--md-green-600: #43a047;--md-green-700: #388e3c;--md-green-800: #2e7d32;--md-green-900: #1b5e20;--md-green-A100: #b9f6ca;--md-green-A200: #69f0ae;--md-green-A400: #00e676;--md-green-A700: #00c853;--md-light-green-50: #f1f8e9;--md-light-green-100: #dcedc8;--md-light-green-200: #c5e1a5;--md-light-green-300: #aed581;--md-light-green-400: #9ccc65;--md-light-green-500: #8bc34a;--md-light-green-600: #7cb342;--md-light-green-700: #689f38;--md-light-green-800: #558b2f;--md-light-green-900: #33691e;--md-light-green-A100: #ccff90;--md-light-green-A200: #b2ff59;--md-light-green-A400: #76ff03;--md-light-green-A700: #64dd17;--md-lime-50: #f9fbe7;--md-lime-100: #f0f4c3;--md-lime-200: #e6ee9c;--md-lime-300: #dce775;--md-lime-400: #d4e157;--md-lime-500: #cddc39;--md-lime-600: #c0ca33;--md-lime-700: #afb42b;--md-lime-800: #9e9d24;--md-lime-900: #827717;--md-lime-A100: #f4ff81;--md-lime-A200: #eeff41;--md-lime-A400: #c6ff00;--md-lime-A700: #aeea00;--md-yellow-50: #fffde7;--md-yellow-100: #fff9c4;--md-yellow-200: #fff59d;--md-yellow-300: #fff176;--md-yellow-400: #ffee58;--md-yellow-500: #ffeb3b;--md-yellow-600: #fdd835;--md-yellow-700: #fbc02d;--md-yellow-800: #f9a825;--md-yellow-900: #f57f17;--md-yellow-A100: #ffff8d;--md-yellow-A200: #ffff00;--md-yellow-A400: #ffea00;--md-yellow-A700: #ffd600;--md-amber-50: #fff8e1;--md-amber-100: #ffecb3;--md-amber-200: #ffe082;--md-amber-300: #ffd54f;--md-amber-400: #ffca28;--md-amber-500: #ffc107;--md-amber-600: #ffb300;--md-amber-700: #ffa000;--md-amber-800: #ff8f00;--md-amber-900: #ff6f00;--md-amber-A100: #ffe57f;--md-amber-A200: #ffd740;--md-amber-A400: #ffc400;--md-amber-A700: #ffab00;--md-orange-50: #fff3e0;--md-orange-100: #ffe0b2;--md-orange-200: #ffcc80;--md-orange-300: #ffb74d;--md-orange-400: #ffa726;--md-orange-500: #ff9800;--md-orange-600: #fb8c00;--md-orange-700: #f57c00;--md-orange-800: #ef6c00;--md-orange-900: #e65100;--md-orange-A100: #ffd180;--md-orange-A200: #ffab40;--md-orange-A400: #ff9100;--md-orange-A700: #ff6d00;--md-deep-orange-50: #fbe9e7;--md-deep-orange-100: #ffccbc;--md-deep-orange-200: #ffab91;--md-deep-orange-300: #ff8a65;--md-deep-orange-400: #ff7043;--md-deep-orange-500: #ff5722;--md-deep-orange-600: #f4511e;--md-deep-orange-700: #e64a19;--md-deep-orange-800: #d84315;--md-deep-orange-900: #bf360c;--md-deep-orange-A100: #ff9e80;--md-deep-orange-A200: #ff6e40;--md-deep-orange-A400: #ff3d00;--md-deep-orange-A700: #dd2c00;--md-brown-50: #efebe9;--md-brown-100: #d7ccc8;--md-brown-200: #bcaaa4;--md-brown-300: #a1887f;--md-brown-400: #8d6e63;--md-brown-500: #795548;--md-brown-600: #6d4c41;--md-brown-700: #5d4037;--md-brown-800: #4e342e;--md-brown-900: #3e2723;--md-grey-50: #fafafa;--md-grey-100: #f5f5f5;--md-grey-200: #eeeeee;--md-grey-300: #e0e0e0;--md-grey-400: #bdbdbd;--md-grey-500: #9e9e9e;--md-grey-600: #757575;--md-grey-700: #616161;--md-grey-800: #424242;--md-grey-900: #212121;--md-blue-grey-50: #eceff1;--md-blue-grey-100: #cfd8dc;--md-blue-grey-200: #b0bec5;--md-blue-grey-300: #90a4ae;--md-blue-grey-400: #78909c;--md-blue-grey-500: #607d8b;--md-blue-grey-600: #546e7a;--md-blue-grey-700: #455a64;--md-blue-grey-800: #37474f;--md-blue-grey-900: #263238}.jupyter-wrapper .jp-Spinner{position:absolute;display:flex;justify-content:center;align-items:center;z-index:10;left:0;top:0;width:100%;height:100%;background:var(--jp-layout-color0);outline:none}.jupyter-wrapper .jp-SpinnerContent{font-size:10px;margin:50px auto;text-indent:-9999em;width:3em;height:3em;border-radius:50%;background:var(--jp-brand-color3);background:linear-gradient(to right, #f37626 10%, rgba(255, 255, 255, 0) 42%);position:relative;animation:load3 1s infinite linear,fadeIn 1s}.jupyter-wrapper .jp-SpinnerContent:before{width:50%;height:50%;background:#f37626;border-radius:100% 0 0 0;position:absolute;top:0;left:0;content:\"\"}.jupyter-wrapper .jp-SpinnerContent:after{background:var(--jp-layout-color0);width:75%;height:75%;border-radius:50%;content:\"\";margin:auto;position:absolute;top:0;left:0;bottom:0;right:0}@keyframes fadeIn{0%{opacity:0}100%{opacity:1}}@keyframes load3{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.jupyter-wrapper button.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:none;box-sizing:border-box;text-align:center;line-height:32px;height:32px;padding:0px 12px;letter-spacing:.8px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled{background:var(--jp-input-background);height:28px;box-sizing:border-box;border:var(--jp-border-width) solid var(--jp-border-color1);padding-left:7px;padding-right:7px;font-size:var(--jp-ui-font-size2);color:var(--jp-ui-font-color0);outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper input.jp-mod-styled:focus{border:var(--jp-border-width) solid var(--md-blue-500);box-shadow:inset 0 0 4px var(--md-blue-300)}.jupyter-wrapper .jp-select-wrapper{display:flex;position:relative;flex-direction:column;padding:1px;background-color:var(--jp-layout-color1);height:28px;box-sizing:border-box;margin-bottom:12px}.jupyter-wrapper .jp-select-wrapper.jp-mod-focused select.jp-mod-styled{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-input-active-background)}.jupyter-wrapper select.jp-mod-styled:hover{background-color:var(--jp-layout-color1);cursor:pointer;color:var(--jp-ui-font-color0);background-color:var(--jp-input-hover-background);box-shadow:inset 0 0px 1px rgba(0,0,0,.5)}.jupyter-wrapper select.jp-mod-styled{flex:1 1 auto;height:32px;width:100%;font-size:var(--jp-ui-font-size2);background:var(--jp-input-background);color:var(--jp-ui-font-color0);padding:0 25px 0 8px;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none}.jupyter-wrapper :root{--jp-private-toolbar-height: calc( 28px + var(--jp-border-width) )}.jupyter-wrapper .jp-Toolbar{color:var(--jp-ui-font-color1);flex:0 0 auto;display:flex;flex-direction:row;border-bottom:var(--jp-border-width) solid var(--jp-toolbar-border-color);box-shadow:var(--jp-toolbar-box-shadow);background:var(--jp-toolbar-background);min-height:var(--jp-toolbar-micro-height);padding:2px;z-index:1}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item.jp-Toolbar-spacer{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-Toolbar-item.jp-Toolbar-kernelStatus{display:inline-block;width:32px;background-repeat:no-repeat;background-position:center;background-size:16px}.jupyter-wrapper .jp-Toolbar>.jp-Toolbar-item{flex:0 0 auto;display:flex;padding-left:1px;padding-right:1px;font-size:var(--jp-ui-font-size1);line-height:var(--jp-private-toolbar-height);height:100%}.jupyter-wrapper div.jp-ToolbarButton{color:transparent;border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px;margin:0px}.jupyter-wrapper button.jp-ToolbarButtonComponent{background:var(--jp-layout-color1);border:none;box-sizing:border-box;outline:none;appearance:none;-webkit-appearance:none;-moz-appearance:none;padding:0px 6px;margin:0px;height:24px;border-radius:var(--jp-border-radius);display:flex;align-items:center;text-align:center;font-size:14px;min-width:unset;min-height:unset}.jupyter-wrapper button.jp-ToolbarButtonComponent:disabled{opacity:.4}.jupyter-wrapper button.jp-ToolbarButtonComponent span{padding:0px;flex:0 0 auto}.jupyter-wrapper button.jp-ToolbarButtonComponent .jp-ToolbarButtonComponent-label{font-size:var(--jp-ui-font-size1);line-height:100%;padding-left:2px;color:var(--jp-ui-font-color1)}.jupyter-wrapper body.p-mod-override-cursor *,.jupyter-wrapper body.lm-mod-override-cursor *{cursor:inherit !important}.jupyter-wrapper .jp-JSONEditor{display:flex;flex-direction:column;width:100%}.jupyter-wrapper .jp-JSONEditor-host{flex:1 1 auto;border:var(--jp-border-width) solid var(--jp-input-border-color);border-radius:0px;background:var(--jp-layout-color0);min-height:50px;padding:1px}.jupyter-wrapper .jp-JSONEditor.jp-mod-error .jp-JSONEditor-host{border-color:red;outline-color:red}.jupyter-wrapper .jp-JSONEditor-header{display:flex;flex:1 0 auto;padding:0 0 0 12px}.jupyter-wrapper .jp-JSONEditor-header label{flex:0 0 auto}.jupyter-wrapper .jp-JSONEditor-commitButton{height:16px;width:16px;background-size:18px;background-repeat:no-repeat;background-position:center}.jupyter-wrapper .jp-JSONEditor-host.jp-mod-focused{background-color:var(--jp-input-active-background);border:1px solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .jp-Editor.jp-mod-dropTarget{border:var(--jp-border-width) solid var(--jp-input-active-border-color);box-shadow:var(--jp-input-box-shadow)}.jupyter-wrapper .CodeMirror{font-family:monospace;height:300px;color:#000;direction:ltr}.jupyter-wrapper .CodeMirror-lines{padding:4px 0}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{padding:0 4px}.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{background-color:#fff}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid #ddd;background-color:#f7f7f7;white-space:nowrap}.jupyter-wrapper .CodeMirror-linenumber{padding:0 3px 0 5px;min-width:20px;text-align:right;color:#999;white-space:nowrap}.jupyter-wrapper .CodeMirror-guttermarker{color:#000}.jupyter-wrapper .CodeMirror-guttermarker-subtle{color:#999}.jupyter-wrapper .CodeMirror-cursor{border-left:1px solid #000;border-right:none;width:0}.jupyter-wrapper .CodeMirror div.CodeMirror-secondarycursor{border-left:1px solid silver}.jupyter-wrapper .cm-fat-cursor .CodeMirror-cursor{width:auto;border:0 !important;background:#7e7}.jupyter-wrapper .cm-fat-cursor div.CodeMirror-cursors{z-index:1}.jupyter-wrapper .cm-fat-cursor-mark{background-color:rgba(20,255,20,.5);-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite}.jupyter-wrapper .cm-animate-fat-cursor{width:auto;border:0;-webkit-animation:blink 1.06s steps(1) infinite;-moz-animation:blink 1.06s steps(1) infinite;animation:blink 1.06s steps(1) infinite;background-color:#7e7}@-moz-keyframes blink{50%{background-color:transparent}}@-webkit-keyframes blink{50%{background-color:transparent}}@keyframes blink{50%{background-color:transparent}}.jupyter-wrapper .cm-tab{display:inline-block;text-decoration:inherit}.jupyter-wrapper .CodeMirror-rulers{position:absolute;left:0;right:0;top:-50px;bottom:0;overflow:hidden}.jupyter-wrapper .CodeMirror-ruler{border-left:1px solid #ccc;top:0;bottom:0;position:absolute}.jupyter-wrapper .cm-s-default .cm-header{color:blue}.jupyter-wrapper .cm-s-default .cm-quote{color:#090}.jupyter-wrapper .cm-negative{color:#d44}.jupyter-wrapper .cm-positive{color:#292}.jupyter-wrapper .cm-header,.jupyter-wrapper .cm-strong{font-weight:bold}.jupyter-wrapper .cm-em{font-style:italic}.jupyter-wrapper .cm-link{text-decoration:underline}.jupyter-wrapper .cm-strikethrough{text-decoration:line-through}.jupyter-wrapper .cm-s-default .cm-keyword{color:#708}.jupyter-wrapper .cm-s-default .cm-atom{color:#219}.jupyter-wrapper .cm-s-default .cm-number{color:#164}.jupyter-wrapper .cm-s-default .cm-def{color:blue}.jupyter-wrapper .cm-s-default .cm-variable-2{color:#05a}.jupyter-wrapper .cm-s-default .cm-variable-3,.jupyter-wrapper .cm-s-default .cm-type{color:#085}.jupyter-wrapper .cm-s-default .cm-comment{color:#a50}.jupyter-wrapper .cm-s-default .cm-string{color:#a11}.jupyter-wrapper .cm-s-default .cm-string-2{color:#f50}.jupyter-wrapper .cm-s-default .cm-meta{color:#555}.jupyter-wrapper .cm-s-default .cm-qualifier{color:#555}.jupyter-wrapper .cm-s-default .cm-builtin{color:#30a}.jupyter-wrapper .cm-s-default .cm-bracket{color:#997}.jupyter-wrapper .cm-s-default .cm-tag{color:#170}.jupyter-wrapper .cm-s-default .cm-attribute{color:#00c}.jupyter-wrapper .cm-s-default .cm-hr{color:#999}.jupyter-wrapper .cm-s-default .cm-link{color:#00c}.jupyter-wrapper .cm-s-default .cm-error{color:red}.jupyter-wrapper .cm-invalidchar{color:red}.jupyter-wrapper .CodeMirror-composing{border-bottom:2px solid}.jupyter-wrapper div.CodeMirror span.CodeMirror-matchingbracket{color:#0b0}.jupyter-wrapper div.CodeMirror span.CodeMirror-nonmatchingbracket{color:#a22}.jupyter-wrapper .CodeMirror-matchingtag{background:rgba(255,150,0,.3)}.jupyter-wrapper .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .CodeMirror{position:relative;overflow:hidden;background:#fff}.jupyter-wrapper .CodeMirror-scroll{overflow:scroll !important;margin-bottom:-30px;margin-right:-30px;padding-bottom:30px;height:100%;outline:none;position:relative}.jupyter-wrapper .CodeMirror-sizer{position:relative;border-right:30px solid transparent}.jupyter-wrapper .CodeMirror-vscrollbar,.jupyter-wrapper .CodeMirror-hscrollbar,.jupyter-wrapper .CodeMirror-scrollbar-filler,.jupyter-wrapper .CodeMirror-gutter-filler{position:absolute;z-index:6;display:none}.jupyter-wrapper .CodeMirror-vscrollbar{right:0;top:0;overflow-x:hidden;overflow-y:scroll}.jupyter-wrapper .CodeMirror-hscrollbar{bottom:0;left:0;overflow-y:hidden;overflow-x:scroll}.jupyter-wrapper .CodeMirror-scrollbar-filler{right:0;bottom:0}.jupyter-wrapper .CodeMirror-gutter-filler{left:0;bottom:0}.jupyter-wrapper .CodeMirror-gutters{position:absolute;left:0;top:0;min-height:100%;z-index:3}.jupyter-wrapper .CodeMirror-gutter{white-space:normal;height:100%;display:inline-block;vertical-align:top;margin-bottom:-30px}.jupyter-wrapper .CodeMirror-gutter-wrapper{position:absolute;z-index:4;background:none !important;border:none !important}.jupyter-wrapper .CodeMirror-gutter-background{position:absolute;top:0;bottom:0;z-index:4}.jupyter-wrapper .CodeMirror-gutter-elt{position:absolute;cursor:default;z-index:4}.jupyter-wrapper .CodeMirror-gutter-wrapper ::selection{background-color:transparent}.jupyter-wrapper .CodeMirror-gutter-wrapper ::-moz-selection{background-color:transparent}.jupyter-wrapper .CodeMirror-lines{cursor:text;min-height:1px}.jupyter-wrapper .CodeMirror pre.CodeMirror-line,.jupyter-wrapper .CodeMirror pre.CodeMirror-line-like{-moz-border-radius:0;-webkit-border-radius:0;border-radius:0;border-width:0;background:transparent;font-family:inherit;font-size:inherit;margin:0;white-space:pre;word-wrap:normal;line-height:inherit;color:inherit;z-index:2;position:relative;overflow:visible;-webkit-tap-highlight-color:transparent;-webkit-font-variant-ligatures:contextual;font-variant-ligatures:contextual}.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line,.jupyter-wrapper .CodeMirror-wrap pre.CodeMirror-line-like{word-wrap:break-word;white-space:pre-wrap;word-break:normal}.jupyter-wrapper .CodeMirror-linebackground{position:absolute;left:0;right:0;top:0;bottom:0;z-index:0}.jupyter-wrapper .CodeMirror-linewidget{position:relative;z-index:2;padding:.1px}.jupyter-wrapper .CodeMirror-rtl pre{direction:rtl}.jupyter-wrapper .CodeMirror-code{outline:none}.jupyter-wrapper .CodeMirror-scroll,.jupyter-wrapper .CodeMirror-sizer,.jupyter-wrapper .CodeMirror-gutter,.jupyter-wrapper .CodeMirror-gutters,.jupyter-wrapper .CodeMirror-linenumber{-moz-box-sizing:content-box;box-sizing:content-box}.jupyter-wrapper .CodeMirror-measure{position:absolute;width:100%;height:0;overflow:hidden;visibility:hidden}.jupyter-wrapper .CodeMirror-cursor{position:absolute;pointer-events:none}.jupyter-wrapper .CodeMirror-measure pre{position:static}.jupyter-wrapper div.CodeMirror-cursors{visibility:hidden;position:relative;z-index:3}.jupyter-wrapper div.CodeMirror-dragcursors{visibility:visible}.jupyter-wrapper .CodeMirror-focused div.CodeMirror-cursors{visibility:visible}.jupyter-wrapper .CodeMirror-selected{background:#d9d9d9}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background:#d7d4f0}.jupyter-wrapper .CodeMirror-crosshair{cursor:crosshair}.jupyter-wrapper .CodeMirror-line::selection,.jupyter-wrapper .CodeMirror-line>span::selection,.jupyter-wrapper .CodeMirror-line>span>span::selection{background:#d7d4f0}.jupyter-wrapper .CodeMirror-line::-moz-selection,.jupyter-wrapper .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .CodeMirror-line>span>span::-moz-selection{background:#d7d4f0}.jupyter-wrapper .cm-searching{background-color:#ffa;background-color:rgba(255,255,0,.4)}.jupyter-wrapper .cm-force-border{padding-right:.1px}@media print{.jupyter-wrapper .CodeMirror div.CodeMirror-cursors{visibility:hidden}}.jupyter-wrapper .cm-tab-wrap-hack:after{content:\"\"}.jupyter-wrapper span.CodeMirror-selectedtext{background:none}.jupyter-wrapper .CodeMirror-dialog{position:absolute;left:0;right:0;background:inherit;z-index:15;padding:.1em .8em;overflow:hidden;color:inherit}.jupyter-wrapper .CodeMirror-dialog-top{border-bottom:1px solid #eee;top:0}.jupyter-wrapper .CodeMirror-dialog-bottom{border-top:1px solid #eee;bottom:0}.jupyter-wrapper .CodeMirror-dialog input{border:none;outline:none;background:transparent;width:20em;color:inherit;font-family:monospace}.jupyter-wrapper .CodeMirror-dialog button{font-size:70%}.jupyter-wrapper .CodeMirror-foldmarker{color:blue;text-shadow:#b9f 1px 1px 2px,#b9f -1px -1px 2px,#b9f 1px -1px 2px,#b9f -1px 1px 2px;font-family:arial;line-height:.3;cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter{width:.7em}.jupyter-wrapper .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{cursor:pointer}.jupyter-wrapper .CodeMirror-foldgutter-open:after{content:\"\u25be\"}.jupyter-wrapper .CodeMirror-foldgutter-folded:after{content:\"\u25b8\"}.jupyter-wrapper .cm-s-material.CodeMirror{background-color:#263238;color:#eff}.jupyter-wrapper .cm-s-material .CodeMirror-gutters{background:#263238;color:#546e7a;border:none}.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker,.jupyter-wrapper .cm-s-material .CodeMirror-guttermarker-subtle,.jupyter-wrapper .cm-s-material .CodeMirror-linenumber{color:#546e7a}.jupyter-wrapper .cm-s-material .CodeMirror-cursor{border-left:1px solid #fc0}.jupyter-wrapper .cm-s-material div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material.CodeMirror-focused div.CodeMirror-selected{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-material .CodeMirror-line>span>span::-moz-selection{background:rgba(128,203,196,.2)}.jupyter-wrapper .cm-s-material .CodeMirror-activeline-background{background:rgba(0,0,0,.5)}.jupyter-wrapper .cm-s-material .cm-keyword{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-operator{color:#89ddff}.jupyter-wrapper .cm-s-material .cm-variable-2{color:#eff}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#f07178}.jupyter-wrapper .cm-s-material .cm-builtin{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-atom{color:#f78c6c}.jupyter-wrapper .cm-s-material .cm-number{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-def{color:#82aaff}.jupyter-wrapper .cm-s-material .cm-string{color:#c3e88d}.jupyter-wrapper .cm-s-material .cm-string-2{color:#f07178}.jupyter-wrapper .cm-s-material .cm-comment{color:#546e7a}.jupyter-wrapper .cm-s-material .cm-variable{color:#f07178}.jupyter-wrapper .cm-s-material .cm-tag{color:#ff5370}.jupyter-wrapper .cm-s-material .cm-meta{color:#ffcb6b}.jupyter-wrapper .cm-s-material .cm-attribute{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-property{color:#c792ea}.jupyter-wrapper .cm-s-material .cm-qualifier{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-variable-3,.jupyter-wrapper .cm-s-material .cm-type{color:#decb6b}.jupyter-wrapper .cm-s-material .cm-error{color:#fff;background-color:#ff5370}.jupyter-wrapper .cm-s-material .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-gutters{background:#3f3f3f !important}.jupyter-wrapper .cm-s-zenburn .CodeMirror-foldgutter-open,.jupyter-wrapper .CodeMirror-foldgutter-folded{color:#999}.jupyter-wrapper .cm-s-zenburn .CodeMirror-cursor{border-left:1px solid #fff}.jupyter-wrapper .cm-s-zenburn{background-color:#3f3f3f;color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-builtin{color:#dcdccc;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-comment{color:#7f9f7f}.jupyter-wrapper .cm-s-zenburn span.cm-keyword{color:#f0dfaf;font-weight:bold}.jupyter-wrapper .cm-s-zenburn span.cm-atom{color:#bfebbf}.jupyter-wrapper .cm-s-zenburn span.cm-def{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-variable{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-variable-2{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-string{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-string-2{color:#cc9393}.jupyter-wrapper .cm-s-zenburn span.cm-number{color:#dcdccc}.jupyter-wrapper .cm-s-zenburn span.cm-tag{color:#93e0e3}.jupyter-wrapper .cm-s-zenburn span.cm-property{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-attribute{color:#dfaf8f}.jupyter-wrapper .cm-s-zenburn span.cm-qualifier{color:#7cb8bb}.jupyter-wrapper .cm-s-zenburn span.cm-meta{color:#f0dfaf}.jupyter-wrapper .cm-s-zenburn span.cm-header{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.cm-operator{color:#f0efd0}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-matchingbracket{box-sizing:border-box;background:transparent;border-bottom:1px solid}.jupyter-wrapper .cm-s-zenburn span.CodeMirror-nonmatchingbracket{border-bottom:1px solid;background:none}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline{background:#000}.jupyter-wrapper .cm-s-zenburn .CodeMirror-activeline-background{background:#000}.jupyter-wrapper .cm-s-zenburn div.CodeMirror-selected{background:#545454}.jupyter-wrapper .cm-s-zenburn .CodeMirror-focused div.CodeMirror-selected{background:#4f4f4f}.jupyter-wrapper .cm-s-abcdef.CodeMirror{background:#0f0f0f;color:#defdef}.jupyter-wrapper .cm-s-abcdef div.CodeMirror-selected{background:#515151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-abcdef .CodeMirror-line>span>span::-moz-selection{background:rgba(56,56,56,.99)}.jupyter-wrapper .cm-s-abcdef .CodeMirror-gutters{background:#555;border-right:2px solid #314151}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker{color:#222}.jupyter-wrapper .cm-s-abcdef .CodeMirror-guttermarker-subtle{color:azure}.jupyter-wrapper .cm-s-abcdef .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-abcdef .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-abcdef span.cm-keyword{color:#b8860b;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-atom{color:#77f}.jupyter-wrapper .cm-s-abcdef span.cm-number{color:violet}.jupyter-wrapper .cm-s-abcdef span.cm-def{color:#fffabc}.jupyter-wrapper .cm-s-abcdef span.cm-variable{color:#abcdef}.jupyter-wrapper .cm-s-abcdef span.cm-variable-2{color:#cacbcc}.jupyter-wrapper .cm-s-abcdef span.cm-variable-3,.jupyter-wrapper .cm-s-abcdef span.cm-type{color:#def}.jupyter-wrapper .cm-s-abcdef span.cm-property{color:#fedcba}.jupyter-wrapper .cm-s-abcdef span.cm-operator{color:#ff0}.jupyter-wrapper .cm-s-abcdef span.cm-comment{color:#7a7b7c;font-style:italic}.jupyter-wrapper .cm-s-abcdef span.cm-string{color:#2b4}.jupyter-wrapper .cm-s-abcdef span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-abcdef span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-abcdef span.cm-builtin{color:#30aabc}.jupyter-wrapper .cm-s-abcdef span.cm-bracket{color:#8a8a8a}.jupyter-wrapper .cm-s-abcdef span.cm-tag{color:#fd4}.jupyter-wrapper .cm-s-abcdef span.cm-attribute{color:#df0}.jupyter-wrapper .cm-s-abcdef span.cm-error{color:red}.jupyter-wrapper .cm-s-abcdef span.cm-header{color:#7fffd4;font-weight:bold}.jupyter-wrapper .cm-s-abcdef span.cm-link{color:#8a2be2}.jupyter-wrapper .cm-s-abcdef .CodeMirror-activeline-background{background:#314151}.jupyter-wrapper .cm-s-base16-light.CodeMirror{background:#f5f5f5;color:#202020}.jupyter-wrapper .cm-s-base16-light div.CodeMirror-selected{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-light .CodeMirror-line>span>span::-moz-selection{background:#e0e0e0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-gutters{background:#f5f5f5;border-right:0px}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-light .CodeMirror-guttermarker-subtle{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-linenumber{color:#b0b0b0}.jupyter-wrapper .cm-s-base16-light .CodeMirror-cursor{border-left:1px solid #505050}.jupyter-wrapper .cm-s-base16-light span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-light span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-property,.jupyter-wrapper .cm-s-base16-light span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-light span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-light span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-light span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-light span.cm-bracket{color:#202020}.jupyter-wrapper .cm-s-base16-light span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-light span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-light span.cm-error{background:#ac4142;color:#505050}.jupyter-wrapper .cm-s-base16-light .CodeMirror-activeline-background{background:#dddcdc}.jupyter-wrapper .cm-s-base16-light .CodeMirror-matchingbracket{color:#f5f5f5 !important;background-color:#6a9fb5 !important}.jupyter-wrapper .cm-s-base16-dark.CodeMirror{background:#151515;color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark div.CodeMirror-selected{background:#303030}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-base16-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(48,48,48,.99)}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-gutters{background:#151515;border-right:0px}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-guttermarker-subtle{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-linenumber{color:#505050}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-cursor{border-left:1px solid #b0b0b0}.jupyter-wrapper .cm-s-base16-dark span.cm-comment{color:#8f5536}.jupyter-wrapper .cm-s-base16-dark span.cm-atom{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-number{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-property,.jupyter-wrapper .cm-s-base16-dark span.cm-attribute{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-keyword{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-string{color:#f4bf75}.jupyter-wrapper .cm-s-base16-dark span.cm-variable{color:#90a959}.jupyter-wrapper .cm-s-base16-dark span.cm-variable-2{color:#6a9fb5}.jupyter-wrapper .cm-s-base16-dark span.cm-def{color:#d28445}.jupyter-wrapper .cm-s-base16-dark span.cm-bracket{color:#e0e0e0}.jupyter-wrapper .cm-s-base16-dark span.cm-tag{color:#ac4142}.jupyter-wrapper .cm-s-base16-dark span.cm-link{color:#aa759f}.jupyter-wrapper .cm-s-base16-dark span.cm-error{background:#ac4142;color:#b0b0b0}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-activeline-background{background:#202020}.jupyter-wrapper .cm-s-base16-dark .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-dracula.CodeMirror,.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{background-color:#282a36 !important;color:#f8f8f2 !important;border:none}.jupyter-wrapper .cm-s-dracula .CodeMirror-gutters{color:#282a36}.jupyter-wrapper .cm-s-dracula .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-dracula .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-dracula .CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dracula .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula span.cm-comment{color:#6272a4}.jupyter-wrapper .cm-s-dracula span.cm-string,.jupyter-wrapper .cm-s-dracula span.cm-string-2{color:#f1fa8c}.jupyter-wrapper .cm-s-dracula span.cm-number{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-variable{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-2{color:#fff}.jupyter-wrapper .cm-s-dracula span.cm-def{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-operator{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-atom{color:#bd93f9}.jupyter-wrapper .cm-s-dracula span.cm-meta{color:#f8f8f2}.jupyter-wrapper .cm-s-dracula span.cm-tag{color:#ff79c6}.jupyter-wrapper .cm-s-dracula span.cm-attribute{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-qualifier{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-property{color:#66d9ef}.jupyter-wrapper .cm-s-dracula span.cm-builtin{color:#50fa7b}.jupyter-wrapper .cm-s-dracula span.cm-variable-3,.jupyter-wrapper .cm-s-dracula span.cm-type{color:#ffb86c}.jupyter-wrapper .cm-s-dracula .CodeMirror-activeline-background{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-dracula .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch.CodeMirror{background:#322931;color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch div.CodeMirror-selected{background:#433b42 !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-gutters{background:#322931;border-right:0px}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-linenumber{color:#797379}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-cursor{border-left:1px solid #989498 !important}.jupyter-wrapper .cm-s-hopscotch span.cm-comment{color:#b33508}.jupyter-wrapper .cm-s-hopscotch span.cm-atom{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-number{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch span.cm-property,.jupyter-wrapper .cm-s-hopscotch span.cm-attribute{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-keyword{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-string{color:#fdcc59}.jupyter-wrapper .cm-s-hopscotch span.cm-variable{color:#8fc13e}.jupyter-wrapper .cm-s-hopscotch span.cm-variable-2{color:#1290bf}.jupyter-wrapper .cm-s-hopscotch span.cm-def{color:#fd8b19}.jupyter-wrapper .cm-s-hopscotch span.cm-error{background:#dd464c;color:#989498}.jupyter-wrapper .cm-s-hopscotch span.cm-bracket{color:#d5d3d5}.jupyter-wrapper .cm-s-hopscotch span.cm-tag{color:#dd464c}.jupyter-wrapper .cm-s-hopscotch span.cm-link{color:#c85e7c}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .cm-s-hopscotch .CodeMirror-activeline-background{background:#302020}.jupyter-wrapper .cm-s-mbo.CodeMirror{background:#2c2c2c;color:#ffffec}.jupyter-wrapper .cm-s-mbo div.CodeMirror-selected{background:#716c62}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mbo .CodeMirror-line>span>span::-moz-selection{background:rgba(113,108,98,.99)}.jupyter-wrapper .cm-s-mbo .CodeMirror-gutters{background:#4e4e4e;border-right:0px}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker{color:#fff}.jupyter-wrapper .cm-s-mbo .CodeMirror-guttermarker-subtle{color:gray}.jupyter-wrapper .cm-s-mbo .CodeMirror-linenumber{color:#dadada}.jupyter-wrapper .cm-s-mbo .CodeMirror-cursor{border-left:1px solid #ffffec}.jupyter-wrapper .cm-s-mbo span.cm-comment{color:#95958a}.jupyter-wrapper .cm-s-mbo span.cm-atom{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-number{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-property,.jupyter-wrapper .cm-s-mbo span.cm-attribute{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-keyword{color:#ffb928}.jupyter-wrapper .cm-s-mbo span.cm-string{color:#ffcf6c}.jupyter-wrapper .cm-s-mbo span.cm-string.cm-property{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-variable-2{color:#00a8c6}.jupyter-wrapper .cm-s-mbo span.cm-def{color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-bracket{color:#fffffc;font-weight:bold}.jupyter-wrapper .cm-s-mbo span.cm-tag{color:#9ddfe9}.jupyter-wrapper .cm-s-mbo span.cm-link{color:#f54b07}.jupyter-wrapper .cm-s-mbo span.cm-error{border-bottom:#636363;color:#ffffec}.jupyter-wrapper .cm-s-mbo span.cm-qualifier{color:#ffffec}.jupyter-wrapper .cm-s-mbo .CodeMirror-activeline-background{background:#494b41}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingbracket{color:#ffb928 !important}.jupyter-wrapper .cm-s-mbo .CodeMirror-matchingtag{background:rgba(255,255,255,.37)}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{color:#999;background-color:#fff}.jupyter-wrapper .cm-s-mdn-like div.CodeMirror-selected{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-mdn-like .CodeMirror-line>span>span::-moz-selection{background:#cfc}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-gutters{background:#f8f8f8;border-left:6px solid rgba(0,83,159,.65);color:#333}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-linenumber{color:#aaa;padding-left:8px}.jupyter-wrapper .cm-s-mdn-like .CodeMirror-cursor{border-left:2px solid #222}.jupyter-wrapper .cm-s-mdn-like .cm-keyword{color:#6262ff}.jupyter-wrapper .cm-s-mdn-like .cm-atom{color:#f90}.jupyter-wrapper .cm-s-mdn-like .cm-number{color:#ca7841}.jupyter-wrapper .cm-s-mdn-like .cm-def{color:#8da6ce}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-2,.jupyter-wrapper .cm-s-mdn-like span.cm-tag{color:#690}.jupyter-wrapper .cm-s-mdn-like span.cm-variable-3,.jupyter-wrapper .cm-s-mdn-like span.cm-def,.jupyter-wrapper .cm-s-mdn-like span.cm-type{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-variable{color:#07a}.jupyter-wrapper .cm-s-mdn-like .cm-property{color:#905}.jupyter-wrapper .cm-s-mdn-like .cm-qualifier{color:#690}.jupyter-wrapper .cm-s-mdn-like .cm-operator{color:#cda869}.jupyter-wrapper .cm-s-mdn-like .cm-comment{color:#777;font-weight:normal}.jupyter-wrapper .cm-s-mdn-like .cm-string{color:#07a;font-style:italic}.jupyter-wrapper .cm-s-mdn-like .cm-string-2{color:#bd6b18}.jupyter-wrapper .cm-s-mdn-like .cm-meta{color:#000}.jupyter-wrapper .cm-s-mdn-like .cm-builtin{color:#9b7536}.jupyter-wrapper .cm-s-mdn-like .cm-tag{color:#997643}.jupyter-wrapper .cm-s-mdn-like .cm-attribute{color:#d6bb6d}.jupyter-wrapper .cm-s-mdn-like .cm-header{color:#ff6400}.jupyter-wrapper .cm-s-mdn-like .cm-hr{color:#aeaeae}.jupyter-wrapper .cm-s-mdn-like .cm-link{color:#ad9361;font-style:italic;text-decoration:none}.jupyter-wrapper .cm-s-mdn-like .cm-error{border-bottom:1px solid red}.jupyter-wrapper div.cm-s-mdn-like .CodeMirror-activeline-background{background:#efefff}.jupyter-wrapper div.cm-s-mdn-like span.CodeMirror-matchingbracket{outline:1px solid gray;color:inherit}.jupyter-wrapper .cm-s-mdn-like.CodeMirror{background-image:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAFcAAAAyCAYAAAAp8UeFAAAHvklEQVR42s2b63bcNgyEQZCSHCdt2vd/0tWF7I+Q6XgMXiTtuvU5Pl57ZQKkKHzEAOtF5KeIJBGJ8uvL599FRFREZhFx8DeXv8trn68RuGaC8TRfo3SNp9dlDDHedyLyTUTeRWStXKPZrjtpZxaRw5hPqozRs1N8/enzIiQRWcCgy4MUA0f+XWliDhyL8Lfyvx7ei/Ae3iQFHyw7U/59pQVIMEEPEz0G7XiwdRjzSfC3UTtz9vchIntxvry5iMgfIhJoEflOz2CQr3F5h/HfeFe+GTdLaKcu9L8LTeQb/R/7GgbsfKedyNdoHsN31uRPWrfZ5wsj/NzzRQHuToIdU3ahwnsKPxXCjJITuOsi7XLc7SG/v5GdALs7wf8JjTFiB5+QvTEfRyGOfX3Lrx8wxyQi3sNq46O7QahQiCsRFgqddjBouVEHOKDgXAQHD9gJCr5sMKkEdjwsarG/ww3BMHBU7OBjXnzdyY7SfCxf5/z6ATccrwlKuwC/jhznnPF4CgVzhhVf4xp2EixcBActO75iZ8/fM9zAs2OMzKdslgXWJ9XG8PQoOAMA5fGcsvORgv0doBXyHrCwfLJAOwo71QLNkb8n2Pl6EWiR7OCibtkPaz4Kc/0NNAze2gju3zOwekALDaCFPI5vjPFmgGY5AZqyGEvH1x7QfIb8YtxMnA/b+QQ0aQDAwc6JMFg8CbQZ4qoYEEHbRwNojuK3EHwd7VALSgq+MNDKzfT58T8qdpADrgW0GmgcAS1lhzztJmkAzcPNOQbsWEALBDSlMKUG0Eq4CLAQWvEVQ9WU57gZJwZtgPO3r9oBTQ9WO8TjqXINx8R0EYpiZEUWOF3FxkbJkgU9B2f41YBrIj5ZfsQa0M5kTgiAAqM3ShXLgu8XMqcrQBvJ0CL5pnTsfMB13oB8athpAq2XOQmcGmoACCLydx7nToa23ATaSIY2ichfOdPTGxlasXMLaL0MLZAOwAKIM+y8CmicobGdCcbbK9DzN+yYGVoNNI5iUKTMyYOjPse4A8SM1MmcXgU0toOq1yO/v8FOxlASyc7TgeYaAMBJHcY1CcCwGI/TK4AmDbDyKYBBtFUkRwto8gygiQEaByFgJ00BH2M8JWwQS1nafDXQCidWyOI8AcjDCSjCLk8ngObuAm3JAHAdubAmOaK06V8MNEsKPJOhobSprwQa6gD7DclRQdqcwL4zxqgBrQcabUiBLclRDKAlWp+etPkBaNMA0AKlrHwTdEByZAA4GM+SNluSY6wAzcMNewxmgig5Ks0nkrSpBvSaQHMdKTBAnLojOdYyGpQ254602ZILPdTD1hdlggdIm74jbTp8vDwF5ZYUeLWGJpWsh6XNyXgcYwVoJQTEhhTYkxzZjiU5npU2TaB979TQehlaAVq4kaGpiPwwwLkYUuBbQwocyQTv1tA0+1UFWoJF3iv1oq+qoSk8EQdJmwHkziIF7oOZk14EGitibAdjLYYK78H5vZOhtWpoI0ATGHs0Q8OMb4Ey+2bU2UYztCtA0wFAs7TplGLRVQCcqaFdGSPCeTI1QNIC52iWNzof6Uib7xjEp07mNNoUYmVosVItHrHzRlLgBn9LFyRHaQCtVUMbtTNhoXWiTOO9k/V8BdAc1Oq0ArSQs6/5SU0hckNy9NnXqQY0PGYo5dWJ7nINaN6o958FWin27aBaWRka1r5myvLOAm0j30eBJqCxHLReVclxhxOEN2JfDWjxBtAC7MIH1fVaGdoOp4qJYDgKtKPSFNID2gSnGldrCqkFZ+5UeQXQBIRrSwocbdZYQT/2LwRahBPBXoHrB8nxaGROST62DKUbQOMMzZIC9abkuELfQzQALWTnDNAm8KHWFOJgJ5+SHIvTPcmx1xQyZRhNL5Qci689aXMEaN/uNIWkEwDAvFpOZmgsBaaGnbs1NPa1Jm32gBZAIh1pCtG7TSH4aE0y1uVY4uqoFPisGlpP2rSA5qTecWn5agK6BzSpgAyD+wFaqhnYoSZ1Vwr8CmlTQbrcO3ZaX0NAEyMbYaAlyquFoLKK3SPby9CeVUPThrSJmkCAE0CrKUQadi4DrdSlWhmah0YL9z9vClH59YGbHx1J8VZTyAjQepJjmXwAKTDQI3omc3p1U4gDUf6RfcdYfrUp5ClAi2J3Ba6UOXGo+K+bQrjjssitG2SJzshaLwMtXgRagUNpYYoVkMSBLM+9GGiJZMvduG6DRZ4qc04DMPtQQxOjEtACmhO7K1AbNbQDEggZyJwscFpAGwENhoBeUwh3bWolhe8BTYVKxQEWrSUn/uhcM5KhvUu/+eQu0Lzhi+VrK0PrZZNDQKs9cpYUuFYgMVpD4/NxenJTiMCNqdUEUf1qZWjppLT5qSkkUZbCwkbZMSuVnu80hfSkzRbQeqCZSAh6huR4VtoM2gHAlLf72smuWgE+VV7XpE25Ab2WFDgyhnSuKbs4GuGzCjR+tIoUuMFg3kgcWKLTwRqanJQ2W00hAsenfaApRC42hbCvK1SlE0HtE9BGgneJO+ELamitD1YjjOYnNYVcraGhtKkW0EqVVeDx733I2NH581k1NNxNLG0i0IJ8/NjVaOZ0tYZ2Vtr0Xv7tPV3hkWp9EFkgS/J0vosngTaSoaG06WHi+xObQkaAdlbanP8B2+2l0f90LmUAAAAASUVORK5CYII=)}.jupyter-wrapper .cm-s-seti.CodeMirror{background-color:#151718 !important;color:#cfd2d1 !important;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-gutters{color:#404b53;background-color:#0e1112;border:none}.jupyter-wrapper .cm-s-seti .CodeMirror-cursor{border-left:solid thin #f8f8f0}.jupyter-wrapper .cm-s-seti .CodeMirror-linenumber{color:#6d8a88}.jupyter-wrapper .cm-s-seti.CodeMirror-focused div.CodeMirror-selected{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-seti .CodeMirror-line>span>span::-moz-selection{background:rgba(255,255,255,.1)}.jupyter-wrapper .cm-s-seti span.cm-comment{color:#41535b}.jupyter-wrapper .cm-s-seti span.cm-string,.jupyter-wrapper .cm-s-seti span.cm-string-2{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-number{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-variable{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-variable-2{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-def{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#ff79c6}.jupyter-wrapper .cm-s-seti span.cm-operator{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-keyword{color:#e6cd69}.jupyter-wrapper .cm-s-seti span.cm-atom{color:#cd3f45}.jupyter-wrapper .cm-s-seti span.cm-meta{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-tag{color:#55b5db}.jupyter-wrapper .cm-s-seti span.cm-attribute{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-qualifier{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-property{color:#a074c4}.jupyter-wrapper .cm-s-seti span.cm-variable-3,.jupyter-wrapper .cm-s-seti span.cm-type{color:#9fca56}.jupyter-wrapper .cm-s-seti span.cm-builtin{color:#9fca56}.jupyter-wrapper .cm-s-seti .CodeMirror-activeline-background{background:#101213}.jupyter-wrapper .cm-s-seti .CodeMirror-matchingbracket{text-decoration:underline;color:#fff !important}.jupyter-wrapper .solarized.base03{color:#002b36}.jupyter-wrapper .solarized.base02{color:#073642}.jupyter-wrapper .solarized.base01{color:#586e75}.jupyter-wrapper .solarized.base00{color:#657b83}.jupyter-wrapper .solarized.base0{color:#839496}.jupyter-wrapper .solarized.base1{color:#93a1a1}.jupyter-wrapper .solarized.base2{color:#eee8d5}.jupyter-wrapper .solarized.base3{color:#fdf6e3}.jupyter-wrapper .solarized.solar-yellow{color:#b58900}.jupyter-wrapper .solarized.solar-orange{color:#cb4b16}.jupyter-wrapper .solarized.solar-red{color:#dc322f}.jupyter-wrapper .solarized.solar-magenta{color:#d33682}.jupyter-wrapper .solarized.solar-violet{color:#6c71c4}.jupyter-wrapper .solarized.solar-blue{color:#268bd2}.jupyter-wrapper .solarized.solar-cyan{color:#2aa198}.jupyter-wrapper .solarized.solar-green{color:#859900}.jupyter-wrapper .cm-s-solarized{line-height:1.45em;color-profile:sRGB;rendering-intent:auto}.jupyter-wrapper .cm-s-solarized.cm-s-dark{color:#839496;background-color:#002b36;text-shadow:#002b36 0 1px}.jupyter-wrapper .cm-s-solarized.cm-s-light{background-color:#fdf6e3;color:#657b83;text-shadow:#eee8d5 0 1px}.jupyter-wrapper .cm-s-solarized .CodeMirror-widget{text-shadow:none}.jupyter-wrapper .cm-s-solarized .cm-header{color:#586e75}.jupyter-wrapper .cm-s-solarized .cm-quote{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-keyword{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .cm-atom{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-number{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-def{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-variable{color:#839496}.jupyter-wrapper .cm-s-solarized .cm-variable-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-variable-3,.jupyter-wrapper .cm-s-solarized .cm-type{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-property{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-operator{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-comment{color:#586e75;font-style:italic}.jupyter-wrapper .cm-s-solarized .cm-string{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-string-2{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-meta{color:#859900}.jupyter-wrapper .cm-s-solarized .cm-qualifier{color:#b58900}.jupyter-wrapper .cm-s-solarized .cm-builtin{color:#d33682}.jupyter-wrapper .cm-s-solarized .cm-bracket{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-matchingbracket{color:#859900}.jupyter-wrapper .cm-s-solarized .CodeMirror-nonmatchingbracket{color:#dc322f}.jupyter-wrapper .cm-s-solarized .cm-tag{color:#93a1a1}.jupyter-wrapper .cm-s-solarized .cm-attribute{color:#2aa198}.jupyter-wrapper .cm-s-solarized .cm-hr{color:transparent;border-top:1px solid #586e75;display:block}.jupyter-wrapper .cm-s-solarized .cm-link{color:#93a1a1;cursor:pointer}.jupyter-wrapper .cm-s-solarized .cm-special{color:#6c71c4}.jupyter-wrapper .cm-s-solarized .cm-em{color:#999;text-decoration:underline;text-decoration-style:dotted}.jupyter-wrapper .cm-s-solarized .cm-error,.jupyter-wrapper .cm-s-solarized .cm-invalidchar{color:#586e75;border-bottom:1px dotted #dc322f}.jupyter-wrapper .cm-s-solarized.cm-s-dark div.CodeMirror-selected{background:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark.CodeMirror ::selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-dark .CodeMirror-line>span>span::-moz-selection{background:rgba(7,54,66,.99)}.jupyter-wrapper .cm-s-solarized.cm-s-light div.CodeMirror-selected{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-light .CodeMirror-line>span>span::selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-ligh .CodeMirror-line>span>span::-moz-selection{background:#eee8d5}.jupyter-wrapper .cm-s-solarized.CodeMirror{-moz-box-shadow:inset 7px 0 12px -6px #000;-webkit-box-shadow:inset 7px 0 12px -6px #000;box-shadow:inset 7px 0 12px -6px #000}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutters{border-right:0}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-gutters{background-color:#073642}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-linenumber{color:#586e75;text-shadow:#021014 0 -1px}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-gutters{background-color:#eee8d5}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-linenumber{color:#839496}.jupyter-wrapper .cm-s-solarized .CodeMirror-linenumber{padding:0 5px}.jupyter-wrapper .cm-s-solarized .CodeMirror-guttermarker-subtle{color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-guttermarker{color:#ddd}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-guttermarker{color:#cb4b16}.jupyter-wrapper .cm-s-solarized .CodeMirror-gutter .CodeMirror-gutter-text{color:#586e75}.jupyter-wrapper .cm-s-solarized .CodeMirror-cursor{border-left:1px solid #819090}.jupyter-wrapper .cm-s-solarized.cm-s-light.cm-fat-cursor .CodeMirror-cursor{background:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-light .cm-animate-fat-cursor{background-color:#7e7}.jupyter-wrapper .cm-s-solarized.cm-s-dark.cm-fat-cursor .CodeMirror-cursor{background:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .cm-animate-fat-cursor{background-color:#586e75}.jupyter-wrapper .cm-s-solarized.cm-s-dark .CodeMirror-activeline-background{background:rgba(255,255,255,.06)}.jupyter-wrapper .cm-s-solarized.cm-s-light .CodeMirror-activeline-background{background:rgba(0,0,0,.06)}.jupyter-wrapper .cm-s-the-matrix.CodeMirror{background:#000;color:lime}.jupyter-wrapper .cm-s-the-matrix div.CodeMirror-selected{background:#2d2d2d}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span::-moz-selection,.jupyter-wrapper .cm-s-the-matrix .CodeMirror-line>span>span::-moz-selection{background:rgba(45,45,45,.99)}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-gutters{background:#060;border-right:2px solid lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker{color:lime}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-guttermarker-subtle{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-linenumber{color:#fff}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-cursor{border-left:1px solid lime}.jupyter-wrapper .cm-s-the-matrix span.cm-keyword{color:#008803;font-weight:bold}.jupyter-wrapper .cm-s-the-matrix span.cm-atom{color:#3ff}.jupyter-wrapper .cm-s-the-matrix span.cm-number{color:#ffb94f}.jupyter-wrapper .cm-s-the-matrix span.cm-def{color:#99c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable{color:#f6c}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-2{color:#c6f}.jupyter-wrapper .cm-s-the-matrix span.cm-variable-3,.jupyter-wrapper .cm-s-the-matrix span.cm-type{color:#96f}.jupyter-wrapper .cm-s-the-matrix span.cm-property{color:#62ffa0}.jupyter-wrapper .cm-s-the-matrix span.cm-operator{color:#999}.jupyter-wrapper .cm-s-the-matrix span.cm-comment{color:#ccc}.jupyter-wrapper .cm-s-the-matrix span.cm-string{color:#39c}.jupyter-wrapper .cm-s-the-matrix span.cm-meta{color:#c9f}.jupyter-wrapper .cm-s-the-matrix span.cm-qualifier{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-builtin{color:#30a}.jupyter-wrapper .cm-s-the-matrix span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-the-matrix span.cm-tag{color:#ffbd40}.jupyter-wrapper .cm-s-the-matrix span.cm-attribute{color:#fff700}.jupyter-wrapper .cm-s-the-matrix span.cm-error{color:red}.jupyter-wrapper .cm-s-the-matrix .CodeMirror-activeline-background{background:#040}.jupyter-wrapper .cm-s-xq-light span.cm-keyword{line-height:1em;font-weight:bold;color:#5a5cad}.jupyter-wrapper .cm-s-xq-light span.cm-atom{color:#6c8cd5}.jupyter-wrapper .cm-s-xq-light span.cm-number{color:#164}.jupyter-wrapper .cm-s-xq-light span.cm-def{text-decoration:underline}.jupyter-wrapper .cm-s-xq-light span.cm-variable{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-2{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-variable-3,.jupyter-wrapper .cm-s-xq-light span.cm-type{color:#000}.jupyter-wrapper .cm-s-xq-light span.cm-comment{color:#0080ff;font-style:italic}.jupyter-wrapper .cm-s-xq-light span.cm-string{color:red}.jupyter-wrapper .cm-s-xq-light span.cm-meta{color:#ff0}.jupyter-wrapper .cm-s-xq-light span.cm-qualifier{color:gray}.jupyter-wrapper .cm-s-xq-light span.cm-builtin{color:#7ea656}.jupyter-wrapper .cm-s-xq-light span.cm-bracket{color:#cc7}.jupyter-wrapper .cm-s-xq-light span.cm-tag{color:#3f7f7f}.jupyter-wrapper .cm-s-xq-light span.cm-attribute{color:#7f007f}.jupyter-wrapper .cm-s-xq-light span.cm-error{color:red}.jupyter-wrapper .cm-s-xq-light .CodeMirror-activeline-background{background:#e8f2ff}.jupyter-wrapper .cm-s-xq-light .CodeMirror-matchingbracket{outline:1px solid gray;color:#000 !important;background:#ff0}.jupyter-wrapper .CodeMirror{line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);border:0;border-radius:0;height:auto}.jupyter-wrapper .CodeMirror pre{padding:0 var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-dialog{background-color:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .CodeMirror-lines{padding:var(--jp-code-padding) 0}.jupyter-wrapper .CodeMirror-linenumber{padding:0 8px}.jupyter-wrapper .jp-CodeMirrorEditor-static{margin:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeMirrorEditor,.jupyter-wrapper .jp-CodeMirrorEditor-static{cursor:text}.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}@media screen and (min-width: 2138px)and (max-width: 4319px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width1) solid var(--jp-editor-cursor-color)}}@media screen and (min-width: 4320px){.jupyter-wrapper .jp-CodeMirrorEditor[data-type=inline] .CodeMirror-cursor{border-left:var(--jp-code-cursor-width2) solid var(--jp-editor-cursor-color)}}.jupyter-wrapper .CodeMirror.jp-mod-readOnly .CodeMirror-cursor{display:none}.jupyter-wrapper .CodeMirror-gutters{border-right:1px solid var(--jp-border-color2);background-color:var(--jp-layout-color0)}.jupyter-wrapper .jp-CollaboratorCursor{border-left:5px solid transparent;border-right:5px solid transparent;border-top:none;border-bottom:3px solid;background-clip:content-box;margin-left:-5px;margin-right:-5px}.jupyter-wrapper .CodeMirror-selectedtext.cm-searching{background-color:var(--jp-search-selected-match-background-color) !important;color:var(--jp-search-selected-match-color) !important}.jupyter-wrapper .cm-searching{background-color:var(--jp-search-unselected-match-background-color) !important;color:var(--jp-search-unselected-match-color) !important}.jupyter-wrapper .CodeMirror-focused .CodeMirror-selected{background-color:var(--jp-editor-selected-focused-background)}.jupyter-wrapper .CodeMirror-selected{background-color:var(--jp-editor-selected-background)}.jupyter-wrapper .jp-CollaboratorCursor-hover{position:absolute;z-index:1;transform:translateX(-50%);color:#fff;border-radius:3px;padding-left:4px;padding-right:4px;padding-top:1px;padding-bottom:1px;text-align:center;font-size:var(--jp-ui-font-size1);white-space:nowrap}.jupyter-wrapper .jp-CodeMirror-ruler{border-left:1px dashed var(--jp-border-color2)}.jupyter-wrapper .CodeMirror.cm-s-jupyter{background:var(--jp-layout-color0);color:var(--jp-content-font-color1)}.jupyter-wrapper .jp-CodeConsole .CodeMirror.cm-s-jupyter,.jupyter-wrapper .jp-Notebook .CodeMirror.cm-s-jupyter{background:transparent}.jupyter-wrapper .cm-s-jupyter .CodeMirror-cursor{border-left:var(--jp-code-cursor-width0) solid var(--jp-editor-cursor-color)}.jupyter-wrapper .cm-s-jupyter span.cm-keyword{color:var(--jp-mirror-editor-keyword-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-atom{color:var(--jp-mirror-editor-atom-color)}.jupyter-wrapper .cm-s-jupyter span.cm-number{color:var(--jp-mirror-editor-number-color)}.jupyter-wrapper .cm-s-jupyter span.cm-def{color:var(--jp-mirror-editor-def-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable{color:var(--jp-mirror-editor-variable-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-2{color:var(--jp-mirror-editor-variable-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-variable-3{color:var(--jp-mirror-editor-variable-3-color)}.jupyter-wrapper .cm-s-jupyter span.cm-punctuation{color:var(--jp-mirror-editor-punctuation-color)}.jupyter-wrapper .cm-s-jupyter span.cm-property{color:var(--jp-mirror-editor-property-color)}.jupyter-wrapper .cm-s-jupyter span.cm-operator{color:var(--jp-mirror-editor-operator-color);font-weight:bold}.jupyter-wrapper .cm-s-jupyter span.cm-comment{color:var(--jp-mirror-editor-comment-color);font-style:italic}.jupyter-wrapper .cm-s-jupyter span.cm-string{color:var(--jp-mirror-editor-string-color)}.jupyter-wrapper .cm-s-jupyter span.cm-string-2{color:var(--jp-mirror-editor-string-2-color)}.jupyter-wrapper .cm-s-jupyter span.cm-meta{color:var(--jp-mirror-editor-meta-color)}.jupyter-wrapper .cm-s-jupyter span.cm-qualifier{color:var(--jp-mirror-editor-qualifier-color)}.jupyter-wrapper .cm-s-jupyter span.cm-builtin{color:var(--jp-mirror-editor-builtin-color)}.jupyter-wrapper .cm-s-jupyter span.cm-bracket{color:var(--jp-mirror-editor-bracket-color)}.jupyter-wrapper .cm-s-jupyter span.cm-tag{color:var(--jp-mirror-editor-tag-color)}.jupyter-wrapper .cm-s-jupyter span.cm-attribute{color:var(--jp-mirror-editor-attribute-color)}.jupyter-wrapper .cm-s-jupyter span.cm-header{color:var(--jp-mirror-editor-header-color)}.jupyter-wrapper .cm-s-jupyter span.cm-quote{color:var(--jp-mirror-editor-quote-color)}.jupyter-wrapper .cm-s-jupyter span.cm-link{color:var(--jp-mirror-editor-link-color)}.jupyter-wrapper .cm-s-jupyter span.cm-error{color:var(--jp-mirror-editor-error-color)}.jupyter-wrapper .cm-s-jupyter span.cm-hr{color:#999}.jupyter-wrapper .cm-s-jupyter span.cm-tab{background:url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);background-position:right;background-repeat:no-repeat}.jupyter-wrapper .cm-s-jupyter .CodeMirror-activeline-background,.jupyter-wrapper .cm-s-jupyter .CodeMirror-gutter{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-RenderedLatex{color:var(--jp-content-font-color1);font-size:var(--jp-content-font-size1);line-height:var(--jp-content-line-height)}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedLatex{padding:var(--jp-code-padding);text-align:left}.jupyter-wrapper .jp-MimeDocument{outline:none}.jupyter-wrapper :root{--jp-private-filebrowser-button-height: 28px;--jp-private-filebrowser-button-width: 48px}.jupyter-wrapper .jp-FileBrowser{display:flex;flex-direction:column;color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1)}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{border-bottom:none;height:auto;margin:var(--jp-toolbar-header-margin);box-shadow:none}.jupyter-wrapper .jp-BreadCrumbs{flex:0 0 auto;margin:4px 12px}.jupyter-wrapper .jp-BreadCrumbs-item{margin:0px 2px;padding:0px 2px;border-radius:var(--jp-border-radius);cursor:pointer}.jupyter-wrapper .jp-BreadCrumbs-item:hover{background-color:var(--jp-layout-color2)}.jupyter-wrapper .jp-BreadCrumbs-item:first-child{margin-left:0px}.jupyter-wrapper .jp-BreadCrumbs-item.jp-mod-dropTarget{background-color:var(--jp-brand-color2);opacity:.7}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{padding:0px}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar{justify-content:space-evenly}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-Toolbar-item{flex:1}.jupyter-wrapper .jp-FileBrowser-toolbar.jp-Toolbar .jp-ToolbarButtonComponent{width:100%}.jupyter-wrapper .jp-DirListing{flex:1 1 auto;display:flex;flex-direction:column;outline:0}.jupyter-wrapper .jp-DirListing-header{flex:0 0 auto;display:flex;flex-direction:row;overflow:hidden;border-top:var(--jp-border-width) solid var(--jp-border-color2);border-bottom:var(--jp-border-width) solid var(--jp-border-color1);box-shadow:var(--jp-toolbar-box-shadow);z-index:2}.jupyter-wrapper .jp-DirListing-headerItem{padding:4px 12px 2px 12px;font-weight:500}.jupyter-wrapper .jp-DirListing-headerItem:hover{background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-name{flex:1 0 84px}.jupyter-wrapper .jp-DirListing-headerItem.jp-id-modified{flex:0 0 112px;border-left:var(--jp-border-width) solid var(--jp-border-color2);text-align:right}.jupyter-wrapper .jp-DirListing-narrow .jp-id-modified,.jupyter-wrapper .jp-DirListing-narrow .jp-DirListing-itemModified{display:none}.jupyter-wrapper .jp-DirListing-headerItem.jp-mod-selected{font-weight:600}.jupyter-wrapper .jp-DirListing-content{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-DirListing.jp-mod-native-drop .jp-DirListing-content{outline:5px dashed rgba(128,128,128,.5);outline-offset:-10px;cursor:copy}.jupyter-wrapper .jp-DirListing-item{display:flex;flex-direction:row;padding:4px 12px;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-selected{color:#fff;background:var(--jp-brand-color1)}.jupyter-wrapper .jp-DirListing-item.jp-mod-dropTarget{background:var(--jp-brand-color3)}.jupyter-wrapper .jp-DirListing-item:hover:not(.jp-mod-selected){background:var(--jp-layout-color2)}.jupyter-wrapper .jp-DirListing-itemIcon{flex:0 0 20px;margin-right:4px}.jupyter-wrapper .jp-DirListing-itemText{flex:1 0 64px;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;user-select:none}.jupyter-wrapper .jp-DirListing-itemModified{flex:0 0 125px;text-align:right}.jupyter-wrapper .jp-DirListing-editor{flex:1 0 64px;outline:none;border:none}.jupyter-wrapper .jp-DirListing-item.jp-mod-running .jp-DirListing-itemIcon:before{color:#32cd32;content:\"\u25cf\";font-size:8px;position:absolute;left:-8px}.jupyter-wrapper .jp-DirListing-item.lm-mod-drag-image,.jupyter-wrapper .jp-DirListing-item.jp-mod-selected.lm-mod-drag-image{font-size:var(--jp-ui-font-size1);padding-left:4px;margin-left:4px;width:160px;background-color:var(--jp-ui-inverse-font-color2);box-shadow:var(--jp-elevation-z2);border-radius:0px;color:var(--jp-ui-font-color1);transform:translateX(-40%) translateY(-58%)}.jupyter-wrapper .jp-DirListing-deadSpace{flex:1 1 auto;margin:0;padding:0;list-style-type:none;overflow:auto;background-color:var(--jp-layout-color1)}.jupyter-wrapper .jp-Document{min-width:120px;min-height:120px;outline:none}.jupyter-wrapper .jp-FileDialog.jp-mod-conflict input{color:red}.jupyter-wrapper .jp-FileDialog .jp-new-name-title{margin-top:12px}.jupyter-wrapper .jp-OutputArea{overflow-y:auto}.jupyter-wrapper .jp-OutputArea-child{display:flex;flex-direction:row}.jupyter-wrapper .jp-OutputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-outprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid transparent;opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-OutputArea-output{height:auto;overflow:auto;user-select:text;-moz-user-select:text;-webkit-user-select:text;-ms-user-select:text}.jupyter-wrapper .jp-OutputArea-child .jp-OutputArea-output{flex-grow:1;flex-shrink:1}.jupyter-wrapper .jp-OutputArea-output.jp-mod-isolated{width:100%;display:block}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated{position:relative}.jupyter-wrapper body.lm-mod-override-cursor .jp-OutputArea-output.jp-mod-isolated:before{content:\"\";position:absolute;top:0;left:0;right:0;bottom:0;background:transparent}.jupyter-wrapper .jp-OutputArea-output pre{border:none;margin:0px;padding:0px;overflow-x:auto;overflow-y:auto;word-break:break-all;word-wrap:break-word;white-space:pre-wrap}.jupyter-wrapper .jp-OutputArea-output.jp-RenderedHTMLCommon table{margin-left:0;margin-right:0}.jupyter-wrapper .jp-OutputArea-output dl,.jupyter-wrapper .jp-OutputArea-output dt,.jupyter-wrapper .jp-OutputArea-output dd{display:block}.jupyter-wrapper .jp-OutputArea-output dl{width:100%;overflow:hidden;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dt{font-weight:bold;float:left;width:20%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea-output dd{float:left;width:80%;padding:0;margin:0}.jupyter-wrapper .jp-OutputArea .jp-OutputArea .jp-OutputArea-prompt{display:none}.jupyter-wrapper .jp-OutputArea-output.jp-OutputArea-executeResult{margin-left:0px;flex:1 1 auto}.jupyter-wrapper .jp-OutputArea-executeResult.jp-RenderedText{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-OutputArea-stdin{line-height:var(--jp-code-line-height);padding-top:var(--jp-code-padding);display:flex}.jupyter-wrapper .jp-Stdin-prompt{color:var(--jp-content-font-color0);padding-right:var(--jp-code-padding);vertical-align:baseline;flex:0 0 auto}.jupyter-wrapper .jp-Stdin-input{font-family:var(--jp-code-font-family);font-size:inherit;color:inherit;background-color:inherit;width:42%;min-width:200px;vertical-align:baseline;padding:0em .25em;margin:0em .25em;flex:0 0 70%}.jupyter-wrapper .jp-Stdin-input:focus{box-shadow:none}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea{height:100%;display:block}.jupyter-wrapper .jp-LinkedOutputView .jp-OutputArea-output:only-child{height:100%}.jupyter-wrapper .jp-Collapser{flex:0 0 var(--jp-cell-collapser-width);padding:0px;margin:0px;border:none;outline:none;background:transparent;border-radius:var(--jp-border-radius);opacity:1}.jupyter-wrapper .jp-Collapser-child{display:block;width:100%;box-sizing:border-box;position:absolute;top:0px;bottom:0px}.jupyter-wrapper .jp-CellHeader,.jupyter-wrapper .jp-CellFooter{height:0px;width:100%;padding:0px;margin:0px;border:none;outline:none;background:transparent}.jupyter-wrapper .jp-InputArea{display:flex;flex-direction:row}.jupyter-wrapper .jp-InputArea-editor{flex:1 1 auto}.jupyter-wrapper .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);border-radius:0px;background:var(--jp-cell-editor-background)}.jupyter-wrapper .jp-InputPrompt{flex:0 0 var(--jp-cell-prompt-width);color:var(--jp-cell-inprompt-font-color);font-family:var(--jp-cell-prompt-font-family);padding:var(--jp-code-padding);letter-spacing:var(--jp-cell-prompt-letter-spacing);opacity:var(--jp-cell-prompt-opacity);line-height:var(--jp-code-line-height);font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid transparent;opacity:var(--jp-cell-prompt-opacity);text-align:right;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}.jupyter-wrapper .jp-Placeholder{display:flex;flex-direction:row;flex:1 1 auto}.jupyter-wrapper .jp-Placeholder-prompt{box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content{flex:1 1 auto;border:none;background:transparent;height:20px;box-sizing:border-box}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon{width:32px;height:16px;border:1px solid transparent;border-radius:var(--jp-border-radius)}.jupyter-wrapper .jp-Placeholder-content .jp-MoreHorizIcon:hover{border:1px solid var(--jp-border-color1);box-shadow:0px 0px 2px 0px rgba(0,0,0,.25);background-color:var(--jp-layout-color0)}.jupyter-wrapper :root{--jp-private-cell-scrolling-output-offset: 5px}.jupyter-wrapper .jp-Cell{padding:var(--jp-cell-padding);margin:0px;border:none;outline:none;background:transparent}.jupyter-wrapper .jp-Cell-inputWrapper,.jupyter-wrapper .jp-Cell-outputWrapper{display:flex;flex-direction:row;padding:0px;margin:0px;overflow:visible}.jupyter-wrapper .jp-Cell-inputArea,.jupyter-wrapper .jp-Cell-outputArea{flex:1 1 auto}.jupyter-wrapper .jp-Cell.jp-mod-noOutputs .jp-Cell-outputCollapser{border:none !important;background:transparent !important}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputCollapser{min-height:var(--jp-cell-collapser-min-height)}.jupyter-wrapper .jp-Cell:not(.jp-mod-noOutputs) .jp-Cell-outputWrapper{margin-top:5px}.jupyter-wrapper .jp-OutputArea-executeResult .jp-RenderedText.jp-OutputArea-output{padding-top:var(--jp-code-padding)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea{overflow-y:auto;max-height:200px;box-shadow:inset 0 0 6px 2px rgba(0,0,0,.3);margin-left:var(--jp-private-cell-scrolling-output-offset)}.jupyter-wrapper .jp-CodeCell.jp-mod-outputsScrolled .jp-OutputArea-prompt{flex:0 0 calc(var(--jp-cell-prompt-width) - var(--jp-private-cell-scrolling-output-offset))}.jupyter-wrapper .jp-MarkdownOutput{flex:1 1 auto;margin-top:0;margin-bottom:0;padding-left:var(--jp-code-padding)}.jupyter-wrapper .jp-MarkdownOutput.jp-RenderedHTMLCommon{overflow:auto}.jupyter-wrapper .jp-NotebookPanel-toolbar{padding:2px}.jupyter-wrapper .jp-Toolbar-item.jp-Notebook-toolbarCellType .jp-select-wrapper.jp-mod-focused{border:none;box-shadow:none}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown select{height:24px;font-size:var(--jp-ui-font-size1);line-height:14px;border-radius:0;display:block}.jupyter-wrapper .jp-Notebook-toolbarCellTypeDropdown span{top:5px !important}.jupyter-wrapper :root{--jp-private-notebook-dragImage-width: 304px;--jp-private-notebook-dragImage-height: 36px;--jp-private-notebook-selected-color: var(--md-blue-400);--jp-private-notebook-active-color: var(--md-green-400)}.jupyter-wrapper .jp-NotebookPanel{display:block;height:100%}.jupyter-wrapper .jp-NotebookPanel.jp-Document{min-width:240px;min-height:120px}.jupyter-wrapper .jp-Notebook{padding:var(--jp-notebook-padding);outline:none;overflow:auto;background:var(--jp-layout-color0)}.jupyter-wrapper .jp-Notebook.jp-mod-scrollPastEnd::after{display:block;content:\"\";min-height:var(--jp-notebook-scroll-padding)}.jupyter-wrapper .jp-Notebook .jp-Cell{overflow:visible}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-InputPrompt{cursor:move}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-InputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell:not(.jp-mod-active) .jp-OutputPrompt{opacity:var(--jp-cell-prompt-not-active-opacity);color:var(--jp-cell-prompt-not-active-font-color)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser{background:var(--jp-brand-color1)}.jupyter-wrapper .jp-Notebook .jp-Cell .jp-Collapser:hover{box-shadow:var(--jp-elevation-z2);background:var(--jp-brand-color1);opacity:var(--jp-cell-collapser-not-active-hover-opacity)}.jupyter-wrapper .jp-Notebook .jp-Cell.jp-mod-active .jp-Collapser:hover{background:var(--jp-brand-color0);opacity:1}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-selected{background:var(--jp-notebook-multiselected-color)}.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Cell.jp-mod-active.jp-mod-selected:not(.jp-mod-multiSelected){background:transparent}.jupyter-wrapper .jp-Notebook.jp-mod-editMode .jp-Cell.jp-mod-active .jp-InputArea-editor{border:var(--jp-border-width) solid var(--jp-cell-editor-active-border-color);box-shadow:var(--jp-input-box-shadow);background-color:var(--jp-cell-editor-active-background)}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropSource{opacity:.5}.jupyter-wrapper .jp-Notebook-cell.jp-mod-dropTarget,.jupyter-wrapper .jp-Notebook.jp-mod-commandMode .jp-Notebook-cell.jp-mod-active.jp-mod-selected.jp-mod-dropTarget{border-top-color:var(--jp-private-notebook-selected-color);border-top-style:solid;border-top-width:2px}.jupyter-wrapper .jp-dragImage{display:flex;flex-direction:row;width:var(--jp-private-notebook-dragImage-width);height:var(--jp-private-notebook-dragImage-height);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background);overflow:visible}.jupyter-wrapper .jp-dragImage-singlePrompt{box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-dragImage .jp-dragImage-content{flex:1 1 auto;z-index:2;font-size:var(--jp-code-font-size);font-family:var(--jp-code-font-family);line-height:var(--jp-code-line-height);padding:var(--jp-code-padding);border:var(--jp-border-width) solid var(--jp-cell-editor-border-color);background:var(--jp-cell-editor-background-color);color:var(--jp-content-font-color3);text-align:left;margin:4px 4px 4px 0px}.jupyter-wrapper .jp-dragImage .jp-dragImage-prompt{flex:0 0 auto;min-width:36px;color:var(--jp-cell-inprompt-font-color);padding:var(--jp-code-padding);padding-left:12px;font-family:var(--jp-cell-prompt-font-family);letter-spacing:var(--jp-cell-prompt-letter-spacing);line-height:1.9;font-size:var(--jp-code-font-size);border:var(--jp-border-width) solid transparent}.jupyter-wrapper .jp-dragImage-multipleBack{z-index:-1;position:absolute;height:32px;width:300px;top:8px;left:8px;background:var(--jp-layout-color2);border:var(--jp-border-width) solid var(--jp-input-border-color);box-shadow:2px 2px 4px 0px rgba(0,0,0,.12)}.jupyter-wrapper .jp-NotebookTools{display:block;min-width:var(--jp-sidebar-min-width);color:var(--jp-ui-font-color1);background:var(--jp-layout-color1);font-size:var(--jp-ui-font-size1);overflow:auto}.jupyter-wrapper .jp-NotebookTools-tool{padding:0px 12px 0 12px}.jupyter-wrapper .jp-ActiveCellTool{padding:12px;background-color:var(--jp-layout-color1);border-top:none !important}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-prompt{flex:0 0 auto;padding-left:0px}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor{flex:1 1 auto;background:var(--jp-cell-editor-background);border-color:var(--jp-cell-editor-border-color)}.jupyter-wrapper .jp-ActiveCellTool .jp-InputArea-editor .CodeMirror{background:transparent}.jupyter-wrapper .jp-MetadataEditorTool{flex-direction:column;padding:12px 0px 12px 0px}.jupyter-wrapper .jp-RankedPanel>:not(:first-child){margin-top:12px}.jupyter-wrapper .jp-KeySelector select.jp-mod-styled{font-size:var(--jp-ui-font-size1);color:var(--jp-ui-font-color0);border:var(--jp-border-width) solid var(--jp-border-color1)}.jupyter-wrapper .jp-KeySelector label,.jupyter-wrapper .jp-MetadataEditorTool label{line-height:1.4}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook{--jp-content-font-size1: var(--jp-content-presentation-font-size1);--jp-code-font-size: var(--jp-code-presentation-font-size)}.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-InputPrompt,.jupyter-wrapper .jp-mod-presentationMode .jp-Notebook .jp-Cell .jp-OutputPrompt{flex:0 0 110px}.jupyter-wrapper .md-typeset__scrollwrap{margin:0}.jupyter-wrapper .jp-MarkdownOutput{padding:0}.jupyter-wrapper h1 .anchor-link,.jupyter-wrapper h2 .anchor-link,.jupyter-wrapper h3 .anchor-link,.jupyter-wrapper h4 .anchor-link,.jupyter-wrapper h5 .anchor-link,.jupyter-wrapper h6 .anchor-link{display:none;margin-left:.5rem;color:var(--md-default-fg-color--lighter)}.jupyter-wrapper h1 .anchor-link:hover,.jupyter-wrapper h2 .anchor-link:hover,.jupyter-wrapper h3 .anchor-link:hover,.jupyter-wrapper h4 .anchor-link:hover,.jupyter-wrapper h5 .anchor-link:hover,.jupyter-wrapper h6 .anchor-link:hover{text-decoration:none;color:var(--md-accent-fg-color)}.jupyter-wrapper h1:hover .anchor-link,.jupyter-wrapper h2:hover .anchor-link,.jupyter-wrapper h3:hover .anchor-link,.jupyter-wrapper h4:hover .anchor-link,.jupyter-wrapper h5:hover .anchor-link,.jupyter-wrapper h6:hover .anchor-link{display:inline-block}.jupyter-wrapper .jp-Cell-inputWrapper .jp-InputPrompt{display:none}.jupyter-wrapper .jp-CodeCell .jp-Cell-inputWrapper .jp-InputPrompt{display:block}.jupyter-wrapper .highlight pre{overflow:auto}.jupyter-wrapper .celltoolbar{border:none;background:#eee;border-radius:2px 2px 0px 0px;width:100%;height:29px;padding-right:4px;box-orient:horizontal;box-align:stretch;display:flex;flex-direction:row;align-items:stretch;box-pack:end;justify-content:flex-start;display:-webkit-flex}.jupyter-wrapper .celltoolbar .tags_button_container{display:flex}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container{display:flex;flex-direction:row;flex-grow:1;overflow:hidden;position:relative}.jupyter-wrapper .celltoolbar .tags_button_container .tag-container .cell-tag{background-color:#fff;white-space:nowrap;margin:3px 4px;padding:0 4px;border-radius:1px;border:1px solid #ccc;box-shadow:none;width:inherit;font-size:11px;font-family:\"Roboto Mono\",SFMono-Regular,Consolas,Menlo,monospace;height:22px;display:inline-block}.jupyter-wrapper .jp-InputArea-editor{width:1px}.jupyter-wrapper .jp-InputPrompt{overflow:unset}.jupyter-wrapper .jp-OutputPrompt{overflow:unset}.jupyter-wrapper .jp-RenderedText{font-size:var(--jp-code-font-size)}.jupyter-wrapper .highlight-ipynb{overflow:auto}.jupyter-wrapper .highlight-ipynb pre{margin:0;padding:5px 10px}.jupyter-wrapper .jp-InputArea-editor{position:relative}.jupyter-wrapper .zeroclipboard-container{position:absolute;top:-3px;right:0;z-index:1000}.jupyter-wrapper .zeroclipboard-container clipboard-copy{-webkit-appearance:button;-moz-appearance:button;padding:7px 5px;font:11px system-ui,sans-serif;display:inline-block;cursor:default}.jupyter-wrapper .zeroclipboard-container .clipboard-copy-icon{padding:4px 4px 2px;color:#57606a;vertical-align:text-bottom}.jupyter-wrapper .clipboard-copy-txt{display:none} /*# sourceMappingURL=mkdocs-jupyter.css.map*/ /*----------------------------------------------------------------------------- | Copyright (c) Jupyter Development Team. | Distributed under the terms of the Modified BSD License. |----------------------------------------------------------------------------*/ /* The following CSS variables define the main, public API for styling JupyterLab. These variables should be used by all plugins wherever possible. In other words, plugins should not define custom colors, sizes, etc unless absolutely necessary. This enables users to change the visual theme of JupyterLab by changing these variables. Many variables appear in an ordered sequence (0,1,2,3). These sequences are designed to work well together, so for example, `--jp-border-color1` should be used with `--jp-layout-color1`. The numbers have the following meanings: * 0: super-primary, reserved for special emphasis * 1: primary, most important under normal situations * 2: secondary, next most important under normal situations * 3: tertiary, next most important under normal situations Throughout JupyterLab, we are mostly following principles from Google's Material Design when selecting colors. We are not, however, following all of MD as it is not optimized for dense, information rich UIs. */ :root { /* Elevation * * We style box-shadows using Material Design's idea of elevation. These particular numbers are taken from here: * * https://github.com/material-components/material-components-web * https://material-components-web.appspot.com/elevation.html */ /* The dark theme shadows need a bit of work, but this will probably also require work on the core layout * colors used in the theme as well. */ --jp-shadow-base-lightness: 32; --jp-shadow-umbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.2 ); --jp-shadow-penumbra-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.14 ); --jp-shadow-ambient-color: rgba( var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), var(--jp-shadow-base-lightness), 0.12 ); --jp-elevation-z0: none; --jp-elevation-z1: 0px 2px 1px -1px var(--jp-shadow-umbra-color), 0px 1px 1px 0px var(--jp-shadow-penumbra-color), 0px 1px 3px 0px var(--jp-shadow-ambient-color); --jp-elevation-z2: 0px 3px 1px -2px var(--jp-shadow-umbra-color), 0px 2px 2px 0px var(--jp-shadow-penumbra-color), 0px 1px 5px 0px var(--jp-shadow-ambient-color); --jp-elevation-z4: 0px 2px 4px -1px var(--jp-shadow-umbra-color), 0px 4px 5px 0px var(--jp-shadow-penumbra-color), 0px 1px 10px 0px var(--jp-shadow-ambient-color); --jp-elevation-z6: 0px 3px 5px -1px var(--jp-shadow-umbra-color), 0px 6px 10px 0px var(--jp-shadow-penumbra-color), 0px 1px 18px 0px var(--jp-shadow-ambient-color); --jp-elevation-z8: 0px 5px 5px -3px var(--jp-shadow-umbra-color), 0px 8px 10px 1px var(--jp-shadow-penumbra-color), 0px 3px 14px 2px var(--jp-shadow-ambient-color); --jp-elevation-z12: 0px 7px 8px -4px var(--jp-shadow-umbra-color), 0px 12px 17px 2px var(--jp-shadow-penumbra-color), 0px 5px 22px 4px var(--jp-shadow-ambient-color); --jp-elevation-z16: 0px 8px 10px -5px var(--jp-shadow-umbra-color), 0px 16px 24px 2px var(--jp-shadow-penumbra-color), 0px 6px 30px 5px var(--jp-shadow-ambient-color); --jp-elevation-z20: 0px 10px 13px -6px var(--jp-shadow-umbra-color), 0px 20px 31px 3px var(--jp-shadow-penumbra-color), 0px 8px 38px 7px var(--jp-shadow-ambient-color); --jp-elevation-z24: 0px 11px 15px -7px var(--jp-shadow-umbra-color), 0px 24px 38px 3px var(--jp-shadow-penumbra-color), 0px 9px 46px 8px var(--jp-shadow-ambient-color); /* Borders * * The following variables, specify the visual styling of borders in JupyterLab. */ --jp-border-width: 1px; --jp-border-color0: var(--md-grey-700); --jp-border-color1: var(--md-grey-700); --jp-border-color2: var(--md-grey-800); --jp-border-color3: var(--md-grey-900); --jp-border-radius: 2px; /* UI Fonts * * The UI font CSS variables are used for the typography all of the JupyterLab * user interface elements that are not directly user generated content. * * The font sizing here is done assuming that the body font size of --jp-ui-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-ui-font-scale-factor: 1.2; --jp-ui-font-size0: 0.83333em; --jp-ui-font-size1: 13px; /* Base font size */ --jp-ui-font-size2: 1.2em; --jp-ui-font-size3: 1.44em; --jp-ui-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Use these font colors against the corresponding main layout colors. * In a light theme, these go from dark to light. */ /* Defaults use Material Design specification */ --jp-ui-font-color0: rgba(255, 255, 255, 1); --jp-ui-font-color1: rgba(255, 255, 255, 0.87); --jp-ui-font-color2: rgba(255, 255, 255, 0.54); --jp-ui-font-color3: rgba(255, 255, 255, 0.38); /* * Use these against the brand/accent/warn/error colors. * These will typically go from light to darker, in both a dark and light theme. */ --jp-ui-inverse-font-color0: rgba(0, 0, 0, 1); --jp-ui-inverse-font-color1: rgba(0, 0, 0, 0.8); --jp-ui-inverse-font-color2: rgba(0, 0, 0, 0.5); --jp-ui-inverse-font-color3: rgba(0, 0, 0, 0.3); /* Content Fonts * * Content font variables are used for typography of user generated content. * * The font sizing here is done assuming that the body font size of --jp-content-font-size1 * is applied to a parent element. When children elements, such as headings, are sized * in em all things will be computed relative to that body size. */ --jp-content-line-height: 1.6; --jp-content-font-scale-factor: 1.2; --jp-content-font-size0: 0.83333em; --jp-content-font-size1: 14px; /* Base font size */ --jp-content-font-size2: 1.2em; --jp-content-font-size3: 1.44em; --jp-content-font-size4: 1.728em; --jp-content-font-size5: 2.0736em; /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-content-presentation-font-size1: 17px; --jp-content-heading-line-height: 1; --jp-content-heading-margin-top: 1.2em; --jp-content-heading-margin-bottom: 0.8em; --jp-content-heading-font-weight: 500; /* Defaults use Material Design specification */ --jp-content-font-color0: rgba(255, 255, 255, 1); --jp-content-font-color1: rgba(255, 255, 255, 1); --jp-content-font-color2: rgba(255, 255, 255, 0.7); --jp-content-font-color3: rgba(255, 255, 255, 0.5); --jp-content-link-color: var(--md-blue-300); --jp-content-font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; /* * Code Fonts * * Code font variables are used for typography of code and other monospaces content. */ --jp-code-font-size: 13px; --jp-code-line-height: 1.3077; /* 17px for 13px base */ --jp-code-padding: 5px; /* 5px for 13px base, codemirror highlighting needs integer px value */ --jp-code-font-family-default: Menlo, Consolas, 'DejaVu Sans Mono', monospace; --jp-code-font-family: var(--jp-code-font-family-default); /* This gives a magnification of about 125% in presentation mode over normal. */ --jp-code-presentation-font-size: 16px; /* may need to tweak cursor width if you change font size */ --jp-code-cursor-width0: 1.4px; --jp-code-cursor-width1: 2px; --jp-code-cursor-width2: 4px; /* Layout * * The following are the main layout colors use in JupyterLab. In a light * theme these would go from light to dark. */ --jp-layout-color0: #111111; --jp-layout-color1: var(--md-grey-900); --jp-layout-color2: var(--md-grey-800); --jp-layout-color3: var(--md-grey-700); --jp-layout-color4: var(--md-grey-600); /* Inverse Layout * * The following are the inverse layout colors use in JupyterLab. In a light * theme these would go from dark to light. */ --jp-inverse-layout-color0: white; --jp-inverse-layout-color1: white; --jp-inverse-layout-color2: var(--md-grey-200); --jp-inverse-layout-color3: var(--md-grey-400); --jp-inverse-layout-color4: var(--md-grey-600); /* Brand/accent */ --jp-brand-color0: var(--md-blue-700); --jp-brand-color1: var(--md-blue-500); --jp-brand-color2: var(--md-blue-300); --jp-brand-color3: var(--md-blue-100); --jp-brand-color4: var(--md-blue-50); --jp-accent-color0: var(--md-green-700); --jp-accent-color1: var(--md-green-500); --jp-accent-color2: var(--md-green-300); --jp-accent-color3: var(--md-green-100); /* State colors (warn, error, success, info) */ --jp-warn-color0: var(--md-orange-700); --jp-warn-color1: var(--md-orange-500); --jp-warn-color2: var(--md-orange-300); --jp-warn-color3: var(--md-orange-100); --jp-error-color0: var(--md-red-700); --jp-error-color1: var(--md-red-500); --jp-error-color2: var(--md-red-300); --jp-error-color3: var(--md-red-100); --jp-success-color0: var(--md-green-700); --jp-success-color1: var(--md-green-500); --jp-success-color2: var(--md-green-300); --jp-success-color3: var(--md-green-100); --jp-info-color0: var(--md-cyan-700); --jp-info-color1: var(--md-cyan-500); --jp-info-color2: var(--md-cyan-300); --jp-info-color3: var(--md-cyan-100); /* Cell specific styles */ --jp-cell-padding: 5px; --jp-cell-collapser-width: 8px; --jp-cell-collapser-min-height: 20px; --jp-cell-collapser-not-active-hover-opacity: 0.6; --jp-cell-editor-background: var(--jp-layout-color1); --jp-cell-editor-border-color: var(--md-grey-700); --jp-cell-editor-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-cell-editor-active-background: var(--jp-layout-color0); --jp-cell-editor-active-border-color: var(--jp-brand-color1); --jp-cell-prompt-width: 64px; --jp-cell-prompt-font-family: var(--jp-code-font-family-default); --jp-cell-prompt-letter-spacing: 0px; --jp-cell-prompt-opacity: 1; --jp-cell-prompt-not-active-opacity: 1; --jp-cell-prompt-not-active-font-color: var(--md-grey-300); /* A custom blend of MD grey and blue 600 * See https://meyerweb.com/eric/tools/color-blend/#546E7A:1E88E5:5:hex */ --jp-cell-inprompt-font-color: #307fc1; /* A custom blend of MD grey and orange 600 * https://meyerweb.com/eric/tools/color-blend/#546E7A:F4511E:5:hex */ --jp-cell-outprompt-font-color: #bf5b3d; /* Notebook specific styles */ --jp-notebook-padding: 10px; --jp-notebook-select-background: var(--jp-layout-color1); --jp-notebook-multiselected-color: rgba(33, 150, 243, 0.24); /* The scroll padding is calculated to fill enough space at the bottom of the notebook to show one single-line cell (with appropriate padding) at the top when the notebook is scrolled all the way to the bottom. We also subtract one pixel so that no scrollbar appears if we have just one single-line cell in the notebook. This padding is to enable a 'scroll past end' feature in a notebook. */ --jp-notebook-scroll-padding: calc( 100% - var(--jp-code-font-size) * var(--jp-code-line-height) - var(--jp-code-padding) - var(--jp-cell-padding) - 1px ); /* Rendermime styles */ --jp-rendermime-error-background: rgba(244, 67, 54, 0.28); --jp-rendermime-table-row-background: var(--md-grey-900); --jp-rendermime-table-row-hover-background: rgba(3, 169, 244, 0.2); /* Dialog specific styles */ --jp-dialog-background: rgba(0, 0, 0, 0.6); /* Console specific styles */ --jp-console-padding: 10px; /* Toolbar specific styles */ --jp-toolbar-border-color: var(--jp-border-color2); --jp-toolbar-micro-height: 8px; --jp-toolbar-background: var(--jp-layout-color1); --jp-toolbar-box-shadow: 0px 0px 2px 0px rgba(0, 0, 0, 0.8); --jp-toolbar-header-margin: 4px 4px 0px 4px; --jp-toolbar-active-background: var(--jp-layout-color0); /* Statusbar specific styles */ --jp-statusbar-height: 24px; /* Input field styles */ --jp-input-box-shadow: inset 0 0 2px var(--md-blue-300); --jp-input-active-background: var(--jp-layout-color0); --jp-input-hover-background: var(--jp-layout-color2); --jp-input-background: var(--md-grey-800); --jp-input-border-color: var(--jp-border-color1); --jp-input-active-border-color: var(--jp-brand-color1); --jp-input-active-box-shadow-color: rgba(19, 124, 189, 0.3); /* General editor styles */ --jp-editor-selected-background: var(--jp-layout-color2); --jp-editor-selected-focused-background: rgba(33, 150, 243, 0.24); --jp-editor-cursor-color: var(--jp-ui-font-color0); /* Code mirror specific styles */ --jp-mirror-editor-keyword-color: var(--md-green-500); --jp-mirror-editor-atom-color: var(--md-blue-300); --jp-mirror-editor-number-color: var(--md-green-400); --jp-mirror-editor-def-color: var(--md-blue-600); --jp-mirror-editor-variable-color: var(--md-grey-300); --jp-mirror-editor-variable-2-color: var(--md-blue-400); --jp-mirror-editor-variable-3-color: var(--md-green-600); --jp-mirror-editor-punctuation-color: var(--md-blue-400); --jp-mirror-editor-property-color: var(--md-blue-400); --jp-mirror-editor-operator-color: #aa22ff; --jp-mirror-editor-comment-color: #408080; --jp-mirror-editor-string-color: #ff7070; --jp-mirror-editor-string-2-color: var(--md-purple-300); --jp-mirror-editor-meta-color: #aa22ff; --jp-mirror-editor-qualifier-color: #555; --jp-mirror-editor-builtin-color: var(--md-green-600); --jp-mirror-editor-bracket-color: #997; --jp-mirror-editor-tag-color: var(--md-green-700); --jp-mirror-editor-attribute-color: var(--md-blue-700); --jp-mirror-editor-header-color: var(--md-blue-500); --jp-mirror-editor-quote-color: var(--md-green-300); --jp-mirror-editor-link-color: var(--md-blue-700); --jp-mirror-editor-error-color: #f00; --jp-mirror-editor-hr-color: #999; /* Vega extension styles */ --jp-vega-background: var(--md-grey-400); /* Sidebar-related styles */ --jp-sidebar-min-width: 250px; /* Search-related styles */ --jp-search-toggle-off-opacity: 0.6; --jp-search-toggle-hover-opacity: 0.8; --jp-search-toggle-on-opacity: 1; --jp-search-selected-match-background-color: rgb(255, 225, 0); --jp-search-selected-match-color: black; --jp-search-unselected-match-background-color: var( --jp-inverse-layout-color0 ); --jp-search-unselected-match-color: var(--jp-ui-inverse-font-color0); /* scrollbar related styles. Supports every browser except Edge. */ /* colors based on JetBrain's Darcula theme */ --jp-scrollbar-background-color: #3f4244; --jp-scrollbar-thumb-color: 88, 96, 97; /* need to specify thumb color as an RGB triplet */ --jp-scrollbar-endpad: 3px; /* the minimum gap between the thumb and the ends of a scrollbar */ /* hacks for setting the thumb shape. These do nothing in Firefox */ --jp-scrollbar-thumb-margin: 3.5px; /* the space in between the sides of the thumb and the track */ --jp-scrollbar-thumb-radius: 9px; /* set to a large-ish value for rounded endcaps on the thumb */ /* Icon colors that work well with light or dark backgrounds */ --jp-icon-contrast-color0: var(--md-purple-600); --jp-icon-contrast-color1: var(--md-green-600); --jp-icon-contrast-color2: var(--md-pink-600); --jp-icon-contrast-color3: var(--md-blue-600); } init_mathjax = function() { if (window.MathJax) { // MathJax loaded MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: \"AMS\", useLabelIds: true } }, tex2jax: { inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ], displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ], processEscapes: true, processEnvironments: true }, displayAlign: 'center', CommonHTML: { linebreaks: { automatic: true } } }); MathJax.Hub.Queue([\"Typeset\", MathJax.Hub]); } } init_mathjax(); Nvidia GPU INT-8 quantization on any transformers model (encoder based) \u00b6 For some context and explanations, please check our documentation here: https://els-rd.github.io/transformer-deploy/quantization_intro/ . Project setup \u00b6 Dependencies installation \u00b6 Your machine should have Nvidia CUDA 11.X, TensorRT 8.2.1 and cuBLAS installed. It's said to be tricky to install, in my experience, just follow Nvidia download page instructions and nothing else , it should work out of the box. Nvidia Docker image could be a good choice too. In [1]: Copied! #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ Check the GPU is enabled and usable. In [2]: Copied! ! nvidia - smi ! nvidia-smi Tue Dec 28 21:46:26 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.44 Driver Version: 495.44 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:03:00.0 On | N/A | | 35% 42C P8 40W / 350W | 263MiB / 24267MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1604 G /usr/lib/xorg/Xorg 159MiB | | 0 N/A N/A 8473 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 106329 G ..._18576.log --shared-files 17MiB | | 0 N/A N/A 110356 G ...AAAAAAAAA= --shared-files 39MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import os from collections import OrderedDict from typing import Dict , List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import pycuda.autoinit import tensorrt as trt import torch import transformers from datasets import load_dataset , load_metric from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext , Logger , Runtime from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , IntervalStrategy , PreTrainedModel , PreTrainedTokenizer , Trainer , TrainingArguments , ) from transformer_deploy.backends.ort_utils import ( cpu_quantization , create_model_for_provider , optimize_onnx , ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine , get_binding_idxs , infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings , track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate import logging import os from collections import OrderedDict from typing import Dict, List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import pycuda.autoinit import tensorrt as trt import torch import transformers from datasets import load_dataset, load_metric from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext, Logger, Runtime from transformers import ( AutoModelForSequenceClassification, AutoTokenizer, IntervalStrategy, PreTrainedModel, PreTrainedTokenizer, Trainer, TrainingArguments, ) from transformer_deploy.backends.ort_utils import ( cpu_quantization, create_model_for_provider, optimize_onnx, ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings, track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate Set logging to error level to ease readability of this notebook on Github. In [4]: Copied! log_level = logging . ERROR logging . getLogger () . setLevel ( log_level ) datasets . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . enable_default_handler () transformers . utils . logging . enable_explicit_format () trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) transformers . logging . set_verbosity_error () log_level = logging.ERROR logging.getLogger().setLevel(log_level) datasets.utils.logging.set_verbosity(log_level) transformers.utils.logging.set_verbosity(log_level) transformers.utils.logging.enable_default_handler() transformers.utils.logging.enable_explicit_format() trt_logger: Logger = trt.Logger(trt.Logger.ERROR) transformers.logging.set_verbosity_error() Preprocess data \u00b6 This part is inspired from an official Notebooks from Hugging Face . There is nothing special to do. Define the task: In [5]: Copied! model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings : Dict [ str , List [ float ]] = dict () runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings: Dict[str, List[float]] = dict() runtime: Runtime = trt.Runtime(trt_logger) profile_index = 0 Preprocess data (task specific): In [6]: Copied! def preprocess_function ( examples ): return tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True , padding = \"max_length\" , max_length = max_seq_len ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) def convert_tensor ( data : OD [ str , List [ List [ int ]]], output : str ) -> OD [ str , Union [ np . ndarray , torch . Tensor ]]: input : OD [ str , Union [ np . ndarray , torch . Tensor ]] = OrderedDict () for k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ]: if k in data : v = data [ k ] if output == \"torch\" : value = torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) elif output == \"np\" : value = np . asarray ( v , dtype = np . int32 ) else : raise Exception ( f \"unknown output type: { output } \" ) input [ k ] = value return input def measure_accuracy ( infer , int64 : bool ) -> float : outputs = list () for start_index in range ( 0 , len ( encoded_dataset [ validation_key ]), batch_size ): end_index = start_index + batch_size data = encoded_dataset [ validation_key ][ start_index : end_index ] inputs : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) if int64 : for k , v in inputs . items (): inputs [ k ] = v . astype ( np . int64 ) output = infer ( inputs ) output = np . argmax ( output [ 0 ], axis = 1 ) . astype ( int ) . tolist () outputs . extend ( output ) return np . mean ( np . array ( outputs ) == np . array ( validation_labels )) def get_trainer ( model : PreTrainedModel ) -> Trainer : trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics , ) transformers . logging . set_verbosity_error () return trainer def preprocess_function(examples): return tokenizer( examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_seq_len ) def compute_metrics(eval_pred): predictions, labels = eval_pred if task != \"stsb\": predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) def convert_tensor(data: OD[str, List[List[int]]], output: str) -> OD[str, Union[np.ndarray, torch.Tensor]]: input: OD[str, Union[np.ndarray, torch.Tensor]] = OrderedDict() for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]: if k in data: v = data[k] if output == \"torch\": value = torch.tensor(v, dtype=torch.long, device=\"cuda\") elif output == \"np\": value = np.asarray(v, dtype=np.int32) else: raise Exception(f\"unknown output type: {output}\") input[k] = value return input def measure_accuracy(infer, int64: bool) -> float: outputs = list() for start_index in range(0, len(encoded_dataset[validation_key]), batch_size): end_index = start_index + batch_size data = encoded_dataset[validation_key][start_index:end_index] inputs: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") if int64: for k, v in inputs.items(): inputs[k] = v.astype(np.int64) output = infer(inputs) output = np.argmax(output[0], axis=1).astype(int).tolist() outputs.extend(output) return np.mean(np.array(outputs) == np.array(validation_labels)) def get_trainer(model: PreTrainedModel) -> Trainer: trainer = Trainer( model, args, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics, ) transformers.logging.set_verbosity_error() return trainer In [7]: Copied! tokenizer : PreTrainedTokenizer = AutoTokenizer . from_pretrained ( model_name , use_fast = True ) dataset = load_dataset ( \"glue\" , task ) metric = load_metric ( \"glue\" , task ) encoded_dataset = dataset . map ( preprocess_function , batched = True ) validation_labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] nb_step = 1000 strategy = IntervalStrategy . STEPS args = TrainingArguments ( f \" { model_name } - { task } \" , evaluation_strategy = strategy , eval_steps = nb_step , logging_steps = nb_step , save_steps = nb_step , save_strategy = strategy , learning_rate = 1e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size * 2 , num_train_epochs = 1 , fp16 = True , group_by_length = True , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = \"accuracy\" , report_to = [], ) tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) dataset = load_dataset(\"glue\", task) metric = load_metric(\"glue\", task) encoded_dataset = dataset.map(preprocess_function, batched=True) validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]] nb_step = 1000 strategy = IntervalStrategy.STEPS args = TrainingArguments( f\"{model_name}-{task}\", evaluation_strategy=strategy, eval_steps=nb_step, logging_steps=nb_step, save_steps=nb_step, save_strategy=strategy, learning_rate=1e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, num_train_epochs=1, fp16=True, group_by_length=True, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=\"accuracy\", report_to=[], ) 0%| | 0/5 [00:00<?, ?it/s] (Standard) fine-tuning model \u00b6 Now that our data are ready, we can download/fine tune the pretrained model. In [8]: Copied! model_fp16 : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = num_labels ) trainer = get_trainer ( model_fp16 ) transformers . logging . set_verbosity_error () trainer . train () print ( trainer . evaluate ()) model_fp16 . save_pretrained ( \"model_trained_fp16\" ) model_fp16: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) trainer = get_trainer(model_fp16) transformers.logging.set_verbosity_error() trainer.train() print(trainer.evaluate()) model_fp16.save_pretrained(\"model_trained_fp16\") [INFO|trainer.py:439] 2021-12-27 09:19:51,063 >> Using amp half precision backend {'loss': 0.6605, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08} {'eval_loss': 0.4653007388114929, 'eval_accuracy': 0.8183392766174223, 'eval_runtime': 18.2981, 'eval_samples_per_second': 536.393, 'eval_steps_per_second': 8.416, 'epoch': 0.08} {'loss': 0.4956, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16} {'eval_loss': 0.4208127558231354, 'eval_accuracy': 0.8346408558329088, 'eval_runtime': 18.3709, 'eval_samples_per_second': 534.268, 'eval_steps_per_second': 8.383, 'epoch': 0.16} {'loss': 0.4662, 'learning_rate': 7.557855280312908e-06, 'epoch': 0.24} {'eval_loss': 0.42171549797058105, 'eval_accuracy': 0.8358634742740703, 'eval_runtime': 18.3642, 'eval_samples_per_second': 534.464, 'eval_steps_per_second': 8.386, 'epoch': 0.24} {'loss': 0.4458, 'learning_rate': 6.7429921773142115e-06, 'epoch': 0.33} {'eval_loss': 0.3808833658695221, 'eval_accuracy': 0.8527763627101376, 'eval_runtime': 18.3578, 'eval_samples_per_second': 534.649, 'eval_steps_per_second': 8.389, 'epoch': 0.33} {'loss': 0.4295, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41} {'eval_loss': 0.383415549993515, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 18.3946, 'eval_samples_per_second': 533.58, 'eval_steps_per_second': 8.372, 'epoch': 0.41} {'loss': 0.4193, 'learning_rate': 5.1148956975228174e-06, 'epoch': 0.49} {'eval_loss': 0.3880891799926758, 'eval_accuracy': 0.8494141619969434, 'eval_runtime': 18.4347, 'eval_samples_per_second': 532.418, 'eval_steps_per_second': 8.354, 'epoch': 0.49} {'loss': 0.4166, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57} {'eval_loss': 0.3630894124507904, 'eval_accuracy': 0.8582781456953642, 'eval_runtime': 18.5126, 'eval_samples_per_second': 530.181, 'eval_steps_per_second': 8.319, 'epoch': 0.57} {'loss': 0.4111, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65} {'eval_loss': 0.3584975004196167, 'eval_accuracy': 0.8596026490066225, 'eval_runtime': 18.4771, 'eval_samples_per_second': 531.198, 'eval_steps_per_second': 8.335, 'epoch': 0.65} {'loss': 0.4002, 'learning_rate': 2.6711212516297265e-06, 'epoch': 0.73} {'eval_loss': 0.36166584491729736, 'eval_accuracy': 0.8625573102394295, 'eval_runtime': 18.489, 'eval_samples_per_second': 530.857, 'eval_steps_per_second': 8.329, 'epoch': 0.73} {'loss': 0.3938, 'learning_rate': 1.8562581486310302e-06, 'epoch': 0.81} {'eval_loss': 0.354215145111084, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 18.4614, 'eval_samples_per_second': 531.651, 'eval_steps_per_second': 8.342, 'epoch': 0.81} {'loss': 0.3951, 'learning_rate': 1.0413950456323338e-06, 'epoch': 0.9} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.512, 'eval_samples_per_second': 530.197, 'eval_steps_per_second': 8.319, 'epoch': 0.9} {'loss': 0.3972, 'learning_rate': 2.265319426336376e-07, 'epoch': 0.98} {'eval_loss': 0.34958672523498535, 'eval_accuracy': 0.8661232806928171, 'eval_runtime': 18.4826, 'eval_samples_per_second': 531.04, 'eval_steps_per_second': 8.332, 'epoch': 0.98} {'train_runtime': 2606.7651, 'train_samples_per_second': 150.647, 'train_steps_per_second': 4.708, 'train_loss': 0.44322973124517767, 'epoch': 1.0} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.5143, 'eval_samples_per_second': 530.13, 'eval_steps_per_second': 8.318, 'epoch': 1.0} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.5143, 'eval_samples_per_second': 530.13, 'eval_steps_per_second': 8.318, 'epoch': 1.0} Add quantization support to any model \u00b6 The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. If you want to know more, check our documentation on: https://els-rd.github.io/transformer-deploy/quantization_intro/ In [9]: Copied! for percentile in [ 99.9 , 99.99 , 99.999 , 99.9999 ]: with QATCalibrate ( method = \"histogram\" , percentile = percentile ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( f \"percentile: { percentile } \" ) print ( trainer . evaluate ()) for percentile in [99.9, 99.99, 99.999, 99.9999]: with QATCalibrate(method=\"histogram\", percentile=percentile) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(f\"percentile: {percentile}\") print(trainer.evaluate()) [INFO|trainer.py:439] 2021-12-27 17:25:51,070 >> Using amp half precision backend percentile: 99.9 {'eval_loss': 0.47421666979789734, 'eval_accuracy': 0.8121242995415181, 'eval_runtime': 47.9158, 'eval_samples_per_second': 204.839, 'eval_steps_per_second': 3.214} {'eval_loss': 0.47421666979789734, 'eval_accuracy': 0.8121242995415181, 'eval_runtime': 47.9158, 'eval_samples_per_second': 204.839, 'eval_steps_per_second': 3.214} [INFO|trainer.py:439] 2021-12-27 17:30:13,795 >> Using amp half precision backend percentile: 99.99 {'eval_loss': 0.3841923773288727, 'eval_accuracy': 0.8487009679062659, 'eval_runtime': 46.6715, 'eval_samples_per_second': 210.3, 'eval_steps_per_second': 3.3} {'eval_loss': 0.3841923773288727, 'eval_accuracy': 0.8487009679062659, 'eval_runtime': 46.6715, 'eval_samples_per_second': 210.3, 'eval_steps_per_second': 3.3} [INFO|trainer.py:439] 2021-12-27 17:34:34,280 >> Using amp half precision backend percentile: 99.999 {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 49.1138, 'eval_samples_per_second': 199.842, 'eval_steps_per_second': 3.136} {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 49.1138, 'eval_samples_per_second': 199.842, 'eval_steps_per_second': 3.136} [INFO|trainer.py:439] 2021-12-27 17:38:54,289 >> Using amp half precision backend percentile: 99.9999 {'eval_loss': 1.0285985469818115, 'eval_accuracy': 0.4956698930208864, 'eval_runtime': 48.0849, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 3.203} {'eval_loss': 1.0285985469818115, 'eval_accuracy': 0.4956698930208864, 'eval_runtime': 48.0849, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 3.203} As you can see, the chosen percentile value has a high impact on the final accuracy. For the rest of the notebook, we apply the 99.999 percentile. In [10]: Copied! with QATCalibrate ( method = \"histogram\" , percentile = 99.999 ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( trainer . evaluate ()) with QATCalibrate(method=\"histogram\", percentile=99.999) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(trainer.evaluate()) [INFO|trainer.py:439] 2021-12-28 13:52:09,215 >> Using amp half precision backend {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 46.5572, 'eval_samples_per_second': 210.816, 'eval_steps_per_second': 3.308} {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 46.5572, 'eval_samples_per_second': 210.816, 'eval_steps_per_second': 3.308} Per layer quantization analysis \u00b6 Below we will run a sensitivity analysis, by enabling quantization of one layer at a time and measuring the accuracy. That way we will be able to detect if the quantization of a specific layer has a larger cost on accuracy than other layers. In [11]: Copied! from pytorch_quantization import nn as quant_nn for i in range ( 12 ): layer_name = f \"layer. { i } \" print ( layer_name ) for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if layer_name in name : module . enable_quant () else : module . disable_quant () trainer . evaluate () print ( \"----\" ) from pytorch_quantization import nn as quant_nn for i in range(12): layer_name = f\"layer.{i}\" print(layer_name) for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if layer_name in name: module.enable_quant() else: module.disable_quant() trainer.evaluate() print(\"----\") layer.0 {'eval_loss': 0.35163024067878723, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 20.695, 'eval_samples_per_second': 474.27, 'eval_steps_per_second': 7.441} ---- layer.1 {'eval_loss': 0.3527306318283081, 'eval_accuracy': 0.8661232806928171, 'eval_runtime': 26.1334, 'eval_samples_per_second': 375.573, 'eval_steps_per_second': 5.893} ---- layer.2 {'eval_loss': 0.3557673394680023, 'eval_accuracy': 0.8629648497198166, 'eval_runtime': 21.1364, 'eval_samples_per_second': 464.366, 'eval_steps_per_second': 7.286} ---- layer.3 {'eval_loss': 0.3551430106163025, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 20.9252, 'eval_samples_per_second': 469.051, 'eval_steps_per_second': 7.36} ---- layer.4 {'eval_loss': 0.35053929686546326, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 21.05, 'eval_samples_per_second': 466.271, 'eval_steps_per_second': 7.316} ---- layer.5 {'eval_loss': 0.35701483488082886, 'eval_accuracy': 0.865206316861946, 'eval_runtime': 20.9236, 'eval_samples_per_second': 469.088, 'eval_steps_per_second': 7.36} ---- layer.6 {'eval_loss': 0.35283517837524414, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 20.8179, 'eval_samples_per_second': 471.469, 'eval_steps_per_second': 7.397} ---- layer.7 {'eval_loss': 0.35288652777671814, 'eval_accuracy': 0.866632705043301, 'eval_runtime': 20.7823, 'eval_samples_per_second': 472.277, 'eval_steps_per_second': 7.41} ---- layer.8 {'eval_loss': 0.35080182552337646, 'eval_accuracy': 0.8672440142638819, 'eval_runtime': 20.737, 'eval_samples_per_second': 473.308, 'eval_steps_per_second': 7.426} ---- layer.9 {'eval_loss': 0.3503498136997223, 'eval_accuracy': 0.8673458991339786, 'eval_runtime': 20.8899, 'eval_samples_per_second': 469.843, 'eval_steps_per_second': 7.372} ---- layer.10 {'eval_loss': 0.3510246276855469, 'eval_accuracy': 0.8658176260825268, 'eval_runtime': 20.8428, 'eval_samples_per_second': 470.905, 'eval_steps_per_second': 7.389} ---- layer.11 {'eval_loss': 0.3509054183959961, 'eval_accuracy': 0.8656138563423331, 'eval_runtime': 20.8451, 'eval_samples_per_second': 470.853, 'eval_steps_per_second': 7.388} ---- It seems that quantization of layers 2 to 6 has the largest accuracy impact. Operator quantization analysis \u00b6 Below we will run a sensitivity analysis, by enabling quantization of one operator type at a time and measuring the accuracy. That way we will be able to detect if a specific operator has a larger cost on accuracy. On Roberta we only quantize matmul and LayerNorm , so we test both candidates. In [12]: Copied! for op in [ \"matmul\" , \"layernorm\" ]: for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if op in name : module . enable_quant () else : module . disable_quant () print ( op ) trainer . evaluate () print ( \"----\" ) for op in [\"matmul\", \"layernorm\"]: for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if op in name: module.enable_quant() else: module.disable_quant() print(op) trainer.evaluate() print(\"----\") matmul {'eval_loss': 0.35049352049827576, 'eval_accuracy': 0.8658176260825268, 'eval_runtime': 26.1972, 'eval_samples_per_second': 374.659, 'eval_steps_per_second': 5.878} ---- layernorm {'eval_loss': 0.35847699642181396, 'eval_accuracy': 0.8597045338767193, 'eval_runtime': 24.3004, 'eval_samples_per_second': 403.903, 'eval_steps_per_second': 6.337} ---- It appears that LayerNorm quantization has a significant accuracy cost. Our goal is to disable quantization for as few operations as possible while preserving accuracy as much as possible. Therefore we will try to only disable quantization for LayerNorm on Layers 2 to 6. In [13]: Copied! disable_layer_names = [ \"layer.2\" , \"layer.3\" , \"layer.4\" , \"layer.6\" ] for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if any ([ f \" { l } .output.layernorm\" in name for l in disable_layer_names ]): print ( f \"disable { name } \" ) module . disable_quant () else : module . enable_quant () trainer . evaluate () disable_layer_names = [\"layer.2\", \"layer.3\", \"layer.4\", \"layer.6\"] for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if any([f\"{l}.output.layernorm\" in name for l in disable_layer_names]): print(f\"disable {name}\") module.disable_quant() else: module.enable_quant() trainer.evaluate() disable roberta.encoder.layer.2.output.layernorm_quantizer_0 disable roberta.encoder.layer.2.output.layernorm_quantizer_1 disable roberta.encoder.layer.3.output.layernorm_quantizer_0 disable roberta.encoder.layer.3.output.layernorm_quantizer_1 disable roberta.encoder.layer.4.output.layernorm_quantizer_0 disable roberta.encoder.layer.4.output.layernorm_quantizer_1 disable roberta.encoder.layer.6.output.layernorm_quantizer_0 disable roberta.encoder.layer.6.output.layernorm_quantizer_1 {'eval_loss': 0.3660135269165039, 'eval_accuracy': 0.8618441161487519, 'eval_runtime': 45.9324, 'eval_samples_per_second': 213.684, 'eval_steps_per_second': 3.353} Out[13]: {'eval_loss': 0.3660135269165039, 'eval_accuracy': 0.8618441161487519, 'eval_runtime': 45.9324, 'eval_samples_per_second': 213.684, 'eval_steps_per_second': 3.353} By just disabling quantization for a single operator on a few layers, we keep most of the performance boost (quantization) but retrieve more than 1 point of accuracy. It's also possible to perform an analysis per quantizer to get a smaller granularity but it's a bit slow to run. If we stop here, it's called a Post Training Quantization (PTQ). Below, we will try to retrieve even more accuracy. Quantization Aware Training (QAT) \u00b6 We retrain the model with 1/10 or 1/100 of the original learning rate. Our goal is to retrieve most of the original accuracy. In [14]: Copied! args . learning_rate = 1e-7 trainer = get_trainer ( model_q ) trainer . train () print ( trainer . evaluate ()) model_q . save_pretrained ( \"model-qat\" ) args.learning_rate = 1e-7 trainer = get_trainer(model_q) trainer.train() print(trainer.evaluate()) model_q.save_pretrained(\"model-qat\") [INFO|trainer.py:439] 2021-12-28 13:54:41,146 >> Using amp half precision backend {'loss': 0.3591, 'learning_rate': 9.188396349413298e-08, 'epoch': 0.08} {'eval_loss': 0.3738575875759125, 'eval_accuracy': 0.8596026490066225, 'eval_runtime': 46.992, 'eval_samples_per_second': 208.865, 'eval_steps_per_second': 3.277, 'epoch': 0.08} {'loss': 0.3182, 'learning_rate': 8.373533246414603e-08, 'epoch': 0.16} {'eval_loss': 0.38133203983306885, 'eval_accuracy': 0.8586856851757514, 'eval_runtime': 45.7335, 'eval_samples_per_second': 214.613, 'eval_steps_per_second': 3.367, 'epoch': 0.16} {'loss': 0.3062, 'learning_rate': 7.558670143415906e-08, 'epoch': 0.24} {'eval_loss': 0.3903615176677704, 'eval_accuracy': 0.8592969943963321, 'eval_runtime': 45.6544, 'eval_samples_per_second': 214.985, 'eval_steps_per_second': 3.373, 'epoch': 0.24} {'loss': 0.2986, 'learning_rate': 6.74380704041721e-08, 'epoch': 0.33} {'eval_loss': 0.39669597148895264, 'eval_accuracy': 0.8577687213448802, 'eval_runtime': 45.6583, 'eval_samples_per_second': 214.966, 'eval_steps_per_second': 3.373, 'epoch': 0.33} {'loss': 0.2994, 'learning_rate': 5.9289439374185136e-08, 'epoch': 0.41} {'eval_loss': 0.394754558801651, 'eval_accuracy': 0.8612328069281712, 'eval_runtime': 45.6439, 'eval_samples_per_second': 215.034, 'eval_steps_per_second': 3.374, 'epoch': 0.41} {'loss': 0.3027, 'learning_rate': 5.1148956975228164e-08, 'epoch': 0.49} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.7496, 'eval_samples_per_second': 214.538, 'eval_steps_per_second': 3.366, 'epoch': 0.49} {'loss': 0.3164, 'learning_rate': 4.300032594524119e-08, 'epoch': 0.57} {'eval_loss': 0.39596375823020935, 'eval_accuracy': 0.8609271523178808, 'eval_runtime': 45.6669, 'eval_samples_per_second': 214.926, 'eval_steps_per_second': 3.372, 'epoch': 0.57} {'loss': 0.3298, 'learning_rate': 3.485984354628422e-08, 'epoch': 0.65} {'eval_loss': 0.3958113491535187, 'eval_accuracy': 0.8599083036169128, 'eval_runtime': 45.6637, 'eval_samples_per_second': 214.941, 'eval_steps_per_second': 3.372, 'epoch': 0.65} {'loss': 0.3379, 'learning_rate': 2.6719361147327245e-08, 'epoch': 0.73} {'eval_loss': 0.39275360107421875, 'eval_accuracy': 0.8577687213448802, 'eval_runtime': 45.7659, 'eval_samples_per_second': 214.461, 'eval_steps_per_second': 3.365, 'epoch': 0.73} {'loss': 0.354, 'learning_rate': 1.8570730117340286e-08, 'epoch': 0.81} {'eval_loss': 0.39236611127853394, 'eval_accuracy': 0.8562404482934284, 'eval_runtime': 45.6972, 'eval_samples_per_second': 214.784, 'eval_steps_per_second': 3.37, 'epoch': 0.81} {'loss': 0.3826, 'learning_rate': 1.0422099087353324e-08, 'epoch': 0.9} {'eval_loss': 0.389812171459198, 'eval_accuracy': 0.8620478858889455, 'eval_runtime': 45.6861, 'eval_samples_per_second': 214.836, 'eval_steps_per_second': 3.371, 'epoch': 0.9} {'loss': 0.4363, 'learning_rate': 2.2734680573663624e-09, 'epoch': 0.98} {'eval_loss': 0.3902811110019684, 'eval_accuracy': 0.8583800305654611, 'eval_runtime': 45.6787, 'eval_samples_per_second': 214.87, 'eval_steps_per_second': 3.371, 'epoch': 0.98} {'train_runtime': 4893.2972, 'train_samples_per_second': 80.253, 'train_steps_per_second': 2.508, 'train_loss': 0.33914821741633805, 'epoch': 1.0} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.6231, 'eval_samples_per_second': 215.132, 'eval_steps_per_second': 3.375, 'epoch': 1.0} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.6231, 'eval_samples_per_second': 215.132, 'eval_steps_per_second': 3.375, 'epoch': 1.0} Export a QDQ Pytorch model to ONNX \u00b6 We need to enable fake quantization mode from Pytorch. In [15]: Copied! data = encoded_dataset [ \"train\" ][ 1 : 3 ] input_torch = convert_tensor ( data , output = \"torch\" ) convert_to_onnx ( model_pytorch = model_q , output_path = \"model_qat.onnx\" , inputs_pytorch = input_torch , quantization = True ) data = encoded_dataset[\"train\"][1:3] input_torch = convert_tensor(data, output=\"torch\") convert_to_onnx(model_pytorch=model_q, output_path=\"model_qat.onnx\", inputs_pytorch=input_torch, quantization=True) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! inputs, amax.item() / bound, 0, /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0]) In [16]: Copied! del model_q QATCalibrate . restore () del model_q QATCalibrate.restore() Benchmark \u00b6 Convert ONNX graph to TensorRT engine \u00b6 In [17]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) engine = build_engine( runtime=runtime, onnx_file_path=\"model_qat.onnx\", logger=trt_logger, min_shape=(1, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=True, ) In [18]: Copied! # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" Prepare input and output buffer \u00b6 In [19]: Copied! stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] stream: Stream = pycuda.driver.Stream() context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] In [20]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") Inference on TensorRT \u00b6 We first check that inference is working correctly: In [21]: Copied! tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print ( tensorrt_output ) tensorrt_output = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print(tensorrt_output) [array([[ 0.11111109, 2.9936233 , -2.5243347 ], [ 3.2135723 , -0.4374885 , -2.4485767 ], [ 2.1678474 , -1.1477091 , -0.7798154 ], [ 1.8148003 , -0.2093072 , -1.416711 ], [ 2.3070638 , 0.27601779, -2.2818418 ], [ 4.1799006 , -0.83163625, -2.8492923 ], [-3.695277 , 2.3409832 , 1.4314314 ], [ 4.1796045 , -1.0709951 , -2.6119678 ], [-0.44781622, -1.4288648 , 1.888488 ], [-2.9845483 , -1.5895646 , 4.117529 ], [ 3.9293122 , -0.68528754, -2.9477124 ], [-2.516609 , 0.34680495, 2.2793124 ], [-3.0710464 , 3.3439813 , 0.08079423], [-2.2859852 , 1.9546673 , 0.37908432], [ 0.3999826 , -1.0603418 , 0.5099453 ], [ 2.9247677 , -0.6867883 , -1.7499886 ], [ 4.1125493 , -0.7771612 , -2.986419 ], [-2.58058 , -2.3291597 , 4.553415 ], [-3.215447 , -1.3902456 , 4.2499046 ], [-2.014185 , 4.117433 , -1.634403 ], [ 4.051285 , -0.64716065, -2.9019048 ], [ 3.742484 , -0.07188296, -3.272956 ], [-3.302061 , -1.0159078 , 3.9711204 ], [ 3.9316242 , -0.33764294, -3.209711 ], [ 3.9900765 , -1.5201662 , -2.1166122 ], [-1.2437494 , 1.410141 , -0.10993958], [-3.1267605 , -0.8212991 , 3.6917076 ], [-2.0607114 , 4.1098857 , -1.4996963 ], [-3.5770578 , -0.736545 , 3.9671996 ], [ 3.776105 , -0.60771704, -2.8707912 ], [ 3.5450761 , -0.14414684, -2.9718893 ], [ 3.4713674 , 0.12106885, -3.189211 ]], dtype=float32)] Measure of the accuracy: In [22]: Copied! infer_trt = lambda inputs : infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) measure_accuracy ( infer = infer_trt , int64 = False ) infer_trt = lambda inputs: infer_tensorrt( context=context, host_inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) measure_accuracy(infer=infer_trt, int64=False) Out[22]: 0.8629648497198166 Latency measures: In [23]: Copied! time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print_timings ( name = \"TensorRT (INT-8)\" , timings = time_buffer ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer) [TensorRT (INT-8)] mean=17.01ms, sd=1.14ms, min=16.64ms, max=27.17ms, median=16.76ms, 95p=18.16ms, 99p=20.06ms In [24]: Copied! del engine , context del engine, context Pytorch baseline \u00b6 Time to get some numbers to compare with. GPU execution \u00b6 We will measure vanilla Pytorch inference on both FP32 and FP16 precision on GPU, it will be our baseline: In [25]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) with torch . inference_mode (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32)\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") with torch.inference_mode(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32)\", timings=time_buffer) [Pytorch (FP32)] mean=76.87ms, sd=1.24ms, min=75.68ms, max=82.38ms, median=76.52ms, 95p=79.29ms, 99p=81.90ms In [26]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16)\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = [] for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16)\", timings=time_buffer) del baseline_model [Pytorch (FP16)] mean=56.24ms, sd=0.67ms, min=55.53ms, max=59.61ms, median=56.05ms, 95p=57.80ms, 99p=58.18ms CPU execution \u00b6 In [27]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_torch_cpu = { k : v . to ( \"cpu\" ) for k , v in input_torch . items ()} torch . set_num_threads ( os . cpu_count ()) with torch . inference_mode (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32) - CPU\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_torch_cpu = {k: v.to(\"cpu\") for k, v in input_torch.items()} torch.set_num_threads(os.cpu_count()) with torch.inference_mode(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32) - CPU\", timings=time_buffer) [Pytorch (FP32) - CPU] mean=4267.96ms, sd=249.08ms, min=3959.59ms, max=4697.79ms, median=4299.22ms, 95p=4632.12ms, 99p=4684.66ms In [28]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16) - CPU\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = [] for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16) - CPU\", timings=time_buffer) del baseline_model [Pytorch (FP16) - CPU] mean=4428.94ms, sd=225.39ms, min=4148.26ms, max=4871.84ms, median=4404.70ms, 95p=4781.81ms, 99p=4853.83ms Below, we will perform dynamic quantization on CPU. In [29]: Copied! quantized_baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) quantized_baseline_model = quantized_baseline_model . eval () quantized_baseline_model = torch . quantization . quantize_dynamic ( quantized_baseline_model , { torch . nn . Linear }, dtype = torch . qint8 ) with torch . inference_mode (): for _ in range ( 3 ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (INT-8) - CPU\" , timings = time_buffer ) quantized_baseline_model = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) quantized_baseline_model = quantized_baseline_model.eval() quantized_baseline_model = torch.quantization.quantize_dynamic( quantized_baseline_model, {torch.nn.Linear}, dtype=torch.qint8 ) with torch.inference_mode(): for _ in range(3): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (INT-8) - CPU\", timings=time_buffer) [Pytorch (INT-8) - CPU] mean=3299.66ms, sd=37.76ms, min=3274.33ms, max=3405.91ms, median=3285.20ms, 95p=3366.88ms, 99p=3398.10ms TensorRT baseline \u00b6 Below we export our finetuned model, the purpose is to only check the performance on mixed precision (FP16, no quantization). In [30]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () convert_to_onnx ( baseline_model , output_path = \"baseline.onnx\" , inputs_pytorch = input_torch , quantization = False ) del baseline_model baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() convert_to_onnx(baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch, quantization=False) del baseline_model In [31]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"baseline.onnx\" , logger = trt_logger , min_shape = ( batch_size , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = False , ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] for _ in range ( 30 ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print_timings ( name = \"TensorRT (FP16)\" , timings = time_buffer ) del engine , context engine = build_engine( runtime=runtime, onnx_file_path=\"baseline.onnx\", logger=trt_logger, min_shape=(batch_size, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=False, ) input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") stream: Stream = pycuda.driver.Stream() context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] for _ in range(30): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print_timings(name=\"TensorRT (FP16)\", timings=time_buffer) del engine, context [TensorRT (FP16)] mean=29.90ms, sd=0.82ms, min=29.30ms, max=33.41ms, median=29.69ms, 95p=31.85ms, 99p=32.79ms ONNX Runtime baseline \u00b6 ONNX Runtime is the go to inference solution from Microsoft. The recent 1.10 version of ONNX Runtime (with TensorRT support) is still a bit buggy on transformer models, that is why we use the 1.9.0 version in the measures below. As before, CPU quantization is dynamic. Function will set ONNX Runtime to use all cores available and enable any possible optimizations. In [32]: Copied! optimize_onnx ( onnx_path = \"baseline.onnx\" , onnx_optim_model_path = \"baseline-optimized.onnx\" , fp16 = True , use_cuda = True , ) optimize_onnx( onnx_path=\"baseline.onnx\", onnx_optim_model_path=\"baseline-optimized.onnx\", fp16=True, use_cuda=True, ) In [33]: Copied! cpu_quantization ( input_model_path = \"baseline-optimized.onnx\" , output_model_path = \"baseline-quantized.onnx\" ) cpu_quantization(input_model_path=\"baseline-optimized.onnx\", output_model_path=\"baseline-quantized.onnx\") Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for th Ignore MatMul due to non constant B: /[MatMul_113] Ignore MatMul due to non constant B: /[MatMul_127] Ignore MatMul due to non constant B: /[MatMul_137] Ignore MatMul due to non constant B: /[MatMul_207] Ignore MatMul due to non constant B: /[MatMul_221] Ignore MatMul due to non constant B: /[MatMul_231] Ignore MatMul due to non constant B: /[MatMul_301] Ignore MatMul due to non constant B: /[MatMul_315] Ignore MatMul due to non constant B: /[MatMul_325] Ignore MatMul due to non constant B: /[MatMul_395] Ignore MatMul due to non constant B: /[MatMul_409] Ignore MatMul due to non constant B: /[MatMul_419] Ignore MatMul due to non constant B: /[MatMul_489] Ignore MatMul due to non constant B: /[MatMul_503] Ignore MatMul due to non constant B: /[MatMul_513] Ignore MatMul due to non constant B: /[MatMul_583] Ignore MatMul due to non constant B: /[MatMul_597] Ignore MatMul due to non constant B: /[MatMul_607] Ignore MatMul due to non constant B: /[MatMul_677] Ignore MatMul due to non constant B: /[MatMul_691] Ignore MatMul due to non constant B: /[MatMul_701] Ignore MatMul due to non constant B: /[MatMul_771] Ignore MatMul due to non constant B: /[MatMul_785] Ignore MatMul due to non constant B: /[MatMul_795] Ignore MatMul due to non constant B: /[MatMul_865] Ignore MatMul due to non constant B: /[MatMul_879] Ignore MatMul due to non constant B: /[MatMul_889] Ignore MatMul due to non constant B: /[MatMul_959] Ignore MatMul due to non constant B: /[MatMul_973] Ignore MatMul due to non constant B: /[MatMul_983] Ignore MatMul due to non constant B: /[MatMul_1053] Ignore MatMul due to non constant B: /[MatMul_1067] Ignore MatMul due to non constant B: /[MatMul_1077] Ignore MatMul due to non constant B: /[MatMul_1147] Ignore MatMul due to non constant B: /[MatMul_1161] Ignore MatMul due to non constant B: /[MatMul_1171] Ignore MatMul due to non constant B: /[Gemm_1187_MatMul] Ignore MatMul due to non constant B: /[Gemm_1189_MatMul] is operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. In [34]: Copied! labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] data = encoded_dataset [ validation_key ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for k , v in inputs_onnx . items (): inputs_onnx [ k ] = v . astype ( np . int64 ) model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) output = model . run ( None , inputs_onnx ) labels = [item[\"label\"] for item in encoded_dataset[validation_key]] data = encoded_dataset[validation_key][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for k, v in inputs_onnx.items(): inputs_onnx[k] = v.astype(np.int64) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") output = model.run(None, inputs_onnx) In [35]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for k , v in inputs_onnx . items (): inputs_onnx [ k ] = v . astype ( np . int64 ) for provider , model_path , benchmark_name , warmup , nb_inference in [ ( \"CUDAExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime GPU (FP32)\" , 10 , 100 ), ( \"CUDAExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime GPU (FP16)\" , 10 , 100 ), ( \"CPUExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime CPU (FP32)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime CPU (FP16)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-quantized.onnx\" , \"ONNX Runtime CPU (INT-8)\" , 3 , 10 ), ]: model = create_model_for_provider ( path = model_path , provider_to_use = provider ) for _ in range ( warmup ): _ = model . run ( None , inputs_onnx ) time_buffer = [] for _ in range ( nb_inference ): with track_infer_time ( time_buffer ): _ = model . run ( None , inputs_onnx ) print_timings ( name = benchmark_name , timings = time_buffer ) del model data = encoded_dataset[\"train\"][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for k, v in inputs_onnx.items(): inputs_onnx[k] = v.astype(np.int64) for provider, model_path, benchmark_name, warmup, nb_inference in [ (\"CUDAExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime GPU (FP32)\", 10, 100), (\"CUDAExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime GPU (FP16)\", 10, 100), (\"CPUExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime CPU (FP32)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime CPU (FP16)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-quantized.onnx\", \"ONNX Runtime CPU (INT-8)\", 3, 10), ]: model = create_model_for_provider(path=model_path, provider_to_use=provider) for _ in range(warmup): _ = model.run(None, inputs_onnx) time_buffer = [] for _ in range(nb_inference): with track_infer_time(time_buffer): _ = model.run(None, inputs_onnx) print_timings(name=benchmark_name, timings=time_buffer) del model [ONNX Runtime GPU (FP32)] mean=76.38ms, sd=4.99ms, min=73.10ms, max=91.05ms, median=73.91ms, 95p=88.30ms, 99p=89.42ms [ONNX Runtime GPU (FP16)] mean=34.21ms, sd=1.68ms, min=33.23ms, max=41.80ms, median=33.70ms, 95p=38.87ms, 99p=40.63ms [ONNX Runtime CPU (FP32)] mean=4023.32ms, sd=92.76ms, min=3895.51ms, max=4267.63ms, median=4013.27ms, 95p=4170.44ms, 99p=4248.19ms [ONNX Runtime CPU (FP16)] mean=3956.61ms, sd=167.65ms, min=3709.88ms, max=4188.62ms, median=3914.53ms, 95p=4180.81ms, 99p=4187.06ms [ONNX Runtime CPU (INT-8)] mean=3336.29ms, sd=168.96ms, min=3170.64ms, max=3765.07ms, median=3299.52ms, 95p=3641.01ms, 99p=3740.26ms Measure of the accuracy with ONNX Runtime engine and CUDA provider: In [36]: Copied! model = create_model_for_provider ( path = \"baseline.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[36]: 0.8663270504330107 In [37]: Copied! model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[37]: 0.8663270504330107 In [38]: Copied! model = create_model_for_provider ( path = \"baseline-quantized.onnx\" , provider_to_use = \"CPUExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline-quantized.onnx\", provider_to_use=\"CPUExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[38]: 0.8650025471217524 In [39]: Copied! del model del model In [40]: Copied!","title":"End to end demo"},{"location":"quantization/#nvidia-gpu-int-8-quantization-on-any-transformers-model-encoder-based","text":"For some context and explanations, please check our documentation here: https://els-rd.github.io/transformer-deploy/quantization_intro/ .","title":"Nvidia GPU INT-8 quantization on any transformers model (encoder based)"},{"location":"quantization/#project-setup","text":"","title":"Project setup"},{"location":"quantization/#dependencies-installation","text":"Your machine should have Nvidia CUDA 11.X, TensorRT 8.2.1 and cuBLAS installed. It's said to be tricky to install, in my experience, just follow Nvidia download page instructions and nothing else , it should work out of the box. Nvidia Docker image could be a good choice too. In [1]: Copied! #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ #! pip3 install git+ssh://git@github.com/ELS-RD/transformer-deploy #! pip3 install datasets sklearn #! pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization\\&subdirectory=tools/pytorch-quantization/ Check the GPU is enabled and usable. In [2]: Copied! ! nvidia - smi ! nvidia-smi Tue Dec 28 21:46:26 2021 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 495.44 Driver Version: 495.44 CUDA Version: 11.5 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 NVIDIA GeForce ... Off | 00000000:03:00.0 On | N/A | | 35% 42C P8 40W / 350W | 263MiB / 24267MiB | 4% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1604 G /usr/lib/xorg/Xorg 159MiB | | 0 N/A N/A 8473 G /usr/bin/gnome-shell 44MiB | | 0 N/A N/A 106329 G ..._18576.log --shared-files 17MiB | | 0 N/A N/A 110356 G ...AAAAAAAAA= --shared-files 39MiB | +-----------------------------------------------------------------------------+ In [3]: Copied! import logging import os from collections import OrderedDict from typing import Dict , List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import pycuda.autoinit import tensorrt as trt import torch import transformers from datasets import load_dataset , load_metric from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext , Logger , Runtime from transformers import ( AutoModelForSequenceClassification , AutoTokenizer , IntervalStrategy , PreTrainedModel , PreTrainedTokenizer , Trainer , TrainingArguments , ) from transformer_deploy.backends.ort_utils import ( cpu_quantization , create_model_for_provider , optimize_onnx , ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine , get_binding_idxs , infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings , track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate import logging import os from collections import OrderedDict from typing import Dict, List from typing import OrderedDict as OD from typing import Union import datasets import numpy as np import pycuda.autoinit import tensorrt as trt import torch import transformers from datasets import load_dataset, load_metric from pycuda._driver import Stream from tensorrt.tensorrt import IExecutionContext, Logger, Runtime from transformers import ( AutoModelForSequenceClassification, AutoTokenizer, IntervalStrategy, PreTrainedModel, PreTrainedTokenizer, Trainer, TrainingArguments, ) from transformer_deploy.backends.ort_utils import ( cpu_quantization, create_model_for_provider, optimize_onnx, ) from transformer_deploy.backends.pytorch_utils import convert_to_onnx from transformer_deploy.backends.trt_utils import build_engine, get_binding_idxs, infer_tensorrt from transformer_deploy.benchmarks.utils import print_timings, track_infer_time from transformer_deploy.QDQModels.calibration_utils import QATCalibrate Set logging to error level to ease readability of this notebook on Github. In [4]: Copied! log_level = logging . ERROR logging . getLogger () . setLevel ( log_level ) datasets . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . set_verbosity ( log_level ) transformers . utils . logging . enable_default_handler () transformers . utils . logging . enable_explicit_format () trt_logger : Logger = trt . Logger ( trt . Logger . ERROR ) transformers . logging . set_verbosity_error () log_level = logging.ERROR logging.getLogger().setLevel(log_level) datasets.utils.logging.set_verbosity(log_level) transformers.utils.logging.set_verbosity(log_level) transformers.utils.logging.enable_default_handler() transformers.utils.logging.enable_explicit_format() trt_logger: Logger = trt.Logger(trt.Logger.ERROR) transformers.logging.set_verbosity_error()","title":"Dependencies installation"},{"location":"quantization/#preprocess-data","text":"This part is inspired from an official Notebooks from Hugging Face . There is nothing special to do. Define the task: In [5]: Copied! model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings : Dict [ str , List [ float ]] = dict () runtime : Runtime = trt . Runtime ( trt_logger ) profile_index = 0 model_name = \"roberta-base\" task = \"mnli\" num_labels = 3 batch_size = 32 max_seq_len = 256 validation_key = \"validation_matched\" timings: Dict[str, List[float]] = dict() runtime: Runtime = trt.Runtime(trt_logger) profile_index = 0 Preprocess data (task specific): In [6]: Copied! def preprocess_function ( examples ): return tokenizer ( examples [ \"premise\" ], examples [ \"hypothesis\" ], truncation = True , padding = \"max_length\" , max_length = max_seq_len ) def compute_metrics ( eval_pred ): predictions , labels = eval_pred if task != \"stsb\" : predictions = np . argmax ( predictions , axis = 1 ) else : predictions = predictions [:, 0 ] return metric . compute ( predictions = predictions , references = labels ) def convert_tensor ( data : OD [ str , List [ List [ int ]]], output : str ) -> OD [ str , Union [ np . ndarray , torch . Tensor ]]: input : OD [ str , Union [ np . ndarray , torch . Tensor ]] = OrderedDict () for k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ]: if k in data : v = data [ k ] if output == \"torch\" : value = torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) elif output == \"np\" : value = np . asarray ( v , dtype = np . int32 ) else : raise Exception ( f \"unknown output type: { output } \" ) input [ k ] = value return input def measure_accuracy ( infer , int64 : bool ) -> float : outputs = list () for start_index in range ( 0 , len ( encoded_dataset [ validation_key ]), batch_size ): end_index = start_index + batch_size data = encoded_dataset [ validation_key ][ start_index : end_index ] inputs : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) if int64 : for k , v in inputs . items (): inputs [ k ] = v . astype ( np . int64 ) output = infer ( inputs ) output = np . argmax ( output [ 0 ], axis = 1 ) . astype ( int ) . tolist () outputs . extend ( output ) return np . mean ( np . array ( outputs ) == np . array ( validation_labels )) def get_trainer ( model : PreTrainedModel ) -> Trainer : trainer = Trainer ( model , args , train_dataset = encoded_dataset [ \"train\" ], eval_dataset = encoded_dataset [ validation_key ], tokenizer = tokenizer , compute_metrics = compute_metrics , ) transformers . logging . set_verbosity_error () return trainer def preprocess_function(examples): return tokenizer( examples[\"premise\"], examples[\"hypothesis\"], truncation=True, padding=\"max_length\", max_length=max_seq_len ) def compute_metrics(eval_pred): predictions, labels = eval_pred if task != \"stsb\": predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) def convert_tensor(data: OD[str, List[List[int]]], output: str) -> OD[str, Union[np.ndarray, torch.Tensor]]: input: OD[str, Union[np.ndarray, torch.Tensor]] = OrderedDict() for k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]: if k in data: v = data[k] if output == \"torch\": value = torch.tensor(v, dtype=torch.long, device=\"cuda\") elif output == \"np\": value = np.asarray(v, dtype=np.int32) else: raise Exception(f\"unknown output type: {output}\") input[k] = value return input def measure_accuracy(infer, int64: bool) -> float: outputs = list() for start_index in range(0, len(encoded_dataset[validation_key]), batch_size): end_index = start_index + batch_size data = encoded_dataset[validation_key][start_index:end_index] inputs: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") if int64: for k, v in inputs.items(): inputs[k] = v.astype(np.int64) output = infer(inputs) output = np.argmax(output[0], axis=1).astype(int).tolist() outputs.extend(output) return np.mean(np.array(outputs) == np.array(validation_labels)) def get_trainer(model: PreTrainedModel) -> Trainer: trainer = Trainer( model, args, train_dataset=encoded_dataset[\"train\"], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics, ) transformers.logging.set_verbosity_error() return trainer In [7]: Copied! tokenizer : PreTrainedTokenizer = AutoTokenizer . from_pretrained ( model_name , use_fast = True ) dataset = load_dataset ( \"glue\" , task ) metric = load_metric ( \"glue\" , task ) encoded_dataset = dataset . map ( preprocess_function , batched = True ) validation_labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] nb_step = 1000 strategy = IntervalStrategy . STEPS args = TrainingArguments ( f \" { model_name } - { task } \" , evaluation_strategy = strategy , eval_steps = nb_step , logging_steps = nb_step , save_steps = nb_step , save_strategy = strategy , learning_rate = 1e-5 , per_device_train_batch_size = batch_size , per_device_eval_batch_size = batch_size * 2 , num_train_epochs = 1 , fp16 = True , group_by_length = True , weight_decay = 0.01 , load_best_model_at_end = True , metric_for_best_model = \"accuracy\" , report_to = [], ) tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True) dataset = load_dataset(\"glue\", task) metric = load_metric(\"glue\", task) encoded_dataset = dataset.map(preprocess_function, batched=True) validation_labels = [item[\"label\"] for item in encoded_dataset[validation_key]] nb_step = 1000 strategy = IntervalStrategy.STEPS args = TrainingArguments( f\"{model_name}-{task}\", evaluation_strategy=strategy, eval_steps=nb_step, logging_steps=nb_step, save_steps=nb_step, save_strategy=strategy, learning_rate=1e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size * 2, num_train_epochs=1, fp16=True, group_by_length=True, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=\"accuracy\", report_to=[], ) 0%| | 0/5 [00:00<?, ?it/s]","title":"Preprocess data"},{"location":"quantization/#standard-fine-tuning-model","text":"Now that our data are ready, we can download/fine tune the pretrained model. In [8]: Copied! model_fp16 : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = num_labels ) trainer = get_trainer ( model_fp16 ) transformers . logging . set_verbosity_error () trainer . train () print ( trainer . evaluate ()) model_fp16 . save_pretrained ( \"model_trained_fp16\" ) model_fp16: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels) trainer = get_trainer(model_fp16) transformers.logging.set_verbosity_error() trainer.train() print(trainer.evaluate()) model_fp16.save_pretrained(\"model_trained_fp16\") [INFO|trainer.py:439] 2021-12-27 09:19:51,063 >> Using amp half precision backend {'loss': 0.6605, 'learning_rate': 9.1875814863103e-06, 'epoch': 0.08} {'eval_loss': 0.4653007388114929, 'eval_accuracy': 0.8183392766174223, 'eval_runtime': 18.2981, 'eval_samples_per_second': 536.393, 'eval_steps_per_second': 8.416, 'epoch': 0.08} {'loss': 0.4956, 'learning_rate': 8.372718383311604e-06, 'epoch': 0.16} {'eval_loss': 0.4208127558231354, 'eval_accuracy': 0.8346408558329088, 'eval_runtime': 18.3709, 'eval_samples_per_second': 534.268, 'eval_steps_per_second': 8.383, 'epoch': 0.16} {'loss': 0.4662, 'learning_rate': 7.557855280312908e-06, 'epoch': 0.24} {'eval_loss': 0.42171549797058105, 'eval_accuracy': 0.8358634742740703, 'eval_runtime': 18.3642, 'eval_samples_per_second': 534.464, 'eval_steps_per_second': 8.386, 'epoch': 0.24} {'loss': 0.4458, 'learning_rate': 6.7429921773142115e-06, 'epoch': 0.33} {'eval_loss': 0.3808833658695221, 'eval_accuracy': 0.8527763627101376, 'eval_runtime': 18.3578, 'eval_samples_per_second': 534.649, 'eval_steps_per_second': 8.389, 'epoch': 0.33} {'loss': 0.4295, 'learning_rate': 5.9289439374185145e-06, 'epoch': 0.41} {'eval_loss': 0.383415549993515, 'eval_accuracy': 0.851044319918492, 'eval_runtime': 18.3946, 'eval_samples_per_second': 533.58, 'eval_steps_per_second': 8.372, 'epoch': 0.41} {'loss': 0.4193, 'learning_rate': 5.1148956975228174e-06, 'epoch': 0.49} {'eval_loss': 0.3880891799926758, 'eval_accuracy': 0.8494141619969434, 'eval_runtime': 18.4347, 'eval_samples_per_second': 532.418, 'eval_steps_per_second': 8.354, 'epoch': 0.49} {'loss': 0.4166, 'learning_rate': 4.30003259452412e-06, 'epoch': 0.57} {'eval_loss': 0.3630894124507904, 'eval_accuracy': 0.8582781456953642, 'eval_runtime': 18.5126, 'eval_samples_per_second': 530.181, 'eval_steps_per_second': 8.319, 'epoch': 0.57} {'loss': 0.4111, 'learning_rate': 3.4851694915254244e-06, 'epoch': 0.65} {'eval_loss': 0.3584975004196167, 'eval_accuracy': 0.8596026490066225, 'eval_runtime': 18.4771, 'eval_samples_per_second': 531.198, 'eval_steps_per_second': 8.335, 'epoch': 0.65} {'loss': 0.4002, 'learning_rate': 2.6711212516297265e-06, 'epoch': 0.73} {'eval_loss': 0.36166584491729736, 'eval_accuracy': 0.8625573102394295, 'eval_runtime': 18.489, 'eval_samples_per_second': 530.857, 'eval_steps_per_second': 8.329, 'epoch': 0.73} {'loss': 0.3938, 'learning_rate': 1.8562581486310302e-06, 'epoch': 0.81} {'eval_loss': 0.354215145111084, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 18.4614, 'eval_samples_per_second': 531.651, 'eval_steps_per_second': 8.342, 'epoch': 0.81} {'loss': 0.3951, 'learning_rate': 1.0413950456323338e-06, 'epoch': 0.9} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.512, 'eval_samples_per_second': 530.197, 'eval_steps_per_second': 8.319, 'epoch': 0.9} {'loss': 0.3972, 'learning_rate': 2.265319426336376e-07, 'epoch': 0.98} {'eval_loss': 0.34958672523498535, 'eval_accuracy': 0.8661232806928171, 'eval_runtime': 18.4826, 'eval_samples_per_second': 531.04, 'eval_steps_per_second': 8.332, 'epoch': 0.98} {'train_runtime': 2606.7651, 'train_samples_per_second': 150.647, 'train_steps_per_second': 4.708, 'train_loss': 0.44322973124517767, 'epoch': 1.0} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.5143, 'eval_samples_per_second': 530.13, 'eval_steps_per_second': 8.318, 'epoch': 1.0} {'eval_loss': 0.3511120676994324, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 18.5143, 'eval_samples_per_second': 530.13, 'eval_steps_per_second': 8.318, 'epoch': 1.0}","title":"(Standard) fine-tuning model"},{"location":"quantization/#add-quantization-support-to-any-model","text":"The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. If you want to know more, check our documentation on: https://els-rd.github.io/transformer-deploy/quantization_intro/ In [9]: Copied! for percentile in [ 99.9 , 99.99 , 99.999 , 99.9999 ]: with QATCalibrate ( method = \"histogram\" , percentile = percentile ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( f \"percentile: { percentile } \" ) print ( trainer . evaluate ()) for percentile in [99.9, 99.99, 99.999, 99.9999]: with QATCalibrate(method=\"histogram\", percentile=percentile) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(f\"percentile: {percentile}\") print(trainer.evaluate()) [INFO|trainer.py:439] 2021-12-27 17:25:51,070 >> Using amp half precision backend percentile: 99.9 {'eval_loss': 0.47421666979789734, 'eval_accuracy': 0.8121242995415181, 'eval_runtime': 47.9158, 'eval_samples_per_second': 204.839, 'eval_steps_per_second': 3.214} {'eval_loss': 0.47421666979789734, 'eval_accuracy': 0.8121242995415181, 'eval_runtime': 47.9158, 'eval_samples_per_second': 204.839, 'eval_steps_per_second': 3.214} [INFO|trainer.py:439] 2021-12-27 17:30:13,795 >> Using amp half precision backend percentile: 99.99 {'eval_loss': 0.3841923773288727, 'eval_accuracy': 0.8487009679062659, 'eval_runtime': 46.6715, 'eval_samples_per_second': 210.3, 'eval_steps_per_second': 3.3} {'eval_loss': 0.3841923773288727, 'eval_accuracy': 0.8487009679062659, 'eval_runtime': 46.6715, 'eval_samples_per_second': 210.3, 'eval_steps_per_second': 3.3} [INFO|trainer.py:439] 2021-12-27 17:34:34,280 >> Using amp half precision backend percentile: 99.999 {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 49.1138, 'eval_samples_per_second': 199.842, 'eval_steps_per_second': 3.136} {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 49.1138, 'eval_samples_per_second': 199.842, 'eval_steps_per_second': 3.136} [INFO|trainer.py:439] 2021-12-27 17:38:54,289 >> Using amp half precision backend percentile: 99.9999 {'eval_loss': 1.0285985469818115, 'eval_accuracy': 0.4956698930208864, 'eval_runtime': 48.0849, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 3.203} {'eval_loss': 1.0285985469818115, 'eval_accuracy': 0.4956698930208864, 'eval_runtime': 48.0849, 'eval_samples_per_second': 204.118, 'eval_steps_per_second': 3.203} As you can see, the chosen percentile value has a high impact on the final accuracy. For the rest of the notebook, we apply the 99.999 percentile. In [10]: Copied! with QATCalibrate ( method = \"histogram\" , percentile = 99.999 ) as qat : model_q : PreTrainedModel = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) model_q = model_q . cuda () qat . setup_model_qat ( model_q ) # prepare quantizer to any model with torch . no_grad (): for start_index in range ( 0 , 128 , batch_size ): end_index = start_index + batch_size data = encoded_dataset [ \"train\" ][ start_index : end_index ] input_torch = { k : torch . tensor ( v , dtype = torch . long , device = \"cuda\" ) for k , v in data . items () if k in [ \"input_ids\" , \"attention_mask\" , \"token_type_ids\" ] } model_q ( ** input_torch ) trainer = get_trainer ( model_q ) print ( trainer . evaluate ()) with QATCalibrate(method=\"histogram\", percentile=99.999) as qat: model_q: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) model_q = model_q.cuda() qat.setup_model_qat(model_q) # prepare quantizer to any model with torch.no_grad(): for start_index in range(0, 128, batch_size): end_index = start_index + batch_size data = encoded_dataset[\"train\"][start_index:end_index] input_torch = { k: torch.tensor(v, dtype=torch.long, device=\"cuda\") for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"] } model_q(**input_torch) trainer = get_trainer(model_q) print(trainer.evaluate()) [INFO|trainer.py:439] 2021-12-28 13:52:09,215 >> Using amp half precision backend {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 46.5572, 'eval_samples_per_second': 210.816, 'eval_steps_per_second': 3.308} {'eval_loss': 0.3939284086227417, 'eval_accuracy': 0.850636780438105, 'eval_runtime': 46.5572, 'eval_samples_per_second': 210.816, 'eval_steps_per_second': 3.308}","title":"Add quantization support to any model"},{"location":"quantization/#per-layer-quantization-analysis","text":"Below we will run a sensitivity analysis, by enabling quantization of one layer at a time and measuring the accuracy. That way we will be able to detect if the quantization of a specific layer has a larger cost on accuracy than other layers. In [11]: Copied! from pytorch_quantization import nn as quant_nn for i in range ( 12 ): layer_name = f \"layer. { i } \" print ( layer_name ) for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if layer_name in name : module . enable_quant () else : module . disable_quant () trainer . evaluate () print ( \"----\" ) from pytorch_quantization import nn as quant_nn for i in range(12): layer_name = f\"layer.{i}\" print(layer_name) for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if layer_name in name: module.enable_quant() else: module.disable_quant() trainer.evaluate() print(\"----\") layer.0 {'eval_loss': 0.35163024067878723, 'eval_accuracy': 0.8663270504330107, 'eval_runtime': 20.695, 'eval_samples_per_second': 474.27, 'eval_steps_per_second': 7.441} ---- layer.1 {'eval_loss': 0.3527306318283081, 'eval_accuracy': 0.8661232806928171, 'eval_runtime': 26.1334, 'eval_samples_per_second': 375.573, 'eval_steps_per_second': 5.893} ---- layer.2 {'eval_loss': 0.3557673394680023, 'eval_accuracy': 0.8629648497198166, 'eval_runtime': 21.1364, 'eval_samples_per_second': 464.366, 'eval_steps_per_second': 7.286} ---- layer.3 {'eval_loss': 0.3551430106163025, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 20.9252, 'eval_samples_per_second': 469.051, 'eval_steps_per_second': 7.36} ---- layer.4 {'eval_loss': 0.35053929686546326, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 21.05, 'eval_samples_per_second': 466.271, 'eval_steps_per_second': 7.316} ---- layer.5 {'eval_loss': 0.35701483488082886, 'eval_accuracy': 0.865206316861946, 'eval_runtime': 20.9236, 'eval_samples_per_second': 469.088, 'eval_steps_per_second': 7.36} ---- layer.6 {'eval_loss': 0.35283517837524414, 'eval_accuracy': 0.8649006622516556, 'eval_runtime': 20.8179, 'eval_samples_per_second': 471.469, 'eval_steps_per_second': 7.397} ---- layer.7 {'eval_loss': 0.35288652777671814, 'eval_accuracy': 0.866632705043301, 'eval_runtime': 20.7823, 'eval_samples_per_second': 472.277, 'eval_steps_per_second': 7.41} ---- layer.8 {'eval_loss': 0.35080182552337646, 'eval_accuracy': 0.8672440142638819, 'eval_runtime': 20.737, 'eval_samples_per_second': 473.308, 'eval_steps_per_second': 7.426} ---- layer.9 {'eval_loss': 0.3503498136997223, 'eval_accuracy': 0.8673458991339786, 'eval_runtime': 20.8899, 'eval_samples_per_second': 469.843, 'eval_steps_per_second': 7.372} ---- layer.10 {'eval_loss': 0.3510246276855469, 'eval_accuracy': 0.8658176260825268, 'eval_runtime': 20.8428, 'eval_samples_per_second': 470.905, 'eval_steps_per_second': 7.389} ---- layer.11 {'eval_loss': 0.3509054183959961, 'eval_accuracy': 0.8656138563423331, 'eval_runtime': 20.8451, 'eval_samples_per_second': 470.853, 'eval_steps_per_second': 7.388} ---- It seems that quantization of layers 2 to 6 has the largest accuracy impact.","title":"Per layer quantization analysis"},{"location":"quantization/#operator-quantization-analysis","text":"Below we will run a sensitivity analysis, by enabling quantization of one operator type at a time and measuring the accuracy. That way we will be able to detect if a specific operator has a larger cost on accuracy. On Roberta we only quantize matmul and LayerNorm , so we test both candidates. In [12]: Copied! for op in [ \"matmul\" , \"layernorm\" ]: for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if op in name : module . enable_quant () else : module . disable_quant () print ( op ) trainer . evaluate () print ( \"----\" ) for op in [\"matmul\", \"layernorm\"]: for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if op in name: module.enable_quant() else: module.disable_quant() print(op) trainer.evaluate() print(\"----\") matmul {'eval_loss': 0.35049352049827576, 'eval_accuracy': 0.8658176260825268, 'eval_runtime': 26.1972, 'eval_samples_per_second': 374.659, 'eval_steps_per_second': 5.878} ---- layernorm {'eval_loss': 0.35847699642181396, 'eval_accuracy': 0.8597045338767193, 'eval_runtime': 24.3004, 'eval_samples_per_second': 403.903, 'eval_steps_per_second': 6.337} ---- It appears that LayerNorm quantization has a significant accuracy cost. Our goal is to disable quantization for as few operations as possible while preserving accuracy as much as possible. Therefore we will try to only disable quantization for LayerNorm on Layers 2 to 6. In [13]: Copied! disable_layer_names = [ \"layer.2\" , \"layer.3\" , \"layer.4\" , \"layer.6\" ] for name , module in model_q . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if any ([ f \" { l } .output.layernorm\" in name for l in disable_layer_names ]): print ( f \"disable { name } \" ) module . disable_quant () else : module . enable_quant () trainer . evaluate () disable_layer_names = [\"layer.2\", \"layer.3\", \"layer.4\", \"layer.6\"] for name, module in model_q.named_modules(): if isinstance(module, quant_nn.TensorQuantizer): if any([f\"{l}.output.layernorm\" in name for l in disable_layer_names]): print(f\"disable {name}\") module.disable_quant() else: module.enable_quant() trainer.evaluate() disable roberta.encoder.layer.2.output.layernorm_quantizer_0 disable roberta.encoder.layer.2.output.layernorm_quantizer_1 disable roberta.encoder.layer.3.output.layernorm_quantizer_0 disable roberta.encoder.layer.3.output.layernorm_quantizer_1 disable roberta.encoder.layer.4.output.layernorm_quantizer_0 disable roberta.encoder.layer.4.output.layernorm_quantizer_1 disable roberta.encoder.layer.6.output.layernorm_quantizer_0 disable roberta.encoder.layer.6.output.layernorm_quantizer_1 {'eval_loss': 0.3660135269165039, 'eval_accuracy': 0.8618441161487519, 'eval_runtime': 45.9324, 'eval_samples_per_second': 213.684, 'eval_steps_per_second': 3.353} Out[13]: {'eval_loss': 0.3660135269165039, 'eval_accuracy': 0.8618441161487519, 'eval_runtime': 45.9324, 'eval_samples_per_second': 213.684, 'eval_steps_per_second': 3.353} By just disabling quantization for a single operator on a few layers, we keep most of the performance boost (quantization) but retrieve more than 1 point of accuracy. It's also possible to perform an analysis per quantizer to get a smaller granularity but it's a bit slow to run. If we stop here, it's called a Post Training Quantization (PTQ). Below, we will try to retrieve even more accuracy.","title":"Operator quantization analysis"},{"location":"quantization/#quantization-aware-training-qat","text":"We retrain the model with 1/10 or 1/100 of the original learning rate. Our goal is to retrieve most of the original accuracy. In [14]: Copied! args . learning_rate = 1e-7 trainer = get_trainer ( model_q ) trainer . train () print ( trainer . evaluate ()) model_q . save_pretrained ( \"model-qat\" ) args.learning_rate = 1e-7 trainer = get_trainer(model_q) trainer.train() print(trainer.evaluate()) model_q.save_pretrained(\"model-qat\") [INFO|trainer.py:439] 2021-12-28 13:54:41,146 >> Using amp half precision backend {'loss': 0.3591, 'learning_rate': 9.188396349413298e-08, 'epoch': 0.08} {'eval_loss': 0.3738575875759125, 'eval_accuracy': 0.8596026490066225, 'eval_runtime': 46.992, 'eval_samples_per_second': 208.865, 'eval_steps_per_second': 3.277, 'epoch': 0.08} {'loss': 0.3182, 'learning_rate': 8.373533246414603e-08, 'epoch': 0.16} {'eval_loss': 0.38133203983306885, 'eval_accuracy': 0.8586856851757514, 'eval_runtime': 45.7335, 'eval_samples_per_second': 214.613, 'eval_steps_per_second': 3.367, 'epoch': 0.16} {'loss': 0.3062, 'learning_rate': 7.558670143415906e-08, 'epoch': 0.24} {'eval_loss': 0.3903615176677704, 'eval_accuracy': 0.8592969943963321, 'eval_runtime': 45.6544, 'eval_samples_per_second': 214.985, 'eval_steps_per_second': 3.373, 'epoch': 0.24} {'loss': 0.2986, 'learning_rate': 6.74380704041721e-08, 'epoch': 0.33} {'eval_loss': 0.39669597148895264, 'eval_accuracy': 0.8577687213448802, 'eval_runtime': 45.6583, 'eval_samples_per_second': 214.966, 'eval_steps_per_second': 3.373, 'epoch': 0.33} {'loss': 0.2994, 'learning_rate': 5.9289439374185136e-08, 'epoch': 0.41} {'eval_loss': 0.394754558801651, 'eval_accuracy': 0.8612328069281712, 'eval_runtime': 45.6439, 'eval_samples_per_second': 215.034, 'eval_steps_per_second': 3.374, 'epoch': 0.41} {'loss': 0.3027, 'learning_rate': 5.1148956975228164e-08, 'epoch': 0.49} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.7496, 'eval_samples_per_second': 214.538, 'eval_steps_per_second': 3.366, 'epoch': 0.49} {'loss': 0.3164, 'learning_rate': 4.300032594524119e-08, 'epoch': 0.57} {'eval_loss': 0.39596375823020935, 'eval_accuracy': 0.8609271523178808, 'eval_runtime': 45.6669, 'eval_samples_per_second': 214.926, 'eval_steps_per_second': 3.372, 'epoch': 0.57} {'loss': 0.3298, 'learning_rate': 3.485984354628422e-08, 'epoch': 0.65} {'eval_loss': 0.3958113491535187, 'eval_accuracy': 0.8599083036169128, 'eval_runtime': 45.6637, 'eval_samples_per_second': 214.941, 'eval_steps_per_second': 3.372, 'epoch': 0.65} {'loss': 0.3379, 'learning_rate': 2.6719361147327245e-08, 'epoch': 0.73} {'eval_loss': 0.39275360107421875, 'eval_accuracy': 0.8577687213448802, 'eval_runtime': 45.7659, 'eval_samples_per_second': 214.461, 'eval_steps_per_second': 3.365, 'epoch': 0.73} {'loss': 0.354, 'learning_rate': 1.8570730117340286e-08, 'epoch': 0.81} {'eval_loss': 0.39236611127853394, 'eval_accuracy': 0.8562404482934284, 'eval_runtime': 45.6972, 'eval_samples_per_second': 214.784, 'eval_steps_per_second': 3.37, 'epoch': 0.81} {'loss': 0.3826, 'learning_rate': 1.0422099087353324e-08, 'epoch': 0.9} {'eval_loss': 0.389812171459198, 'eval_accuracy': 0.8620478858889455, 'eval_runtime': 45.6861, 'eval_samples_per_second': 214.836, 'eval_steps_per_second': 3.371, 'epoch': 0.9} {'loss': 0.4363, 'learning_rate': 2.2734680573663624e-09, 'epoch': 0.98} {'eval_loss': 0.3902811110019684, 'eval_accuracy': 0.8583800305654611, 'eval_runtime': 45.6787, 'eval_samples_per_second': 214.87, 'eval_steps_per_second': 3.371, 'epoch': 0.98} {'train_runtime': 4893.2972, 'train_samples_per_second': 80.253, 'train_steps_per_second': 2.508, 'train_loss': 0.33914821741633805, 'epoch': 1.0} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.6231, 'eval_samples_per_second': 215.132, 'eval_steps_per_second': 3.375, 'epoch': 1.0} {'eval_loss': 0.39516741037368774, 'eval_accuracy': 0.8626591951095263, 'eval_runtime': 45.6231, 'eval_samples_per_second': 215.132, 'eval_steps_per_second': 3.375, 'epoch': 1.0}","title":"Quantization Aware Training (QAT)"},{"location":"quantization/#export-a-qdq-pytorch-model-to-onnx","text":"We need to enable fake quantization mode from Pytorch. In [15]: Copied! data = encoded_dataset [ \"train\" ][ 1 : 3 ] input_torch = convert_tensor ( data , output = \"torch\" ) convert_to_onnx ( model_pytorch = model_q , output_path = \"model_qat.onnx\" , inputs_pytorch = input_torch , quantization = True ) data = encoded_dataset[\"train\"][1:3] input_torch = convert_tensor(data, output=\"torch\") convert_to_onnx(model_pytorch=model_q, output_path=\"model_qat.onnx\", inputs_pytorch=input_torch, quantization=True) /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! inputs, amax.item() / bound, 0, /home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs! quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0]) In [16]: Copied! del model_q QATCalibrate . restore () del model_q QATCalibrate.restore()","title":"Export a QDQ Pytorch model to ONNX"},{"location":"quantization/#benchmark","text":"","title":"Benchmark"},{"location":"quantization/#convert-onnx-graph-to-tensorrt-engine","text":"In [17]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"model_qat.onnx\" , logger = trt_logger , min_shape = ( 1 , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = True , ) engine = build_engine( runtime=runtime, onnx_file_path=\"model_qat.onnx\", logger=trt_logger, min_shape=(1, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=True, ) In [18]: Copied! # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\" # same as above, but from the terminal # !/usr/src/tensorrt/bin/trtexec --onnx=model_qat.onnx --shapes=input_ids:32x256,attention_mask:32x256 --best --workspace=10000 --saveEngine=\"test.plan\"","title":"Convert ONNX graph to TensorRT engine"},{"location":"quantization/#prepare-input-and-output-buffer","text":"In [19]: Copied! stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] stream: Stream = pycuda.driver.Stream() context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] In [20]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\")","title":"Prepare input and output buffer"},{"location":"quantization/#inference-on-tensorrt","text":"We first check that inference is working correctly: In [21]: Copied! tensorrt_output = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print ( tensorrt_output ) tensorrt_output = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print(tensorrt_output) [array([[ 0.11111109, 2.9936233 , -2.5243347 ], [ 3.2135723 , -0.4374885 , -2.4485767 ], [ 2.1678474 , -1.1477091 , -0.7798154 ], [ 1.8148003 , -0.2093072 , -1.416711 ], [ 2.3070638 , 0.27601779, -2.2818418 ], [ 4.1799006 , -0.83163625, -2.8492923 ], [-3.695277 , 2.3409832 , 1.4314314 ], [ 4.1796045 , -1.0709951 , -2.6119678 ], [-0.44781622, -1.4288648 , 1.888488 ], [-2.9845483 , -1.5895646 , 4.117529 ], [ 3.9293122 , -0.68528754, -2.9477124 ], [-2.516609 , 0.34680495, 2.2793124 ], [-3.0710464 , 3.3439813 , 0.08079423], [-2.2859852 , 1.9546673 , 0.37908432], [ 0.3999826 , -1.0603418 , 0.5099453 ], [ 2.9247677 , -0.6867883 , -1.7499886 ], [ 4.1125493 , -0.7771612 , -2.986419 ], [-2.58058 , -2.3291597 , 4.553415 ], [-3.215447 , -1.3902456 , 4.2499046 ], [-2.014185 , 4.117433 , -1.634403 ], [ 4.051285 , -0.64716065, -2.9019048 ], [ 3.742484 , -0.07188296, -3.272956 ], [-3.302061 , -1.0159078 , 3.9711204 ], [ 3.9316242 , -0.33764294, -3.209711 ], [ 3.9900765 , -1.5201662 , -2.1166122 ], [-1.2437494 , 1.410141 , -0.10993958], [-3.1267605 , -0.8212991 , 3.6917076 ], [-2.0607114 , 4.1098857 , -1.4996963 ], [-3.5770578 , -0.736545 , 3.9671996 ], [ 3.776105 , -0.60771704, -2.8707912 ], [ 3.5450761 , -0.14414684, -2.9718893 ], [ 3.4713674 , 0.12106885, -3.189211 ]], dtype=float32)] Measure of the accuracy: In [22]: Copied! infer_trt = lambda inputs : infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) measure_accuracy ( infer = infer_trt , int64 = False ) infer_trt = lambda inputs: infer_tensorrt( context=context, host_inputs=inputs, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) measure_accuracy(infer=infer_trt, int64=False) Out[22]: 0.8629648497198166 Latency measures: In [23]: Copied! time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print_timings ( name = \"TensorRT (INT-8)\" , timings = time_buffer ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print_timings(name=\"TensorRT (INT-8)\", timings=time_buffer) [TensorRT (INT-8)] mean=17.01ms, sd=1.14ms, min=16.64ms, max=27.17ms, median=16.76ms, 95p=18.16ms, 99p=20.06ms In [24]: Copied! del engine , context del engine, context","title":"Inference on TensorRT"},{"location":"quantization/#pytorch-baseline","text":"Time to get some numbers to compare with.","title":"Pytorch baseline"},{"location":"quantization/#gpu-execution","text":"We will measure vanilla Pytorch inference on both FP32 and FP16 precision on GPU, it will be our baseline: In [25]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) with torch . inference_mode (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32)\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") with torch.inference_mode(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32)\", timings=time_buffer) [Pytorch (FP32)] mean=76.87ms, sd=1.24ms, min=75.68ms, max=82.38ms, median=76.52ms, 95p=79.29ms, 99p=81.90ms In [26]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 30 ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16)\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(30): _ = baseline_model(**input_torch) torch.cuda.synchronize() time_buffer = [] for _ in range(100): with track_infer_time(time_buffer): _ = baseline_model(**input_torch) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16)\", timings=time_buffer) del baseline_model [Pytorch (FP16)] mean=56.24ms, sd=0.67ms, min=55.53ms, max=59.61ms, median=56.05ms, 95p=57.80ms, 99p=58.18ms","title":"GPU execution"},{"location":"quantization/#cpu-execution","text":"In [27]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . eval () data = encoded_dataset [ \"train\" ][ 0 : batch_size ] input_torch : OD [ str , torch . Tensor ] = convert_tensor ( data = data , output = \"torch\" ) input_torch_cpu = { k : v . to ( \"cpu\" ) for k , v in input_torch . items ()} torch . set_num_threads ( os . cpu_count ()) with torch . inference_mode (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP32) - CPU\" , timings = time_buffer ) baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.eval() data = encoded_dataset[\"train\"][0:batch_size] input_torch: OD[str, torch.Tensor] = convert_tensor(data=data, output=\"torch\") input_torch_cpu = {k: v.to(\"cpu\") for k, v in input_torch.items()} torch.set_num_threads(os.cpu_count()) with torch.inference_mode(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP32) - CPU\", timings=time_buffer) [Pytorch (FP32) - CPU] mean=4267.96ms, sd=249.08ms, min=3959.59ms, max=4697.79ms, median=4299.22ms, 95p=4632.12ms, 99p=4684.66ms In [28]: Copied! with torch . inference_mode (): with torch . cuda . amp . autocast (): for _ in range ( 3 ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = [] for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (FP16) - CPU\" , timings = time_buffer ) del baseline_model with torch.inference_mode(): with torch.cuda.amp.autocast(): for _ in range(3): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = [] for _ in range(10): with track_infer_time(time_buffer): _ = baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (FP16) - CPU\", timings=time_buffer) del baseline_model [Pytorch (FP16) - CPU] mean=4428.94ms, sd=225.39ms, min=4148.26ms, max=4871.84ms, median=4404.70ms, 95p=4781.81ms, 99p=4853.83ms Below, we will perform dynamic quantization on CPU. In [29]: Copied! quantized_baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) quantized_baseline_model = quantized_baseline_model . eval () quantized_baseline_model = torch . quantization . quantize_dynamic ( quantized_baseline_model , { torch . nn . Linear }, dtype = torch . qint8 ) with torch . inference_mode (): for _ in range ( 3 ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () time_buffer = list () for _ in range ( 10 ): with track_infer_time ( time_buffer ): _ = quantized_baseline_model ( ** input_torch_cpu ) torch . cuda . synchronize () print_timings ( name = \"Pytorch (INT-8) - CPU\" , timings = time_buffer ) quantized_baseline_model = AutoModelForSequenceClassification.from_pretrained( \"model_trained_fp16\", num_labels=num_labels ) quantized_baseline_model = quantized_baseline_model.eval() quantized_baseline_model = torch.quantization.quantize_dynamic( quantized_baseline_model, {torch.nn.Linear}, dtype=torch.qint8 ) with torch.inference_mode(): for _ in range(3): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() time_buffer = list() for _ in range(10): with track_infer_time(time_buffer): _ = quantized_baseline_model(**input_torch_cpu) torch.cuda.synchronize() print_timings(name=\"Pytorch (INT-8) - CPU\", timings=time_buffer) [Pytorch (INT-8) - CPU] mean=3299.66ms, sd=37.76ms, min=3274.33ms, max=3405.91ms, median=3285.20ms, 95p=3366.88ms, 99p=3398.10ms","title":"CPU execution"},{"location":"quantization/#tensorrt-baseline","text":"Below we export our finetuned model, the purpose is to only check the performance on mixed precision (FP16, no quantization). In [30]: Copied! baseline_model = AutoModelForSequenceClassification . from_pretrained ( \"model_trained_fp16\" , num_labels = num_labels ) baseline_model = baseline_model . cuda () convert_to_onnx ( baseline_model , output_path = \"baseline.onnx\" , inputs_pytorch = input_torch , quantization = False ) del baseline_model baseline_model = AutoModelForSequenceClassification.from_pretrained(\"model_trained_fp16\", num_labels=num_labels) baseline_model = baseline_model.cuda() convert_to_onnx(baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch, quantization=False) del baseline_model In [31]: Copied! engine = build_engine ( runtime = runtime , onnx_file_path = \"baseline.onnx\" , logger = trt_logger , min_shape = ( batch_size , max_seq_len ), optimal_shape = ( batch_size , max_seq_len ), max_shape = ( batch_size , max_seq_len ), workspace_size = 10000 * 1024 * 1024 , fp16 = True , int8 = False , ) input_np : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) stream : Stream = pycuda . driver . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] for _ in range ( 30 ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) time_buffer = list () for _ in range ( 100 ): with track_infer_time ( time_buffer ): _ = infer_tensorrt ( context = context , host_inputs = input_np , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) print_timings ( name = \"TensorRT (FP16)\" , timings = time_buffer ) del engine , context engine = build_engine( runtime=runtime, onnx_file_path=\"baseline.onnx\", logger=trt_logger, min_shape=(batch_size, max_seq_len), optimal_shape=(batch_size, max_seq_len), max_shape=(batch_size, max_seq_len), workspace_size=10000 * 1024 * 1024, fp16=True, int8=False, ) input_np: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") stream: Stream = pycuda.driver.Stream() context: IExecutionContext = engine.create_execution_context() context.set_optimization_profile_async(profile_index=profile_index, stream_handle=stream.handle) input_binding_idxs, output_binding_idxs = get_binding_idxs(engine, profile_index) # type: List[int], List[int] for _ in range(30): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) time_buffer = list() for _ in range(100): with track_infer_time(time_buffer): _ = infer_tensorrt( context=context, host_inputs=input_np, input_binding_idxs=input_binding_idxs, output_binding_idxs=output_binding_idxs, stream=stream, ) print_timings(name=\"TensorRT (FP16)\", timings=time_buffer) del engine, context [TensorRT (FP16)] mean=29.90ms, sd=0.82ms, min=29.30ms, max=33.41ms, median=29.69ms, 95p=31.85ms, 99p=32.79ms","title":"TensorRT baseline"},{"location":"quantization/#onnx-runtime-baseline","text":"ONNX Runtime is the go to inference solution from Microsoft. The recent 1.10 version of ONNX Runtime (with TensorRT support) is still a bit buggy on transformer models, that is why we use the 1.9.0 version in the measures below. As before, CPU quantization is dynamic. Function will set ONNX Runtime to use all cores available and enable any possible optimizations. In [32]: Copied! optimize_onnx ( onnx_path = \"baseline.onnx\" , onnx_optim_model_path = \"baseline-optimized.onnx\" , fp16 = True , use_cuda = True , ) optimize_onnx( onnx_path=\"baseline.onnx\", onnx_optim_model_path=\"baseline-optimized.onnx\", fp16=True, use_cuda=True, ) In [33]: Copied! cpu_quantization ( input_model_path = \"baseline-optimized.onnx\" , output_model_path = \"baseline-quantized.onnx\" ) cpu_quantization(input_model_path=\"baseline-optimized.onnx\", output_model_path=\"baseline-quantized.onnx\") Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for th Ignore MatMul due to non constant B: /[MatMul_113] Ignore MatMul due to non constant B: /[MatMul_127] Ignore MatMul due to non constant B: /[MatMul_137] Ignore MatMul due to non constant B: /[MatMul_207] Ignore MatMul due to non constant B: /[MatMul_221] Ignore MatMul due to non constant B: /[MatMul_231] Ignore MatMul due to non constant B: /[MatMul_301] Ignore MatMul due to non constant B: /[MatMul_315] Ignore MatMul due to non constant B: /[MatMul_325] Ignore MatMul due to non constant B: /[MatMul_395] Ignore MatMul due to non constant B: /[MatMul_409] Ignore MatMul due to non constant B: /[MatMul_419] Ignore MatMul due to non constant B: /[MatMul_489] Ignore MatMul due to non constant B: /[MatMul_503] Ignore MatMul due to non constant B: /[MatMul_513] Ignore MatMul due to non constant B: /[MatMul_583] Ignore MatMul due to non constant B: /[MatMul_597] Ignore MatMul due to non constant B: /[MatMul_607] Ignore MatMul due to non constant B: /[MatMul_677] Ignore MatMul due to non constant B: /[MatMul_691] Ignore MatMul due to non constant B: /[MatMul_701] Ignore MatMul due to non constant B: /[MatMul_771] Ignore MatMul due to non constant B: /[MatMul_785] Ignore MatMul due to non constant B: /[MatMul_795] Ignore MatMul due to non constant B: /[MatMul_865] Ignore MatMul due to non constant B: /[MatMul_879] Ignore MatMul due to non constant B: /[MatMul_889] Ignore MatMul due to non constant B: /[MatMul_959] Ignore MatMul due to non constant B: /[MatMul_973] Ignore MatMul due to non constant B: /[MatMul_983] Ignore MatMul due to non constant B: /[MatMul_1053] Ignore MatMul due to non constant B: /[MatMul_1067] Ignore MatMul due to non constant B: /[MatMul_1077] Ignore MatMul due to non constant B: /[MatMul_1147] Ignore MatMul due to non constant B: /[MatMul_1161] Ignore MatMul due to non constant B: /[MatMul_1171] Ignore MatMul due to non constant B: /[Gemm_1187_MatMul] Ignore MatMul due to non constant B: /[Gemm_1189_MatMul] is operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator Attention. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. Warning: Unsupported operator BiasGelu. No schema registered for this operator. Warning: Unsupported operator SkipLayerNormalization. No schema registered for this operator. In [34]: Copied! labels = [ item [ \"label\" ] for item in encoded_dataset [ validation_key ]] data = encoded_dataset [ validation_key ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for k , v in inputs_onnx . items (): inputs_onnx [ k ] = v . astype ( np . int64 ) model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) output = model . run ( None , inputs_onnx ) labels = [item[\"label\"] for item in encoded_dataset[validation_key]] data = encoded_dataset[validation_key][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for k, v in inputs_onnx.items(): inputs_onnx[k] = v.astype(np.int64) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") output = model.run(None, inputs_onnx) In [35]: Copied! data = encoded_dataset [ \"train\" ][ 0 : batch_size ] inputs_onnx : OD [ str , np . ndarray ] = convert_tensor ( data = data , output = \"np\" ) for k , v in inputs_onnx . items (): inputs_onnx [ k ] = v . astype ( np . int64 ) for provider , model_path , benchmark_name , warmup , nb_inference in [ ( \"CUDAExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime GPU (FP32)\" , 10 , 100 ), ( \"CUDAExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime GPU (FP16)\" , 10 , 100 ), ( \"CPUExecutionProvider\" , \"baseline.onnx\" , \"ONNX Runtime CPU (FP32)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-optimized.onnx\" , \"ONNX Runtime CPU (FP16)\" , 3 , 10 ), ( \"CPUExecutionProvider\" , \"baseline-quantized.onnx\" , \"ONNX Runtime CPU (INT-8)\" , 3 , 10 ), ]: model = create_model_for_provider ( path = model_path , provider_to_use = provider ) for _ in range ( warmup ): _ = model . run ( None , inputs_onnx ) time_buffer = [] for _ in range ( nb_inference ): with track_infer_time ( time_buffer ): _ = model . run ( None , inputs_onnx ) print_timings ( name = benchmark_name , timings = time_buffer ) del model data = encoded_dataset[\"train\"][0:batch_size] inputs_onnx: OD[str, np.ndarray] = convert_tensor(data=data, output=\"np\") for k, v in inputs_onnx.items(): inputs_onnx[k] = v.astype(np.int64) for provider, model_path, benchmark_name, warmup, nb_inference in [ (\"CUDAExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime GPU (FP32)\", 10, 100), (\"CUDAExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime GPU (FP16)\", 10, 100), (\"CPUExecutionProvider\", \"baseline.onnx\", \"ONNX Runtime CPU (FP32)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-optimized.onnx\", \"ONNX Runtime CPU (FP16)\", 3, 10), (\"CPUExecutionProvider\", \"baseline-quantized.onnx\", \"ONNX Runtime CPU (INT-8)\", 3, 10), ]: model = create_model_for_provider(path=model_path, provider_to_use=provider) for _ in range(warmup): _ = model.run(None, inputs_onnx) time_buffer = [] for _ in range(nb_inference): with track_infer_time(time_buffer): _ = model.run(None, inputs_onnx) print_timings(name=benchmark_name, timings=time_buffer) del model [ONNX Runtime GPU (FP32)] mean=76.38ms, sd=4.99ms, min=73.10ms, max=91.05ms, median=73.91ms, 95p=88.30ms, 99p=89.42ms [ONNX Runtime GPU (FP16)] mean=34.21ms, sd=1.68ms, min=33.23ms, max=41.80ms, median=33.70ms, 95p=38.87ms, 99p=40.63ms [ONNX Runtime CPU (FP32)] mean=4023.32ms, sd=92.76ms, min=3895.51ms, max=4267.63ms, median=4013.27ms, 95p=4170.44ms, 99p=4248.19ms [ONNX Runtime CPU (FP16)] mean=3956.61ms, sd=167.65ms, min=3709.88ms, max=4188.62ms, median=3914.53ms, 95p=4180.81ms, 99p=4187.06ms [ONNX Runtime CPU (INT-8)] mean=3336.29ms, sd=168.96ms, min=3170.64ms, max=3765.07ms, median=3299.52ms, 95p=3641.01ms, 99p=3740.26ms Measure of the accuracy with ONNX Runtime engine and CUDA provider: In [36]: Copied! model = create_model_for_provider ( path = \"baseline.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[36]: 0.8663270504330107 In [37]: Copied! model = create_model_for_provider ( path = \"baseline-optimized.onnx\" , provider_to_use = \"CUDAExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline-optimized.onnx\", provider_to_use=\"CUDAExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[37]: 0.8663270504330107 In [38]: Copied! model = create_model_for_provider ( path = \"baseline-quantized.onnx\" , provider_to_use = \"CPUExecutionProvider\" ) infer_ort = lambda tokens : model . run ( None , tokens ) measure_accuracy ( infer = infer_ort , int64 = True ) model = create_model_for_provider(path=\"baseline-quantized.onnx\", provider_to_use=\"CPUExecutionProvider\") infer_ort = lambda tokens: model.run(None, tokens) measure_accuracy(infer=infer_ort, int64=True) Out[38]: 0.8650025471217524 In [39]: Copied! del model del model In [40]: Copied!","title":"ONNX Runtime baseline"},{"location":"quantization_ast/","text":"Add quantization support to any model # The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. That way, quantization will work out of the box for the final user. The process is based on Python AST modification, basically we parse the model source code in RAM, we convert it to a tree, then we patch the tree to add the QDQ nodes and we replace, still in RAM, the original module source code. Our library also offer the option to restore original behavior. In theory it works for any model. However, not related to quantization, some models are not fully compliant with TensorRT (unsupported operators, etc.). For those models, we rewrite some part of the source code, these patches are manually written but are applied to the model at run time (like the AST manipulation). Info concrete examples on Roberta architecture: in Hugging Face library, there is a cumsum operator used during the position embedding generation. Something very simple. It takes as input an integer tensor and output an integer tensor. It happens that the cumsum operator from TensorRT supports float but not integer (https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md). It leads to a crash during the model conversion with a strange error message. Converting the input to float tensor fixes the issue. The process is simple: Calibrate, after that you have a PTQ second fine tuning, it's the QAT (optional) Info there are many ways to get a QDQ model, you can modify Pytorch source code (including doing it at runtime like here), patch ONNX graph (this approach is used at Microsoft for instance but only support PTQ , not QAT as ONNX file can't be trained on Pytorch for now) or leverage the new FX Pytorch interface (it's a bit experimental and it seems to miss some feature to support Nvidia QAT library). Modifying the source code is the most straightforward, and doing it through AST is the least intrusive (no need to duplicate the work of HF). Concretly, minimal user code looks like that: apply_quantization.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from transformers import AutoModelForSequenceClassification from transformer_deploy.QDQModels.calibration_utils import QATCalibrate my_data_loader = ... with QATCalibrate () as qat : model_q = AutoModelForSequenceClassification . from_pretrained ( \"my/model\" ) model_q . cuda () qat . setup_model_qat ( model_q ) with torch . no_grad (): for data in my_data_loader : model_q ( ** data ) # <- calibration happens here model_q . save_model_to_file ( \"path/to/somewhere\" ) ... and then in shell convert_model -m path/to/somewhere --quantization ... Info The first context manager will enable quantization support, and the setup_model_qat() method will add the QDQ nodes.","title":"How is it implemented?"},{"location":"quantization_ast/#add-quantization-support-to-any-model","text":"The idea is to take the source code of a specific model and add automatically QDQ nodes. QDQ nodes will be placed before and after an operation that we want to quantize, that\u2019s inside these nodes that the information to perform the mapping between high precision and low precision number is stored. That way, quantization will work out of the box for the final user. The process is based on Python AST modification, basically we parse the model source code in RAM, we convert it to a tree, then we patch the tree to add the QDQ nodes and we replace, still in RAM, the original module source code. Our library also offer the option to restore original behavior. In theory it works for any model. However, not related to quantization, some models are not fully compliant with TensorRT (unsupported operators, etc.). For those models, we rewrite some part of the source code, these patches are manually written but are applied to the model at run time (like the AST manipulation). Info concrete examples on Roberta architecture: in Hugging Face library, there is a cumsum operator used during the position embedding generation. Something very simple. It takes as input an integer tensor and output an integer tensor. It happens that the cumsum operator from TensorRT supports float but not integer (https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md). It leads to a crash during the model conversion with a strange error message. Converting the input to float tensor fixes the issue. The process is simple: Calibrate, after that you have a PTQ second fine tuning, it's the QAT (optional) Info there are many ways to get a QDQ model, you can modify Pytorch source code (including doing it at runtime like here), patch ONNX graph (this approach is used at Microsoft for instance but only support PTQ , not QAT as ONNX file can't be trained on Pytorch for now) or leverage the new FX Pytorch interface (it's a bit experimental and it seems to miss some feature to support Nvidia QAT library). Modifying the source code is the most straightforward, and doing it through AST is the least intrusive (no need to duplicate the work of HF). Concretly, minimal user code looks like that: apply_quantization.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from transformers import AutoModelForSequenceClassification from transformer_deploy.QDQModels.calibration_utils import QATCalibrate my_data_loader = ... with QATCalibrate () as qat : model_q = AutoModelForSequenceClassification . from_pretrained ( \"my/model\" ) model_q . cuda () qat . setup_model_qat ( model_q ) with torch . no_grad (): for data in my_data_loader : model_q ( ** data ) # <- calibration happens here model_q . save_model_to_file ( \"path/to/somewhere\" ) ... and then in shell convert_model -m path/to/somewhere --quantization ... Info The first context manager will enable quantization support, and the setup_model_qat() method will add the QDQ nodes.","title":"Add quantization support to any model"},{"location":"quantization_intro/","text":"Nvidia GPU INT-8 quantization # What is it about? # Quantization is one of the most effective and generic approaches to make model inference faster. Basically, it replaces high precision float numbers in model tensors encoded in 32 or 16 bits by lower precision ones encoded in 8 bits or less: it takes less memory computation is easier / faster It can be applied to any model in theory, and, if done well, it should maintain accuracy. The purpose of this notebook is to show a process to perform quantization on any Transformer architectures. Moreover, the library is designed to offer a simple API and still let advanced users tweak the algorithm. Benchmark # TL;DR We benchmarked Pytorch and Nvidia TensorRT, on both CPU and GPU , with/without quantization, our methods provide the fastest inference by large margin. Framework Precision Latency (ms) Accuracy Speedup Hardware Pytorch FP32 4267 86.6 % X 0.02 CPU Pytorch FP16 4428 86.6 % X 0.02 CPU Pytorch INT-8 3300 85.9 % X 0.02 CPU Pytorch FP32 77 86.6 % X 1 GPU Pytorch FP16 56 86.6 % X 1.38 GPU ONNX Runtime FP32 76 86.6 % X 1.01 GPU ONNX Runtime FP16 34 86.6 % X 2.26 GPU ONNX Runtime FP32 4023 86.6 % X 0.02 CPU ONNX Runtime FP16 3957 86.6 % X 0.02 CPU ONNX Runtime INT-8 3336 86.5 % X 0.02 CPU TensorRT FP16 30 86.6 % X 2.57 GPU TensorRT ( our method ) INT-8 17 86.2 % X 4.53 GPU Note measures done on a Nvidia RTX 3090 GPU + 12 cores i7 Intel CPU (support AVX-2 instruction) Roberta base architecture flavor with batch of size 32 / seq len 256, similar results obtained for other sizes/seq len not included in the table. Accuracy obtained after a single epoch, no LR search or any hyper parameter optimization Check the end to end demo to see where these numbers are from.","title":"Why using quantization?"},{"location":"quantization_intro/#nvidia-gpu-int-8-quantization","text":"","title":"Nvidia GPU INT-8 quantization"},{"location":"quantization_intro/#what-is-it-about","text":"Quantization is one of the most effective and generic approaches to make model inference faster. Basically, it replaces high precision float numbers in model tensors encoded in 32 or 16 bits by lower precision ones encoded in 8 bits or less: it takes less memory computation is easier / faster It can be applied to any model in theory, and, if done well, it should maintain accuracy. The purpose of this notebook is to show a process to perform quantization on any Transformer architectures. Moreover, the library is designed to offer a simple API and still let advanced users tweak the algorithm.","title":"What is it about?"},{"location":"quantization_intro/#benchmark","text":"TL;DR We benchmarked Pytorch and Nvidia TensorRT, on both CPU and GPU , with/without quantization, our methods provide the fastest inference by large margin. Framework Precision Latency (ms) Accuracy Speedup Hardware Pytorch FP32 4267 86.6 % X 0.02 CPU Pytorch FP16 4428 86.6 % X 0.02 CPU Pytorch INT-8 3300 85.9 % X 0.02 CPU Pytorch FP32 77 86.6 % X 1 GPU Pytorch FP16 56 86.6 % X 1.38 GPU ONNX Runtime FP32 76 86.6 % X 1.01 GPU ONNX Runtime FP16 34 86.6 % X 2.26 GPU ONNX Runtime FP32 4023 86.6 % X 0.02 CPU ONNX Runtime FP16 3957 86.6 % X 0.02 CPU ONNX Runtime INT-8 3336 86.5 % X 0.02 CPU TensorRT FP16 30 86.6 % X 2.57 GPU TensorRT ( our method ) INT-8 17 86.2 % X 4.53 GPU Note measures done on a Nvidia RTX 3090 GPU + 12 cores i7 Intel CPU (support AVX-2 instruction) Roberta base architecture flavor with batch of size 32 / seq len 256, similar results obtained for other sizes/seq len not included in the table. Accuracy obtained after a single epoch, no LR search or any hyper parameter optimization Check the end to end demo to see where these numbers are from.","title":"Benchmark"},{"location":"quantization_ptq/","text":"Post Training Quantization ( PTQ ) and Quantization Aware Training ( QAT ) # What is it? # A PTQ is basically a fine tuned model where we add quantization nodes and that we calibrate. Calibration is a key step in the static quantization process. Its quality depends on the final accuracy (the inference speed will stay the same). Moreover, a good PTQ is a good basis for a good Quantization Aware Training ( QAT ). By calling with QATCalibrate(...) as qat: , the lib will patch Transformer model AST (source code) in RAM, basically adding quantization support to each model. How to tune a PTQ ? # One of the things we try to guess during the calibration is what range of tensor values capture most of the information stored in the tensor. Indeed, a FP32 tensor can store at the same time very large and very small values, we obviously can't do the same with a 8-bits integer tensors and a scale. An 8-bits integer can only encode 255 values so we need to fix some limits and say, if a value is outside our limits, it just takes a maximum value instead of its real one. For instance, if we say our range is -1000 to +1000 and a tensor contains the value +4000, it will be replaced by the maximum value, +1000. As said before, we will use the histogram method to find the perfect range. We also need to choose a percentile. Usually, you will choose something very close to 100. Danger If the percentile is too small, we put too many values outside the covered range. Values outside the range will be replaced by a single maximum value and you lose some granularity in model weights. If the percentile is too big, your range will be very large and because 8-bits signed integers can only encode values between -127 to +127, even when you use a scale you lose in granularity. Therefore, we in our demo, we launched a grid search on percentile hyper parameter and retrived 1 accuracy point with very little effort. One step further, the QAT # If it's not enough, the last step is to just fine tune a second the model. It helps to retrieve a bit of accuracy.","title":"PTQ and QAT, who are they?"},{"location":"quantization_ptq/#post-training-quantization-ptq-and-quantization-aware-training-qat","text":"","title":"Post Training Quantization (PTQ) and Quantization Aware Training (QAT)"},{"location":"quantization_ptq/#what-is-it","text":"A PTQ is basically a fine tuned model where we add quantization nodes and that we calibrate. Calibration is a key step in the static quantization process. Its quality depends on the final accuracy (the inference speed will stay the same). Moreover, a good PTQ is a good basis for a good Quantization Aware Training ( QAT ). By calling with QATCalibrate(...) as qat: , the lib will patch Transformer model AST (source code) in RAM, basically adding quantization support to each model.","title":"What is it?"},{"location":"quantization_ptq/#how-to-tune-a-ptq","text":"One of the things we try to guess during the calibration is what range of tensor values capture most of the information stored in the tensor. Indeed, a FP32 tensor can store at the same time very large and very small values, we obviously can't do the same with a 8-bits integer tensors and a scale. An 8-bits integer can only encode 255 values so we need to fix some limits and say, if a value is outside our limits, it just takes a maximum value instead of its real one. For instance, if we say our range is -1000 to +1000 and a tensor contains the value +4000, it will be replaced by the maximum value, +1000. As said before, we will use the histogram method to find the perfect range. We also need to choose a percentile. Usually, you will choose something very close to 100. Danger If the percentile is too small, we put too many values outside the covered range. Values outside the range will be replaced by a single maximum value and you lose some granularity in model weights. If the percentile is too big, your range will be very large and because 8-bits signed integers can only encode values between -127 to +127, even when you use a scale you lose in granularity. Therefore, we in our demo, we launched a grid search on percentile hyper parameter and retrived 1 accuracy point with very little effort.","title":"How to tune a PTQ?"},{"location":"quantization_ptq/#one-step-further-the-qat","text":"If it's not enough, the last step is to just fine tune a second the model. It helps to retrieve a bit of accuracy.","title":"One step further, the QAT"},{"location":"quantization_theory/","text":"Some theory # A (very) short intro to INT-8 quantization # Basic idea behind model quantization is to replace tensors made of float numbers (usually encoded on 32 bits) by lower precision representation (integers encoded on 8 bits for Nvidia GPUs). Therefore computation is faster and model memory footprint is lower. Making tensor storage smaller makes memory transfer faster... and is also a source of computation acceleration. This approach is very interesting for its trade-off: you reduce inference time significantly, and it costs close to nothing in accuracy. Replacing float numbers by integers is done through a mapping. This step is called calibration , and its purpose is to compute for each tensor or each channel of a tensor (one of its dimensions) a range covering most weights and then define a scale and a distribution center to map float numbers to 8 bits integers. There are several ways to perform quantization, depending of how and when the calibration is performed: dynamically: the mapping is done online, during the inference, there are some overhead but it's usually the easiest to leverage, end user has very few configuration to set, statically, after training ( post training quantization or PTQ ): this way is efficient because quantization is done offline, before inference, but it may have an accuracy cost, statically, after training ( quantization aware training or QAT ): like a PTQ followed by a second fine tuning. Same efficiency but usually slightly better accuracy. Nvidia GPUs don't support dynamic quantization, CPU supports all types of quantization. Compared to PTQ , QAT better preserves accuracy and should be preferred in most cases. During the quantization aware training : in the inside, Pytorch will train with high precision float numbers, on the outside, Pytorch will simulate that a quantization has already been applied and output results accordingly (for loss computation for instance) The simulation process is done through the add of quantization / dequantization nodes, most often called QDQ , it's an abbreviation you will see often in the quantization world. Want to learn more about quantization? You can check this high quality blog post for more information. The process is well described in this Nvidia presentation Why does it matter? # CPU quantization is supported out of the box by Pytorch and ONNX Runtime . GPU quantization on the other side requires specific tools and process to be applied . In the specific case of Transformer models, few demos from Nvidia and Microsoft exist; they are all for the old vanilla Bert architecture. It doesn't support modern architectures out of the box, like Albert , Roberta , Deberta or Electra .","title":"What is the theory?"},{"location":"quantization_theory/#some-theory","text":"","title":"Some theory"},{"location":"quantization_theory/#a-very-short-intro-to-int-8-quantization","text":"Basic idea behind model quantization is to replace tensors made of float numbers (usually encoded on 32 bits) by lower precision representation (integers encoded on 8 bits for Nvidia GPUs). Therefore computation is faster and model memory footprint is lower. Making tensor storage smaller makes memory transfer faster... and is also a source of computation acceleration. This approach is very interesting for its trade-off: you reduce inference time significantly, and it costs close to nothing in accuracy. Replacing float numbers by integers is done through a mapping. This step is called calibration , and its purpose is to compute for each tensor or each channel of a tensor (one of its dimensions) a range covering most weights and then define a scale and a distribution center to map float numbers to 8 bits integers. There are several ways to perform quantization, depending of how and when the calibration is performed: dynamically: the mapping is done online, during the inference, there are some overhead but it's usually the easiest to leverage, end user has very few configuration to set, statically, after training ( post training quantization or PTQ ): this way is efficient because quantization is done offline, before inference, but it may have an accuracy cost, statically, after training ( quantization aware training or QAT ): like a PTQ followed by a second fine tuning. Same efficiency but usually slightly better accuracy. Nvidia GPUs don't support dynamic quantization, CPU supports all types of quantization. Compared to PTQ , QAT better preserves accuracy and should be preferred in most cases. During the quantization aware training : in the inside, Pytorch will train with high precision float numbers, on the outside, Pytorch will simulate that a quantization has already been applied and output results accordingly (for loss computation for instance) The simulation process is done through the add of quantization / dequantization nodes, most often called QDQ , it's an abbreviation you will see often in the quantization world. Want to learn more about quantization? You can check this high quality blog post for more information. The process is well described in this Nvidia presentation","title":"A (very) short intro to INT-8 quantization"},{"location":"quantization_theory/#why-does-it-matter","text":"CPU quantization is supported out of the box by Pytorch and ONNX Runtime . GPU quantization on the other side requires specific tools and process to be applied . In the specific case of Transformer models, few demos from Nvidia and Microsoft exist; they are all for the old vanilla Bert architecture. It doesn't support modern architectures out of the box, like Albert , Roberta , Deberta or Electra .","title":"Why does it matter?"},{"location":"run/","text":"Run in a single command # Tip You can run commands below from Docker directly (no need to install Nvidia dependencies outside Docker one), like: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:latest \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend onnx \\ --seq-len 128 128 128\" With the single command below, you will: download the model and its tokenizer from Hugging Face hub, convert the model to ONNX graph, optimize the model with ONNX Runtime and save artefact ( model.onnx ), the model with TensorRT and save artefact ( model.plan ), benchmark each backend (including Pytorch), generate configuration files for Triton inference server convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend onnx --seq-len 128 128 128 --batch-size 1 32 32 # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=8.75ms, sd=0.30ms, min=8.60ms, max=11.20ms, median=8.68ms, 95p=9.15ms, 99p=10.77ms # [Pytorch (FP16)] mean=6.75ms, sd=0.22ms, min=6.66ms, max=8.99ms, median=6.71ms, 95p=6.88ms, 99p=7.95ms # [ONNX Runtime (FP32)] mean=8.10ms, sd=0.43ms, min=7.93ms, max=11.76ms, median=8.02ms, 95p=8.39ms, 99p=11.30ms # [ONNX Runtime (optimized)] mean=3.66ms, sd=0.23ms, min=3.57ms, max=6.46ms, median=3.62ms, 95p=3.70ms, 99p=4.95ms Info 128 128 128 -> minimum, optimal, maximum sequence length, to help TensorRT better optimize your model. Better to have the same value for seq len to get best performances from TensorRT ( ONNX Runtime has not this limitation). 1 32 32 -> batch size, same as above. Good idea to get 1 as minimum value. No impact on TensorRT performance. Launch Nvidia Triton inference server to play with both ONNX and TensorRT models: docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:21.12-py3 \\ bash -c \"pip install transformers sentencepiece && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside. Right now, only TensorRT 8.0.3 backend is available in Triton. Until the TensorRT 8.2 backend is available, we advise you to only use ONNX Runtime backend. Query the inference server: First you need to convert your strings to a binary file following the format of demo/query_body.bin . The Json part is straightforward, the second part a bit less. Please follow the code below for the recipe for a single string. If you have several strings, just concatenate the results. import struct text : str = \"This live event is great. I will sign-up for Infinity. \\n \" text_b = text . encode ( 'UTF-8' ) print ( struct . pack ( \"<I\" , len ( text_b )) + text_b ) # <I means little-endian unsigned integers, followed by the number of elements Tip check Nvidia implementation from https://github.com/triton-inference-server/client/blob/530bcac5f1574aa2222930076200544eb274245c/src/python/library/tritonclient/utils/ init .py#L187 for more information. # https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md # @ means no data conversion (curl feature) curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" check demo folder to discover more performant ways to query the server from Python or elsewhere.","title":"Run (1 command)"},{"location":"run/#run-in-a-single-command","text":"Tip You can run commands below from Docker directly (no need to install Nvidia dependencies outside Docker one), like: docker run -it --rm --gpus all \\ -v $PWD :/project ghcr.io/els-rd/transformer-deploy:latest \\ bash -c \"cd /project && \\ convert_model -m \\\"philschmid/MiniLM-L6-H384-uncased-sst2\\\" \\ --backend onnx \\ --seq-len 128 128 128\" With the single command below, you will: download the model and its tokenizer from Hugging Face hub, convert the model to ONNX graph, optimize the model with ONNX Runtime and save artefact ( model.onnx ), the model with TensorRT and save artefact ( model.plan ), benchmark each backend (including Pytorch), generate configuration files for Triton inference server convert_model -m philschmid/MiniLM-L6-H384-uncased-sst2 --backend onnx --seq-len 128 128 128 --batch-size 1 32 32 # ... # Inference done on NVIDIA GeForce RTX 3090 # latencies: # [Pytorch (FP32)] mean=8.75ms, sd=0.30ms, min=8.60ms, max=11.20ms, median=8.68ms, 95p=9.15ms, 99p=10.77ms # [Pytorch (FP16)] mean=6.75ms, sd=0.22ms, min=6.66ms, max=8.99ms, median=6.71ms, 95p=6.88ms, 99p=7.95ms # [ONNX Runtime (FP32)] mean=8.10ms, sd=0.43ms, min=7.93ms, max=11.76ms, median=8.02ms, 95p=8.39ms, 99p=11.30ms # [ONNX Runtime (optimized)] mean=3.66ms, sd=0.23ms, min=3.57ms, max=6.46ms, median=3.62ms, 95p=3.70ms, 99p=4.95ms Info 128 128 128 -> minimum, optimal, maximum sequence length, to help TensorRT better optimize your model. Better to have the same value for seq len to get best performances from TensorRT ( ONNX Runtime has not this limitation). 1 32 32 -> batch size, same as above. Good idea to get 1 as minimum value. No impact on TensorRT performance. Launch Nvidia Triton inference server to play with both ONNX and TensorRT models: docker run -it --rm --gpus all -p8000:8000 -p8001:8001 -p8002:8002 --shm-size 256m \\ -v $PWD /triton_models:/models nvcr.io/nvidia/tritonserver:21.12-py3 \\ bash -c \"pip install transformers sentencepiece && tritonserver --model-repository=/models\" As you can see we install Transformers and then launch the server itself. This is of course a bad practice, you should make your own 2 lines Dockerfile with Transformers inside. Right now, only TensorRT 8.0.3 backend is available in Triton. Until the TensorRT 8.2 backend is available, we advise you to only use ONNX Runtime backend. Query the inference server: First you need to convert your strings to a binary file following the format of demo/query_body.bin . The Json part is straightforward, the second part a bit less. Please follow the code below for the recipe for a single string. If you have several strings, just concatenate the results. import struct text : str = \"This live event is great. I will sign-up for Infinity. \\n \" text_b = text . encode ( 'UTF-8' ) print ( struct . pack ( \"<I\" , len ( text_b )) + text_b ) # <I means little-endian unsigned integers, followed by the number of elements Tip check Nvidia implementation from https://github.com/triton-inference-server/client/blob/530bcac5f1574aa2222930076200544eb274245c/src/python/library/tritonclient/utils/ init .py#L187 for more information. # https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_binary_data.md # @ means no data conversion (curl feature) curl -X POST http://localhost:8000/v2/models/transformer_onnx_inference/versions/1/infer \\ --data-binary \"@demo/query_body.bin\" \\ --header \"Inference-Header-Content-Length: 161\" check demo folder to discover more performant ways to query the server from Python or elsewhere.","title":"Run in a single command"},{"location":"setup_local/","text":"Installation # Tip Use Docker if you don't want to install all Nvidia dependencies (for a first try for instance). In the long term, local install is probably a better idea. 6 rules to install locally Nvidia dependencies You may have heard or experienced difficulties in installing Nvidia dependencies, or making them detected by your system. If you are on Debian / Ubuntu, it should be easy . 1st rule : don't follow install guides found on reddit, blogs, etc. they are never up to date 2nd rule : don't follow install guides from Nvidia dependency manual, they are not always up to date 3rd rule : only follow install guides from Nvidia downlad pages , they are the only ones with updated instructions 4th rule : uninstall all your Nvidia dependencies not coming directly from a Nvidia repo (including the Ubuntu driver) and reinstall them from Nvidia repositories 5th rule : if your OS version is recent and not listed in compatible/tested OS of a dependency, just take the dependency tested latest OS version, it will work otherwise Twitter/forums would be full of complaints. 6th rule : choose the network .deb option when possible (meaning add a repo to get updates). Local .deb means manual update. The list of dependencies you will need to run this library locally: CUDA >= 11.4.x cuDNN 8.2 TensorRT 8.2.1 (GA) Optional, to run this library from Docker (so you don't have to install all other dependencies): nvidia-docker You may need to login with a free Nvidia account to download some dependencies. Tip To be able to leverage your CUDA installation by the Pycuda dependency, don't forget to add to CUDA path to your $PATH env variable. Otherwise, you will have issues, like Pycuda failing to compile. Then, it's the usual git clone: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy for CPU / GPU support: pip3 install \".[GPU]\" -f https://download.pytorch.org/whl/cu113/torch_stable.html --extra-index-url https://pypi.ngc.nvidia.com # if you want to perform GPU quantization (recommended) pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg = pytorch-quantization \\& subdirectory = tools/pytorch-quantization/ for CPU only support: pip3 install \".[CPU]\" -f https://download.pytorch.org/whl/cpu/torch_stable.html To build your own version of the Docker image: make build_docker","title":"Installation (local or Docker only)"},{"location":"setup_local/#installation","text":"Tip Use Docker if you don't want to install all Nvidia dependencies (for a first try for instance). In the long term, local install is probably a better idea. 6 rules to install locally Nvidia dependencies You may have heard or experienced difficulties in installing Nvidia dependencies, or making them detected by your system. If you are on Debian / Ubuntu, it should be easy . 1st rule : don't follow install guides found on reddit, blogs, etc. they are never up to date 2nd rule : don't follow install guides from Nvidia dependency manual, they are not always up to date 3rd rule : only follow install guides from Nvidia downlad pages , they are the only ones with updated instructions 4th rule : uninstall all your Nvidia dependencies not coming directly from a Nvidia repo (including the Ubuntu driver) and reinstall them from Nvidia repositories 5th rule : if your OS version is recent and not listed in compatible/tested OS of a dependency, just take the dependency tested latest OS version, it will work otherwise Twitter/forums would be full of complaints. 6th rule : choose the network .deb option when possible (meaning add a repo to get updates). Local .deb means manual update. The list of dependencies you will need to run this library locally: CUDA >= 11.4.x cuDNN 8.2 TensorRT 8.2.1 (GA) Optional, to run this library from Docker (so you don't have to install all other dependencies): nvidia-docker You may need to login with a free Nvidia account to download some dependencies. Tip To be able to leverage your CUDA installation by the Pycuda dependency, don't forget to add to CUDA path to your $PATH env variable. Otherwise, you will have issues, like Pycuda failing to compile. Then, it's the usual git clone: git clone git@github.com:ELS-RD/transformer-deploy.git cd transformer-deploy for CPU / GPU support: pip3 install \".[GPU]\" -f https://download.pytorch.org/whl/cu113/torch_stable.html --extra-index-url https://pypi.ngc.nvidia.com # if you want to perform GPU quantization (recommended) pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg = pytorch-quantization \\& subdirectory = tools/pytorch-quantization/ for CPU only support: pip3 install \".[CPU]\" -f https://download.pytorch.org/whl/cpu/torch_stable.html To build your own version of the Docker image: make build_docker","title":"Installation"},{"location":"reference/convert/","text":"This module contains code related to client interface. check_accuracy ( engine_name , pytorch_output , engine_output , tolerance ) # Compare engine predictions with a reference. Assert that the difference is under a threshold. Parameters: Name Type Description Default engine_name str string used in error message, if any required pytorch_output List[numpy.ndarray] reference output used for the comparaison required engine_output List[numpy.ndarray] output from the engine required tolerance float if difference in outputs is above threshold, an error will be raised required Source code in src/transformer_deploy/convert.py def check_accuracy ( engine_name : str , pytorch_output : List [ np . ndarray ], engine_output : List [ np . ndarray ], tolerance : float ) -> None : \"\"\" Compare engine predictions with a reference. Assert that the difference is under a threshold. :param engine_name: string used in error message, if any :param pytorch_output: reference output used for the comparaison :param engine_output: output from the engine :param tolerance: if difference in outputs is above threshold, an error will be raised \"\"\" discrepency = compare_outputs ( pytorch_output = pytorch_output , engine_output = engine_output ) assert discrepency < tolerance , ( f \" { engine_name } discrepency is too high ( { discrepency : .2f } > { tolerance } ): \\n \" f \"Pythorch: \\n { pytorch_output } \\n \" f \"VS \\n \" f \" { engine_name } : \\n { engine_output } \\n \" f \"Diff: \\n \" f \" { np . asarray ( pytorch_output ) - np . asarray ( engine_output ) } \\n \" \"Tolerance can be increased with --atol parameter.\" ) launch_inference ( infer , inputs , nb_measures ) # Perform inference and measure latency Parameters: Name Type Description Default infer Callable a lambda which will perform the inference required inputs List[Dict[str, Union[numpy.ndarray, torch.Tensor]]] tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) required nb_measures int number of measures to perform for the latency measure required Returns: Type Description Tuple[List[numpy.ndarray], List[float]] a tuple of model output and inference latencies Source code in src/transformer_deploy/convert.py def launch_inference ( infer : Callable , inputs : List [ Dict [ str , Union [ np . ndarray , torch . Tensor ]]], nb_measures : int ) -> Tuple [ List [ np . ndarray ], List [ float ]]: \"\"\" Perform inference and measure latency :param infer: a lambda which will perform the inference :param inputs: tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) :param nb_measures: number of measures to perform for the latency measure :return: a tuple of model output and inference latencies \"\"\" assert type ( inputs ) == list assert len ( inputs ) > 0 outputs = list () for batch_input in inputs : output = infer ( batch_input ) outputs . append ( output ) time_buffer : List [ float ] = list () for _ in range ( nb_measures ): with track_infer_time ( time_buffer ): _ = infer ( inputs [ 0 ]) return outputs , time_buffer","title":"Convert"},{"location":"reference/convert/#src.transformer_deploy.convert.check_accuracy","text":"Compare engine predictions with a reference. Assert that the difference is under a threshold. Parameters: Name Type Description Default engine_name str string used in error message, if any required pytorch_output List[numpy.ndarray] reference output used for the comparaison required engine_output List[numpy.ndarray] output from the engine required tolerance float if difference in outputs is above threshold, an error will be raised required Source code in src/transformer_deploy/convert.py def check_accuracy ( engine_name : str , pytorch_output : List [ np . ndarray ], engine_output : List [ np . ndarray ], tolerance : float ) -> None : \"\"\" Compare engine predictions with a reference. Assert that the difference is under a threshold. :param engine_name: string used in error message, if any :param pytorch_output: reference output used for the comparaison :param engine_output: output from the engine :param tolerance: if difference in outputs is above threshold, an error will be raised \"\"\" discrepency = compare_outputs ( pytorch_output = pytorch_output , engine_output = engine_output ) assert discrepency < tolerance , ( f \" { engine_name } discrepency is too high ( { discrepency : .2f } > { tolerance } ): \\n \" f \"Pythorch: \\n { pytorch_output } \\n \" f \"VS \\n \" f \" { engine_name } : \\n { engine_output } \\n \" f \"Diff: \\n \" f \" { np . asarray ( pytorch_output ) - np . asarray ( engine_output ) } \\n \" \"Tolerance can be increased with --atol parameter.\" )","title":"check_accuracy()"},{"location":"reference/convert/#src.transformer_deploy.convert.launch_inference","text":"Perform inference and measure latency Parameters: Name Type Description Default infer Callable a lambda which will perform the inference required inputs List[Dict[str, Union[numpy.ndarray, torch.Tensor]]] tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) required nb_measures int number of measures to perform for the latency measure required Returns: Type Description Tuple[List[numpy.ndarray], List[float]] a tuple of model output and inference latencies Source code in src/transformer_deploy/convert.py def launch_inference ( infer : Callable , inputs : List [ Dict [ str , Union [ np . ndarray , torch . Tensor ]]], nb_measures : int ) -> Tuple [ List [ np . ndarray ], List [ float ]]: \"\"\" Perform inference and measure latency :param infer: a lambda which will perform the inference :param inputs: tensor compatible with the lambda (Torch tensor for Pytorch, or numpy otherwise) :param nb_measures: number of measures to perform for the latency measure :return: a tuple of model output and inference latencies \"\"\" assert type ( inputs ) == list assert len ( inputs ) > 0 outputs = list () for batch_input in inputs : output = infer ( batch_input ) outputs . append ( output ) time_buffer : List [ float ] = list () for _ in range ( nb_measures ): with track_infer_time ( time_buffer ): _ = infer ( inputs [ 0 ]) return outputs , time_buffer","title":"launch_inference()"},{"location":"reference/QDQModels/QDQAlbert/","text":"This module add quantization support to all Albert architecture based models.","title":"QDQAlbert"},{"location":"reference/QDQModels/QDQBert/","text":"This module add quantization support to all Bert architecture based models.","title":"QDQBert"},{"location":"reference/QDQModels/QDQDeberta/","text":"This module add quantization support to all Deberta architecture based models. For now, Deberta export to ONNX doesn't work well. This PR may help: https://github.com/microsoft/DeBERTa/pull/6 get_attention_mask ( self , attention_mask ) # Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def get_attention_mask ( self , attention_mask ): \"\"\" Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. \"\"\" if attention_mask . dim () <= 2 : extended_attention_mask = attention_mask . unsqueeze ( 1 ) . unsqueeze ( 2 ) attention_mask = extended_attention_mask * extended_attention_mask . squeeze ( - 2 ) . unsqueeze ( - 1 ) # unecessary conversion, byte == unsigned integer -> not supported by TensorRT # attention_mask = attention_mask.byte() elif attention_mask . dim () == 3 : attention_mask = attention_mask . unsqueeze ( 1 ) return attention_mask symbolic ( g , self , mask , dim ) # Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def symbolic ( g , self , mask , dim ): \"\"\" Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. \"\"\" import torch.onnx.symbolic_helper as sym_help from torch.onnx.symbolic_opset9 import masked_fill , softmax mask_cast_value = g . op ( \"Cast\" , mask , to_i = sym_help . cast_pytorch_to_onnx [ \"Long\" ]) # r_mask = g.op(\"Sub\", g.op(\"Constant\", value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value) # replace Byte by Char to get signed numbers r_mask = g . op ( \"Cast\" , g . op ( \"Sub\" , g . op ( \"Constant\" , value_t = torch . tensor ( 1 , dtype = torch . int64 )), mask_cast_value ), to_i = sym_help . cast_pytorch_to_onnx [ \"Char\" ], ) output = masked_fill ( g , self , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( float ( \"-inf\" )))) output = softmax ( g , output , dim ) return masked_fill ( g , output , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( 0 , dtype = torch . int8 )))","title":"QDQDeberta"},{"location":"reference/QDQModels/QDQDeberta/#src.transformer_deploy.QDQModels.QDQDeberta.get_attention_mask","text":"Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def get_attention_mask ( self , attention_mask ): \"\"\" Override existing get_attention_mask method in DebertaV2Encoder class. This one uses signed integers instead of unsigned one. \"\"\" if attention_mask . dim () <= 2 : extended_attention_mask = attention_mask . unsqueeze ( 1 ) . unsqueeze ( 2 ) attention_mask = extended_attention_mask * extended_attention_mask . squeeze ( - 2 ) . unsqueeze ( - 1 ) # unecessary conversion, byte == unsigned integer -> not supported by TensorRT # attention_mask = attention_mask.byte() elif attention_mask . dim () == 3 : attention_mask = attention_mask . unsqueeze ( 1 ) return attention_mask","title":"get_attention_mask()"},{"location":"reference/QDQModels/QDQDeberta/#src.transformer_deploy.QDQModels.QDQDeberta.symbolic","text":"Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. Source code in src/transformer_deploy/QDQModels/QDQDeberta.py def symbolic ( g , self , mask , dim ): \"\"\" Override existing symbolic static function of Xsoftmax class. This one uses signed integers instead of unsigned one. Symbolic function are used during ONNX conversion instead of Pytorch code. \"\"\" import torch.onnx.symbolic_helper as sym_help from torch.onnx.symbolic_opset9 import masked_fill , softmax mask_cast_value = g . op ( \"Cast\" , mask , to_i = sym_help . cast_pytorch_to_onnx [ \"Long\" ]) # r_mask = g.op(\"Sub\", g.op(\"Constant\", value_t=torch.tensor(1, dtype=torch.int64)), mask_cast_value) # replace Byte by Char to get signed numbers r_mask = g . op ( \"Cast\" , g . op ( \"Sub\" , g . op ( \"Constant\" , value_t = torch . tensor ( 1 , dtype = torch . int64 )), mask_cast_value ), to_i = sym_help . cast_pytorch_to_onnx [ \"Char\" ], ) output = masked_fill ( g , self , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( float ( \"-inf\" )))) output = softmax ( g , output , dim ) return masked_fill ( g , output , r_mask , g . op ( \"Constant\" , value_t = torch . tensor ( 0 , dtype = torch . int8 )))","title":"symbolic()"},{"location":"reference/QDQModels/QDQDistilbert/","text":"This module add quantization support to all Distilbert architecture based models.","title":"QDQDistilbert"},{"location":"reference/QDQModels/QDQElectra/","text":"This module add quantization support to all Electra architecture based models.","title":"QDQElectra"},{"location":"reference/QDQModels/QDQRoberta/","text":"This module add quantization support to all Roberta architecture based models. qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ) # Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. Source code in src/transformer_deploy/QDQModels/QDQRoberta.py def qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ): \"\"\" Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. \"\"\" # QDQ change below # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA. # int() -> float() because of a limitations in cumsum operator implementation in TensorRT mask = input_ids . ne ( padding_idx ) . float () incremental_indices = ( torch . cumsum ( mask , dim = 1 ) . type_as ( mask ) + past_key_values_length ) * mask return incremental_indices . long () + padding_idx","title":"QDQRoberta"},{"location":"reference/QDQModels/QDQRoberta/#src.transformer_deploy.QDQModels.QDQRoberta.qdq_create_position_tensorrt","text":"Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. Source code in src/transformer_deploy/QDQModels/QDQRoberta.py def qdq_create_position_tensorrt ( input_ids , padding_idx , past_key_values_length = 0 ): \"\"\" Override qdq_create_position_tensorrt function. It appeared that cumsum operator in TensorRT doesn't support integer type. see https://github.com/onnx/onnx-tensorrt/blob/master/docs/operators.md This override uses float instead. \"\"\" # QDQ change below # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA. # int() -> float() because of a limitations in cumsum operator implementation in TensorRT mask = input_ids . ne ( padding_idx ) . float () incremental_indices = ( torch . cumsum ( mask , dim = 1 ) . type_as ( mask ) + past_key_values_length ) * mask return incremental_indices . long () + padding_idx","title":"qdq_create_position_tensorrt()"},{"location":"reference/QDQModels/ast_operator_patch/","text":"Contains code to match and patch specific AST patterns. Patch2ArgsNode ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class Patch2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names __init__ ( self , op ) special # Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) PatchAdd2ArgsNode ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchAdd2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ] __init__ ( self , op ) special # Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ] should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) PatchLayer ( PatchNode ) # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchLayer ( PatchNode ): def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return [] __init__ ( self , origin_module , origin_layer , target_module , target_layer ) special # Patch source code in the form a.b(...) to c.d(...) Parameters: Name Type Description Default origin_module str module to patch required origin_layer str layer/method to patch required target_module str new module to use required target_layer str new layer/method to use required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return [] should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) PatchNode # Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchNode ( object ): __metaclass__ = abc . ABCMeta torch_op_to_quantize : str @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" ) @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) @staticmethod def _wrap_attr ( quantizer_name : str , tensor_var : ast . expr ) -> ast . Call : \"\"\" Generate quantization wrapping each attribute of a torch operation to optimize (matmul, add, etc.) :param quantizer_name: generated quantization name :param tensor_var: the variable to wrap :return: the ast tree to replace the original variable \"\"\" return ast . Call ( func = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = quantizer_name , ctx = ast . Load ()), args = [ tensor_var ], keywords = [], ) def get_quant_name ( self , node_id : int ) -> str : return f \" { self . torch_op_to_quantize . lower () } _quantizer_ { node_id } \" __metaclass__ ( type ) # Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls ) __instancecheck__ ( cls , instance ) special # Override for isinstance(instance, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod # Create and return a new object. See help(type) for accurate signature. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls __subclasscheck__ ( cls , subclass ) special # Override for issubclass(subclass, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) register ( cls , subclass ) # Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) patch ( self , node , ** kwargs ) # Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) should_patch ( self , node ) # Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" )","title":"Ast operator patch"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class Patch2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names","title":"Patch2ArgsNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.__init__","text":"Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a, b) to torch.op(self.q1(a), self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] q_attr_names = list () for index in range ( 2 ): # only apply transfo to the 2 first args arg = node . args [ index ] q_name = self . get_quant_name ( nb_quant_node + len ( q_attr_names )) q_attr_names . append ( q_name ) node . args [ index ] = self . _wrap_attr ( q_name , arg ) return q_attr_names","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.Patch2ArgsNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == \"torch\" and node . func . attr == self . torch_op_to_quantize )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchAdd2ArgsNode ( PatchNode ): def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ]","title":"PatchAdd2ArgsNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.__init__","text":"Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) Parameters: Name Type Description Default op str operator to match required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , op : str ): \"\"\" Patch source code in the form torch.op(a + b) to torch.op(self.q1(a) + self.q1(b)) :param op: operator to match \"\"\" self . torch_op_to_quantize = op","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: assert \"nb_quant_node\" in kwargs , \"missing nb_quant_node paramter\" nb_quant_node : int = kwargs [ \"nb_quant_node\" ] left_name = self . get_quant_name ( nb_quant_node ) right_name = self . get_quant_name ( nb_quant_node + 1 ) node . args [ 0 ] . left = self . _wrap_attr ( left_name , node . args [ 0 ] . left ) node . args [ 0 ] . right = self . _wrap_attr ( right_name , node . args [ 0 ] . right ) return [ left_name , right_name ]","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchAdd2ArgsNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and node . func . attr == self . torch_op_to_quantize and isinstance ( node . args , list ) and len ( node . args ) == 1 and isinstance ( node . args [ 0 ], ast . BinOp ) and isinstance ( node . args [ 0 ] . op , ast . Add ) )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchLayer ( PatchNode ): def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer ) def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return []","title":"PatchLayer"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.__init__","text":"Patch source code in the form a.b(...) to c.d(...) Parameters: Name Type Description Default origin_module str module to patch required origin_layer str layer/method to patch required target_module str new module to use required target_layer str new layer/method to use required Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __init__ ( self , origin_module : str , origin_layer : str , target_module : str , target_layer : str ): \"\"\" Patch source code in the form a.b(...) to c.d(...) :param origin_module: module to patch :param origin_layer: layer/method to patch :param target_module: new module to use :param target_layer: new layer/method to use \"\"\" self . origin_module = origin_module self . origin_layer = origin_layer self . target_module = target_module self . target_layer = target_layer","title":"__init__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: node . func . value . id = self . target_module node . func . attr = self . target_layer return []","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchLayer.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def should_patch ( self , node : ast . AST ) -> bool : return ( isinstance ( node , ast . Call ) and isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value . id == self . origin_module and node . func . attr == self . origin_layer )","title":"should_patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode","text":"Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class PatchNode ( object ): __metaclass__ = abc . ABCMeta torch_op_to_quantize : str @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" ) @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" ) @staticmethod def _wrap_attr ( quantizer_name : str , tensor_var : ast . expr ) -> ast . Call : \"\"\" Generate quantization wrapping each attribute of a torch operation to optimize (matmul, add, etc.) :param quantizer_name: generated quantization name :param tensor_var: the variable to wrap :return: the ast tree to replace the original variable \"\"\" return ast . Call ( func = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = quantizer_name , ctx = ast . Load ()), args = [ tensor_var ], keywords = [], ) def get_quant_name ( self , node_id : int ) -> str : return f \" { self . torch_op_to_quantize . lower () } _quantizer_ { node_id } \"","title":"PatchNode"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py class ABCMeta ( type ): \"\"\"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). \"\"\" def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass ) def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance ) def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass ) def _dump_registry ( cls , file = None ): \"\"\"Debug helper to print the ABC registry.\"\"\" print ( f \"Class: { cls . __module__ } . { cls . __qualname__ } \" , file = file ) print ( f \"Inv. counter: { get_cache_token () } \" , file = file ) ( _abc_registry , _abc_cache , _abc_negative_cache , _abc_negative_cache_version ) = _get_dump ( cls ) print ( f \"_abc_registry: { _abc_registry !r} \" , file = file ) print ( f \"_abc_cache: { _abc_cache !r} \" , file = file ) print ( f \"_abc_negative_cache: { _abc_negative_cache !r} \" , file = file ) print ( f \"_abc_negative_cache_version: { _abc_negative_cache_version !r} \" , file = file ) def _abc_registry_clear ( cls ): \"\"\"Clear the registry (for debugging or testing).\"\"\" _reset_registry ( cls ) def _abc_caches_clear ( cls ): \"\"\"Clear the caches (for debugging or testing).\"\"\" _reset_caches ( cls )","title":"__metaclass__"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__instancecheck__","text":"Override for isinstance(instance, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __instancecheck__ ( cls , instance ): \"\"\"Override for isinstance(instance, cls).\"\"\" return _abc_instancecheck ( cls , instance )","title":"__instancecheck__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__new__","text":"Create and return a new object. See help(type) for accurate signature. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __new__ ( mcls , name , bases , namespace , ** kwargs ): cls = super () . __new__ ( mcls , name , bases , namespace , ** kwargs ) _abc_init ( cls ) return cls","title":"__new__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.__subclasscheck__","text":"Override for issubclass(subclass, cls). Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def __subclasscheck__ ( cls , subclass ): \"\"\"Override for issubclass(subclass, cls).\"\"\" return _abc_subclasscheck ( cls , subclass )","title":"__subclasscheck__()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.__metaclass__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py def register ( cls , subclass ): \"\"\"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. \"\"\" return _abc_register ( cls , subclass )","title":"register()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.patch","text":"Patch node by adding quantizer nodes around the operator provided during the init Parameters: Name Type Description Default node AST node to patch required kwargs additional parameters, like nb_quant_node for the number of existing quantizer node {} Returns: Type Description List[str] return list of generated quantizer node names Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def patch ( self , node : ast . AST , ** kwargs ) -> List [ str ]: \"\"\" Patch node by adding quantizer nodes around the operator provided during the __init__ :param node: node to patch :param kwargs: additional parameters, like nb_quant_node for the number of existing quantizer node :return: return list of generated quantizer node names \"\"\" raise Exception ( \"to implement\" )","title":"patch()"},{"location":"reference/QDQModels/ast_operator_patch/#src.transformer_deploy.QDQModels.ast_operator_patch.PatchNode.should_patch","text":"Check if a node should be patched Parameters: Name Type Description Default node AST node to check required Returns: Type Description bool return True if it matches the operator provided during the init Source code in src/transformer_deploy/QDQModels/ast_operator_patch.py @abc . abstractmethod def should_patch ( self , node : ast . AST ) -> bool : \"\"\" Check if a node should be patched :param node: node to check :return: return True if it matches the operator provided during the __init__ \"\"\" raise Exception ( \"to implement\" )","title":"should_patch()"},{"location":"reference/QDQModels/ast_utils/","text":"Contains the code to patch model AST in RAM. PatchModule dataclass # PatchModule(module: str, monkey_patch: Dict[str, Tuple[Callable, str]] = ) Source code in src/transformer_deploy/QDQModels/ast_utils.py @dataclass class PatchModule : module : str monkey_patch : Dict [ str , Tuple [ Callable , str ]] = field ( default_factory = dict ) def print_code ( self ): for class_name , cl in self . monkey_patch . items (): print ( \"---------\" ) print ( class_name ) inspect . getsource ( cl ) def restore ( self ): model_module = importlib . import_module ( name = self . module ) importlib . reload ( model_module ) add_init_quantizer ( head_node , q_attr_names ) # Add initialization of quantizer to init () Parameters: Name Type Description Default head_node Module node related to a class to optimize required q_attr_names List[str] list of quantizer names to init required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_init_quantizer ( head_node : ast . Module , q_attr_names : List [ str ]) -> ast . Module : \"\"\" Add initialization of quantizer to __init__() :param head_node: node related to a class to optimize :param q_attr_names: list of quantizer names to init :return: modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.FunctionDef if isinstance ( node , ast . FunctionDef ) and node . name == \"__init__\" : for name in q_attr_names : quantizer = init_quantizer ( name ) node . body . append ( quantizer ) return head_node add_qdq_to_class_name ( head_node , new_class_name ) # Change the name of the class to optimize (may help in debugging / error messages) Parameters: Name Type Description Default head_node Module node related to the class to optimize required new_class_name str new name to use required Returns: Type Description Module the modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_qdq_to_class_name ( head_node : ast . Module , new_class_name : str ) -> ast . Module : \"\"\" Change the name of the class to optimize (may help in debugging / error messages) :param head_node: node related to the class to optimize :param new_class_name: new name to use :return: the modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.ClassDef if isinstance ( node , ast . ClassDef ): node . name = new_class_name return head_node add_quant_to_module ( module_to_patch , new_module_name ) # Modify a class to add quantization operations around each torch operation to optimize. Parameters: Name Type Description Default module_to_patch type Pytorch module to patch required new_module_name str new name for the module required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quant_to_module ( module_to_patch : type , new_module_name : str ) -> ast . Module : \"\"\" Modify a class to add quantization operations around each torch operation to optimize. :param module_to_patch: Pytorch module to patch :param new_module_name: new name for the module :return: modified ast tree \"\"\" source_code = inspect . getsource ( module_to_patch ) head = ast . parse ( source_code ) head , nodes_to_add = patch_nodes ( head ) add_init_quantizer ( head_node = head , q_attr_names = nodes_to_add ) head = add_qdq_to_class_name ( head_node = head , new_class_name = new_module_name ) return head add_quantization_to_model ( module_path , class_to_patch ) # Add quantization support to a model. Parameters: Name Type Description Default module_path str model module to optimize required class_to_patch Optional[List[str]] name of modules to patch, if None it will be auto-detected. required Returns: Type Description backup of original classes Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quantization_to_model ( module_path : str , class_to_patch : Optional [ List [ str ]], ): \"\"\" Add quantization support to a model. :param module_path: model module to optimize :param class_to_patch: name of modules to patch, if None it will be auto-detected. :return: backup of original classes \"\"\" model_module = importlib . import_module ( name = module_path ) load_missing_imports ( model_module ) if class_to_patch is None or len ( class_to_patch ) == 0 : class_to_patch = list_class_to_patch ( model_module = model_module ) logging . info ( f \"modify class { ', ' . join ( class_to_patch ) } \" ) for class_name in class_to_patch : module_to_patch = getattr ( model_module , class_name ) head = add_quant_to_module ( module_to_patch = module_to_patch , new_module_name = class_name ) head = ast . fix_missing_locations ( head ) module_patched : code = compile ( head , filename = \"<ast modif - transformer deploy>\" , mode = \"exec\" ) # execute the code in the module context so it overrides the original classes and leverage existing imports exec ( module_patched , model_module . __dict__ , model_module . __dict__ ) contains_op ( node ) # Check if a tree contains some operations to optimize. Parameters: Name Type Description Default node AST Head of the ast tree required Returns: Type Description bool True if ast tree contains operations to optimize Source code in src/transformer_deploy/QDQModels/ast_utils.py def contains_op ( node : ast . AST ) -> bool : \"\"\" Check if a tree contains some operations to optimize. :param node: Head of the ast tree :return: True if ast tree contains operations to optimize \"\"\" for node in ast . walk ( node ): for op in op_to_quant : if op . should_patch ( node = node ): return True return False init_quantizer ( name ) # Generate quantization node initialization to add to the end of init () Parameters: Name Type Description Default name str generated name of the node required Returns: Type Description Assign quantization init ast node Source code in src/transformer_deploy/QDQModels/ast_utils.py def init_quantizer ( name : str ) -> ast . Assign : \"\"\" Generate quantization node initialization to add to the end of __init__() :param name: generated name of the node :return: quantization init ast node \"\"\" quant_linear = ast . Attribute ( value = ast . Name ( id = \"quant_nn\" , ctx = ast . Load ()), attr = \"QuantLinear\" , ctx = ast . Load ()) default_quant_desc_input = ast . Attribute ( value = quant_linear , attr = \"default_quant_desc_input\" , ctx = ast . Load ()) tensor_quant = ast . Name ( id = \"TensorQuantizer\" , ctx = ast . Load ()) quant_value = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = name , ctx = ast . Store ()) return ast . Assign ( targets = [ quant_value ], value = ast . Call ( func = tensor_quant , args = [ default_quant_desc_input ], keywords = []), ) list_class_to_patch ( model_module ) # List all classes which contain operations to be optimized. Parameters: Name Type Description Default model_module Pytorch module required Returns: Type Description List[str] the list of module names to be optimized Source code in src/transformer_deploy/QDQModels/ast_utils.py def list_class_to_patch ( model_module ) -> List [ str ]: \"\"\" List all classes which contain operations to be optimized. :param model_module: Pytorch module :return: the list of module names to be optimized \"\"\" module_names : List [ str ] = list () module_source_code = inspect . getsource ( model_module ) head_node = ast . parse ( module_source_code ) for node in ast . walk ( head_node ): if isinstance ( node , ast . ClassDef ) and contains_op ( node = node ): module_names . append ( node . name ) return module_names load_missing_imports ( model_module ) # Execute some imports in the context of a module. Override Linear layer by its quantized version Parameters: Name Type Description Default model_module module to use for the imports required Source code in src/transformer_deploy/QDQModels/ast_utils.py def load_missing_imports ( model_module ) -> None : \"\"\" Execute some imports in the context of a module. Override Linear layer by its quantized version :param model_module: module to use for the imports \"\"\" import_code = \"\"\" from pytorch_quantization import nn as quant_nn from pytorch_quantization.nn import TensorQuantizer \"\"\" # remove extra spaces import_code = inspect . cleandoc ( import_code ) # execute the code in the module context exec ( import_code , model_module . __dict__ , model_module . __dict__ ) patch_nodes ( head_node ) # Replace an operation to optimize by its optimized version. May have to generate some quantization node names. Parameters: Name Type Description Default head_node Module ast node to modify required Returns: Type Description Tuple[ast.Module, List[str]] the modified ast tree and the list of generated quantization nodes Source code in src/transformer_deploy/QDQModels/ast_utils.py def patch_nodes ( head_node : ast . Module ) -> Tuple [ ast . Module , List [ str ]]: \"\"\" Replace an operation to optimize by its optimized version. May have to generate some quantization node names. :param head_node: ast node to modify :return: the modified ast tree and the list of generated quantization nodes \"\"\" q_attr_names : List [ str ] = list () for node in ast . walk ( head_node ): # type: ast.Call for op in op_to_quant : if op . should_patch ( node = node ): quant_names = op . patch ( node = node , nb_quant_node = len ( q_attr_names )) q_attr_names . extend ( quant_names ) return head_node , q_attr_names","title":"Ast utils"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.PatchModule","text":"PatchModule(module: str, monkey_patch: Dict[str, Tuple[Callable, str]] = ) Source code in src/transformer_deploy/QDQModels/ast_utils.py @dataclass class PatchModule : module : str monkey_patch : Dict [ str , Tuple [ Callable , str ]] = field ( default_factory = dict ) def print_code ( self ): for class_name , cl in self . monkey_patch . items (): print ( \"---------\" ) print ( class_name ) inspect . getsource ( cl ) def restore ( self ): model_module = importlib . import_module ( name = self . module ) importlib . reload ( model_module )","title":"PatchModule"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_init_quantizer","text":"Add initialization of quantizer to init () Parameters: Name Type Description Default head_node Module node related to a class to optimize required q_attr_names List[str] list of quantizer names to init required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_init_quantizer ( head_node : ast . Module , q_attr_names : List [ str ]) -> ast . Module : \"\"\" Add initialization of quantizer to __init__() :param head_node: node related to a class to optimize :param q_attr_names: list of quantizer names to init :return: modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.FunctionDef if isinstance ( node , ast . FunctionDef ) and node . name == \"__init__\" : for name in q_attr_names : quantizer = init_quantizer ( name ) node . body . append ( quantizer ) return head_node","title":"add_init_quantizer()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_qdq_to_class_name","text":"Change the name of the class to optimize (may help in debugging / error messages) Parameters: Name Type Description Default head_node Module node related to the class to optimize required new_class_name str new name to use required Returns: Type Description Module the modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_qdq_to_class_name ( head_node : ast . Module , new_class_name : str ) -> ast . Module : \"\"\" Change the name of the class to optimize (may help in debugging / error messages) :param head_node: node related to the class to optimize :param new_class_name: new name to use :return: the modified ast tree \"\"\" for node in ast . walk ( head_node ): # type: ast.ClassDef if isinstance ( node , ast . ClassDef ): node . name = new_class_name return head_node","title":"add_qdq_to_class_name()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_quant_to_module","text":"Modify a class to add quantization operations around each torch operation to optimize. Parameters: Name Type Description Default module_to_patch type Pytorch module to patch required new_module_name str new name for the module required Returns: Type Description Module modified ast tree Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quant_to_module ( module_to_patch : type , new_module_name : str ) -> ast . Module : \"\"\" Modify a class to add quantization operations around each torch operation to optimize. :param module_to_patch: Pytorch module to patch :param new_module_name: new name for the module :return: modified ast tree \"\"\" source_code = inspect . getsource ( module_to_patch ) head = ast . parse ( source_code ) head , nodes_to_add = patch_nodes ( head ) add_init_quantizer ( head_node = head , q_attr_names = nodes_to_add ) head = add_qdq_to_class_name ( head_node = head , new_class_name = new_module_name ) return head","title":"add_quant_to_module()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.add_quantization_to_model","text":"Add quantization support to a model. Parameters: Name Type Description Default module_path str model module to optimize required class_to_patch Optional[List[str]] name of modules to patch, if None it will be auto-detected. required Returns: Type Description backup of original classes Source code in src/transformer_deploy/QDQModels/ast_utils.py def add_quantization_to_model ( module_path : str , class_to_patch : Optional [ List [ str ]], ): \"\"\" Add quantization support to a model. :param module_path: model module to optimize :param class_to_patch: name of modules to patch, if None it will be auto-detected. :return: backup of original classes \"\"\" model_module = importlib . import_module ( name = module_path ) load_missing_imports ( model_module ) if class_to_patch is None or len ( class_to_patch ) == 0 : class_to_patch = list_class_to_patch ( model_module = model_module ) logging . info ( f \"modify class { ', ' . join ( class_to_patch ) } \" ) for class_name in class_to_patch : module_to_patch = getattr ( model_module , class_name ) head = add_quant_to_module ( module_to_patch = module_to_patch , new_module_name = class_name ) head = ast . fix_missing_locations ( head ) module_patched : code = compile ( head , filename = \"<ast modif - transformer deploy>\" , mode = \"exec\" ) # execute the code in the module context so it overrides the original classes and leverage existing imports exec ( module_patched , model_module . __dict__ , model_module . __dict__ )","title":"add_quantization_to_model()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.contains_op","text":"Check if a tree contains some operations to optimize. Parameters: Name Type Description Default node AST Head of the ast tree required Returns: Type Description bool True if ast tree contains operations to optimize Source code in src/transformer_deploy/QDQModels/ast_utils.py def contains_op ( node : ast . AST ) -> bool : \"\"\" Check if a tree contains some operations to optimize. :param node: Head of the ast tree :return: True if ast tree contains operations to optimize \"\"\" for node in ast . walk ( node ): for op in op_to_quant : if op . should_patch ( node = node ): return True return False","title":"contains_op()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.init_quantizer","text":"Generate quantization node initialization to add to the end of init () Parameters: Name Type Description Default name str generated name of the node required Returns: Type Description Assign quantization init ast node Source code in src/transformer_deploy/QDQModels/ast_utils.py def init_quantizer ( name : str ) -> ast . Assign : \"\"\" Generate quantization node initialization to add to the end of __init__() :param name: generated name of the node :return: quantization init ast node \"\"\" quant_linear = ast . Attribute ( value = ast . Name ( id = \"quant_nn\" , ctx = ast . Load ()), attr = \"QuantLinear\" , ctx = ast . Load ()) default_quant_desc_input = ast . Attribute ( value = quant_linear , attr = \"default_quant_desc_input\" , ctx = ast . Load ()) tensor_quant = ast . Name ( id = \"TensorQuantizer\" , ctx = ast . Load ()) quant_value = ast . Attribute ( value = ast . Name ( id = \"self\" , ctx = ast . Load ()), attr = name , ctx = ast . Store ()) return ast . Assign ( targets = [ quant_value ], value = ast . Call ( func = tensor_quant , args = [ default_quant_desc_input ], keywords = []), )","title":"init_quantizer()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.list_class_to_patch","text":"List all classes which contain operations to be optimized. Parameters: Name Type Description Default model_module Pytorch module required Returns: Type Description List[str] the list of module names to be optimized Source code in src/transformer_deploy/QDQModels/ast_utils.py def list_class_to_patch ( model_module ) -> List [ str ]: \"\"\" List all classes which contain operations to be optimized. :param model_module: Pytorch module :return: the list of module names to be optimized \"\"\" module_names : List [ str ] = list () module_source_code = inspect . getsource ( model_module ) head_node = ast . parse ( module_source_code ) for node in ast . walk ( head_node ): if isinstance ( node , ast . ClassDef ) and contains_op ( node = node ): module_names . append ( node . name ) return module_names","title":"list_class_to_patch()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.load_missing_imports","text":"Execute some imports in the context of a module. Override Linear layer by its quantized version Parameters: Name Type Description Default model_module module to use for the imports required Source code in src/transformer_deploy/QDQModels/ast_utils.py def load_missing_imports ( model_module ) -> None : \"\"\" Execute some imports in the context of a module. Override Linear layer by its quantized version :param model_module: module to use for the imports \"\"\" import_code = \"\"\" from pytorch_quantization import nn as quant_nn from pytorch_quantization.nn import TensorQuantizer \"\"\" # remove extra spaces import_code = inspect . cleandoc ( import_code ) # execute the code in the module context exec ( import_code , model_module . __dict__ , model_module . __dict__ )","title":"load_missing_imports()"},{"location":"reference/QDQModels/ast_utils/#src.transformer_deploy.QDQModels.ast_utils.patch_nodes","text":"Replace an operation to optimize by its optimized version. May have to generate some quantization node names. Parameters: Name Type Description Default head_node Module ast node to modify required Returns: Type Description Tuple[ast.Module, List[str]] the modified ast tree and the list of generated quantization nodes Source code in src/transformer_deploy/QDQModels/ast_utils.py def patch_nodes ( head_node : ast . Module ) -> Tuple [ ast . Module , List [ str ]]: \"\"\" Replace an operation to optimize by its optimized version. May have to generate some quantization node names. :param head_node: ast node to modify :return: the modified ast tree and the list of generated quantization nodes \"\"\" q_attr_names : List [ str ] = list () for node in ast . walk ( head_node ): # type: ast.Call for op in op_to_quant : if op . should_patch ( node = node ): quant_names = op . patch ( node = node , nb_quant_node = len ( q_attr_names )) q_attr_names . extend ( quant_names ) return head_node , q_attr_names","title":"patch_nodes()"},{"location":"reference/QDQModels/calibration_utils/","text":"Setup and run quantization calibration QATCalibrate # Source code in src/transformer_deploy/QDQModels/calibration_utils.py class QATCalibrate : def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc ) def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () def __enter__ ( self ): add_qdq () self . setup_nvidia_qat () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : self . finalize_calibration () __init__ ( self , method = 'histogram' , percentile = 99.999 , per_channel = True ) special # Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. Parameters: Name Type Description Default method str the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". 'histogram' percentile float for histogram method, what do you define as an outlier value 99.999 per_channel bool calibration granularity. per channel == per dimension. True Source code in src/transformer_deploy/QDQModels/calibration_utils.py def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel finalize_calibration ( self ) # Disable calibration process and enable quantized nodes. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () restore () staticmethod # Restore behavior without quantization support. Source code in src/transformer_deploy/QDQModels/calibration_utils.py @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () setup_model_qat ( self , model ) # Enable calibration on each tensor to quantize. Parameters: Name Type Description Default model PreTrainedModel model to optimize required Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () setup_nvidia_qat ( self ) # Setup Nvidia QAT library global variables. Should be called before initializing a model. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc )","title":"Calibration utils"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate","text":"Source code in src/transformer_deploy/QDQModels/calibration_utils.py class QATCalibrate : def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc ) def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable () def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda () @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq () def __enter__ ( self ): add_qdq () self . setup_nvidia_qat () return self def __exit__ ( self , exc_type , exc_val , exc_tb ): if exc_type is None : self . finalize_calibration ()","title":"QATCalibrate"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.__init__","text":"Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. Parameters: Name Type Description Default method str the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". 'histogram' percentile float for histogram method, what do you define as an outlier value 99.999 per_channel bool calibration granularity. per channel == per dimension. True Source code in src/transformer_deploy/QDQModels/calibration_utils.py def __init__ ( self , method : str = \"histogram\" , percentile : float = 99.999 , per_channel : bool = True ): \"\"\" Calibration will learn how a float tensor should be mapped to an integer tensor. Will learn range, bias and scale. Quantization targets signe 8 bits integers as it's the best supported type for Nvidia GPUs (there are dedicated 8 bits integer tensor cores on most modern Nvidia GPU architectures). Don't forget to call setup_model_qat at some point. :param method: the method calibration to use. One of [histogram, percentile]. Recommended method for transformers is \"histogram\". :param percentile: for histogram method, what do you define as an outlier value :param per_channel: calibration granularity. per channel == per dimension. \"\"\" assert torch . cuda . is_available (), \"CUDA not available\" self . model : Optional [ PreTrainedModel ] = None assert method in [ \"histogram\" , \"max\" , ], f \"unknown calibration method (for NLP): { method } \" self . calib_method : str = method self . calibration_percentile : float = percentile self . calibration_per_channel : bool = per_channel","title":"__init__()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.finalize_calibration","text":"Disable calibration process and enable quantized nodes. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def finalize_calibration ( self ) -> None : \"\"\" Disable calibration process and enable quantized nodes. \"\"\" calib_method = \"max\" if self . calib_method == \"max\" else \"percentile\" for _ , module in self . model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : if isinstance ( module . _calibrator , calib . MaxCalibrator ): module . load_calib_amax () else : # strict=False -> avoid Exception when some quantizer are never used # (because of a condition for instance) module . load_calib_amax ( calib_method , percentile = self . calibration_percentile , strict = False ) module . enable_quant () module . disable_calib () else : module . enable () # move back model to GPU memory self . model . cuda ()","title":"finalize_calibration()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.restore","text":"Restore behavior without quantization support. Source code in src/transformer_deploy/QDQModels/calibration_utils.py @staticmethod def restore (): \"\"\" Restore behavior without quantization support. \"\"\" remove_qdq ()","title":"restore()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.setup_model_qat","text":"Enable calibration on each tensor to quantize. Parameters: Name Type Description Default model PreTrainedModel model to optimize required Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_model_qat ( self , model : PreTrainedModel ) -> None : \"\"\" Enable calibration on each tensor to quantize. :param model: model to optimize \"\"\" self . model = model model = self . model . cuda () # Find the TensorQuantizer and enable calibration for name , module in model . named_modules (): if isinstance ( module , quant_nn . TensorQuantizer ): if module . _calibrator is not None : module . disable_quant () module . enable_calib () else : module . disable ()","title":"setup_model_qat()"},{"location":"reference/QDQModels/calibration_utils/#src.transformer_deploy.QDQModels.calibration_utils.QATCalibrate.setup_nvidia_qat","text":"Setup Nvidia QAT library global variables. Should be called before initializing a model. Source code in src/transformer_deploy/QDQModels/calibration_utils.py def setup_nvidia_qat ( self ) -> None : \"\"\" Setup Nvidia QAT library global variables. Should be called before initializing a model. \"\"\" input_desc = QuantDescriptor ( num_bits = 8 , calib_method = self . calib_method ) axis = ( 0 ,) if self . calibration_per_channel else None weight_desc = QuantDescriptor ( num_bits = 8 , axis = axis ) quant_nn . QuantLinear . set_default_quant_desc_input ( input_desc ) quant_nn . QuantLinear . set_default_quant_desc_weight ( weight_desc )","title":"setup_nvidia_qat()"},{"location":"reference/QDQModels/patch/","text":"Simple to use wrapper to patch transformer models AST add_qdq ( modules_to_patch = None ) # Add quantization support to each tested model by modifyin their AST. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def add_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Add quantization support to each tested model by modifyin their AST. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"add quantization to module { patch . module } \" ) patch_model ( patch ) patch_model ( patch ) # Perform modifications to model to make it work with ONNX export and quantization. Parameters: Name Type Description Default patch PatchModule an object containing all the information to perform a modification required Source code in src/transformer_deploy/QDQModels/patch.py def patch_model ( patch : PatchModule ) -> None : \"\"\" Perform modifications to model to make it work with ONNX export and quantization. :param patch: an object containing all the information to perform a modification \"\"\" add_quantization_to_model ( module_path = patch . module , class_to_patch = None ) model_module = importlib . import_module ( patch . module ) for target , ( modified_object , object_name ) in patch . monkey_patch . items (): source_code = inspect . getsource ( modified_object ) source_code += f \" \\n { target } = { object_name } \" exec ( source_code , model_module . __dict__ , model_module . __dict__ ) remove_qdq ( modules_to_patch = None ) # Restore AST of modified modules. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def remove_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Restore AST of modified modules. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"restore module { patch . module } \" ) patch . restore ()","title":"Patch"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.add_qdq","text":"Add quantization support to each tested model by modifyin their AST. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def add_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Add quantization support to each tested model by modifyin their AST. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"add quantization to module { patch . module } \" ) patch_model ( patch )","title":"add_qdq()"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.patch_model","text":"Perform modifications to model to make it work with ONNX export and quantization. Parameters: Name Type Description Default patch PatchModule an object containing all the information to perform a modification required Source code in src/transformer_deploy/QDQModels/patch.py def patch_model ( patch : PatchModule ) -> None : \"\"\" Perform modifications to model to make it work with ONNX export and quantization. :param patch: an object containing all the information to perform a modification \"\"\" add_quantization_to_model ( module_path = patch . module , class_to_patch = None ) model_module = importlib . import_module ( patch . module ) for target , ( modified_object , object_name ) in patch . monkey_patch . items (): source_code = inspect . getsource ( modified_object ) source_code += f \" \\n { target } = { object_name } \" exec ( source_code , model_module . __dict__ , model_module . __dict__ )","title":"patch_model()"},{"location":"reference/QDQModels/patch/#src.transformer_deploy.QDQModels.patch.remove_qdq","text":"Restore AST of modified modules. Parameters: Name Type Description Default modules_to_patch Optional[List[transformer_deploy.QDQModels.ast_utils.PatchModule]] list of operator to target None Source code in src/transformer_deploy/QDQModels/patch.py def remove_qdq ( modules_to_patch : Optional [ List [ PatchModule ]] = None ) -> None : \"\"\" Restore AST of modified modules. :param modules_to_patch: list of operator to target \"\"\" if modules_to_patch is None : modules_to_patch = tested_models for patch in modules_to_patch : logging . info ( f \"restore module { patch . module } \" ) patch . restore ()","title":"remove_qdq()"},{"location":"reference/backends/ort_utils/","text":"All the tooling to ease ONNX Runtime usage. cpu_quantization ( input_model_path , output_model_path ) # ONNX CPU only dynamic quantization Parameters: Name Type Description Default input_model_path str ONNX graph (float) to quantize required output_model_path str where to save quantized model required Source code in src/transformer_deploy/backends/ort_utils.py def cpu_quantization ( input_model_path : str , output_model_path : str ) -> None : \"\"\" ONNX CPU only dynamic quantization :param input_model_path: ONNX graph (float) to quantize :param output_model_path: where to save quantized model \"\"\" quantize_dynamic ( model_input = input_model_path , model_output = output_model_path , op_types_to_quantize = [ \"MatMul\" , \"Attention\" ], weight_type = QuantType . QInt8 , per_channel = True , reduce_range = True , extra_options = { \"WeightSymmetric\" : False , \"MatMulConstBOnly\" : True }, ) create_model_for_provider ( path , provider_to_use , nb_threads = 12 , nb_instances = 0 ) # Create an ONNX Runtime instance. Parameters: Name Type Description Default path str path to ONNX file required provider_to_use Union[str, List] provider to use for inference required nb_threads int intra_op_num_threads to use 12 nb_instances int inter_op_num_threads to use 0 Returns: Type Description InferenceSession ONNX Runtime inference session Source code in src/transformer_deploy/backends/ort_utils.py def create_model_for_provider ( path : str , provider_to_use : Union [ str , List ], nb_threads : int = multiprocessing . cpu_count (), nb_instances : int = 0 ) -> InferenceSession : \"\"\" Create an ONNX Runtime instance. :param path: path to ONNX file :param provider_to_use: provider to use for inference :param nb_threads: intra_op_num_threads to use :param nb_instances: inter_op_num_threads to use :return: ONNX Runtime inference session \"\"\" options = SessionOptions () options . graph_optimization_level = GraphOptimizationLevel . ORT_ENABLE_ALL if type ( provider_to_use ) != list : provider_to_use = [ provider_to_use ] if provider_to_use == [ \"CPUExecutionProvider\" ]: options . execution_mode = ExecutionMode . ORT_SEQUENTIAL if nb_instances <= 1 else ExecutionMode . ORT_PARALLEL options . intra_op_num_threads = nb_threads if nb_instances > 1 : options . inter_op_num_threads = nb_instances return InferenceSession ( path , options , providers = provider_to_use ) optimize_onnx ( onnx_path , onnx_optim_model_path , fp16 , use_cuda , num_attention_heads = 0 , hidden_size = 0 ) # ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. Parameters: Name Type Description Default onnx_path str ONNX input path required onnx_optim_model_path str where to save optimized model required fp16 bool use mixed precision (faster inference) required use_cuda bool perform optimization on GPU (should ) required num_attention_heads int number of attention heads of a model (0 -> try to detect) 0 hidden_size int hidden layer size of a model (0 -> try to detect) 0 Source code in src/transformer_deploy/backends/ort_utils.py def optimize_onnx ( onnx_path : str , onnx_optim_model_path : str , fp16 : bool , use_cuda : bool , num_attention_heads : int = 0 , hidden_size : int = 0 , ) -> None : \"\"\" ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. :param onnx_path: ONNX input path :param onnx_optim_model_path: where to save optimized model :param fp16: use mixed precision (faster inference) :param use_cuda: perform optimization on GPU (should ) :param num_attention_heads: number of attention heads of a model (0 -> try to detect) :param hidden_size: hidden layer size of a model (0 -> try to detect) \"\"\" optimization_options = FusionOptions ( \"bert\" ) optimization_options . enable_gelu_approximation = False # additional optimization optimized_model : BertOnnxModel = optimizer . optimize_model ( input = onnx_path , model_type = \"bert\" , use_gpu = use_cuda , opt_level = 1 , num_heads = num_attention_heads , # automatic detection with 0 may not work with opset 13 or distilbert models hidden_size = hidden_size , # automatic detection with 0 optimization_options = optimization_options , ) if fp16 : optimized_model . convert_float_to_float16 () # FP32 -> FP16 logging . info ( f \"optimizations applied: { optimized_model . get_fused_operator_statistics () } \" ) optimized_model . save_model_to_file ( onnx_optim_model_path )","title":"Ort utils"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.cpu_quantization","text":"ONNX CPU only dynamic quantization Parameters: Name Type Description Default input_model_path str ONNX graph (float) to quantize required output_model_path str where to save quantized model required Source code in src/transformer_deploy/backends/ort_utils.py def cpu_quantization ( input_model_path : str , output_model_path : str ) -> None : \"\"\" ONNX CPU only dynamic quantization :param input_model_path: ONNX graph (float) to quantize :param output_model_path: where to save quantized model \"\"\" quantize_dynamic ( model_input = input_model_path , model_output = output_model_path , op_types_to_quantize = [ \"MatMul\" , \"Attention\" ], weight_type = QuantType . QInt8 , per_channel = True , reduce_range = True , extra_options = { \"WeightSymmetric\" : False , \"MatMulConstBOnly\" : True }, )","title":"cpu_quantization()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.create_model_for_provider","text":"Create an ONNX Runtime instance. Parameters: Name Type Description Default path str path to ONNX file required provider_to_use Union[str, List] provider to use for inference required nb_threads int intra_op_num_threads to use 12 nb_instances int inter_op_num_threads to use 0 Returns: Type Description InferenceSession ONNX Runtime inference session Source code in src/transformer_deploy/backends/ort_utils.py def create_model_for_provider ( path : str , provider_to_use : Union [ str , List ], nb_threads : int = multiprocessing . cpu_count (), nb_instances : int = 0 ) -> InferenceSession : \"\"\" Create an ONNX Runtime instance. :param path: path to ONNX file :param provider_to_use: provider to use for inference :param nb_threads: intra_op_num_threads to use :param nb_instances: inter_op_num_threads to use :return: ONNX Runtime inference session \"\"\" options = SessionOptions () options . graph_optimization_level = GraphOptimizationLevel . ORT_ENABLE_ALL if type ( provider_to_use ) != list : provider_to_use = [ provider_to_use ] if provider_to_use == [ \"CPUExecutionProvider\" ]: options . execution_mode = ExecutionMode . ORT_SEQUENTIAL if nb_instances <= 1 else ExecutionMode . ORT_PARALLEL options . intra_op_num_threads = nb_threads if nb_instances > 1 : options . inter_op_num_threads = nb_instances return InferenceSession ( path , options , providers = provider_to_use )","title":"create_model_for_provider()"},{"location":"reference/backends/ort_utils/#src.transformer_deploy.backends.ort_utils.optimize_onnx","text":"ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. Parameters: Name Type Description Default onnx_path str ONNX input path required onnx_optim_model_path str where to save optimized model required fp16 bool use mixed precision (faster inference) required use_cuda bool perform optimization on GPU (should ) required num_attention_heads int number of attention heads of a model (0 -> try to detect) 0 hidden_size int hidden layer size of a model (0 -> try to detect) 0 Source code in src/transformer_deploy/backends/ort_utils.py def optimize_onnx ( onnx_path : str , onnx_optim_model_path : str , fp16 : bool , use_cuda : bool , num_attention_heads : int = 0 , hidden_size : int = 0 , ) -> None : \"\"\" ONNX Runtime transformer graph optimization. Performs some operator fusion (merge several nodes of the graph in a single one) and may convert some nodes to reduced precision. :param onnx_path: ONNX input path :param onnx_optim_model_path: where to save optimized model :param fp16: use mixed precision (faster inference) :param use_cuda: perform optimization on GPU (should ) :param num_attention_heads: number of attention heads of a model (0 -> try to detect) :param hidden_size: hidden layer size of a model (0 -> try to detect) \"\"\" optimization_options = FusionOptions ( \"bert\" ) optimization_options . enable_gelu_approximation = False # additional optimization optimized_model : BertOnnxModel = optimizer . optimize_model ( input = onnx_path , model_type = \"bert\" , use_gpu = use_cuda , opt_level = 1 , num_heads = num_attention_heads , # automatic detection with 0 may not work with opset 13 or distilbert models hidden_size = hidden_size , # automatic detection with 0 optimization_options = optimization_options , ) if fp16 : optimized_model . convert_float_to_float16 () # FP32 -> FP16 logging . info ( f \"optimizations applied: { optimized_model . get_fused_operator_statistics () } \" ) optimized_model . save_model_to_file ( onnx_optim_model_path )","title":"optimize_onnx()"},{"location":"reference/backends/pytorch_utils/","text":"Utils related to Pytorch inference. convert_to_onnx ( model_pytorch , output_path , inputs_pytorch , quantization ) # Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. ONNX opset 12 used for non quantized models, and 13 otherwise. Parameters: Name Type Description Default model_pytorch PreTrainedModel Pytorch model (transformers) required output_path str where to save ONNX file required inputs_pytorch OrderedDict[str, torch.Tensor] Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) required quantization bool model is quantized required Source code in src/transformer_deploy/backends/pytorch_utils.py def convert_to_onnx ( model_pytorch : PreTrainedModel , output_path : str , inputs_pytorch : Od [ str , torch . Tensor ], quantization : bool ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. ONNX opset 12 used for non quantized models, and 13 otherwise. :param model_pytorch: Pytorch model (transformers) :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param quantization: model is quantized \"\"\" if quantization : try : from pytorch_quantization.nn import TensorQuantizer except ImportError : raise ImportError ( \"It seems that pytorch-quantization is not yet installed. \" \"It is required when you enable the quantization flag and use CUDA device.\" \"Please find installation instructions on \" \"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization or use: \\n \" \"pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization \\\\ &\" \"subdirectory=tools/pytorch-quantization/\" ) TensorQuantizer . use_fb_fake_quant = True # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = 13 , # the ONNX version to use, >= 13 supports channel quantized model do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) if quantization : TensorQuantizer . use_fb_fake_quant = False get_model_size ( path ) # Find number of attention heads and hidden layer size of a model Parameters: Name Type Description Default path str path to model required Returns: Type Description Tuple[int, int] tupple of # of attention heads and hidden layer size (0 if not found) Source code in src/transformer_deploy/backends/pytorch_utils.py def get_model_size ( path : str ) -> Tuple [ int , int ]: \"\"\" Find number of attention heads and hidden layer size of a model :param path: path to model :return: tupple of # of attention heads and hidden layer size (0 if not found) \"\"\" config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = path ) num_attention_heads = getattr ( config , \"num_attention_heads\" , 0 ) hidden_size = getattr ( config , \"hidden_size\" , 0 ) return num_attention_heads , hidden_size infer_classification_pytorch ( model , run_on_cuda ) # Perform Pytorch inference for classification task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], numpy.ndarray] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_classification_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], np . ndarray ]: \"\"\" Perform Pytorch inference for classification task :param model: Pytorch model (transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> np . ndarray : model_output = model ( ** inputs ) . logits . detach () . cpu () . numpy () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer infer_feature_extraction_pytorch ( model , run_on_cuda ) # Perform Pytorch inference for feature extraction task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (sentence-transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], numpy.ndarray] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_feature_extraction_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], np . ndarray ]: \"\"\" Perform Pytorch inference for feature extraction task :param model: Pytorch model (sentence-transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> np . ndarray : model_output = model ( ** inputs ) . detach () . cpu () . numpy () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"Pytorch utils"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.convert_to_onnx","text":"Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. ONNX opset 12 used for non quantized models, and 13 otherwise. Parameters: Name Type Description Default model_pytorch PreTrainedModel Pytorch model (transformers) required output_path str where to save ONNX file required inputs_pytorch OrderedDict[str, torch.Tensor] Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) required quantization bool model is quantized required Source code in src/transformer_deploy/backends/pytorch_utils.py def convert_to_onnx ( model_pytorch : PreTrainedModel , output_path : str , inputs_pytorch : Od [ str , torch . Tensor ], quantization : bool ) -> None : \"\"\" Convert a Pytorch model to an ONNX graph by tracing the provided input inside the Pytorch code. ONNX opset 12 used for non quantized models, and 13 otherwise. :param model_pytorch: Pytorch model (transformers) :param output_path: where to save ONNX file :param inputs_pytorch: Tensor, can be dummy data, shape is not important as we declare all axes as dynamic. Should be on the same device than the model (CPU or GPU) :param quantization: model is quantized \"\"\" if quantization : try : from pytorch_quantization.nn import TensorQuantizer except ImportError : raise ImportError ( \"It seems that pytorch-quantization is not yet installed. \" \"It is required when you enable the quantization flag and use CUDA device.\" \"Please find installation instructions on \" \"https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization or use: \\n \" \"pip3 install git+ssh://git@github.com/NVIDIA/TensorRT#egg=pytorch-quantization \\\\ &\" \"subdirectory=tools/pytorch-quantization/\" ) TensorQuantizer . use_fb_fake_quant = True # dynamic axis == variable length axis dynamic_axis = OrderedDict () for k in inputs_pytorch . keys (): dynamic_axis [ k ] = { 0 : \"batch_size\" , 1 : \"sequence\" } dynamic_axis [ \"output\" ] = { 0 : \"batch_size\" } with torch . no_grad (): torch . onnx . export ( model_pytorch , # model to optimize args = tuple ( inputs_pytorch . values ()), # tuple of multiple inputs f = output_path , # output path / file object opset_version = 13 , # the ONNX version to use, >= 13 supports channel quantized model do_constant_folding = True , # simplify model (replace constant expressions) input_names = list ( inputs_pytorch . keys ()), # input names output_names = [ \"output\" ], # output axis name dynamic_axes = dynamic_axis , # declare dynamix axis for each input / output training = TrainingMode . EVAL , # always put the model in evaluation mode verbose = False , ) if quantization : TensorQuantizer . use_fb_fake_quant = False","title":"convert_to_onnx()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.get_model_size","text":"Find number of attention heads and hidden layer size of a model Parameters: Name Type Description Default path str path to model required Returns: Type Description Tuple[int, int] tupple of # of attention heads and hidden layer size (0 if not found) Source code in src/transformer_deploy/backends/pytorch_utils.py def get_model_size ( path : str ) -> Tuple [ int , int ]: \"\"\" Find number of attention heads and hidden layer size of a model :param path: path to model :return: tupple of # of attention heads and hidden layer size (0 if not found) \"\"\" config = AutoConfig . from_pretrained ( pretrained_model_name_or_path = path ) num_attention_heads = getattr ( config , \"num_attention_heads\" , 0 ) hidden_size = getattr ( config , \"hidden_size\" , 0 ) return num_attention_heads , hidden_size","title":"get_model_size()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.infer_classification_pytorch","text":"Perform Pytorch inference for classification task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], numpy.ndarray] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_classification_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], np . ndarray ]: \"\"\" Perform Pytorch inference for classification task :param model: Pytorch model (transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> np . ndarray : model_output = model ( ** inputs ) . logits . detach () . cpu () . numpy () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"infer_classification_pytorch()"},{"location":"reference/backends/pytorch_utils/#src.transformer_deploy.backends.pytorch_utils.infer_feature_extraction_pytorch","text":"Perform Pytorch inference for feature extraction task Parameters: Name Type Description Default model PreTrainedModel Pytorch model (sentence-transformers) required run_on_cuda bool True if should be ran on GPU required Returns: Type Description Callable[[Dict[str, torch.Tensor]], numpy.ndarray] a function to perform inference Source code in src/transformer_deploy/backends/pytorch_utils.py def infer_feature_extraction_pytorch ( model : PreTrainedModel , run_on_cuda : bool ) -> Callable [[ Dict [ str , torch . Tensor ]], np . ndarray ]: \"\"\" Perform Pytorch inference for feature extraction task :param model: Pytorch model (sentence-transformers) :param run_on_cuda: True if should be ran on GPU :return: a function to perform inference \"\"\" def infer ( inputs : Dict [ str , torch . Tensor ]) -> np . ndarray : model_output = model ( ** inputs ) . detach () . cpu () . numpy () # noqa: F821 if run_on_cuda : torch . cuda . synchronize () return model_output return infer","title":"infer_feature_extraction_pytorch()"},{"location":"reference/backends/st_utils/","text":"Utils related to sentence-transformers. STransformerWrapper ( Module ) # Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. Source code in src/transformer_deploy/backends/st_utils.py class STransformerWrapper ( nn . Module ): \"\"\" Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. \"\"\" def __init__ ( self , model : \"SentenceTransformer\" ): super () . __init__ () self . model = model def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ] forward ( self , * kargs , ** kwargs ) # Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/backends/st_utils.py def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ] load_sentence_transformers ( path ) # Load sentence-transformers model and wrap it to make it behave like any other transformers model Parameters: Name Type Description Default path str path to the model required Returns: Type Description STransformerWrapper wrapped sentence-transformers model Source code in src/transformer_deploy/backends/st_utils.py def load_sentence_transformers ( path : str ) -> STransformerWrapper : \"\"\" Load sentence-transformers model and wrap it to make it behave like any other transformers model :param path: path to the model :return: wrapped sentence-transformers model \"\"\" try : from sentence_transformers import SentenceTransformer except ImportError : raise Exception ( \"sentence-transformers library is not present, please install it: pip install sentence-transformers\" ) model : SentenceTransformer = SentenceTransformer ( path ) return STransformerWrapper ( model = model )","title":"St utils"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.STransformerWrapper","text":"Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. Source code in src/transformer_deploy/backends/st_utils.py class STransformerWrapper ( nn . Module ): \"\"\" Wrap sentence-transformers model to provide a forward function with multiple inputs as expected by ONNX export tool. \"\"\" def __init__ ( self , model : \"SentenceTransformer\" ): super () . __init__ () self . model = model def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ]","title":"STransformerWrapper"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.STransformerWrapper.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in src/transformer_deploy/backends/st_utils.py def forward ( self , * kargs , ** kwargs ): inputs = dict () if len ( kargs ) >= 2 : inputs [ \"input_ids\" ] = kargs [ 0 ] inputs [ \"attention_mask\" ] = kargs [ - 1 ] if len ( kargs ) == 3 : inputs [ \"token_type_ids\" ] = kargs [ 1 ] if len ( kwargs ) > 0 : inputs = kwargs assert 2 <= len ( inputs ) <= 3 , f \"unexpected number of inputs: { len ( inputs ) } \" outputs = self . model . forward ( input = inputs ) return outputs [ \"sentence_embedding\" ]","title":"forward()"},{"location":"reference/backends/st_utils/#src.transformer_deploy.backends.st_utils.load_sentence_transformers","text":"Load sentence-transformers model and wrap it to make it behave like any other transformers model Parameters: Name Type Description Default path str path to the model required Returns: Type Description STransformerWrapper wrapped sentence-transformers model Source code in src/transformer_deploy/backends/st_utils.py def load_sentence_transformers ( path : str ) -> STransformerWrapper : \"\"\" Load sentence-transformers model and wrap it to make it behave like any other transformers model :param path: path to the model :return: wrapped sentence-transformers model \"\"\" try : from sentence_transformers import SentenceTransformer except ImportError : raise Exception ( \"sentence-transformers library is not present, please install it: pip install sentence-transformers\" ) model : SentenceTransformer = SentenceTransformer ( path ) return STransformerWrapper ( model = model )","title":"load_sentence_transformers()"},{"location":"reference/backends/trt_utils/","text":"All the tooling to ease TensorRT usage. build_engine ( runtime , onnx_file_path , logger , min_shape , optimal_shape , max_shape , workspace_size , fp16 , int8 ) # Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size don't hurt performance and is highly advised. Parameters: Name Type Description Default runtime Runtime global variable shared accross inference call / model building required onnx_file_path str path to the ONNX file required logger Logger specific logger to TensorRT required min_shape Tuple[int, int] the minimal shape of input tensors. It's advised to set first dimension (batch size) to 1 required optimal_shape Tuple[int, int] input tensor shape used for optimizations required max_shape Tuple[int, int] maximal input tensor shape required workspace_size int GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. required fp16 bool enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. required int8 bool enable INT-8 quantization, best performance but model should have been quantized. required Returns: Type Description ICudaEngine TensorRT engine to use during inference Source code in src/transformer_deploy/backends/trt_utils.py def build_engine ( runtime : Runtime , onnx_file_path : str , logger : Logger , min_shape : Tuple [ int , int ], optimal_shape : Tuple [ int , int ], max_shape : Tuple [ int , int ], workspace_size : int , fp16 : bool , int8 : bool , ) -> ICudaEngine : \"\"\" Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size don't hurt performance and is highly advised. :param runtime: global variable shared accross inference call / model building :param onnx_file_path: path to the ONNX file :param logger: specific logger to TensorRT :param min_shape: the minimal shape of input tensors. It's advised to set first dimension (batch size) to 1 :param optimal_shape: input tensor shape used for optimizations :param max_shape: maximal input tensor shape :param workspace_size: GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. :param fp16: enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. :param int8: enable INT-8 quantization, best performance but model should have been quantized. :return: TensorRT engine to use during inference \"\"\" with trt . Builder ( logger ) as builder : # type: Builder with builder . create_network ( flags = 1 << int ( trt . NetworkDefinitionCreationFlag . EXPLICIT_BATCH ) ) as network_definition : # type: INetworkDefinition with trt . OnnxParser ( network_definition , logger ) as parser : # type: OnnxParser builder . max_batch_size = max_shape [ 0 ] # max batch size config : IBuilderConfig = builder . create_builder_config () config . max_workspace_size = workspace_size # to enable complete trt inspector debugging, only for TensorRT >= 8.2 # config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED # disable CUDNN optimizations config . set_tactic_sources ( tactic_sources = 1 << int ( trt . TacticSource . CUBLAS ) | 1 << int ( trt . TacticSource . CUBLAS_LT ) ) if int8 : config . set_flag ( trt . BuilderFlag . INT8 ) if fp16 : config . set_flag ( trt . BuilderFlag . FP16 ) config . set_flag ( trt . BuilderFlag . DISABLE_TIMING_CACHE ) # https://github.com/NVIDIA/TensorRT/issues/1196 (sometimes big diff in output when using FP16) config . set_flag ( trt . BuilderFlag . PREFER_PRECISION_CONSTRAINTS ) with open ( onnx_file_path , \"rb\" ) as f : parser . parse ( f . read ()) profile : IOptimizationProfile = builder . create_optimization_profile () for num_input in range ( network_definition . num_inputs ): profile . set_shape ( input = network_definition . get_input ( num_input ) . name , min = min_shape , opt = optimal_shape , max = max_shape , ) config . add_optimization_profile ( profile ) if fp16 : network_definition = fix_fp16_network ( network_definition ) trt_engine = builder . build_serialized_network ( network_definition , config ) engine : ICudaEngine = runtime . deserialize_cuda_engine ( trt_engine ) assert engine is not None , \"error during engine generation, check error messages above :-(\" return engine fix_fp16_network ( network_definition ) # Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. Parameters: Name Type Description Default network_definition INetworkDefinition graph generated by TensorRT after parsing ONNX file (during the model building) required Returns: Type Description INetworkDefinition patched network definition Source code in src/transformer_deploy/backends/trt_utils.py def fix_fp16_network ( network_definition : INetworkDefinition ) -> INetworkDefinition : \"\"\" Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. :param network_definition: graph generated by TensorRT after parsing ONNX file (during the model building) :return: patched network definition \"\"\" # search for patterns which may overflow in FP16 precision, we force FP32 precisions for those nodes for layer_index in range ( network_definition . num_layers - 1 ): layer : ILayer = network_definition . get_layer ( layer_index ) next_layer : ILayer = network_definition . get_layer ( layer_index + 1 ) # POW operation usually followed by mean reduce if layer . type == trt . LayerType . ELEMENTWISE and next_layer . type == trt . LayerType . REDUCE : # casting to get access to op attribute layer . __class__ = IElementWiseLayer next_layer . __class__ = IReduceLayer if layer . op == trt . ElementWiseOperation . POW : layer . precision = trt . DataType . FLOAT next_layer . precision = trt . DataType . FLOAT layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) next_layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition get_binding_idxs ( engine , profile_index ) # Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings Parameters: Name Type Description Default engine ICudaEngine TensorRT engine generated during the model building required profile_index int profile to use (several profiles can be set during building) required Returns: Type Description input and output tensor indexes Source code in src/transformer_deploy/backends/trt_utils.py def get_binding_idxs ( engine : trt . ICudaEngine , profile_index : int ): \"\"\" Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings :param engine: TensorRT engine generated during the model building :param profile_index: profile to use (several profiles can be set during building) :return: input and output tensor indexes \"\"\" num_bindings_per_profile = engine . num_bindings // engine . num_optimization_profiles start_binding = profile_index * num_bindings_per_profile end_binding = start_binding + num_bindings_per_profile # Separate input and output binding indices for convenience input_binding_idxs : List [ int ] = [] output_binding_idxs : List [ int ] = [] for binding_index in range ( start_binding , end_binding ): if engine . binding_is_input ( binding_index ): input_binding_idxs . append ( binding_index ) else : output_binding_idxs . append ( binding_index ) return input_binding_idxs , output_binding_idxs infer_tensorrt ( context , host_inputs , input_binding_idxs , output_binding_idxs , stream ) # Perform inference with TensorRT. Parameters: Name Type Description Default context IExecutionContext shared variable required host_inputs OrderedDict[str, numpy.ndarray] input tensor required input_binding_idxs List[int] input tensor indexes required output_binding_idxs List[int] output tensor indexes required stream Stream GPU stream to synchronize memories required Returns: Type Description ndarray output tensor Source code in src/transformer_deploy/backends/trt_utils.py def infer_tensorrt ( context : IExecutionContext , host_inputs : OrderedDict [ str , np . ndarray ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], stream : Stream , ) -> np . ndarray : \"\"\" Perform inference with TensorRT. :param context: shared variable :param host_inputs: input tensor :param input_binding_idxs: input tensor indexes :param output_binding_idxs: output tensor indexes :param stream: GPU stream to synchronize memories :return: output tensor \"\"\" input_list : List [ ndarray ] = list () device_inputs : List [ DeviceAllocation ] = list () for tensor in host_inputs . values (): # warning: small change in output if int64 is used instead of int32 tensor_int32 : np . ndarray = np . asarray ( tensor , dtype = np . int32 ) input_list . append ( tensor_int32 ) # allocate GPU memory for input tensors device_input : DeviceAllocation = cuda . mem_alloc ( tensor_int32 . nbytes ) device_inputs . append ( device_input ) cuda . memcpy_htod_async ( device_input , tensor_int32 . ravel (), stream ) # calculate input shape, bind it, allocate GPU memory for the output host_outputs , device_outputs = setup_binding_shapes ( context , input_list , input_binding_idxs , output_binding_idxs ) bindings = device_inputs + device_outputs assert context . execute_async_v2 ( bindings , stream_handle = stream . handle ), \"failure during execution of inference\" for h_output , d_output in zip ( host_outputs , device_outputs ): cuda . memcpy_dtoh_async ( h_output , d_output ) # GPU to host stream . synchronize () # sync all CUDA ops return host_outputs load_engine ( runtime , engine_file_path , profile_index = 0 ) # Load serialized TensorRT engine. Parameters: Name Type Description Default runtime Runtime shared variable required engine_file_path str path to the serialized engine required profile_index int which profile to load, 0 if you have not used multiple profiles 0 Returns: Type Description Callable[[Dict[str, numpy.ndarray]], numpy.ndarray] A function to perform inference Source code in src/transformer_deploy/backends/trt_utils.py def load_engine ( runtime : Runtime , engine_file_path : str , profile_index : int = 0 ) -> Callable [[ Dict [ str , np . ndarray ]], np . ndarray ]: \"\"\" Load serialized TensorRT engine. :param runtime: shared variable :param engine_file_path: path to the serialized engine :param profile_index: which profile to load, 0 if you have not used multiple profiles :return: A function to perform inference \"\"\" with open ( file = engine_file_path , mode = \"rb\" ) as f : engine : ICudaEngine = runtime . deserialize_cuda_engine ( f . read ()) stream : Stream = cuda . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) # retrieve input/output IDs input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] def tensorrt_model ( inputs : Dict [ str , np . ndarray ]) -> np . ndarray : return infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) return tensorrt_model save_engine ( engine , engine_file_path ) # Serialize TensorRT engine to file. Parameters: Name Type Description Default engine ICudaEngine TensorRT engine required engine_file_path str output path required Source code in src/transformer_deploy/backends/trt_utils.py def save_engine ( engine : ICudaEngine , engine_file_path : str ) -> None : \"\"\" Serialize TensorRT engine to file. :param engine: TensorRT engine :param engine_file_path: output path \"\"\" with open ( engine_file_path , \"wb\" ) as f : f . write ( engine . serialize ()) setup_binding_shapes ( context , host_inputs , input_binding_idxs , output_binding_idxs ) # Reserve memory in GPU for input and output tensors. Parameters: Name Type Description Default context IExecutionContext TensorRT context shared accross inference steps required host_inputs List[numpy.ndarray] input tensor required input_binding_idxs List[int] indexes of each input vector (should be the same than during building) required output_binding_idxs List[int] indexes of each output vector (should be the same than during building) required Returns: Type Description Tuple[List[numpy.ndarray], List[pycuda._driver.DeviceAllocation]] tensors where output will be stored and GPU memory address of output tensor Source code in src/transformer_deploy/backends/trt_utils.py def setup_binding_shapes ( context : trt . IExecutionContext , host_inputs : List [ np . ndarray ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> Tuple [ List [ np . ndarray ], List [ DeviceAllocation ]]: \"\"\" Reserve memory in GPU for input and output tensors. :param context: TensorRT context shared accross inference steps :param host_inputs: input tensor :param input_binding_idxs: indexes of each input vector (should be the same than during building) :param output_binding_idxs: indexes of each output vector (should be the same than during building) :return: tensors where output will be stored and GPU memory address of output tensor \"\"\" # explicitly set dynamic input shapes, so dynamic output shapes can be computed internally for host_input , binding_index in zip ( host_inputs , input_binding_idxs ): context . set_binding_shape ( binding_index , host_input . shape ) assert context . all_binding_shapes_specified host_outputs : List [ np . ndarray ] = [] device_outputs : List [ DeviceAllocation ] = [] for binding_index in output_binding_idxs : output_shape = context . get_binding_shape ( binding_index ) # allocate buffers to hold output results after copying back to host buffer = np . empty ( output_shape , dtype = np . float32 ) host_outputs . append ( buffer ) # allocate output buffers on device device_outputs . append ( cuda . mem_alloc ( buffer . nbytes )) return host_outputs , device_outputs","title":"Trt utils"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.build_engine","text":"Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size don't hurt performance and is highly advised. Parameters: Name Type Description Default runtime Runtime global variable shared accross inference call / model building required onnx_file_path str path to the ONNX file required logger Logger specific logger to TensorRT required min_shape Tuple[int, int] the minimal shape of input tensors. It's advised to set first dimension (batch size) to 1 required optimal_shape Tuple[int, int] input tensor shape used for optimizations required max_shape Tuple[int, int] maximal input tensor shape required workspace_size int GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. required fp16 bool enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. required int8 bool enable INT-8 quantization, best performance but model should have been quantized. required Returns: Type Description ICudaEngine TensorRT engine to use during inference Source code in src/transformer_deploy/backends/trt_utils.py def build_engine ( runtime : Runtime , onnx_file_path : str , logger : Logger , min_shape : Tuple [ int , int ], optimal_shape : Tuple [ int , int ], max_shape : Tuple [ int , int ], workspace_size : int , fp16 : bool , int8 : bool , ) -> ICudaEngine : \"\"\" Convert ONNX file to TensorRT engine. It supports dynamic shape, however it's advised to keep sequence length fix as it hurts performance otherwise. Dynamic batch size don't hurt performance and is highly advised. :param runtime: global variable shared accross inference call / model building :param onnx_file_path: path to the ONNX file :param logger: specific logger to TensorRT :param min_shape: the minimal shape of input tensors. It's advised to set first dimension (batch size) to 1 :param optimal_shape: input tensor shape used for optimizations :param max_shape: maximal input tensor shape :param workspace_size: GPU memory to use during the building, more is always better. If there is not enough memory, some optimization may fail, and the whole conversion process will crash. :param fp16: enable FP16 precision, it usually provide a 20-30% boost compared to ONNX Runtime. :param int8: enable INT-8 quantization, best performance but model should have been quantized. :return: TensorRT engine to use during inference \"\"\" with trt . Builder ( logger ) as builder : # type: Builder with builder . create_network ( flags = 1 << int ( trt . NetworkDefinitionCreationFlag . EXPLICIT_BATCH ) ) as network_definition : # type: INetworkDefinition with trt . OnnxParser ( network_definition , logger ) as parser : # type: OnnxParser builder . max_batch_size = max_shape [ 0 ] # max batch size config : IBuilderConfig = builder . create_builder_config () config . max_workspace_size = workspace_size # to enable complete trt inspector debugging, only for TensorRT >= 8.2 # config.profiling_verbosity = trt.ProfilingVerbosity.DETAILED # disable CUDNN optimizations config . set_tactic_sources ( tactic_sources = 1 << int ( trt . TacticSource . CUBLAS ) | 1 << int ( trt . TacticSource . CUBLAS_LT ) ) if int8 : config . set_flag ( trt . BuilderFlag . INT8 ) if fp16 : config . set_flag ( trt . BuilderFlag . FP16 ) config . set_flag ( trt . BuilderFlag . DISABLE_TIMING_CACHE ) # https://github.com/NVIDIA/TensorRT/issues/1196 (sometimes big diff in output when using FP16) config . set_flag ( trt . BuilderFlag . PREFER_PRECISION_CONSTRAINTS ) with open ( onnx_file_path , \"rb\" ) as f : parser . parse ( f . read ()) profile : IOptimizationProfile = builder . create_optimization_profile () for num_input in range ( network_definition . num_inputs ): profile . set_shape ( input = network_definition . get_input ( num_input ) . name , min = min_shape , opt = optimal_shape , max = max_shape , ) config . add_optimization_profile ( profile ) if fp16 : network_definition = fix_fp16_network ( network_definition ) trt_engine = builder . build_serialized_network ( network_definition , config ) engine : ICudaEngine = runtime . deserialize_cuda_engine ( trt_engine ) assert engine is not None , \"error during engine generation, check error messages above :-(\" return engine","title":"build_engine()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.fix_fp16_network","text":"Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. Parameters: Name Type Description Default network_definition INetworkDefinition graph generated by TensorRT after parsing ONNX file (during the model building) required Returns: Type Description INetworkDefinition patched network definition Source code in src/transformer_deploy/backends/trt_utils.py def fix_fp16_network ( network_definition : INetworkDefinition ) -> INetworkDefinition : \"\"\" Mixed precision on TensorRT can generate scores very far from Pytorch because of some operator being saturated. Indeed, FP16 can't store very large and very small numbers like FP32. Here, we search for some patterns of operators to keep in FP32, in most cases, it is enough to fix the inference and don't hurt performances. :param network_definition: graph generated by TensorRT after parsing ONNX file (during the model building) :return: patched network definition \"\"\" # search for patterns which may overflow in FP16 precision, we force FP32 precisions for those nodes for layer_index in range ( network_definition . num_layers - 1 ): layer : ILayer = network_definition . get_layer ( layer_index ) next_layer : ILayer = network_definition . get_layer ( layer_index + 1 ) # POW operation usually followed by mean reduce if layer . type == trt . LayerType . ELEMENTWISE and next_layer . type == trt . LayerType . REDUCE : # casting to get access to op attribute layer . __class__ = IElementWiseLayer next_layer . __class__ = IReduceLayer if layer . op == trt . ElementWiseOperation . POW : layer . precision = trt . DataType . FLOAT next_layer . precision = trt . DataType . FLOAT layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) next_layer . set_output_type ( index = 0 , dtype = trt . DataType . FLOAT ) return network_definition","title":"fix_fp16_network()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.get_binding_idxs","text":"Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings Parameters: Name Type Description Default engine ICudaEngine TensorRT engine generated during the model building required profile_index int profile to use (several profiles can be set during building) required Returns: Type Description input and output tensor indexes Source code in src/transformer_deploy/backends/trt_utils.py def get_binding_idxs ( engine : trt . ICudaEngine , profile_index : int ): \"\"\" Calculate start/end binding indices for current context's profile https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html#opt_profiles_bindings :param engine: TensorRT engine generated during the model building :param profile_index: profile to use (several profiles can be set during building) :return: input and output tensor indexes \"\"\" num_bindings_per_profile = engine . num_bindings // engine . num_optimization_profiles start_binding = profile_index * num_bindings_per_profile end_binding = start_binding + num_bindings_per_profile # Separate input and output binding indices for convenience input_binding_idxs : List [ int ] = [] output_binding_idxs : List [ int ] = [] for binding_index in range ( start_binding , end_binding ): if engine . binding_is_input ( binding_index ): input_binding_idxs . append ( binding_index ) else : output_binding_idxs . append ( binding_index ) return input_binding_idxs , output_binding_idxs","title":"get_binding_idxs()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.infer_tensorrt","text":"Perform inference with TensorRT. Parameters: Name Type Description Default context IExecutionContext shared variable required host_inputs OrderedDict[str, numpy.ndarray] input tensor required input_binding_idxs List[int] input tensor indexes required output_binding_idxs List[int] output tensor indexes required stream Stream GPU stream to synchronize memories required Returns: Type Description ndarray output tensor Source code in src/transformer_deploy/backends/trt_utils.py def infer_tensorrt ( context : IExecutionContext , host_inputs : OrderedDict [ str , np . ndarray ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], stream : Stream , ) -> np . ndarray : \"\"\" Perform inference with TensorRT. :param context: shared variable :param host_inputs: input tensor :param input_binding_idxs: input tensor indexes :param output_binding_idxs: output tensor indexes :param stream: GPU stream to synchronize memories :return: output tensor \"\"\" input_list : List [ ndarray ] = list () device_inputs : List [ DeviceAllocation ] = list () for tensor in host_inputs . values (): # warning: small change in output if int64 is used instead of int32 tensor_int32 : np . ndarray = np . asarray ( tensor , dtype = np . int32 ) input_list . append ( tensor_int32 ) # allocate GPU memory for input tensors device_input : DeviceAllocation = cuda . mem_alloc ( tensor_int32 . nbytes ) device_inputs . append ( device_input ) cuda . memcpy_htod_async ( device_input , tensor_int32 . ravel (), stream ) # calculate input shape, bind it, allocate GPU memory for the output host_outputs , device_outputs = setup_binding_shapes ( context , input_list , input_binding_idxs , output_binding_idxs ) bindings = device_inputs + device_outputs assert context . execute_async_v2 ( bindings , stream_handle = stream . handle ), \"failure during execution of inference\" for h_output , d_output in zip ( host_outputs , device_outputs ): cuda . memcpy_dtoh_async ( h_output , d_output ) # GPU to host stream . synchronize () # sync all CUDA ops return host_outputs","title":"infer_tensorrt()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.load_engine","text":"Load serialized TensorRT engine. Parameters: Name Type Description Default runtime Runtime shared variable required engine_file_path str path to the serialized engine required profile_index int which profile to load, 0 if you have not used multiple profiles 0 Returns: Type Description Callable[[Dict[str, numpy.ndarray]], numpy.ndarray] A function to perform inference Source code in src/transformer_deploy/backends/trt_utils.py def load_engine ( runtime : Runtime , engine_file_path : str , profile_index : int = 0 ) -> Callable [[ Dict [ str , np . ndarray ]], np . ndarray ]: \"\"\" Load serialized TensorRT engine. :param runtime: shared variable :param engine_file_path: path to the serialized engine :param profile_index: which profile to load, 0 if you have not used multiple profiles :return: A function to perform inference \"\"\" with open ( file = engine_file_path , mode = \"rb\" ) as f : engine : ICudaEngine = runtime . deserialize_cuda_engine ( f . read ()) stream : Stream = cuda . Stream () context : IExecutionContext = engine . create_execution_context () context . set_optimization_profile_async ( profile_index = profile_index , stream_handle = stream . handle ) # retrieve input/output IDs input_binding_idxs , output_binding_idxs = get_binding_idxs ( engine , profile_index ) # type: List[int], List[int] def tensorrt_model ( inputs : Dict [ str , np . ndarray ]) -> np . ndarray : return infer_tensorrt ( context = context , host_inputs = inputs , input_binding_idxs = input_binding_idxs , output_binding_idxs = output_binding_idxs , stream = stream , ) return tensorrt_model","title":"load_engine()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.save_engine","text":"Serialize TensorRT engine to file. Parameters: Name Type Description Default engine ICudaEngine TensorRT engine required engine_file_path str output path required Source code in src/transformer_deploy/backends/trt_utils.py def save_engine ( engine : ICudaEngine , engine_file_path : str ) -> None : \"\"\" Serialize TensorRT engine to file. :param engine: TensorRT engine :param engine_file_path: output path \"\"\" with open ( engine_file_path , \"wb\" ) as f : f . write ( engine . serialize ())","title":"save_engine()"},{"location":"reference/backends/trt_utils/#src.transformer_deploy.backends.trt_utils.setup_binding_shapes","text":"Reserve memory in GPU for input and output tensors. Parameters: Name Type Description Default context IExecutionContext TensorRT context shared accross inference steps required host_inputs List[numpy.ndarray] input tensor required input_binding_idxs List[int] indexes of each input vector (should be the same than during building) required output_binding_idxs List[int] indexes of each output vector (should be the same than during building) required Returns: Type Description Tuple[List[numpy.ndarray], List[pycuda._driver.DeviceAllocation]] tensors where output will be stored and GPU memory address of output tensor Source code in src/transformer_deploy/backends/trt_utils.py def setup_binding_shapes ( context : trt . IExecutionContext , host_inputs : List [ np . ndarray ], input_binding_idxs : List [ int ], output_binding_idxs : List [ int ], ) -> Tuple [ List [ np . ndarray ], List [ DeviceAllocation ]]: \"\"\" Reserve memory in GPU for input and output tensors. :param context: TensorRT context shared accross inference steps :param host_inputs: input tensor :param input_binding_idxs: indexes of each input vector (should be the same than during building) :param output_binding_idxs: indexes of each output vector (should be the same than during building) :return: tensors where output will be stored and GPU memory address of output tensor \"\"\" # explicitly set dynamic input shapes, so dynamic output shapes can be computed internally for host_input , binding_index in zip ( host_inputs , input_binding_idxs ): context . set_binding_shape ( binding_index , host_input . shape ) assert context . all_binding_shapes_specified host_outputs : List [ np . ndarray ] = [] device_outputs : List [ DeviceAllocation ] = [] for binding_index in output_binding_idxs : output_shape = context . get_binding_shape ( binding_index ) # allocate buffers to hold output results after copying back to host buffer = np . empty ( output_shape , dtype = np . float32 ) host_outputs . append ( buffer ) # allocate output buffers on device device_outputs . append ( cuda . mem_alloc ( buffer . nbytes )) return host_outputs , device_outputs","title":"setup_binding_shapes()"},{"location":"reference/benchmarks/utils/","text":"Shared functions related to benchmarks. generate_input ( seq_len , batch_size , include_token_ids , device = 'cuda' ) # Generate dummy inputs. Parameters: Name Type Description Default seq_len int number of token per input. required batch_size int first dimension of the tensor required include_token_ids bool should we add token_type_ids required device str where to store tensors (Pytorch only). One of [cpu, cuda] 'cuda' Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, numpy.ndarray]] a tuple of tensors, Pytorch and numpy Source code in src/transformer_deploy/benchmarks/utils.py def generate_input ( seq_len : int , batch_size : int , include_token_ids : bool , device : str = \"cuda\" ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , np . ndarray ]]: \"\"\" Generate dummy inputs. :param seq_len: number of token per input. :param batch_size: first dimension of the tensor :param include_token_ids: should we add token_type_ids :param device: where to store tensors (Pytorch only). One of [cpu, cuda] :return: a tuple of tensors, Pytorch and numpy \"\"\" assert device in [ \"cpu\" , \"cuda\" ] shape = ( batch_size , seq_len ) inputs_pytorch : OrderedDict [ str , torch . Tensor ] = OrderedDict () inputs_pytorch [ \"input_ids\" ] = torch . randint ( high = 100 , size = shape , dtype = torch . long , device = device ) if include_token_ids : inputs_pytorch [ \"token_type_ids\" ] = torch . ones ( size = shape , dtype = torch . long , device = device ) inputs_pytorch [ \"attention_mask\" ] = torch . ones ( size = shape , dtype = torch . long , device = device ) inputs_onnx : Dict [ str , np . ndarray ] = { k : np . ascontiguousarray ( v . detach () . cpu () . numpy ()) for k , v in inputs_pytorch . items () } return inputs_pytorch , inputs_onnx print_timings ( name , timings ) # Format and print latencies Parameters: Name Type Description Default name str engine name required timings List[float] latencies measured during the inference required Source code in src/transformer_deploy/benchmarks/utils.py def print_timings ( name : str , timings : List [ float ]) -> None : \"\"\" Format and print latencies :param name: engine name :param timings: latencies measured during the inference \"\"\" mean_time = 1e3 * np . mean ( timings ) std_time = 1e3 * np . std ( timings ) min_time = 1e3 * np . min ( timings ) max_time = 1e3 * np . max ( timings ) median , percent_95_time , percent_99_time = 1e3 * np . percentile ( timings , [ 50 , 95 , 99 ]) print ( f \"[ { name } ] \" f \"mean= { mean_time : .2f } ms, \" f \"sd= { std_time : .2f } ms, \" f \"min= { min_time : .2f } ms, \" f \"max= { max_time : .2f } ms, \" f \"median= { median : .2f } ms, \" f \"95p= { percent_95_time : .2f } ms, \" f \"99p= { percent_99_time : .2f } ms\" ) setup_logging ( level = 20 ) # Set the generic Python logger Parameters: Name Type Description Default level int logger level 20 Source code in src/transformer_deploy/benchmarks/utils.py def setup_logging ( level : int = logging . INFO ) -> None : \"\"\" Set the generic Python logger :param level: logger level \"\"\" logging . basicConfig ( format = \" %(asctime)s %(levelname)-8s %(message)s \" , datefmt = \"%m/ %d /%Y %H:%M:%S\" , level = level ) track_infer_time ( buffer ) # A context manager to perform latency measures Parameters: Name Type Description Default buffer List[int] a List where to save latencies for each input required Source code in src/transformer_deploy/benchmarks/utils.py @contextmanager def track_infer_time ( buffer : List [ int ]) -> None : \"\"\" A context manager to perform latency measures :param buffer: a List where to save latencies for each input \"\"\" start = time . perf_counter () yield end = time . perf_counter () buffer . append ( end - start )","title":"Utils"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.generate_input","text":"Generate dummy inputs. Parameters: Name Type Description Default seq_len int number of token per input. required batch_size int first dimension of the tensor required include_token_ids bool should we add token_type_ids required device str where to store tensors (Pytorch only). One of [cpu, cuda] 'cuda' Returns: Type Description Tuple[Dict[str, torch.Tensor], Dict[str, numpy.ndarray]] a tuple of tensors, Pytorch and numpy Source code in src/transformer_deploy/benchmarks/utils.py def generate_input ( seq_len : int , batch_size : int , include_token_ids : bool , device : str = \"cuda\" ) -> Tuple [ Dict [ str , torch . Tensor ], Dict [ str , np . ndarray ]]: \"\"\" Generate dummy inputs. :param seq_len: number of token per input. :param batch_size: first dimension of the tensor :param include_token_ids: should we add token_type_ids :param device: where to store tensors (Pytorch only). One of [cpu, cuda] :return: a tuple of tensors, Pytorch and numpy \"\"\" assert device in [ \"cpu\" , \"cuda\" ] shape = ( batch_size , seq_len ) inputs_pytorch : OrderedDict [ str , torch . Tensor ] = OrderedDict () inputs_pytorch [ \"input_ids\" ] = torch . randint ( high = 100 , size = shape , dtype = torch . long , device = device ) if include_token_ids : inputs_pytorch [ \"token_type_ids\" ] = torch . ones ( size = shape , dtype = torch . long , device = device ) inputs_pytorch [ \"attention_mask\" ] = torch . ones ( size = shape , dtype = torch . long , device = device ) inputs_onnx : Dict [ str , np . ndarray ] = { k : np . ascontiguousarray ( v . detach () . cpu () . numpy ()) for k , v in inputs_pytorch . items () } return inputs_pytorch , inputs_onnx","title":"generate_input()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.print_timings","text":"Format and print latencies Parameters: Name Type Description Default name str engine name required timings List[float] latencies measured during the inference required Source code in src/transformer_deploy/benchmarks/utils.py def print_timings ( name : str , timings : List [ float ]) -> None : \"\"\" Format and print latencies :param name: engine name :param timings: latencies measured during the inference \"\"\" mean_time = 1e3 * np . mean ( timings ) std_time = 1e3 * np . std ( timings ) min_time = 1e3 * np . min ( timings ) max_time = 1e3 * np . max ( timings ) median , percent_95_time , percent_99_time = 1e3 * np . percentile ( timings , [ 50 , 95 , 99 ]) print ( f \"[ { name } ] \" f \"mean= { mean_time : .2f } ms, \" f \"sd= { std_time : .2f } ms, \" f \"min= { min_time : .2f } ms, \" f \"max= { max_time : .2f } ms, \" f \"median= { median : .2f } ms, \" f \"95p= { percent_95_time : .2f } ms, \" f \"99p= { percent_99_time : .2f } ms\" )","title":"print_timings()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.setup_logging","text":"Set the generic Python logger Parameters: Name Type Description Default level int logger level 20 Source code in src/transformer_deploy/benchmarks/utils.py def setup_logging ( level : int = logging . INFO ) -> None : \"\"\" Set the generic Python logger :param level: logger level \"\"\" logging . basicConfig ( format = \" %(asctime)s %(levelname)-8s %(message)s \" , datefmt = \"%m/ %d /%Y %H:%M:%S\" , level = level )","title":"setup_logging()"},{"location":"reference/benchmarks/utils/#src.transformer_deploy.benchmarks.utils.track_infer_time","text":"A context manager to perform latency measures Parameters: Name Type Description Default buffer List[int] a List where to save latencies for each input required Source code in src/transformer_deploy/benchmarks/utils.py @contextmanager def track_infer_time ( buffer : List [ int ]) -> None : \"\"\" A context manager to perform latency measures :param buffer: a List where to save latencies for each input \"\"\" start = time . perf_counter () yield end = time . perf_counter () buffer . append ( end - start )","title":"track_infer_time()"},{"location":"reference/templates/triton/","text":"Generate Nvidia Triton server configuration files. Configuration # Source code in src/transformer_deploy/templates/triton.py class Configuration : def __init__ ( self , workind_directory : str , model_name : str , model_type : ModelType , batch_size : int , nb_output : int , nb_instance : int , include_token_type : bool , device : str , ): \"\"\" Configuration file setup. :param workind_directory: path to the working directory :param model_name: model name to use (used to call TensorRT API) :param model_type: type of model (ONNX or TensorRT) :param batch_size: dynamic batch size to use (0 to disable) :param nb_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param include_token_type: does the model expect to receive among tensor input one for token type? :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name = model_name self . model_name += \"_onnx\" if model_type == ModelType . ONNX else \"_tensorrt\" self . model_folder_name = f \" { self . model_name } _model\" self . tokenizer_folder_name = f \" { self . model_name } _tokenize\" self . inference_folder_name = f \" { self . model_name } _inference\" self . batch_size = batch_size self . nb_model_output = nb_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . include_token_type = include_token_type self . workind_directory = workind_directory if model_type == ModelType . ONNX : self . input_type = \"TYPE_INT64\" self . inference_platform = \"onnxruntime_onnx\" elif model_type == ModelType . TensorRT : self . input_type = \"TYPE_INT32\" self . inference_platform = \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { model_type } \" ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" def __get_tokens ( self ): \"\"\" Generate input tensor configuration :return: input tensor configuration string \"\"\" token_type = \"\" if self . include_token_type : token_type = f \"\"\" {{ name: \"token_type_ids\" data_type: { self . input_type } dims: [-1, -1] }} , \"\"\" return f \"\"\" {{ name: \"input_ids\" data_type: { self . input_type } dims: [-1, -1] }} , { token_type } {{ name: \"attention_mask\" data_type: { self . input_type } dims: [-1, -1] }} \"\"\" def __instance_group ( self ): \"\"\" Generate instance configuration. :return: instance configuration \"\"\" return f \"\"\" instance_group [ {{ count: { self . nb_instance } kind: { self . device_kind } }} ] \"\"\" . strip () def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: { self . batch_size } platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . __get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} { self . __instance_group () } \"\"\" . strip () def get_tokenize_conf ( self ): \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" name: \" { self . tokenizer_folder_name } \" max_batch_size: { self . batch_size } backend: \"python\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . __get_tokens () } ] { self . __instance_group () } \"\"\" . strip () def get_inference_conf ( self ): \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" input_token_type_ids = \"\" if self . include_token_type : input_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () output_token_type_ids = \"\" if self . include_token_type : output_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () return f \"\"\" name: \" { self . inference_folder_name } \" max_batch_size: { self . batch_size } platform: \"ensemble\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} ensemble_scheduling {{ step [ {{ model_name: \" { self . tokenizer_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { input_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { output_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () def create_folders ( self , tokenizer : PreTrainedTokenizer , model_path : str ) -> None : \"\"\" Generate configuration folder layout. :param tokenizer: tokenizer to use :param model_path: ouput path \"\"\" wd_path = Path ( self . workind_directory ) wd_path . mkdir ( parents = True , exist_ok = True ) for folder_name , conf_func in [ ( self . model_folder_name , self . get_model_conf ), ( self . tokenizer_folder_name , self . get_tokenize_conf ), ( self . inference_folder_name , self . get_inference_conf ), ]: current_folder = wd_path . joinpath ( folder_name ) current_folder . mkdir ( exist_ok = True ) conf = conf_func () current_folder . joinpath ( \"config.pbtxt\" ) . write_text ( conf ) version_folder = current_folder . joinpath ( \"1\" ) version_folder . mkdir ( exist_ok = True ) tokenizer_model_folder_path = wd_path . joinpath ( self . tokenizer_folder_name ) . joinpath ( \"1\" ) tokenizer . save_pretrained ( str ( tokenizer_model_folder_path . absolute ())) source_code : str = inspect . getsource ( python_tokenizer ) Path ( tokenizer_model_folder_path ) . joinpath ( \"model.py\" ) . write_text ( source_code ) model_folder_path = wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" )) __init__ ( self , workind_directory , model_name , model_type , batch_size , nb_output , nb_instance , include_token_type , device ) special # Configuration file setup. Parameters: Name Type Description Default workind_directory str path to the working directory required model_name str model name to use (used to call TensorRT API) required model_type ModelType type of model (ONNX or TensorRT) required batch_size int dynamic batch size to use (0 to disable) required nb_output int number of tensor outputs required nb_instance int number of parallel instances to use. Mainly useful to optimize CPU inference. required include_token_type bool does the model expect to receive among tensor input one for token type? required device str where perform is done. One of [cpu, cuda] required Source code in src/transformer_deploy/templates/triton.py def __init__ ( self , workind_directory : str , model_name : str , model_type : ModelType , batch_size : int , nb_output : int , nb_instance : int , include_token_type : bool , device : str , ): \"\"\" Configuration file setup. :param workind_directory: path to the working directory :param model_name: model name to use (used to call TensorRT API) :param model_type: type of model (ONNX or TensorRT) :param batch_size: dynamic batch size to use (0 to disable) :param nb_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param include_token_type: does the model expect to receive among tensor input one for token type? :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name = model_name self . model_name += \"_onnx\" if model_type == ModelType . ONNX else \"_tensorrt\" self . model_folder_name = f \" { self . model_name } _model\" self . tokenizer_folder_name = f \" { self . model_name } _tokenize\" self . inference_folder_name = f \" { self . model_name } _inference\" self . batch_size = batch_size self . nb_model_output = nb_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . include_token_type = include_token_type self . workind_directory = workind_directory if model_type == ModelType . ONNX : self . input_type = \"TYPE_INT64\" self . inference_platform = \"onnxruntime_onnx\" elif model_type == ModelType . TensorRT : self . input_type = \"TYPE_INT32\" self . inference_platform = \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { model_type } \" ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" create_folders ( self , tokenizer , model_path ) # Generate configuration folder layout. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required model_path str ouput path required Source code in src/transformer_deploy/templates/triton.py def create_folders ( self , tokenizer : PreTrainedTokenizer , model_path : str ) -> None : \"\"\" Generate configuration folder layout. :param tokenizer: tokenizer to use :param model_path: ouput path \"\"\" wd_path = Path ( self . workind_directory ) wd_path . mkdir ( parents = True , exist_ok = True ) for folder_name , conf_func in [ ( self . model_folder_name , self . get_model_conf ), ( self . tokenizer_folder_name , self . get_tokenize_conf ), ( self . inference_folder_name , self . get_inference_conf ), ]: current_folder = wd_path . joinpath ( folder_name ) current_folder . mkdir ( exist_ok = True ) conf = conf_func () current_folder . joinpath ( \"config.pbtxt\" ) . write_text ( conf ) version_folder = current_folder . joinpath ( \"1\" ) version_folder . mkdir ( exist_ok = True ) tokenizer_model_folder_path = wd_path . joinpath ( self . tokenizer_folder_name ) . joinpath ( \"1\" ) tokenizer . save_pretrained ( str ( tokenizer_model_folder_path . absolute ())) source_code : str = inspect . getsource ( python_tokenizer ) Path ( tokenizer_model_folder_path ) . joinpath ( \"model.py\" ) . write_text ( source_code ) model_folder_path = wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" )) get_inference_conf ( self ) # Generate inference step configuration. Returns: Type Description inference step configuration Source code in src/transformer_deploy/templates/triton.py def get_inference_conf ( self ): \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" input_token_type_ids = \"\" if self . include_token_type : input_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () output_token_type_ids = \"\" if self . include_token_type : output_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () return f \"\"\" name: \" { self . inference_folder_name } \" max_batch_size: { self . batch_size } platform: \"ensemble\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} ensemble_scheduling {{ step [ {{ model_name: \" { self . tokenizer_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { input_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { output_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () get_model_conf ( self ) # Generate model configuration. Returns: Type Description str model configuration Source code in src/transformer_deploy/templates/triton.py def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: { self . batch_size } platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . __get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} { self . __instance_group () } \"\"\" . strip () get_tokenize_conf ( self ) # Generate tokenization step configuration. Returns: Type Description tokenization step configuration Source code in src/transformer_deploy/templates/triton.py def get_tokenize_conf ( self ): \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" name: \" { self . tokenizer_folder_name } \" max_batch_size: { self . batch_size } backend: \"python\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . __get_tokens () } ] { self . __instance_group () } \"\"\" . strip () ModelType ( Enum ) # Type of model to use Source code in src/transformer_deploy/templates/triton.py class ModelType ( Enum ): \"\"\" Type of model to use \"\"\" ONNX = 1 TensorRT = 2","title":"Triton"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration","text":"Source code in src/transformer_deploy/templates/triton.py class Configuration : def __init__ ( self , workind_directory : str , model_name : str , model_type : ModelType , batch_size : int , nb_output : int , nb_instance : int , include_token_type : bool , device : str , ): \"\"\" Configuration file setup. :param workind_directory: path to the working directory :param model_name: model name to use (used to call TensorRT API) :param model_type: type of model (ONNX or TensorRT) :param batch_size: dynamic batch size to use (0 to disable) :param nb_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param include_token_type: does the model expect to receive among tensor input one for token type? :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name = model_name self . model_name += \"_onnx\" if model_type == ModelType . ONNX else \"_tensorrt\" self . model_folder_name = f \" { self . model_name } _model\" self . tokenizer_folder_name = f \" { self . model_name } _tokenize\" self . inference_folder_name = f \" { self . model_name } _inference\" self . batch_size = batch_size self . nb_model_output = nb_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . include_token_type = include_token_type self . workind_directory = workind_directory if model_type == ModelType . ONNX : self . input_type = \"TYPE_INT64\" self . inference_platform = \"onnxruntime_onnx\" elif model_type == ModelType . TensorRT : self . input_type = \"TYPE_INT32\" self . inference_platform = \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { model_type } \" ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\" def __get_tokens ( self ): \"\"\" Generate input tensor configuration :return: input tensor configuration string \"\"\" token_type = \"\" if self . include_token_type : token_type = f \"\"\" {{ name: \"token_type_ids\" data_type: { self . input_type } dims: [-1, -1] }} , \"\"\" return f \"\"\" {{ name: \"input_ids\" data_type: { self . input_type } dims: [-1, -1] }} , { token_type } {{ name: \"attention_mask\" data_type: { self . input_type } dims: [-1, -1] }} \"\"\" def __instance_group ( self ): \"\"\" Generate instance configuration. :return: instance configuration \"\"\" return f \"\"\" instance_group [ {{ count: { self . nb_instance } kind: { self . device_kind } }} ] \"\"\" . strip () def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: { self . batch_size } platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . __get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} { self . __instance_group () } \"\"\" . strip () def get_tokenize_conf ( self ): \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" name: \" { self . tokenizer_folder_name } \" max_batch_size: { self . batch_size } backend: \"python\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . __get_tokens () } ] { self . __instance_group () } \"\"\" . strip () def get_inference_conf ( self ): \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" input_token_type_ids = \"\" if self . include_token_type : input_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () output_token_type_ids = \"\" if self . include_token_type : output_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () return f \"\"\" name: \" { self . inference_folder_name } \" max_batch_size: { self . batch_size } platform: \"ensemble\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} ensemble_scheduling {{ step [ {{ model_name: \" { self . tokenizer_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { input_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { output_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip () def create_folders ( self , tokenizer : PreTrainedTokenizer , model_path : str ) -> None : \"\"\" Generate configuration folder layout. :param tokenizer: tokenizer to use :param model_path: ouput path \"\"\" wd_path = Path ( self . workind_directory ) wd_path . mkdir ( parents = True , exist_ok = True ) for folder_name , conf_func in [ ( self . model_folder_name , self . get_model_conf ), ( self . tokenizer_folder_name , self . get_tokenize_conf ), ( self . inference_folder_name , self . get_inference_conf ), ]: current_folder = wd_path . joinpath ( folder_name ) current_folder . mkdir ( exist_ok = True ) conf = conf_func () current_folder . joinpath ( \"config.pbtxt\" ) . write_text ( conf ) version_folder = current_folder . joinpath ( \"1\" ) version_folder . mkdir ( exist_ok = True ) tokenizer_model_folder_path = wd_path . joinpath ( self . tokenizer_folder_name ) . joinpath ( \"1\" ) tokenizer . save_pretrained ( str ( tokenizer_model_folder_path . absolute ())) source_code : str = inspect . getsource ( python_tokenizer ) Path ( tokenizer_model_folder_path ) . joinpath ( \"model.py\" ) . write_text ( source_code ) model_folder_path = wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" ))","title":"Configuration"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration.__init__","text":"Configuration file setup. Parameters: Name Type Description Default workind_directory str path to the working directory required model_name str model name to use (used to call TensorRT API) required model_type ModelType type of model (ONNX or TensorRT) required batch_size int dynamic batch size to use (0 to disable) required nb_output int number of tensor outputs required nb_instance int number of parallel instances to use. Mainly useful to optimize CPU inference. required include_token_type bool does the model expect to receive among tensor input one for token type? required device str where perform is done. One of [cpu, cuda] required Source code in src/transformer_deploy/templates/triton.py def __init__ ( self , workind_directory : str , model_name : str , model_type : ModelType , batch_size : int , nb_output : int , nb_instance : int , include_token_type : bool , device : str , ): \"\"\" Configuration file setup. :param workind_directory: path to the working directory :param model_name: model name to use (used to call TensorRT API) :param model_type: type of model (ONNX or TensorRT) :param batch_size: dynamic batch size to use (0 to disable) :param nb_output: number of tensor outputs :param nb_instance: number of parallel instances to use. Mainly useful to optimize CPU inference. :param include_token_type: does the model expect to receive among tensor input one for token type? :param device: where perform is done. One of [cpu, cuda] \"\"\" self . model_name = model_name self . model_name += \"_onnx\" if model_type == ModelType . ONNX else \"_tensorrt\" self . model_folder_name = f \" { self . model_name } _model\" self . tokenizer_folder_name = f \" { self . model_name } _tokenize\" self . inference_folder_name = f \" { self . model_name } _inference\" self . batch_size = batch_size self . nb_model_output = nb_output assert nb_instance > 0 , f \"nb_instance== { nb_instance } : nb model instances should be positive\" self . nb_instance = nb_instance self . include_token_type = include_token_type self . workind_directory = workind_directory if model_type == ModelType . ONNX : self . input_type = \"TYPE_INT64\" self . inference_platform = \"onnxruntime_onnx\" elif model_type == ModelType . TensorRT : self . input_type = \"TYPE_INT32\" self . inference_platform = \"tensorrt_plan\" else : raise Exception ( f \"unknown model type: { model_type } \" ) self . device_kind = \"KIND_GPU\" if device == \"cuda\" else \"KIND_CPU\"","title":"__init__()"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration.create_folders","text":"Generate configuration folder layout. Parameters: Name Type Description Default tokenizer PreTrainedTokenizer tokenizer to use required model_path str ouput path required Source code in src/transformer_deploy/templates/triton.py def create_folders ( self , tokenizer : PreTrainedTokenizer , model_path : str ) -> None : \"\"\" Generate configuration folder layout. :param tokenizer: tokenizer to use :param model_path: ouput path \"\"\" wd_path = Path ( self . workind_directory ) wd_path . mkdir ( parents = True , exist_ok = True ) for folder_name , conf_func in [ ( self . model_folder_name , self . get_model_conf ), ( self . tokenizer_folder_name , self . get_tokenize_conf ), ( self . inference_folder_name , self . get_inference_conf ), ]: current_folder = wd_path . joinpath ( folder_name ) current_folder . mkdir ( exist_ok = True ) conf = conf_func () current_folder . joinpath ( \"config.pbtxt\" ) . write_text ( conf ) version_folder = current_folder . joinpath ( \"1\" ) version_folder . mkdir ( exist_ok = True ) tokenizer_model_folder_path = wd_path . joinpath ( self . tokenizer_folder_name ) . joinpath ( \"1\" ) tokenizer . save_pretrained ( str ( tokenizer_model_folder_path . absolute ())) source_code : str = inspect . getsource ( python_tokenizer ) Path ( tokenizer_model_folder_path ) . joinpath ( \"model.py\" ) . write_text ( source_code ) model_folder_path = wd_path . joinpath ( self . model_folder_name ) . joinpath ( \"1\" ) shutil . copy ( model_path , os . path . join ( model_folder_path , \"model.bin\" ))","title":"create_folders()"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration.get_inference_conf","text":"Generate inference step configuration. Returns: Type Description inference step configuration Source code in src/transformer_deploy/templates/triton.py def get_inference_conf ( self ): \"\"\" Generate inference step configuration. :return: inference step configuration \"\"\" input_token_type_ids = \"\" if self . include_token_type : input_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () output_token_type_ids = \"\" if self . include_token_type : output_token_type_ids = \"\"\" { key: \"token_type_ids\" value: \"token_type_ids\" }, \"\"\" . strip () return f \"\"\" name: \" { self . inference_folder_name } \" max_batch_size: { self . batch_size } platform: \"ensemble\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} ensemble_scheduling {{ step [ {{ model_name: \" { self . tokenizer_folder_name } \" model_version: -1 input_map {{ key: \"TEXT\" value: \"TEXT\" }} output_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { input_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] }} , {{ model_name: \" { self . model_folder_name } \" model_version: -1 input_map [ {{ key: \"input_ids\" value: \"input_ids\" }} , { output_token_type_ids } {{ key: \"attention_mask\" value: \"attention_mask\" }} ] output_map {{ key: \"output\" value: \"output\" }} }} ] }} \"\"\" . strip ()","title":"get_inference_conf()"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration.get_model_conf","text":"Generate model configuration. Returns: Type Description str model configuration Source code in src/transformer_deploy/templates/triton.py def get_model_conf ( self ) -> str : \"\"\" Generate model configuration. :return: model configuration \"\"\" return f \"\"\" name: \" { self . model_folder_name } \" max_batch_size: { self . batch_size } platform: \" { self . inference_platform } \" default_model_filename: \"model.bin\" input [ { self . __get_tokens () } ] output {{ name: \"output\" data_type: TYPE_FP32 dims: [-1, { self . nb_model_output } ] }} { self . __instance_group () } \"\"\" . strip ()","title":"get_model_conf()"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.Configuration.get_tokenize_conf","text":"Generate tokenization step configuration. Returns: Type Description tokenization step configuration Source code in src/transformer_deploy/templates/triton.py def get_tokenize_conf ( self ): \"\"\" Generate tokenization step configuration. :return: tokenization step configuration \"\"\" return f \"\"\" name: \" { self . tokenizer_folder_name } \" max_batch_size: { self . batch_size } backend: \"python\" input [ {{ name: \"TEXT\" data_type: TYPE_STRING dims: [ -1 ] }} ] output [ { self . __get_tokens () } ] { self . __instance_group () } \"\"\" . strip ()","title":"get_tokenize_conf()"},{"location":"reference/templates/triton/#src.transformer_deploy.templates.triton.ModelType","text":"Type of model to use Source code in src/transformer_deploy/templates/triton.py class ModelType ( Enum ): \"\"\" Type of model to use \"\"\" ONNX = 1 TensorRT = 2","title":"ModelType"},{"location":"reference/utils/args/","text":"Command line args parser parse_args ( commands = None ) # Parse command line arguments Parameters: Name Type Description Default commands List[str] to provide command line programatically None Returns: Type Description Namespace parsed command line Source code in src/transformer_deploy/utils/args.py def parse_args ( commands : List [ str ] = None ) -> argparse . Namespace : \"\"\" Parse command line arguments :param commands: to provide command line programatically :return: parsed command line \"\"\" parser = argparse . ArgumentParser ( description = \"optimize and deploy transformers\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( \"-m\" , \"--model\" , required = True , help = \"path to model or URL to Hugging Face hub\" ) parser . add_argument ( \"-t\" , \"--tokenizer\" , help = \"path to tokenizer or URL to Hugging Face hub\" ) parser . add_argument ( \"--sentence-transformers\" , action = \"store_true\" , help = \"use sentence-transformers to load model\" ) parser . add_argument ( \"--auth-token\" , default = None , help = ( \"Hugging Face Hub auth token. Set to `None` (default) for public models. \" \"For private models, use `True` to use local cached token, or a string of your HF API token\" ), ) parser . add_argument ( \"-b\" , \"--batch-size\" , default = [ 1 , 1 , 1 ], help = \"batch sizes to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-s\" , \"--seq-len\" , default = [ 16 , 16 , 16 ], help = \"sequence lengths to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-q\" , \"--quantization\" , action = \"store_true\" , help = \"INT-8 GPU quantization support\" ) parser . add_argument ( \"-w\" , \"--workspace-size\" , default = 10000 , help = \"workspace size in MiB (TensorRT)\" , type = int ) parser . add_argument ( \"-o\" , \"--output\" , default = \"triton_models\" , help = \"name to be used for \" ) parser . add_argument ( \"-n\" , \"--name\" , default = \"transformer\" , help = \"model name to be used in triton server\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"display detailed information\" ) parser . add_argument ( \"--backend\" , default = [ \"onnx\" ], help = \"backend to use. multiple args accepted.\" , nargs = \"*\" , choices = [ \"onnx\" , \"tensorrt\" ], ) parser . add_argument ( \"-d\" , \"--device\" , default = None , help = \"device to use. If not set, will be cuda if available.\" , choices = [ \"cpu\" , \"cuda\" ], ) parser . add_argument ( \"--nb-threads\" , default = 1 , help = \"# of CPU threads to use for inference\" , type = int ) parser . add_argument ( \"--nb-instances\" , default = 1 , help = \"# of model instances, may improve throughput (Triton)\" , type = int ) parser . add_argument ( \"--warmup\" , default = 10 , help = \"# of inferences to warm each model\" , type = int ) parser . add_argument ( \"--nb-measures\" , default = 1000 , help = \"# of inferences for benchmarks\" , type = int ) parser . add_argument ( \"--seed\" , default = 123 , help = \"seed for random inputs, etc.\" , type = int ) parser . add_argument ( \"--atol\" , default = 3e-1 , help = \"tolerance when comparing outputs to Pytorch ones\" , type = float ) args , _ = parser . parse_known_args ( args = commands ) return args","title":"Args"},{"location":"reference/utils/args/#src.transformer_deploy.utils.args.parse_args","text":"Parse command line arguments Parameters: Name Type Description Default commands List[str] to provide command line programatically None Returns: Type Description Namespace parsed command line Source code in src/transformer_deploy/utils/args.py def parse_args ( commands : List [ str ] = None ) -> argparse . Namespace : \"\"\" Parse command line arguments :param commands: to provide command line programatically :return: parsed command line \"\"\" parser = argparse . ArgumentParser ( description = \"optimize and deploy transformers\" , formatter_class = argparse . ArgumentDefaultsHelpFormatter ) parser . add_argument ( \"-m\" , \"--model\" , required = True , help = \"path to model or URL to Hugging Face hub\" ) parser . add_argument ( \"-t\" , \"--tokenizer\" , help = \"path to tokenizer or URL to Hugging Face hub\" ) parser . add_argument ( \"--sentence-transformers\" , action = \"store_true\" , help = \"use sentence-transformers to load model\" ) parser . add_argument ( \"--auth-token\" , default = None , help = ( \"Hugging Face Hub auth token. Set to `None` (default) for public models. \" \"For private models, use `True` to use local cached token, or a string of your HF API token\" ), ) parser . add_argument ( \"-b\" , \"--batch-size\" , default = [ 1 , 1 , 1 ], help = \"batch sizes to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-s\" , \"--seq-len\" , default = [ 16 , 16 , 16 ], help = \"sequence lengths to optimize for (min, optimal, max). Used by TensorRT and benchmarks.\" , type = int , nargs = 3 , ) parser . add_argument ( \"-q\" , \"--quantization\" , action = \"store_true\" , help = \"INT-8 GPU quantization support\" ) parser . add_argument ( \"-w\" , \"--workspace-size\" , default = 10000 , help = \"workspace size in MiB (TensorRT)\" , type = int ) parser . add_argument ( \"-o\" , \"--output\" , default = \"triton_models\" , help = \"name to be used for \" ) parser . add_argument ( \"-n\" , \"--name\" , default = \"transformer\" , help = \"model name to be used in triton server\" ) parser . add_argument ( \"-v\" , \"--verbose\" , action = \"store_true\" , help = \"display detailed information\" ) parser . add_argument ( \"--backend\" , default = [ \"onnx\" ], help = \"backend to use. multiple args accepted.\" , nargs = \"*\" , choices = [ \"onnx\" , \"tensorrt\" ], ) parser . add_argument ( \"-d\" , \"--device\" , default = None , help = \"device to use. If not set, will be cuda if available.\" , choices = [ \"cpu\" , \"cuda\" ], ) parser . add_argument ( \"--nb-threads\" , default = 1 , help = \"# of CPU threads to use for inference\" , type = int ) parser . add_argument ( \"--nb-instances\" , default = 1 , help = \"# of model instances, may improve throughput (Triton)\" , type = int ) parser . add_argument ( \"--warmup\" , default = 10 , help = \"# of inferences to warm each model\" , type = int ) parser . add_argument ( \"--nb-measures\" , default = 1000 , help = \"# of inferences for benchmarks\" , type = int ) parser . add_argument ( \"--seed\" , default = 123 , help = \"seed for random inputs, etc.\" , type = int ) parser . add_argument ( \"--atol\" , default = 3e-1 , help = \"tolerance when comparing outputs to Pytorch ones\" , type = float ) args , _ = parser . parse_known_args ( args = commands ) return args","title":"parse_args()"}]}