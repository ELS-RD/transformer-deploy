{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Callable, Dict, Optional, Set\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "from onnx import ModelProto\n",
    "from torch.nn import Linear\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PretrainedConfig, T5ForConditionalGeneration, TensorType\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Stack\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx\n",
    "import onnx\n",
    "from onnx import GraphProto, ModelProto, helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Loading Hugging Face model / tokenizer\n",
    "\n",
    "We use a specific branch of OnnxRuntime with a better management of if/else/then ONNX node:\n",
    "\n",
    "```shell\n",
    "git clone --recursive https://github.com/Microsoft/onnxruntime\n",
    "cd onnxruntime\n",
    "git fetch origin hari/location_plan_implicit_inputs\n",
    "git checkout -b hari/location_plan_implicit_inputs FETCH_HEAD\n",
    "CUDACXX=/usr/local/cuda-11.4/bin/nvcc ./build.sh \\\n",
    "    --config Release \\\n",
    "    --build_wheel \\\n",
    "    --parallel \\\n",
    "    --use_cuda \\\n",
    "    --cuda_home /usr/local/cuda-11.4 \\\n",
    "    --cudnn_home /usr/lib/x86_\n",
    "    -linux-gnu/ \\\n",
    "    --skip_test\n",
    "```\n",
    "\n",
    "> to clear previous compilation, delete content of `./build` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids: torch.Tensor = tokenizer(\"Studies show that\", return_tensors=TensorType.PYTORCH).input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "out_full: Seq2SeqLMOutput = model(input_ids=input_ids, decoder_input_ids=input_ids)\n",
    "\n",
    "\n",
    "def are_equal(a: torch.Tensor, b: torch.Tensor, atol: float = 1e-2) -> None:\n",
    "    assert np.allclose(a=a.detach().cpu().numpy(), b=b.detach().cpu().numpy(), atol=1e-2), f\"{a}\\n\\nVS\\n\\n{b}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Export to ONNX\n",
    "\n",
    "## Export encoder part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\")\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model.encoder,\n",
    "    output_path=\"test-enc.onnx\",\n",
    "    inputs_pytorch={\"input_ids\": input_ids},\n",
    "    var_output_seq=True,\n",
    "    quantization=False,\n",
    ")\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-enc.onnx\", onnx_optim_model_path=\"test-enc-opt.onnx\", architecture=\"bert\", use_cuda=True, fp16=True\n",
    ")\n",
    "\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "enc_onnx_out = inference_onnx_binding(\n",
    "    model_onnx=enc_onnx,\n",
    "    inputs={\"input_ids\": input_ids},\n",
    "    device=input_ids.device.type,\n",
    "    output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    ")[\"output\"]\n",
    "\n",
    "are_equal(a=enc_onnx_out, b=out_enc.last_hidden_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Export decoder\n",
    "\n",
    "### Wrapper to include some post processing on the decoder output\n",
    "\n",
    "The post processing is mainly a projection of the decoder output on a matrix with one of its dimension equal to model vocabulary size, so we have scores for each possible token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class ExportT5(torch.nn.Module):\n",
    "    def __init__(self, decoder: T5Stack, lm_head: Linear):\n",
    "        super(ExportT5, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, past_key_values: Tuple = None):\n",
    "        out_dec = self.decoder.forward(\n",
    "            input_ids=input_ids, encoder_hidden_states=encoder_hidden_states, past_key_values=past_key_values\n",
    "        )\n",
    "        # Rescale output before projecting on vocab\n",
    "        out_dec[\"last_hidden_state\"] = out_dec[\"last_hidden_state\"] * (model.model_dim**-0.5)\n",
    "        out_dec[\"last_hidden_state\"] = self.lm_head(out_dec[\"last_hidden_state\"])\n",
    "        return out_dec\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model_decoder = ExportT5(decoder=model.decoder, lm_head=model.lm_head).eval()\n",
    "out_model_export: torch.Tensor = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "are_equal(a=out_model_export[\"last_hidden_state\"], b=out_full.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Export decoder part to ONNX\n",
    "\n",
    "Export 2 versions of the decoder, one without cache support and one with it.\n",
    "Both models share most of their weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:529: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n",
      "In-place op on output of tensor.shape. See https://pytorch.org/docs/master/onnx.html#avoid-inplace-operations-when-using-tensor-shape-in-tracing-mode\n"
     ]
    }
   ],
   "source": [
    "model_decoder.cuda()\n",
    "# decoder output one step before\n",
    "out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_ids\": input_ids[:, -1:].type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "    \"past_key_values\": out_dec_pytorch.past_key_values,\n",
    "}\n",
    "\n",
    "# TODO replace hard coded axis names by generated one when generation works as expected\n",
    "input_names = [\n",
    "    \"input_ids\",\n",
    "    \"encoder_hidden_states\",\n",
    "    \"past_key_values.0.decoder.key\",\n",
    "    \"past_key_values.0.decoder.value\",\n",
    "    \"past_key_values.0.encoder.key\",\n",
    "    \"past_key_values.0.encoder.value\",\n",
    "    \"past_key_values.1.decoder.key\",\n",
    "    \"past_key_values.1.decoder.value\",\n",
    "    \"past_key_values.1.encoder.key\",\n",
    "    \"past_key_values.1.encoder.value\",\n",
    "    \"past_key_values.2.decoder.key\",\n",
    "    \"past_key_values.2.decoder.value\",\n",
    "    \"past_key_values.2.encoder.key\",\n",
    "    \"past_key_values.2.encoder.value\",\n",
    "    \"past_key_values.3.decoder.key\",\n",
    "    \"past_key_values.3.decoder.value\",\n",
    "    \"past_key_values.3.encoder.key\",\n",
    "    \"past_key_values.3.encoder.value\",\n",
    "    \"past_key_values.4.decoder.key\",\n",
    "    \"past_key_values.4.decoder.value\",\n",
    "    \"past_key_values.4.encoder.key\",\n",
    "    \"past_key_values.4.encoder.value\",\n",
    "    \"past_key_values.5.decoder.key\",\n",
    "    \"past_key_values.5.decoder.value\",\n",
    "    \"past_key_values.5.encoder.key\",\n",
    "    \"past_key_values.5.encoder.value\",\n",
    "]\n",
    "\n",
    "output_names = [\n",
    "    \"logits\",\n",
    "    \"present.0.decoder.key\",\n",
    "    \"present.0.decoder.value\",\n",
    "    \"present.0.encoder.key\",\n",
    "    \"present.0.encoder.value\",\n",
    "    \"present.1.decoder.key\",\n",
    "    \"present.1.decoder.value\",\n",
    "    \"present.1.encoder.key\",\n",
    "    \"present.1.encoder.value\",\n",
    "    \"present.2.decoder.key\",\n",
    "    \"present.2.decoder.value\",\n",
    "    \"present.2.encoder.key\",\n",
    "    \"present.2.encoder.value\",\n",
    "    \"present.3.decoder.key\",\n",
    "    \"present.3.decoder.value\",\n",
    "    \"present.3.encoder.key\",\n",
    "    \"present.3.encoder.value\",\n",
    "    \"present.4.decoder.key\",\n",
    "    \"present.4.decoder.value\",\n",
    "    \"present.4.encoder.key\",\n",
    "    \"present.4.encoder.value\",\n",
    "    \"present.5.decoder.key\",\n",
    "    \"present.5.decoder.value\",\n",
    "    \"present.5.encoder.key\",\n",
    "    \"present.5.encoder.value\",\n",
    "]\n",
    "\n",
    "dynamic_axis = {\n",
    "    \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
    "    \"encoder_hidden_states\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"logits\": {0: \"batch\", 1: \"decoder_sequence\"},\n",
    "    \"present.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model_decoder,\n",
    "        (model_inputs,),\n",
    "        f=\"test-dec-cache.onnx\",\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axis,\n",
    "        do_constant_folding=True,\n",
    "        opset_version=13,\n",
    "    )\n",
    "\n",
    "model_inputs_no_cache = {\n",
    "    \"input_ids\": input_ids.type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model_decoder,\n",
    "        (model_inputs_no_cache,),\n",
    "        f=\"test-dec-no-cache.onnx\",\n",
    "        input_names=list(model_inputs_no_cache.keys()),\n",
    "        output_names=output_names,\n",
    "        dynamic_axes={k: v for k, v in dynamic_axis.items() if \"past_key_values\" not in k},\n",
    "        do_constant_folding=True,\n",
    "        opset_version=13,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Merge ONNX computation graph to deduplicate weights\n",
    "\n",
    "TODO remove unecessary initializer to avoid warning messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: cache_node_onnx::Slice_1256 - size: 0.01\n",
      "name: cache_node_onnx::Slice_1257 - size: 0.01\n",
      "name: cache_node_onnx::Slice_1267 - size: 0.01\n",
      "name: cache_node_onnx::Slice_1268 - size: 0.01\n"
     ]
    }
   ],
   "source": [
    "onnx_model_cache = onnx.load(\"test-dec-cache.onnx\")\n",
    "onnx_model_no_cache = onnx.load(\"test-dec-no-cache.onnx\")\n",
    "\n",
    "prefix = \"cache_node_\"\n",
    "mapping_initializer_cache_to_no_cache = dict()\n",
    "to_add = list()\n",
    "for node_cache in onnx_model_cache.graph.initializer:\n",
    "    found = False\n",
    "    for node_no_cache in onnx_model_no_cache.graph.initializer:\n",
    "        if node_cache.raw_data == node_no_cache.raw_data:\n",
    "            found = True\n",
    "            mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name\n",
    "            break\n",
    "    if not found:\n",
    "        node_cache.name = prefix + node_cache.name\n",
    "        to_add.append(node_cache)\n",
    "        mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name\n",
    "        print(f\"name: {node_cache.name} - size: {len(node_cache.raw_data)/1024:.2f}\")\n",
    "\n",
    "onnx_model_no_cache.graph.initializer.extend(to_add)\n",
    "# I/O model names should not be prefixed\n",
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "for node in onnx_model_cache.graph.node:\n",
    "    for index, input_name in enumerate(node.input):\n",
    "        if input_name in model_io_names:\n",
    "            continue\n",
    "        node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name)\n",
    "    for index, output_name in enumerate(node.output):\n",
    "        if output_name in model_io_names:\n",
    "            continue\n",
    "        node.output[index] = prefix + output_name\n",
    "    node.name = prefix + node.name\n",
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "prefix = \"init_\"\n",
    "cache = dict()\n",
    "for node in onnx_model_no_cache.graph.initializer:\n",
    "    if node.name in model_io_names:\n",
    "        new_name = prefix + node.name\n",
    "        cache[node.name] = new_name\n",
    "        node.name = new_name\n",
    "\n",
    "for node in onnx_model_no_cache.graph.node:\n",
    "    for index, n in enumerate(node.input):\n",
    "        node.input[index] = cache.get(n, n)\n",
    "\n",
    "# mandatory for subgraph in if/else node\n",
    "assert len(onnx_model_cache.graph.output) == len(onnx_model_no_cache.graph.output)\n",
    "\n",
    "graph_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_cache.graph.node),\n",
    "    name=\"graph-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "graph_no_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_no_cache.graph.node),\n",
    "    name=\"graph-no-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "enable_cache_input = onnx.helper.make_tensor_value_info(name=\"enable_cache\", elem_type=onnx.TensorProto.BOOL, shape=[1])\n",
    "\n",
    "if_node = onnx.helper.make_node(\n",
    "    op_type=\"If\",\n",
    "    inputs=[\"enable_cache\"],\n",
    "    outputs=[o.name for o in list(onnx_model_no_cache.graph.output)],\n",
    "    then_branch=graph_cache,\n",
    "    else_branch=graph_no_cache,\n",
    ")\n",
    "\n",
    "if_graph_def: GraphProto = helper.make_graph(\n",
    "    nodes=[if_node],\n",
    "    name=\"if-model\",\n",
    "    inputs=list(onnx_model_cache.graph.input) + [enable_cache_input],\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=list(onnx_model_no_cache.graph.initializer),\n",
    ")\n",
    "\n",
    "\n",
    "model_def: ModelProto = helper.make_model(\n",
    "    if_graph_def, producer_name=\"onnx-example\", opset_imports=[helper.make_opsetid(onnx.defs.ONNX_DOMAIN, 13)]\n",
    ")\n",
    "onnx.checker.check_model(model_def)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check ONNX decoder output\n",
    "\n",
    "Compare ONNX output with and without cache, plus compare with Pytorch output."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-27 21:25:03.383764077 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1221'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383782849 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1154'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383787567 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1003'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383791360 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_919'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383796637 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1070'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383799992 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_218'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383803318 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_617'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383809697 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_852'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383815849 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1204'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383819136 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_1053'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383823672 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_449'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383827116 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_365'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383830119 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_466'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383834366 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_550'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383838356 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383841587 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_701'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383844737 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_751'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383848266 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_768'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.383852668 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Div_902'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384003928 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1321'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384008987 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1304'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384013059 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1140'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384017316 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_298'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384020617 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_399'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384029577 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_761'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384034189 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384037432 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_959'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384040775 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1041'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.38404480"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n",
    "model_decoder = model_decoder.cuda()\n",
    "input_ids = input_ids.cuda()\n",
    "model = model.eval()\n",
    "model_decoder = model_decoder.eval()\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "dec_onnx = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmark new model on real scenario\n",
    "\n",
    "TODO: print graph of cache / no cache latency for each seq len + show how to justify what we measure\n",
    "TODO: convert the model to FP16 + add some explanation regarding aggressive conversion https://pytorch.org/docs/stable/amp.html?utm_source=pocket_mylist\n",
    "TODO: add experiment with TensorRT in mixed precision\n",
    "TODO: try https://github.com/microsoft/onnxruntime/pull/11320 + https://github.com/microsoft/onnxruntime/pull/8702 + https://github.com/microsoft/onnxruntime/issues/11254"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_580'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384049488 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1222'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384052909 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_498'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384059094 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_416'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384062797 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_778'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384065949 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_1123'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384069147 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_597'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384072247 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_679'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384077006 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_860'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.384080819 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Div_942'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407664584 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Unsqueeze_152'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407674192 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Unsqueeze_150'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407680043 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::ConstantOfShape_164'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407827285 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::ConstantOfShape_136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407832692 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.407837665 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Unsqueeze_124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.431456469 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Expand_165'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.431601296 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Expand_137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472664533 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1218'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472673799 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1201'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472677450 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_1198'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472681158 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1067'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472684366 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1050'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472687754 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_997'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472691232 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_916'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472695766 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_896'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472698973 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1151'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472702027 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_1148'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472705799 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_597'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472710185 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_594'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472714430 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_1064'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472718115 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_748'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472721293 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_212'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472725008 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_745'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472728609 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_359'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472733386 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_611'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472736560 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_849'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472739677 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_443'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472742857 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_446'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472745853 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_1047'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472748881 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_460'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472751886 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_913'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472754991 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_215'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472757969 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_698'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472761196 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_544'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472764187 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_547'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472767206 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_362'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472770386 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_463'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472773769 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_765'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472776869 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_1000'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472779836 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_846'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472782948 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_614'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472785967 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_762'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472789760 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Add_899'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472793241 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_1215'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472796238 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Pow_695'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472953467 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1315'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472959183 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1219'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472962601 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1216'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472965975 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1134'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472969206 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1298'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472972442 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1120'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472976098 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1038'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472979336 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1035'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472983006 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_295'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472986911 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_171'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472990421 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472993784 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1137'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.472998443 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_758'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473001687 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_936'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473005536 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_1318'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473009868 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_956'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473013979 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_772'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473020347 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_168'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473024327 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_393'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473027609 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_492'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473031578 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_396'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473034974 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_410'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473038212 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_413'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473041591 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_775'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473045102 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_854'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473048851 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_495'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473052818 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_574'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473056269 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_577'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473059494 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_591'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473062872 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_594'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473066125 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_292'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473069748 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_676'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473073488 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_755'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473077016 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_857'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473080293 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_673'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473083789 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_1117'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473087149 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Add_939'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.473090573 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Pow_953'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.494170950 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_onnx::Mul_1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-27 21:25:03.494314492 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'onnx::Mul_1325'. It is not used by any node and should be removed from the model.\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    out_enc_pytorch: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "    previous_step_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder(\n",
    "        input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc_pytorch.last_hidden_state\n",
    "    )\n",
    "    out_dec_pytorch: BaseModelOutputWithPastAndCrossAttentions = model_decoder(\n",
    "        input_ids=input_ids, encoder_hidden_states=out_enc_pytorch.last_hidden_state\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def decoder_pytorch_inference(decoder_input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, **_):\n",
    "    with torch.inference_mode():\n",
    "        return model_decoder(input_ids=decoder_input_ids, encoder_hidden_states=encoder_hidden_states)\n",
    "\n",
    "\n",
    "def decoder_onnx_inference(\n",
    "    decoder_input_ids: torch.Tensor,\n",
    "    encoder_hidden_states: torch.Tensor,\n",
    "    enable_cache: torch.Tensor,\n",
    "    past_key_values: Optional[torch.Tensor],\n",
    "):\n",
    "    inputs_onnx_dict = {\n",
    "        \"input_ids\": decoder_input_ids,\n",
    "        \"encoder_hidden_states\": encoder_hidden_states,\n",
    "        \"enable_cache\": enable_cache,\n",
    "    }\n",
    "\n",
    "    if past_key_values is not None:\n",
    "        for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(past_key_values):\n",
    "            inputs_onnx_dict[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "            inputs_onnx_dict[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "            inputs_onnx_dict[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "            inputs_onnx_dict[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "\n",
    "    output_shape = {\"logits\": tuple(decoder_input_ids.shape) + (int(model.config.vocab_size),)}\n",
    "    enc_batch, enc_seq_len, _ = encoder_hidden_states.shape\n",
    "    if past_key_values is None:\n",
    "        dec_batch, dec_seq_len = decoder_input_ids.shape\n",
    "    else:\n",
    "        # dec_batch, _ = decoder_input_ids.shape\n",
    "        # seq len can't be guessed from input_ids which is always 1\n",
    "        dec_batch, _, dec_seq_len, _ = past_key_values[0][0].shape\n",
    "        dec_seq_len += 1\n",
    "\n",
    "    for i in range(int(model.config.num_layers)):\n",
    "        output_shape[f\"present.{i}.decoder.key\"] = (\n",
    "            int(dec_batch),\n",
    "            int(model.config.num_heads),\n",
    "            int(dec_seq_len),\n",
    "            int(model.config.d_kv),\n",
    "        )\n",
    "        output_shape[f\"present.{i}.decoder.value\"] = (\n",
    "            int(dec_batch),\n",
    "            int(model.config.num_heads),\n",
    "            int(dec_seq_len),\n",
    "            int(model.config.d_kv),\n",
    "        )\n",
    "        output_shape[f\"present.{i}.encoder.key\"] = (\n",
    "            int(dec_batch),\n",
    "            int(model.config.num_heads),\n",
    "            int(enc_seq_len),\n",
    "            int(model.config.d_kv),\n",
    "        )\n",
    "        output_shape[f\"present.{i}.encoder.value\"] = (\n",
    "            int(dec_batch),\n",
    "            int(model.config.num_heads),\n",
    "            int(enc_seq_len),\n",
    "            int(model.config.d_kv),\n",
    "        )\n",
    "    result_dict = inference_onnx_binding(\n",
    "        model_onnx=dec_onnx,\n",
    "        inputs=inputs_onnx_dict,\n",
    "        device=decoder_input_ids.device.type,\n",
    "        output_shape=output_shape,\n",
    "    )\n",
    "    past_states = list()\n",
    "    for i in range(model.config.num_layers):\n",
    "        kv = (\n",
    "            result_dict[f\"present.{i}.decoder.key\"],\n",
    "            result_dict[f\"present.{i}.decoder.value\"],\n",
    "            result_dict[f\"present.{i}.encoder.key\"],\n",
    "            result_dict[f\"present.{i}.encoder.value\"],\n",
    "        )\n",
    "        past_states.append(kv)\n",
    "    torch.cuda.synchronize()\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(\n",
    "        last_hidden_state=result_dict[\"logits\"],\n",
    "        past_key_values=past_states,\n",
    "    )\n",
    "\n",
    "\n",
    "out_dec_onnx_no_cache = decoder_onnx_inference(\n",
    "    decoder_input_ids=input_ids,\n",
    "    encoder_hidden_states=out_enc_pytorch.last_hidden_state,\n",
    "    enable_cache=torch.tensor([False], device=\"cuda\", dtype=torch.bool),\n",
    "    past_key_values=None,\n",
    ")\n",
    "are_equal(a=out_dec_onnx_no_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :])\n",
    "\n",
    "# check that past states are identical between ONNX and Pytorch\n",
    "assert len(out_dec_onnx_no_cache.past_key_values) == len(out_dec_pytorch.past_key_values)\n",
    "for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip(\n",
    "    out_dec_onnx_no_cache.past_key_values, out_dec_pytorch.past_key_values\n",
    "):\n",
    "    are_equal(a=o_dec_k, b=p_dec_k)\n",
    "    are_equal(a=o_dev_v, b=p_dev_v)\n",
    "    are_equal(a=o_enc_k, b=p_enc_k)\n",
    "    are_equal(a=o_enc_v, b=p_enc_v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "out_dec_onnx_cache = decoder_onnx_inference(\n",
    "    decoder_input_ids=input_ids[:, -1:],\n",
    "    encoder_hidden_states=out_enc_pytorch.last_hidden_state,\n",
    "    enable_cache=torch.tensor([True], device=\"cuda\", dtype=torch.bool),\n",
    "    past_key_values=previous_step_pytorch.past_key_values,\n",
    ")\n",
    "\n",
    "are_equal(a=out_dec_onnx_cache.last_hidden_state[:, -1:, :], b=out_dec_pytorch.last_hidden_state[:, -1:, :])\n",
    "\n",
    "# check that past states are identical between ONNX and Pytorch\n",
    "assert len(out_dec_onnx_cache.past_key_values) == len(out_dec_pytorch.past_key_values)\n",
    "for (o_dec_k, o_dev_v, o_enc_k, o_enc_v), (p_dec_k, p_dev_v, p_enc_k, p_enc_v) in zip(\n",
    "    out_dec_onnx_cache.past_key_values, out_dec_pytorch.past_key_values\n",
    "):\n",
    "    are_equal(a=o_dec_k, b=p_dec_k)\n",
    "    are_equal(a=o_dev_v, b=p_dev_v)\n",
    "    are_equal(a=o_enc_k, b=p_enc_k)\n",
    "    are_equal(a=o_enc_v, b=p_enc_v)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Studien Studien zeigen, Studien e Studies show that a study study reveals that study studies show. ; studies, show.</s>\n",
      "Studies show that</s>.. ; studies studies show, :; –;; (;);;.; études -</s>\n"
     ]
    }
   ],
   "source": [
    "def encoder_onnx_inference(input_ids: torch.Tensor, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    last_hidden_state = inference_onnx_binding(\n",
    "        model_onnx=enc_onnx,  # noqa: F821\n",
    "        inputs={\"input_ids\": input_ids},\n",
    "        output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    "        device=input_ids.device.type,\n",
    "    )[\"output\"]\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=last_hidden_state)\n",
    "\n",
    "\n",
    "def encoder_pytorch_inference(input_ids, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    with torch.inference_mode():\n",
    "        res = model.encoder(input_ids=input_ids)\n",
    "        return res\n",
    "\n",
    "\n",
    "# https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py\n",
    "class ExtT5(torch.nn.Module, GenerationMixin):\n",
    "    def __init__(self, config: PretrainedConfig, device: torch.device, encoder_func: Callable, decoder_func: Callable):\n",
    "        super(ExtT5, self).__init__()\n",
    "        self.main_input_name = \"input_ids\"  # https://github.com/huggingface/transformers/pull/14803\n",
    "        self.config: PretrainedConfig = config\n",
    "        self.device: torch.device = device\n",
    "\n",
    "        self.encoder_func = encoder_func\n",
    "        self.decoder_func = decoder_func\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder_func\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder_func\n",
    "\n",
    "    # from transformers library (modeling_t5.py)\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_decoder_past = ()\n",
    "        for layer_past_states in past:\n",
    "            # get the correct batch idx from layer past batch dim\n",
    "            # batch dim of `past` is at 2nd position\n",
    "            reordered_layer_past_states = ()\n",
    "            for layer_past_state in layer_past_states:\n",
    "                # need to set correct `past` for each of the four key / value states\n",
    "                reordered_layer_past_states = reordered_layer_past_states + (\n",
    "                    layer_past_state.index_select(0, beam_idx),\n",
    "                )\n",
    "\n",
    "            assert reordered_layer_past_states[0].shape == layer_past_states[0].shape\n",
    "            assert len(reordered_layer_past_states) == len(layer_past_states)\n",
    "\n",
    "            reordered_decoder_past = reordered_decoder_past + (reordered_layer_past_states,)\n",
    "        return reordered_decoder_past\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, use_cache=None, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        params = {\n",
    "            \"encoder_hidden_states\": kwargs[\"encoder_outputs\"][\"last_hidden_state\"],\n",
    "        }\n",
    "        if past is None:\n",
    "            params[self.main_input_name] = input_ids\n",
    "            params[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "        else:\n",
    "            params[self.main_input_name] = input_ids[:, -1:]\n",
    "            params[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool)\n",
    "            params[\"past_key_values\"] = past\n",
    "\n",
    "        return params\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        encoder_hidden_states: torch.Tensor,\n",
    "        enable_cache: torch.Tensor,\n",
    "        past_key_values: Optional[torch.Tensor] = None,\n",
    "        **_,\n",
    "    ):\n",
    "        dec_output = self.get_decoder()(\n",
    "            decoder_input_ids=input_ids,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            enable_cache=enable_cache,\n",
    "            past_key_values=past_key_values,\n",
    "        )\n",
    "        return Seq2SeqLMOutput(logits=dec_output.last_hidden_state, past_key_values=dec_output.past_key_values)\n",
    "\n",
    "\n",
    "model.config.use_cache = True\n",
    "model_gen = (\n",
    "    ExtT5(\n",
    "        config=model.config,\n",
    "        device=model.device,\n",
    "        encoder_func=encoder_onnx_inference,  # encoder_pytorch_inference\n",
    "        decoder_func=decoder_onnx_inference,  # decoder_pytorch_inference\n",
    "    )\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "with torch.inference_mode():\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model_gen.generate(inputs=input_ids, min_length=30, max_length=60, num_beams=4, no_repeat_ngram_size=2)[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(\n",
    "                input_ids=input_ids,\n",
    "                decoder_input_ids=input_ids,\n",
    "                min_length=30,\n",
    "                max_length=60,\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.362773418426514\n",
      "12.342561721801758\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "for _ in range(3):\n",
    "    model_gen.generate(inputs=input_ids, max_length=500, num_beams=4, no_repeat_ngram_size=2, min_length=500)\n",
    "print(time() - start)\n",
    "\n",
    "model.config.use_cache = False\n",
    "with torch.inference_mode():\n",
    "    start = time()\n",
    "    for _ in range(3):\n",
    "        model.generate(inputs=input_ids, max_length=500, num_beams=4, no_repeat_ngram_size=2, min_length=500)\n",
    "    print(time() - start)\n",
    "\n",
    "model = model.cpu()\n",
    "model_decoder = model_decoder.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with torch.inference_mode():\n",
    "#     out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "#\n",
    "# input_ort = dict()\n",
    "# input_ort[\"input_ids\"] = input_ids\n",
    "# input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "# input_ort[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# result_no_cache = inference_onnx_binding(\n",
    "#     model_onnx=dec_onnx,\n",
    "#     inputs=input_ort,\n",
    "#     device=input_ids.device.type,\n",
    "#     output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.vocab_size),)},\n",
    "# )\n",
    "#\n",
    "# input_ort[\"input_ids\"] = input_ort[\"input_ids\"][:, -1:]\n",
    "# input_ort[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# with torch.inference_mode():\n",
    "#     out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "#\n",
    "#\n",
    "# result_cache = inference_onnx_binding(\n",
    "#     model_onnx=dec_onnx,\n",
    "#     inputs=input_ort,\n",
    "#     device=input_ort[\"input_ids\"].device.type,\n",
    "#     output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.vocab_size),)},\n",
    "# )\n",
    "#\n",
    "# assert np.allclose(a=result_cache[\"logits\"][:, -1:, :].detach().cpu(), b=result_no_cache[\"logits\"][:, -1:, :].detach().cpu(), atol=1e-2)\n",
    "#\n",
    "# with torch.inference_mode():\n",
    "#     result_python = model_decoder(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# assert np.allclose(\n",
    "#     a=result_no_cache[\"logits\"][:, -1:, :].detach().cpu(), b=result_python.last_hidden_state[:, -1:, :].detach().cpu().numpy(), atol=1e-2\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# inputs_onnx_dict = {\"input_ids\": input_ids[:, -1:],\n",
    "#                     \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "#                     \"enable_cache\": torch.tensor([True], device=\"cuda\", dtype=torch.bool),\n",
    "#                     }\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(out_dec_pytorch.past_key_values):\n",
    "#     inputs_onnx_dict[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "#     inputs_onnx_dict[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "#     inputs_onnx_dict[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "#     inputs_onnx_dict[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "#\n",
    "# for ref_k, ref_v in input_ort.items():\n",
    "#     assert ref_k in inputs_onnx_dict, f\"missing key: {ref_k}\"\n",
    "#     assert torch.equal(ref_v, inputs_onnx_dict[ref_k]), f\"unexpected diff: {ref_k}\"\n",
    "#\n",
    "# assert len(input_ort) == len(inputs_onnx_dict)\n",
    "\n",
    "\n",
    "# del result_no_cache\n",
    "# del result_cache\n",
    "# del dec_onnx\n",
    "# del input_ort\n",
    "# del out_enc\n",
    "# del out_dec_pytorch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# output_shape = {\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.vocab_size),)}\n",
    "# enc_batch, enc_seq_len, _ = inputs_onnx_dict[\"encoder_hidden_states\"].shape\n",
    "# # USE ORIGINAL INPUT IDs AND NOT THE LAST TOKEN ONE\n",
    "# dec_batch, dec_seq_len = input_ids.shape\n",
    "# for i in range(int(model.config.num_layers)):\n",
    "#     output_shape[f\"present.{i}.decoder.key\"] = (int(dec_batch), int(model.config.num_heads), int(dec_seq_len), int(model.config.d_kv))\n",
    "#     output_shape[f\"present.{i}.decoder.value\"] = (int(dec_batch), int(model.config.num_heads), int(dec_seq_len), int(model.config.d_kv))\n",
    "#     output_shape[f\"present.{i}.encoder.key\"] = (int(dec_batch), int(model.config.num_heads), int(enc_seq_len), int(model.config.d_kv))\n",
    "#     output_shape[f\"present.{i}.encoder.value\"] = (int(dec_batch), int(model.config.num_heads), int(enc_seq_len), int(model.config.d_kv))\n",
    "#\n",
    "# result_dict = inference_onnx_binding(\n",
    "#     model_onnx=dec_onnx,\n",
    "#     inputs=inputs_onnx_dict,\n",
    "#     device=inputs_onnx_dict[\"input_ids\"].device.type,\n",
    "#     output_shape=output_shape,\n",
    "# )\n",
    "# past_states = list()\n",
    "# for i in range(model.config.num_layers):\n",
    "#     kv = (\n",
    "#         result_dict[f\"present.{i}.decoder.key\"],\n",
    "#         result_dict[f\"present.{i}.decoder.value\"],\n",
    "#         result_dict[f\"present.{i}.encoder.key\"],\n",
    "#         result_dict[f\"present.{i}.encoder.value\"],\n",
    "#     )\n",
    "#     past_states.append(kv)\n",
    "#\n",
    "# print(result_dict[\"logits\"][:, -1:, :])\n",
    "# print(result_python.last_hidden_state[:, -1:, :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model = model.to(\"cuda\")\n",
    "# model_decoder = model_decoder.cuda()\n",
    "\n",
    "# input_ids = input_ids.cuda()\n",
    "# model_def.SerializeToString()\n",
    "# \"test-dec-no-cache.onnx\"\n",
    "# dec_onnx = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def decoder_onnx_inference(input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, enable_cache: torch.Tensor, past_key_values: Optional[torch.Tensor]):\n",
    "#     inputs_dict = {\"input_ids\": input_ids, \"encoder_hidden_states\": encoder_hidden_states, \"enable_cache\": enable_cache}\n",
    "#     if past_key_values is not None:\n",
    "#         for i in range(len(past_key_values)):\n",
    "#             inputs_dict[f\"past_key_values.{i}.decoder.key\"] = past_key_values[i][0].detach().cuda()\n",
    "#             inputs_dict[f\"past_key_values.{i}.decoder.value\"] = past_key_values[i][1].detach().cuda()\n",
    "#             inputs_dict[f\"past_key_values.{i}.encoder.key\"] = past_key_values[i][2].detach().cuda()\n",
    "#             inputs_dict[f\"past_key_values.{i}.encoder.value\"] = past_key_values[i][3].detach().cuda()\n",
    "#\n",
    "#     output_shape = {\"logits\": tuple(input_ids.shape) + (int(model.config.vocab_size),)}\n",
    "#     dec_batch, dec_seq_len = input_ids.shape\n",
    "#     print(f\"input_ids {input_ids.shape}\")\n",
    "#     enc_batch, enc_seq_len, _ = encoder_hidden_states.shape\n",
    "#     print(f\"encoder_hidden_states {encoder_hidden_states.shape}\")\n",
    "#     for i in range(int(model.config.num_layers)):\n",
    "#         output_shape[f\"present.{i}.decoder.key\"] = (int(dec_batch), int(model.config.num_heads), int(dec_seq_len), int(model.config.d_kv))\n",
    "#         output_shape[f\"present.{i}.decoder.value\"] = (int(dec_batch), int(model.config.num_heads), int(dec_seq_len), int(model.config.d_kv))\n",
    "#         output_shape[f\"present.{i}.encoder.key\"] = (int(dec_batch), int(model.config.num_heads), int(enc_seq_len), int(model.config.d_kv))\n",
    "#         output_shape[f\"present.{i}.encoder.value\"] = (int(dec_batch), int(model.config.num_heads), int(enc_seq_len), int(model.config.d_kv))\n",
    "#\n",
    "#     result_dict = inference_onnx_binding(\n",
    "#         model_onnx=dec_onnx,\n",
    "#         inputs=inputs_dict,\n",
    "#         device=input_ids.device.type,\n",
    "#         output_shape=output_shape,\n",
    "#     )\n",
    "#     past_states = list()\n",
    "#     for i in range(model.config.num_layers):\n",
    "#         kv = (\n",
    "#             result_dict[f\"present.{i}.decoder.key\"],\n",
    "#             result_dict[f\"present.{i}.decoder.value\"],\n",
    "#             result_dict[f\"present.{i}.encoder.key\"],\n",
    "#             result_dict[f\"present.{i}.encoder.value\"],\n",
    "#         )\n",
    "#         past_states.append(kv)\n",
    "#\n",
    "#     return BaseModelOutputWithPastAndCrossAttentions(\n",
    "#         last_hidden_state=result_dict[\"logits\"],\n",
    "#         past_key_values=past_states,\n",
    "#     )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "#\n",
    "# dec_onnx_no_cache = decoder_onnx_inference(\n",
    "#     input_ids=input_ids,\n",
    "#     encoder_hidden_states=out_enc.last_hidden_state.detach(),\n",
    "#     enable_cache=torch.tensor([False], device=\"cuda\", dtype=torch.bool),\n",
    "#     past_key_values=None,\n",
    "# )\n",
    "# assert np.allclose(a=dec_onnx_no_cache.last_hidden_state.detach().cpu().numpy(), b=out_full.logits.detach().cpu().numpy(), atol=1e-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# previous_step_dec = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "# dec_onnx_cache = decoder_onnx_inference(\n",
    "#     input_ids=input_ids[:, -1:],\n",
    "#     encoder_hidden_states=out_enc.last_hidden_state.detach(),\n",
    "#     enable_cache=torch.tensor([True], device=\"cuda\", dtype=torch.bool),\n",
    "#     past_key_values=previous_step_dec.past_key_values,\n",
    "# )\n",
    "#\n",
    "# assert np.allclose(a=dec_onnx_cache.last_hidden_state.detach().cpu().numpy(), b=out_full.logits[:,-1,:].detach().cpu().numpy(), atol=2e-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.cuda.synchronize()\n",
    "# model_decoder.eval()\n",
    "# with torch.inference_mode():\n",
    "#     print(model_decoder(input_ids=input_ids[:, -1:], encoder_hidden_states=out_enc.last_hidden_state, past_key_values=previous_step_dec.past_key_values).last_hidden_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# torch.cuda.synchronize()\n",
    "# model = model.eval()\n",
    "# with torch.inference_mode():\n",
    "#     out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "#     out_dec_pytorch = model_decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# model_to_export = ExportT5(decoder=model.decoder, lm_head=model.lm_head).eval()\n",
    "# out_model_export: torch.Tensor = model_to_export(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "# assert np.allclose(out_model_export.detach().cpu().numpy(), out_full.logits.detach().cpu().numpy(), atol=1e-5)\n",
    "#\n",
    "# inputs_onnx = {\"input_ids\": input_ids, \"encoder_hidden_states\": out_enc.last_hidden_state}\n",
    "#\n",
    "# convert_to_onnx(\n",
    "#     model_pytorch=model_to_export,\n",
    "#     output_path=\"test-dec.onnx\",\n",
    "#     inputs_pytorch=inputs_onnx,\n",
    "#     var_output_seq=False,\n",
    "#     quantization=False,\n",
    "#     fix_output_dim_size=False,  # specific to decoder part\n",
    "# )\n",
    "# optimize_onnx(\n",
    "#     onnx_path=\"test-dec.onnx\",\n",
    "#     onnx_optim_model_path=\"test-dec-opt.onnx\",\n",
    "#     architecture=\"bert\",\n",
    "#     use_cuda=True,\n",
    "#     fp16=True,\n",
    "#     num_attention_heads=model.config.num_heads,\n",
    "#     hidden_size=model.config.d_model,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "# runtime: Runtime = trt.Runtime(trt_logger)\n",
    "# trt_model_name = \"trt-t5-dec.plan\"\n",
    "#\n",
    "# # create only of does not exist because it's slow to run...\n",
    "#\n",
    "# # 768 for base model, 512 for small, make it dependent from the Pytorch model configuration\n",
    "# input_id_shape = TensorRTShape(min_shape=[5, 1], optimal_shape=[5, 500], max_shape=[5, 500], input_name=\"input_ids\")\n",
    "# encoder_hidden_states_shape = TensorRTShape(\n",
    "#     min_shape=[5, 1, 512], optimal_shape=[5, 500 // 2, 512], max_shape=[5, 500, 512], input_name=\"encoder_hidden_states\"\n",
    "# )\n",
    "#\n",
    "#\n",
    "# model = model.cuda()\n",
    "# model_onnx: ModelProto = onnx.load(\"test-dec.onnx\")\n",
    "# model_onnx_all_nodes = add_output_nodes(model=model_onnx)\n",
    "# onnx_graph: Dict[str, Set[str]] = get_adjency_dict(model=model_onnx)\n",
    "# ort_model_all_nodes = create_model_for_provider(model_onnx_all_nodes.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "#\n",
    "#\n",
    "# # use info from tokenizer size and max shape provided through the command line\n",
    "# def get_random_input():\n",
    "#     input = torch.randint(high=tokenizer.vocab_size, size=(5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "#     hidden_state = model.encoder(input_ids=input).last_hidden_state.detach().cpu().numpy()\n",
    "#     return {\"input_ids\": input.detach().cpu().numpy(), \"encoder_hidden_states\": hidden_state}\n",
    "#\n",
    "#\n",
    "# keep_fp32 = get_list_fp32_nodes(\n",
    "#     onnx_graph=onnx_graph, model=ort_model_all_nodes, get_input=get_random_input, nb_try=200\n",
    "# )\n",
    "# model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# engine: ICudaEngine = build_engine(\n",
    "#     runtime=runtime,\n",
    "#     onnx_file_path=\"test-dec.onnx\",\n",
    "#     logger=trt_logger,\n",
    "#     workspace_size=20000 * 1024**2,\n",
    "#     fp16=True,\n",
    "#     int8=False,\n",
    "#     input_shapes=[input_id_shape, encoder_hidden_states_shape],\n",
    "#     fp16_fix=get_fix_fp16_network_func(keep_fp32=keep_fp32),\n",
    "# )\n",
    "# save_engine(engine, trt_model_name)\n",
    "#\n",
    "# tensorrt_model = load_engine(runtime=runtime, engine_file_path=trt_model_name)\n",
    "# a = tensorrt_model(\n",
    "#     {\n",
    "#         \"input_ids\": input_ids.type(torch.int32).repeat((5, 1)),\n",
    "#         \"encoder_hidden_states\": out_enc.last_hidden_state.repeat((5, 1, 1)),\n",
    "#     }\n",
    "# )\n",
    "# print(a[0])\n",
    "#\n",
    "# benchmark_input = torch.ones((5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "# benchmark_enc_output = out_enc.last_hidden_state.repeat((5, 1, 1))\n",
    "# for _ in range(10):\n",
    "#     tensorrt_model(\n",
    "#         {\n",
    "#             \"input_ids\": benchmark_input,\n",
    "#             \"encoder_hidden_states\": benchmark_enc_output,\n",
    "#         }\n",
    "#     )\n",
    "# start = time()\n",
    "# for _ in range(100):\n",
    "#     tensorrt_model(\n",
    "#         {\n",
    "#             \"input_ids\": benchmark_input,\n",
    "#             \"encoder_hidden_states\": benchmark_enc_output,\n",
    "#         }\n",
    "#     )\n",
    "# print(time() - start)\n",
    "#\n",
    "# dec_onnx = create_model_for_provider(\"test-dec-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "# dec_onnx_out = decoder_onnx_inference(input_ids=input_ids, last_hidden_state=out_enc.last_hidden_state)\n",
    "#\n",
    "#\n",
    "# for _ in range(10):\n",
    "#     decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "# start = time()\n",
    "# for _ in range(100):\n",
    "#     decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "# print(time() - start)\n",
    "#\n",
    "# model.cuda()\n",
    "# for _ in range(10):\n",
    "#     model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "# start = time()\n",
    "# for _ in range(100):\n",
    "#     model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "# print(time() - start)\n",
    "\n",
    "# TensorRT, ONNX Runtime, Pytorch\n",
    "\n",
    "# sequence 500\n",
    "# 0.8640644550323486\n",
    "# 0.6695075035095215\n",
    "# 1.1308434009552002\n",
    "\n",
    "# sequence 250\n",
    "# 0.9177014827728271\n",
    "# 0.6861860752105713\n",
    "# 1.1923034191131592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "# model.decoder(\n",
    "#     input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state, past_key_values=None\n",
    "# ).last_hidden_state[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "# model.decoder(\n",
    "#     input_ids=input_ids[:, -1:],\n",
    "#     encoder_hidden_states=out_enc.last_hidden_state,\n",
    "#     past_key_values=out_dec_pytorch.past_key_values,\n",
    "# ).last_hidden_state[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# from transformers.onnx.features import FeaturesManager\n",
    "#\n",
    "# feature = \"seq2seq-lm-with-past\"\n",
    "# model = FeaturesManager.get_model_from_feature(feature, model_name)\n",
    "# model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "# onnx_config = model_onnx_config(model.config)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     model.config.return_dict = True\n",
    "#     model.eval()\n",
    "#\n",
    "#     # Check if we need to override certain configuration item\n",
    "#     if onnx_config.values_override is not None:\n",
    "#         for override_config_key, override_config_value in onnx_config.values_override.items():\n",
    "#             setattr(model.config, override_config_key, override_config_value)\n",
    "#\n",
    "#     # Ensure inputs match\n",
    "#     model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH)\n",
    "#     for k, v in model_inputs.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             model_inputs[k] = model_inputs[k].type(torch.int32)\n",
    "#     onnx_outputs = list(onnx_config.outputs.keys())\n",
    "#\n",
    "#     onnx_config.patch_ops()\n",
    "#\n",
    "#     # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "#     torch.onnx.export(\n",
    "#         model,\n",
    "#         (model_inputs,),\n",
    "#         f=\"test-dec-cache.onnx\",\n",
    "#         input_names=list(onnx_config.inputs.keys()),\n",
    "#         output_names=onnx_outputs,\n",
    "#         dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())},\n",
    "#         do_constant_folding=True,\n",
    "#         use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()),\n",
    "#         enable_onnx_checker=True,\n",
    "#         opset_version=13,\n",
    "#     )\n",
    "#\n",
    "#     onnx_config.restore_ops()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CPUExecutionProvider\")\n",
    "# input_ort = dict()\n",
    "# input_ort[\"input_ids\"] = input_ids[:, -1:].type(torch.int32).detach().cpu().numpy()\n",
    "# input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "#\n",
    "# ort_cache.run([\"logits\"], input_ort)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ort_no_cache = create_model_for_provider(\"test-dec-no-cache.onnx\", \"CPUExecutionProvider\")\n",
    "# input_no_cache = dict()\n",
    "# input_no_cache[\"input_ids\"] = input_ids.type(torch.int32).detach().cpu().numpy()\n",
    "# input_no_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "#\n",
    "# ort_no_cache.run([\"logits\"], input_no_cache)[0][:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids[:, -1:]\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach()\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.cuda()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.cuda()\n",
    "#\n",
    "#\n",
    "# print(inference_onnx_binding(model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (1, 1, 512)})[\n",
    "#     \"logits\"\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ort_cache = create_model_for_provider(\"test-dec-no-cache.onnx\", \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach()\n",
    "#\n",
    "# print(inference_onnx_binding(model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (1, 4, 512)})[\n",
    "#     \"logits\"\n",
    "# ][:,-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "# out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CPUExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids.type(torch.int32).detach().cpu().numpy()\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "# input_cache[\"enable_cache\"] = np.array([False])\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "#\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0][:,-1,:][:, :10])\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0].shape)\n",
    "#\n",
    "#\n",
    "# input_cache[\"enable_cache\"] = np.array([True])\n",
    "# input_cache[\"input_ids\"] = input_cache[\"input_ids\"][:, -1:]\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0][:,-1,:][:, :10])\n",
    "# print(ort_cache.run([\"logits\"], input_cache)[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "#\n",
    "# gc.collect()\n",
    "# model.cuda()\n",
    "# input_ids_benchmark = torch.ones((4, 200), dtype=torch.int32, device=\"cuda\")\n",
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids_benchmark)  #\n",
    "# out_dec_pytorch = model.decoder(input_ids=input_ids_benchmark[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "# input_ort = dict()\n",
    "# input_ort[\"input_ids\"] = input_ids_benchmark.type(torch.int32)\n",
    "# input_ort[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "# input_ort[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.key\"] = k_dec\n",
    "#     input_ort[f\"past_key_values.{index}.decoder.value\"] = v_dec\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.key\"] = k_enc\n",
    "#     input_ort[f\"past_key_values.{index}.encoder.value\"] = v_enc\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     result_dict = inference_onnx_binding(\n",
    "#         model_onnx=ort_cache,\n",
    "#         inputs=input_ort,\n",
    "#         device=input_ids_benchmark.device.type,\n",
    "#         output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.d_model),)},\n",
    "#     )\n",
    "# print(time() - start)\n",
    "# print(result_dict[\"logits\"][:, -1:, :][0, :, :10])\n",
    "#\n",
    "# input_ort[\"enable_cache\"] = torch.tensor([True], device=\"cuda\", dtype=torch.bool)\n",
    "# input_ort[\"input_ids\"] = input_ort[\"input_ids\"][:, -1:].type(torch.int32)\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     result_dict = inference_onnx_binding(\n",
    "#         model_onnx=ort_cache,\n",
    "#         inputs=input_ort,\n",
    "#         device=input_ids.device.type,\n",
    "#         output_shape={\"logits\": tuple(input_ort[\"input_ids\"].shape) + (int(model.config.d_model),)},\n",
    "#     )\n",
    "# print(time() - start)\n",
    "# print(result_dict[\"logits\"][:, -1:, :][0, :, :10])\n",
    "#\n",
    "# del input_ids_benchmark\n",
    "# del ort_cache\n",
    "# del input_ort\n",
    "# del out_enc\n",
    "# del out_dec_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# del input_ids_benchmark\n",
    "# del out_enc\n",
    "# del out_dec_pytorch\n",
    "# del input_ort\n",
    "# del ort_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tout_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0))\n",
    "# out_dec_pytorch = model.decoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0), encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids.type(torch.int32)\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state\n",
    "# input_cache[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = torch.zeros((1,8,1,64), dtype=torch.float32, device=\"cuda\") # v_enc.detach().cpu().numpy()\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     result_dict = inference_onnx_binding(\n",
    "#     model_onnx=ort_cache,\n",
    "#     inputs=input_cache,\n",
    "#     device=input_ids.device.type,\n",
    "#     output_shape={\"logits\" : tuple(input_ids.shape) + (int(model.config.vocab_size),)},\n",
    "#     )\n",
    "# print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0))\n",
    "# out_dec_pytorch = model.decoder(input_ids=torch.range(1, 1000, dtype=torch.int32, device=\"cuda\").unsqueeze(0), encoder_hidden_states=out_enc.last_hidden_state)\n",
    "#\n",
    "# ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "# input_cache = dict()\n",
    "# input_cache[\"input_ids\"] = input_ids[:, :-1].type(torch.int32).detach().cpu().numpy()\n",
    "# input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "# input_cache[\"enable_cache\"] = torch.tensor([False], device=\"cuda\", dtype=torch.bool)\n",
    "#\n",
    "# for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "#     out_dec_pytorch.past_key_values\n",
    "# ):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "#     input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "#\n",
    "#\n",
    "# start = time()\n",
    "# for _ in range(10):\n",
    "#     ort_cache.run([\"logits\"], input_cache)\n",
    "# print(time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}