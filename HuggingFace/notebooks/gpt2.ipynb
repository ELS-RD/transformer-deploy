{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "235d2f1b-439e-4cd0-8286-1d63a13f2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from tensorrt.tensorrt import Logger, Runtime\n",
    "from torch.nn import Module\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import BatchEncoding\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    ")\n",
    "from transformers import PretrainedConfig\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider\n",
    "from transformer_deploy.backends.ort_utils import optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import get_model_size\n",
    "from transformer_deploy.backends.trt_utils import load_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Download HuggingFace GPT-2 model \n",
    "\n",
    "First, we download the original HuggingFace PyTorch GPT-2 model from HuggingFace model hubs, together with its associated tokernizer.\n",
    "\n",
    "The GPT-2 variants supported by TensorRT 8 are: gpt2 (117M), gpt2-large (774M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download model and tokernizer\n",
    "model_name = \"gpt2\"  # choices: gpt2 | gpt2-large\n",
    "\n",
    "model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# to avoid error message or passing some args to each generate call\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Inference with PyTorch model\n",
    "\n",
    "#### Single example inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5c5fe7-7733-49b5-89c5-c8278ff54fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   616,  3290,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "----\n",
      "tensor([[[ -35.2362,  -35.3266,  -38.9753,  ...,  -44.4645,  -43.9974,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5831, -116.5724,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [ -88.7435,  -89.8643,  -93.1977,  ...,  -92.3839,  -96.1782,\n",
      "           -92.1273],\n",
      "         [ -85.1646,  -88.3379,  -92.8703,  ...,  -99.8017,  -94.7657,\n",
      "           -90.9330],\n",
      "         [-116.7280, -119.3950, -121.7259,  ..., -129.1003, -124.6102,\n",
      "          -121.6092],\n",
      "         [ -61.9847,  -63.7082,  -65.6898,  ...,  -76.0924,  -71.7898,\n",
      "           -66.1154]]])\n"
     ]
    }
   ],
   "source": [
    "# carry out inference with a single sample\n",
    "inputs = tokenizer(\"Hello, my dog is \", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(\"----\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c0468b-976a-4a08-98d3-e87578ec067f",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `gpt2_inference` which executes the inference on a single batch repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "`TimingProfile` is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecdf8f00-0562-482b-9bec-b0b7596aec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HuggingFace.GPT2.measurements import gpt2_inference\n",
    "# from HuggingFace.NNDF.networks import TimingProfile\n",
    "#\n",
    "# # Benchmarking TensorRT performance on single batch\n",
    "# output, decoder_e2e_median_time = gpt2_inference(\n",
    "#     model.to(\"cuda:0\"), inputs.input_ids.to(\"cuda:0\"), TimingProfile(iterations=10, number=1, warmup=1)\n",
    "# )\n",
    "# decoder_e2e_median_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805756f-81f9-43cf-88f6-b205ecd23034",
   "metadata": {},
   "source": [
    "#### Open-end text generation\n",
    "Next, we will employ the PyTorch model for the open-end text generation task, which GPT-2 is particularly good at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c3d01fc-9928-486b-9d15-de84d46528e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HuggingFace.GPT2.GPT2ModelConfig import GPT2ModelTRTConfig\n",
    "#\n",
    "# sample_output = model.to(\"cuda:0\").generate(\n",
    "#     inputs.input_ids.to(\"cuda:0\"),\n",
    "#     max_length=GPT2ModelTRTConfig.MAX_SEQUENCE_LENGTH[\"gpt2\"],\n",
    "#     num_beams=5,\n",
    "#     num_return_sequences=3,\n",
    "#     do_sample=True,\n",
    "# )\n",
    "#\n",
    "# # de-tokenize model output to raw text\n",
    "# for s in sample_output:\n",
    "#     print(tokenizer.decode(s, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b016c2f-7982-44ac-81e5-d3854391a8b6",
   "metadata": {},
   "source": [
    "For benchmarking purposes, we will employ a helper function `full_inference_greedy` which executes the inference repeatedly and measures end to end execution time. Let's take note of this execution time for later comparison with TensorRT. \n",
    " \n",
    "TimingProfile is a named tuple that specifies the number of experiments and number of times to call the function per iteration (and number of warm-up calls although it is not used here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93aea249-529e-4b5e-9759-e0c8370391a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from HuggingFace.GPT2.measurements import full_inference_greedy\n",
    "#\n",
    "# # get complete decoder inference result and its timing profile\n",
    "# sample_output, full_e2e_median_runtime = full_inference_greedy(\n",
    "#     model.to(\"cuda:0\"),\n",
    "#     inputs.input_ids,\n",
    "#     TimingProfile(iterations=10, number=1, warmup=1),\n",
    "#     max_length=64,\n",
    "# )\n",
    "# full_e2e_median_runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Convert to ONNX format\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ae8a8",
   "metadata": {},
   "source": [
    "## Build graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f5f081",
   "metadata": {},
   "source": [
    "### Build ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a80b6a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    }
   ],
   "source": [
    "input_ids: BatchEncoding = tokenizer(\n",
    "    \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\"\n",
    ")\n",
    "# some inference engines don't support int64 tensor as inputs, we convert tensors when required\n",
    "for k, v in input_ids.items():  # type: str, torch.Tensor\n",
    "    if v.dtype in [torch.int64, torch.long]:\n",
    "        input_ids[k] = v.type(torch.int32)\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model,\n",
    "    output_path=\"test-gpt2.onnx\",\n",
    "    inputs_pytorch=dict(input_ids),\n",
    "    quantization=False,\n",
    "    var_output_seq=True,\n",
    ")\n",
    "\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a148c2",
   "metadata": {},
   "source": [
    "### Optimize ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15f49fd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fusion_base:Fused LayerNormalization count: 25\n",
      "INFO:fusion_base:Fused FastGelu count: 12\n",
      "INFO:fusion_utils:Remove reshape node Reshape_9 since its input shape is same as output: ['batch_size', 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_19 since its input shape is same as output: [1, 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_2700 since its input shape is same as output: ['batch_size', 'sequence', 768]\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 23 nodes are removed\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 864 nodes are removed\n",
      "INFO:onnx_model_gpt2:postprocess: remove Reshape count:72\n",
      "INFO:fusion_base:Fused FastGelu(add bias) count: 12\n",
      "INFO:onnx_model_bert:opset verion: 13\n",
      "INFO:onnx_model_bert:Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:root:optimizations applied: {'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:onnx_model:Sort graphs in topological order\n",
      "INFO:onnx_model:Output model to test-gpt2-opt.onnx\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "num_attention_heads, hidden_size = get_model_size(path=model_name)\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-gpt2.onnx\",\n",
    "    onnx_optim_model_path=\"test-gpt2-opt.onnx\",\n",
    "    fp16=True,\n",
    "    use_cuda=True,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    architecture=\"gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578751c",
   "metadata": {},
   "source": [
    "## Build TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "747ebae8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/24/2022-14:00:27] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[01/24/2022-14:00:27] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 13595, GPU 10348 (MiB)\n",
      "[01/24/2022-14:00:27] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[01/24/2022-14:00:27] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 13595, GPU 10348 (MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 664882988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/24/2022-14:00:27] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:27] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:27] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:29] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:29] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:29] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:29] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:29] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:29] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:29] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:29] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:30] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:30] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:30] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:30] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:30] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:30] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:30] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:30] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:31] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:31] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:32] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:32] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:32] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:32] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:32] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:32] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:33] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:33] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:34] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:34] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:34] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:34] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:35] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:35] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:36] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:37] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:37] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:37] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:38] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:38] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:38] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:38] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:40] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:40] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:41] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:41] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:42] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:42] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:42] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:43] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:45] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:45] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:46] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:46] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:47] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:47] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:48] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:48] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:51] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:51] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:52] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:52] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:53] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:53] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:54] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:54] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:58] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:00:58] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:00:59] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:00] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:01] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:01] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:02] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:02] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:07] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:07] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:08] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:08] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:10] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/24/2022-14:01:10] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/24/2022-14:01:12] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 14612, GPU 10437 (MiB)\n",
      "[01/24/2022-14:01:12] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[01/24/2022-14:13:34] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[01/24/2022-14:13:47] [TRT] [I] Total Host Persistent Memory: 288\n",
      "[01/24/2022-14:13:47] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[01/24/2022-14:13:47] [TRT] [I] Total Scratch Memory: 52166400\n",
      "[01/24/2022-14:13:47] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 384 MiB, GPU 8939 MiB\n",
      "[01/24/2022-14:13:47] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.014771ms to assign 4 blocks to 4 nodes requiring 90766336 bytes.\n",
      "[01/24/2022-14:13:47] [TRT] [I] Total Activation Memory: 90766336\n",
      "[01/24/2022-14:13:47] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 15104, GPU 10822 (MiB)\n",
      "[01/24/2022-14:13:47] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +512, now: CPU 0, GPU 1855 (MiB)\n",
      "[01/24/2022-14:13:48] [TRT] [I] Loaded engine size: 978 MiB\n",
      "[01/24/2022-14:13:48] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 16081, GPU 10790 (MiB)\n",
      "[01/24/2022-14:13:48] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +487, now: CPU 0, GPU 1830 (MiB)\n"
     ]
    }
   ],
   "source": [
    "from transformer_deploy.backends.trt_utils import build_engine, save_engine\n",
    "from tensorrt import ICudaEngine\n",
    "\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.INFO)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "if True:\n",
    "    engine: ICudaEngine = build_engine(\n",
    "        runtime=runtime,\n",
    "        onnx_file_path=\"test-gpt2.onnx\",\n",
    "        logger=trt_logger,\n",
    "        min_shape=(1, 1),\n",
    "        optimal_shape=(1, 128),  # num beam -> batch size\n",
    "        max_shape=(1, 384),  # num beam -> batch size\n",
    "        workspace_size=12000 * 1024 * 1024,\n",
    "        fp16=True,\n",
    "        int8=False,\n",
    "    )\n",
    "    save_engine(engine, \"test-gpt2.plan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe349f",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f6165",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa67b468",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformer_deploy.backends.ort_utils import inference_onnx_binding\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from typing import Callable, Dict\n",
    "\n",
    "\n",
    "class GPTModelWrapper(Module, GenerationMixin):\n",
    "    def __init__(\n",
    "        self, config: PretrainedConfig, device: torch.device, inference: Callable[[torch.Tensor], torch.Tensor]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config: PretrainedConfig = config\n",
    "        self.device: torch.device = device\n",
    "        self.inference: Callable[[torch.Tensor], torch.Tensor] = inference\n",
    "        self.infer_time = list()\n",
    "        self.to(device=device)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "        }\n",
    "\n",
    "    def forward(self, input_ids, **_):\n",
    "        start = time.time()\n",
    "        logits = self.inference(input_ids)\n",
    "        self.infer_time.append(time.time() - start)\n",
    "        return CausalLMOutputWithCrossAttentions(logits=logits)\n",
    "\n",
    "    def timing(self) -> float:\n",
    "        return np.sum(self.infer_time)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"Here is some text to encode Hello World\",\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac120c3",
   "metadata": {},
   "source": [
    "## Pytorch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f34f5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "torch: 0.36\n",
      "infer timing: 0.34\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "def inference_torch(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids)\n",
    "    return model.lm_head(transformer_outputs.last_hidden_state)\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "inputs.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    gpt2_model = GPTModelWrapper(config=model.config, device=model.device, inference=inference_torch)\n",
    "    sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "    for _ in range(2):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    gpt2_model.infer_time.clear()\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=True)\n",
    "    print(f\"torch: {(time.time() - start)/60:.2f}\")\n",
    "    print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "    print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "_ = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420d8d8",
   "metadata": {},
   "source": [
    "## Naive ONNX Runtime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd67d6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "onnx: 0.66\n",
      "infer timing: 0.64\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_naive(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids.detach().cpu().numpy().astype(np.int32)}\n",
    "    logit = model_onnx.run(None, data)\n",
    "    np_logit = np.array(logit)\n",
    "    return torch.squeeze(torch.from_numpy(np_logit), dim=0)\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cpu\"), inference=inference_onnx_naive)\n",
    "inputs.to(\"cpu\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"onnx: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de843e68",
   "metadata": {},
   "source": [
    "## Optimized ONNX Runtime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fd5c3ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "onnx binding: 0.15\n",
      "infer timing: 0.13\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_onnx_optimized)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"onnx binding: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de4c17",
   "metadata": {},
   "source": [
    "## TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4941aab6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/24/2022-14:13:52] [TRT] [I] Loaded engine size: 978 MiB\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 15811, GPU 10528 (MiB)\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +487, now: CPU 0, GPU 1560 (MiB)\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 14832, GPU 10528 (MiB)\n",
      "[01/24/2022-14:13:53] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +87, now: CPU 0, GPU 1647 (MiB)\n",
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "tensorrt: 0.09\n",
      "infer timing: 0.08\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine(\n",
    "    engine_file_path=\"test-gpt2.plan\", runtime=runtime\n",
    ")\n",
    "\n",
    "\n",
    "def inference_tensorrt(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return tensorrt_model(data)[0]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_tensorrt)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"tensorrt: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del tensorrt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110e38c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}