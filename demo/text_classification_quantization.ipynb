{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A recipe to perform Nvidia GPU int-8 quantization on most transformers model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, Nvidia added to Hugging Face `transformer` library a new model called `QDQBert`.\n",
    "The single purpose of this model is to show how to add GPU quantization to vanilla Bert.\n",
    "There are also some demo scripts to demonstrate the use of the model on SQuaD task.\n",
    "\n",
    "**GPU quantization is a way to double the inference speed of your GPU**.\n",
    "It can be applied to any model in theory, and unlike distillation, if done well, it should not decrease your model accuracy.\n",
    "\n",
    "Unfortunately, these extreme perforamances are not easy to get, it requires some good knowledge of TensorRT API, ONNX export, or quantization process. The purpose of this tutorial is to show a good enough process to perform quantization.\n",
    "\n",
    "## What is int-8 quantization?\n",
    "\n",
    "Basic idea behind the expression int-8 quantization is that instead of doing deep learning computations with `float` numbers (usually encoded on 32 bits), you use integers (encoded on 8 bits). On a large matrix multiplication it has 2 effects:\n",
    "\n",
    "* it reduces by a large margin the size in memory, making **memory transfer faster** (on GPU, many operations are very fast to compute, and memory transfer is the main bottleneck, they are called memory bound)\n",
    "* it also makes **computation faster** accelerating the slowest operations (in transformer, mainly big matrix multiplication during the self attention comptutation)\n",
    "\n",
    "A 8-bit integer can encode values from -128 to +127, and no decimal (as it's an integer).\n",
    "So a 8-bit integer can't encode values like `1280.872654`.\n",
    "\n",
    "However we can use our integer if it's associated to a scale (a FP32 scale). For instance, for a scale of 20, I can set my integer to 64 (64*20=1280), it's not exactly `1280.872654` but it's close enough.\n",
    "\n",
    "That's why we need to perform a step called `calibration` during which the range of values and the scale (encoded as a FP32 float) will be computed.\n",
    "\n",
    "Basically, we know that by converting a FP32 to an int-8 and its scale, we will lose some information, and the goal of the calibration is to minimize this loss.\n",
    "\n",
    "If in a matrix, values go from -1.5 to +2, it may be encoded as an integer taking value from -128 to +127, associated to a scale of 64 (2*64=128)\n",
    "\n",
    "\n",
    "[A good documentation on quantization](https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf)\n",
    "\n",
    "\n",
    "## Why a dedicated tutorial?\n",
    "\n",
    "The code from Nvidia only supports out of the box vanilla `Bert` model (and not similar models, like RoBerta & co).\n",
    "The demo from Nvidia is on the SQuaD task, it's cool but it makes the code a lot less clear that needed.\n",
    "\n",
    "To be both simple and cover most use cases, in this tutorial we will see:\n",
    "\n",
    "* how to perform GPU quantization on **any** transformer model (not just Bert) using a simple trick\n",
    "* how to to apply quantization to a common task like classification (which is easier to understand than question answering)\n",
    "* measure performance gain (latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install `master` branch of `transfomers` library to use a new model: **QDQBert** and `transformer-deploy` to leverage `TensorRT` models (TensorRT API is not something simple to master, it's highly advised to use a wrapper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MOsHUjgdIrIW"
   },
   "outputs": [],
   "source": [
    "#! pip install git+https://github.com/huggingface/transformers\n",
    "#! pip install git+https://github.com/ELS-RD/transformer-deploy\n",
    "#! pip install sklearn datasets -U\n",
    "#! pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2OzrD4f-3ydk",
    "outputId": "54cc2ea6-6969-4e01-f9f9-78c5fc91ff85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  1 18:59:19 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:03:00.0  On |                  N/A |\r\n",
      "| 79%   60C    P8    52W / 350W |    311MiB / 24267MiB |      2%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1975      G   /usr/lib/xorg/Xorg                188MiB |\r\n",
      "|    0   N/A  N/A      7865      G   /usr/bin/gnome-shell               40MiB |\r\n",
      "|    0   N/A  N/A     35082      G   ...AAAAAAAAA= --shared-files       41MiB |\r\n",
      "|    0   N/A  N/A    161588      G   ..._49620.log --shared-files       12MiB |\r\n",
      "|    0   N/A  N/A    706814      G   ...AAAAAAAAA= --shared-files       25MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# check that the GPU is enabled\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEJBSTyZIrIb"
   },
   "source": [
    "# Fine-tuning a model on a text classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is inspired from [official Notebooks from Hugging Face](https://github.com/huggingface/notebooks/blob/master/examples/text_classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YZbiBDuGIrId"
   },
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "task = \"mnli\"\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model_checkpoint = \"roberta-base\"\n",
    "batch_size = 32\n",
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QYTpxXIrIl"
   },
   "source": [
    "We will use the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library to download the data and get the metric we need to use for evaluation (to compare our model to the benchmark). This can be easily done with the functions `load_dataset` and `load_metric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d786beb2d0d2475f80ba9b915e2500a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['premise', 'hypothesis', 'label', 'idx'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "import datasets\n",
    "\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVx71GdAIrJH"
   },
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
    "\n",
    "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "- we download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "b6be028de2ae4ff691538eedb33793af",
      "a3e2c73d393d4e58a371f3da3dd80e6d",
      "b4d3f284fc4c4061b58d43a738f9bc78",
      "8a11c8fed672470b8335dc575a4a220e",
      "08286a6371584b4186014ecb5d5f164d",
      "68c4c867096d41a78740fdee30edcadb",
      "7d520bdde27742abb42803843721d101",
      "f8a0053903c64e75ac25eab5b24d5871",
      "93dbcc6d23a743bab0da8af6ee5e2825",
      "d1ecc3d380fc4758b03190b23686a2f1",
      "2d3a08166846438db79b0f89314fe76a",
      "5e2185bd6e4f4a10b89ac606868a43bd",
      "f44d2beebfe44186b0ac8016e89e4b49",
      "2eac6b4817e14d7fae396e6458b940fa",
      "af16284f77594397a69ad0e322b5e736",
      "a20579a9e7364fb485d79bdc4feb54dc",
      "cae29b9c6d45412fab70977fcd0f3234",
      "927ad6ade85a402594074fa90ab558c2",
      "30646fa2c0dc494e9dbcbd4dc598410e",
      "7a75099f99054645bf3fc1b778dac7e6",
      "d5d015711ae04d2f801577fc50af6c15",
      "4b13c3b3435f4689b29d48e0a35bebd6",
      "be4affe852b348de8fe1362582b08da9",
      "c6c100b71f26405fb960598feb5eee03",
      "99e94791043b4499b06601f7524f9b14",
      "26bc2038bed74279813ab5af09a2724c",
      "9bc6e14b912249e3b7d02f31bcc74667",
      "196ffc99ad5a40109d9b1cfe12032b62",
      "d5c8ff9e3bd849059fa7b30eab5fc940",
      "7ff32d18c9f0473893a6a6b2941c54b0",
      "0022faf286b44e858e638ccd5ded38b0",
      "6e54ce781ca54ad283911fa4774e3361",
      "969b6fdac1d6418d89a683db1e6ec6b2",
      "092db03992f24951b494fbb81da5b9d6",
      "023900ca566446eab5905b25b16a3de7",
      "994cf2338c7c4899952e25723445693c",
      "6aa2f5d46f1f454198d8e69517549ff1",
      "72b8f11065254e5ca488cd346b5add54",
      "c7bd52ef524c4d279dfcaa3aebe4a2c5",
      "d9a0852554284d36b6b121f579b06b41",
      "4320b12de9d14c459cc88319e2d7622a",
      "7b483d17d1d14fdd922600f0c906fc2f",
      "14648b8262944f5faac134a7c0184e47",
      "10678736bd534c63aebda414da01b4db"
     ]
    },
    "id": "eXNLu_-nIrJI",
    "outputId": "10b2f739-6277-44c2-fd31-0de3a9ab9fa8"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "We pass along `use_fast=True` to the call above to use one of the fast tokenizers (backed by Rust) from the ðŸ¤— Tokenizers library. Those fast tokenizers are available for almost all models, but if you got an error with the previous call, remove that argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "fyGdtK9oIrJM"
   },
   "outputs": [],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbqtC4MrIrJO"
   },
   "source": [
    "We can double check it does work on our current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19GG646uIrJO",
    "outputId": "b9d1e5e8-21ca-43ea-85c6-f4315d50d96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: Conceptually cream skimming has two basic dimensions - product and geography.\n",
      "Sentence 2: Product and geography are what make cream skimming work. \n"
     ]
    }
   ],
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C0hcmp9IrJQ"
   },
   "source": [
    "We can them write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "vc0BSBLIIrJQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True, padding=\"max_length\", max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "65017db07d7f4e798ede741cc92488f0",
      "6fa74604c68543a38392fa0e1587f707",
      "86cc326e574a4fada7224e6f0c209e9a",
      "af5b646f89024c139c695a1f058fb772",
      "37cda4cae81a4d94aa831fb40b5c3b26",
      "28b7346a9b8c4b198dd9dbea1be013b6",
      "561b1ede331a40c1a2bff9422e8eea0e",
      "aecf7f063234416abf3f24766481cb89",
      "21ef195fa88f49c4a2c057f8028177a2",
      "5b1ad9f5d02c4b298a02ce6041692057",
      "56fd7584b0844590936519ec3851922e",
      "bbe3a471efb04ea8b5aabc4be819d585",
      "59418bbeb20547e5b5e1a5728262c757",
      "a61d366d91c34697a55f62b754e1f3a5",
      "1bea379404df429b9852b62a938661ae",
      "c801e1727de44b67aa7cb1c3d970e1fe",
      "b8722dc10d4447fe9630cbf169260cc8",
      "a9b98fd93fcd4fc4a2b2aa88c82835d0",
      "300f01e3547648f3983a83d3d3118c54",
      "a4c444f06c0847c09a44917084d3908d",
      "7c875ecd9cb54405a6c45969bcb4b4c6",
      "4552ee8ca6bd4a0b956651cc23f4ff3c",
      "3bfff454943b4b04a12ec29bbe28e0aa",
      "154200a8bc0b44fe8d0419fd56c6539d",
      "cedca6e55b84443e82f3d01471d61048",
      "a7d355f456eb4d3995dd91c5917a72c1",
      "b264b220d9c444bd9da46a7e6c8fd5ed",
      "4fae966b76844c869cdea1e53891e26f",
      "a0a2918e9772475cac51124b3b83fcaf",
      "a02624219ee84f50b1a3032eaa030a39",
      "5f032f56105f463a8680aa2482d0b162",
      "7701ec898fd443f1b35b187aea3651e9",
      "8399339998564d21ba5db6f0514c02c6"
     ]
    },
    "id": "DDtsaJeVIrJT",
    "outputId": "0eeb1cb2-e308-493b-807e-532eeae5f4fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1f1a13d917d99a50.arrow\n",
      "Loading cached processed dataset at /home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-9b9dbd19d82c4713.arrow\n",
      "Loading cached processed dataset at /home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-b37b7241dc97daf7.arrow\n",
      "Loading cached processed dataset at /home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f78b5169435f1ed4.arrow\n",
      "Loading cached processed dataset at /home/geantvert/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-87c8e6fc7a3e0678.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since all our tasks are about sentence classification, we use the `AutoModelForSequenceClassification` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which is always 2, except for STS-B which is a regression problem and MNLI where we have 3 labels):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "ac14ba24dcf3404db9fd303dbb24d7a5",
      "4e91efae49b64f038fd3fbfcfd2be510",
      "17b83e0d0fb947d7bf20319ff930e8fc",
      "1da1d80871f545bbb21bf5a84d2120a0",
      "c593f2e45e244637821cc5721788bf2c",
      "cbbb20b5d01a4450bfb8dfbf8048d64f",
      "854cfd13416543fba8221093b903658b",
      "7ec6da801d0d45c4bb80eeab5518e124",
      "8585eab4b3fe4992bd7e7c4596e2483b",
      "990482eebca2424bb5ecbd114007e02c",
      "c92a19dfa84142af91522bc22f21fca6",
      "78601982b0e04b80adaa502db2ef685a",
      "167874df55014291be95cd390b1e60d3",
      "d6426fea2eda41dd9a31cb3f35b0877e",
      "163146c2f23440bcbf782116a35b5684",
      "0dab554959dc44b3b313ee8ae91ca88d",
      "f651eecbb6d44c24820cf6fe5ab92e7b",
      "a51b461c062f4636bfa4b48823d0709b",
      "cced5f1cccc2400a8fbfd7a6eaedc666",
      "cf9597523c024514b9b3e66bc77e3fa8",
      "f01fdef82047471e8c1b780cae5379cc",
      "e1f08cf954ae4aea818c90d893486c77"
     ]
    },
    "id": "KPMoLPBn_1vN",
    "outputId": "58dca4e7-fc5c-4fd1-a8d4-755aa1e956cb"
   },
   "outputs": [],
   "source": [
    "import pytorch_quantization.nn as quant_nn\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from typing import Dict, OrderedDict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedModel,\n",
    "    QDQBertForSequenceClassification,\n",
    "    BertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    IntervalStrategy,\n",
    ")\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import calib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_to_onnx(model_pytorch: PreTrainedModel, output_path: str, inputs_pytorch: Dict[str, torch.Tensor]) -> None:\n",
    "    with torch.no_grad():\n",
    "        torch.onnx.export(\n",
    "            model_pytorch,  # model to optimize\n",
    "            args=(inputs_pytorch[\"input_ids\"], inputs_pytorch[\"attention_mask\"]),  # tuple of multiple inputs , inputs_pytorch[\"token_type_ids\"]\n",
    "            f=output_path,  # output path / file object\n",
    "            opset_version=13,  # the ONNX version to use\n",
    "            do_constant_folding=True,  # simplify model (replace constant expressions)\n",
    "            input_names=[\"input_ids\", \"attention_mask\"],  # input names \"token_type_ids\"\n",
    "            output_names=[\"model_output\"],  # output name\n",
    "            dynamic_axes={  # declare dynamix axis for each input / output (dynamic axis == variable length axis)\n",
    "                \"input_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "                \"attention_mask\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "                #\"token_type_ids\": {0: \"batch_size\", 1: \"sequence\"},\n",
    "                \"model_output\": {0: \"batch_size\"},\n",
    "            },\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "nb_step = 1000\n",
    "strategy = IntervalStrategy.STEPS\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = strategy,\n",
    "    eval_steps=nb_step,\n",
    "    logging_steps=nb_step,\n",
    "    save_steps=nb_step,\n",
    "    save_strategy = strategy,\n",
    "    learning_rate=1e-5,  # 7.5e-6 https://github.com/pytorch/fairseq/issues/2057#issuecomment-643674771\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    group_by_length=False,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transplant weights from one model into Bert architecture\n",
    "\n",
    "First, you need to know that not all models are quantization compliant. The optimization engine (`TensorRT`) search for some patterns and will fail to opimize the model if it doesn't find them. It requires the code to be written in a certain way. For that reason we will try to reuse what works.\n",
    "\n",
    "We will leverage the fact that since Bert have been released, very few improvements have been brought to the transformer architecture (at least encoder only models).\n",
    "Indeed, better model appeared, and most of the work has been done to improve the pretraining step.\n",
    "So the idea will be to take the weights from those new models and put them inside Bert.\n",
    "\n",
    "The reason of this process is to avoid the modification of the source code of these others model.\n",
    "Copy-pasting quantization part of QDQModel to another one is not hard (there are only few blocks modified) but would require some work on the user side, making quantization harder that it should be.\n",
    "The process described below is not perfect but should work for most users.\n",
    "\n",
    "**steps**:\n",
    "\n",
    "* load Bert model\n",
    "* retrieve layer/weight names\n",
    "* load target model (here Roberta)\n",
    "* replace weight/layer names with those from Roberta\n",
    "* override the architecture name in model configuration\n",
    "\n",
    "If there is no 1 to 1 correspondance (it happens), try to keep at least embeddings and self attention. Of course, it's possible that if a model is very different, the transplant may cost some accuracy. In our experience, if your trainset is big enough it should not happen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
    "bert_keys = list(model_bert.state_dict().keys())\n",
    "del model_bert\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "model.save_pretrained(\"roberta-in-bert\")\n",
    "del model\n",
    "model_weights: OrderedDict[str, Tensor] = torch.load(\"roberta-in-bert/pytorch_model.bin\")\n",
    "\n",
    "\n",
    "# a too simple check\n",
    "# IRL, check layer names and find a way to map self attention and embeddings from the original model to Bert\n",
    "assert len(model_weights) == len(bert_keys)\n",
    "\n",
    "for bert_key in bert_keys:\n",
    "    # pop remove the first weights from the Ordered dict ...\n",
    "    _, weight = model_weights.popitem(last=False)\n",
    "    # ... and we re-insert them, in order, with a new key\n",
    "    model_weights[bert_key] = weight\n",
    "\n",
    "# we re-export the weights\n",
    "torch.save(model_weights, \"roberta-in-bert/pytorch_model.bin\")\n",
    "del model_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We override the architecture name to make `transformers` believe it is Bert..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====> change architecture to bert base <======\n",
    "import json\n",
    "\n",
    "with open(\"roberta-in-bert/config.json\") as f:\n",
    "    content = json.load(f)\n",
    "    content['architectures'] = [\"bert\"]\n",
    "\n",
    "with open(\"roberta-in-bert/config.json\", mode=\"w\") as f:\n",
    "    json.dump(content, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "\n",
    "When you create a classification model from a pretrained one, the last layer are randomly initialized.\n",
    "We don't want to take these totally random values to compute the calibration of tensors.\n",
    "Moreover, our trainset is a bit small, and it's easy to overfit.\n",
    "\n",
    "Therefore, we train our `Roberta into Bert` model on 1/6 of the train set.\n",
    "The goal is to slightly update the weights to the new architecture, not to get the best score.\n",
    "\n",
    "> another approach is to fully train your model, perform calibration, and then retrain it on a small part of the data with a low learning rate (usually 1/10 of the original one).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using amp half precision backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 07:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.723500</td>\n",
       "      <td>0.532824</td>\n",
       "      <td>0.792562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.549100</td>\n",
       "      <td>0.483588</td>\n",
       "      <td>0.809068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-1000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-1000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-2000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-2000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-2000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from roberta-base-finetuned-mnli/checkpoint-2000 (score: 0.8090677534386144).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='154' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in roberta-in-bert-trained/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4835878908634186, 'eval_accuracy': 0.8090677534386144, 'eval_runtime': 19.1183, 'eval_samples_per_second': 513.384, 'eval_steps_per_second': 8.055, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-in-bert-trained/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertForSequenceClassification.from_pretrained(\"roberta-in-bert\", num_labels=num_labels)\n",
    "model_bert = model_bert.cuda()\n",
    "\n",
    "args.max_steps = 2000\n",
    "trainer = Trainer(\n",
    "    model_bert,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "model_bert.save_pretrained(\"roberta-in-bert-trained\")\n",
    "del trainer\n",
    "del model_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will start the quantization process.\n",
    "It follow those steps:\n",
    "\n",
    "* perform the calibration\n",
    "* perform a quantization aware training\n",
    "\n",
    "By passing validation values to the model, we will calibrate it, meaning it will get the right range / scale to convert FP32 weights to int-8 ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activate histogram calibration\n",
    "\n",
    "There are several kinds of calbrators, below we use the percentile one (99.99p) (`histogram`), basically, its purpose is to just remove the most extreme values before computing range / scale.\n",
    "The other option is `max`, it's much faster but expect lower accuracy.\n",
    "\n",
    "Second calibration option, choose between calibration done at the tensor level or per channel (more fine grained, slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also use \"max\" instead of \"historgram\"\n",
    "input_desc = QuantDescriptor(num_bits=8, calib_method=\"histogram\")\n",
    "# below we do per-channel quantization for weights, set axis to None to get a per tensor calibration\n",
    "weight_desc = QuantDescriptor(num_bits=8, axis=(0,))\n",
    "quant_nn.QuantLinear.set_default_quant_desc_input(input_desc)\n",
    "quant_nn.QuantLinear.set_default_quant_desc_weight(weight_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform calibration\n",
    "\n",
    "During this step we will enable the calibration nodes, and pass some representative data to the model.\n",
    "It will then be used to compute the scale/range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roberta-in-bert-trained/config.json\n",
      "You are using a model of type bert to instantiate a model of type qdqbert. This is not supported for all configurations of models and can yield errors.\n",
      "Model config QDQBertConfig {\n",
      "  \"_name_or_path\": \"roberta-in-bert\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"qdqbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file roberta-in-bert-trained/pytorch_model.bin\n",
      "I1201 19:07:22.556784 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.557633 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.558312 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.559234 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.570187 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.570979 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.572141 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.573173 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.587157 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.587976 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.588648 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.589257 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.589886 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.590491 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.591093 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.591677 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.604110 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.604965 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.605788 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.606476 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.608637 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.609386 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.633528 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.634159 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.634558 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.635272 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.659617 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.660160 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.660545 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.661419 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.662083 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.662428 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.672487 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.672940 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.673371 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.673864 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.686383 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.687393 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.688224 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.689111 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.701097 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.702034 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.702666 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.703149 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.703629 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.704051 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.704477 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.704874 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.716984 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.717729 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.718373 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.718999 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.719850 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.720462 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.741302 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.741899 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.742321 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.742763 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.769119 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.769680 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.770210 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.771095 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.771780 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.772149 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.783096 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.783622 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.784070 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.784542 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.794926 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:22.795491 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.795853 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.796230 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.807137 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.807927 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.808553 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.809141 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.809771 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.810495 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.811114 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.811691 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.822019 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.822805 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.823449 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.824043 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.824914 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.825503 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.846135 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.846942 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.847608 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.848223 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.871000 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.871783 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.872462 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.873089 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.873962 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.874621 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.884854 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.885457 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.886066 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.886688 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.899109 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.899768 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.900748 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.901568 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.911406 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.912059 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.912927 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.913561 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.914347 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.915078 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.915945 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.916547 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.927549 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.928246 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.928908 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.929500 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.930580 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.931228 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.951412 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.952154 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.952960 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.953933 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.974907 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.975683 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.976436 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.977675 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:22.978751 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.979352 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.991469 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:22.992411 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:22.993380 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:22.994207 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.005038 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.005791 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.006553 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.007203 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.017255 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.017829 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.018506 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.019221 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.019879 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.020379 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.020857 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.021332 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.031261 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.031948 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.032585 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.033192 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.034027 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.034641 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.058038 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:23.058811 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.059446 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.060050 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.079895 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.080400 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.080774 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.081119 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.081963 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.082303 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.092197 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.092940 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.093587 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.094182 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.104809 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.105530 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.106288 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.107035 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.117332 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.117931 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.118697 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.119329 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.120085 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.120682 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.121240 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.121816 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.131671 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.132045 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.132399 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.132738 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.133545 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.133875 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.153260 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.153972 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.154779 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.155387 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.179506 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.180439 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.181103 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.181725 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.182586 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.183223 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.194004 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.195079 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.196422 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.196865 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.208515 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.209109 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.209630 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.210128 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.221330 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.221704 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.222061 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.222406 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.222798 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.223171 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.223591 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.223986 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.235521 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.236109 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.236990 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.237547 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.238431 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.239129 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.262135 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.262805 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.263666 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.264140 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.287677 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.288471 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.289235 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.290506 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.291714 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.292328 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.303590 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.304338 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.304985 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.305601 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.316265 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.316930 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.317543 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.318154 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:23.328983 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.329701 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.330348 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.331055 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.331630 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.332135 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.332654 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.333236 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.343668 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.344560 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.345513 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.346140 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.346822 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.347609 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.370694 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.371253 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.371692 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.372114 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.395322 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.396442 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.397025 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.397402 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.398226 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.398845 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.412521 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.413233 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.413730 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.414105 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.424964 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.425662 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.426278 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.426805 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.437678 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.438106 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.438483 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.438826 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.439212 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.439549 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.439886 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.440611 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.451024 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.451713 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.452302 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.452893 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.453708 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.454290 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.479227 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.480020 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.480688 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.481298 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.503707 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.504791 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.505503 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.506218 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.507561 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.508140 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.519340 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.519988 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.520587 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.521208 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.534733 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.535418 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.536240 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.537214 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.548016 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.548620 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.549396 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.550175 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.550990 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.551932 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.552865 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.553758 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.564883 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.565361 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.565732 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.566088 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.567098 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.567529 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.590969 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.592016 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.592971 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.593897 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:23.619890 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.620731 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.621435 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.622109 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.623023 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.623664 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.634196 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.634888 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.635725 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.636688 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.657259 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.657959 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.658629 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.659231 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.672862 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.673597 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.674302 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.674978 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.675721 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.676385 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.677051 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.677688 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.689925 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.690466 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.690964 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.691452 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.692585 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.693069 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.717059 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.717769 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.718341 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.718904 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.741673 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.742345 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.742954 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.743494 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.744401 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.745055 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.755685 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.756275 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.756812 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.757349 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.767277 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.767896 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.768507 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.769093 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.779711 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.780525 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.781466 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.782547 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.783240 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.783874 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.784468 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.785077 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.795897 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.796726 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.797205 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.797985 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.798608 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.798972 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.822418 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.823235 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.823901 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.824537 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.847674 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:07:23.848276 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:07:23.848831 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.849364 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:07:23.850354 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:07:23.851019 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "All model checkpoint weights were used when initializing QDQBertForSequenceClassification.\n",
      "\n",
      "All the weights of QDQBertForSequenceClassification were initialized from the model checkpoint at roberta-in-bert-trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QDQBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc9c4f7d994a43238fc98b0a4a82a76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.008212 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.008661 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.008988 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.009320 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.009638 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.009947 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.010272 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.010650 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.011093 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.011521 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.011932 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.012334 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.012732 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.013158 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.013524 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.015070 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.015362 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.015657 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.015948 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.016235 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.016535 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.016821 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.017107 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.017389 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.017682 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.017965 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.018249 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.018543 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.018842 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.019130 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.019419 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.019713 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.020005 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.020288 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.020587 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.020866 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.021157 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.021440 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.021725 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.022008 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.022335 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.022688 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.023005 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.023291 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.023587 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.023878 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.024167 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.024456 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.024747 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.025036 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.025317 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.025598 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.025888 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.026176 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.026575 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.029913 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.030232 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.030552 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.030859 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.031172 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.031473 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.031761 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.032055 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.032341 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.032632 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.032923 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.033217 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.033504 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.033799 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.034101 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.034383 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.034724 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.035037 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.035333 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.035627 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.035917 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.036211 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.036498 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.036792 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.037077 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.037374 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.037710 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.038072 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.038429 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.038773 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.039572 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.040231 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.040872 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.041512 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.042466 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.042991 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.043453 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.043893 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.044385 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.044853 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.045287 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.045724 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.046156 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.046822 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.047271 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.047720 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.048171 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.048631 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.049073 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.049527 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.049985 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.050450 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.050935 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.051433 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.051962 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.052431 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.052842 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.053302 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.053708 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.054098 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.054503 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.054916 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.055331 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.055722 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.056115 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.056530 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.056917 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.057300 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.057678 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.058071 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.058478 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.058875 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.059275 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.059674 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.060062 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.060468 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.060877 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.061275 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.061657 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.062041 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.062422 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.062809 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.063184 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.063562 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.063941 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.064331 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.064721 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.065103 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.065503 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.065927 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.066313 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.066744 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.067138 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.067527 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.067911 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.068341 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.068727 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.069152 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.069553 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.069929 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.070332 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.070743 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.071141 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.071579 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.072017 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.072420 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.072801 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.073190 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.073567 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.073958 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.074336 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.074850 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.075454 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.075989 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.076356 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.076695 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.077060 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.077408 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.077838 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.078272 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.078711 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.079138 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.079486 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.079871 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.080217 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.080585 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.080916 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.081352 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.081695 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.082050 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.082368 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.082800 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.083172 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.083823 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.084179 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.084587 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.084959 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.092437 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.093339 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.094649 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.099028 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.099557 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.100039 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.100597 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.100946 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.101308 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.101635 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.102412 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.102815 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.103189 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.103542 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.103915 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.104271 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.104646 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.105073 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.105480 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.105823 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.106204 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.106556 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.106930 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.107255 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.107614 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.107955 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.108338 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.108674 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.109040 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.109416 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.109760 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.110115 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.110567 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.110929 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.111307 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.111885 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.112311 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.112704 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.115879 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.116308 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.116706 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.117094 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.117460 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.117829 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.118185 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.118570 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.118924 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.119296 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.119665 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.120041 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.120388 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.120766 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.121122 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.121490 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.121852 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.122225 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.122589 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.122989 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.123348 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.123733 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.125864 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.126237 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.126607 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.127273 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.127610 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.128101 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.128500 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.128862 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.129232 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.129593 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.129964 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.130370 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.131626 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.132100 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.132482 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.132864 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.133640 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.134005 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.134386 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.134849 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.135259 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.135945 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.136400 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.136772 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.137124 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.137450 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.137776 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.138093 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.138446 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.138790 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.139108 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.139424 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.139753 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.140066 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.140388 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.140700 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.141023 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.141333 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.141650 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.141971 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.142299 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.142667 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.142989 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.143304 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.143644 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.144004 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.144335 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.144657 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.144972 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.145276 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.145586 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.145884 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.146196 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.146507 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.146830 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.147159 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.147554 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.148074 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.148501 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.148970 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.149340 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.149654 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.149964 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.150276 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.150603 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.150904 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.151208 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.151526 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.151910 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.152282 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.152642 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.153004 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.153381 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.153741 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.154193 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.154572 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.154937 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.155293 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.155663 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.156019 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.156385 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.156755 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.157115 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.157469 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.157838 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.158190 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.158565 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.158932 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.159324 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.159684 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.160052 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.160408 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.160774 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.161141 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.161501 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.161860 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.162240 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.162612 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.163017 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.163409 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.163841 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.164212 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.165174 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.165847 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.166251 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.166768 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.167108 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.167433 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.167788 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.168111 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.168436 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.168760 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.169080 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.169394 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.169714 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.170035 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.170361 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.170723 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.171069 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.171420 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.171787 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.172127 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.172460 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.172802 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.173132 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.173461 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.173796 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.174127 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.174479 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.174819 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.175151 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.175482 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.175847 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.176198 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.176537 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.176899 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.177248 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.177613 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.177950 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.178279 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.178630 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.178949 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.179286 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.179632 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.179993 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.180335 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.180680 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.181045 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.181429 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.181766 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.182112 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.182474 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.182854 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.183246 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.183611 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.183982 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.184353 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.184703 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.185055 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.185404 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.185768 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.186109 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.186471 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.186816 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.187159 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.187501 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.187842 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.188180 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.188540 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.188892 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.189239 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.189582 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.189941 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.190281 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.190637 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.190984 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.191337 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.191706 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.192056 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.192404 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.192754 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.193092 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.193433 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.193799 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.194635 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.195046 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.195410 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.195775 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.196141 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.196555 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.196925 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.197286 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.197643 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.198002 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.198354 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.198724 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.199099 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.199455 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.199810 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.200161 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.200533 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.200872 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.201215 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.201560 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.201900 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.202245 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.202603 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.202946 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.203296 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.203634 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.203979 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.204316 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.204662 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.204997 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.205339 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.205677 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.206024 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.206361 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.206710 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.207053 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.207409 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.207747 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.208091 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.208422 140057592104768 tensor_quantizer.py:179] Enable MaxCalibrator\n",
      "I1201 19:07:24.208777 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.209120 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n",
      "I1201 19:07:24.209468 140057592104768 tensor_quantizer.py:183] Disable `quant` stage.\n",
      "I1201 19:07:24.209815 140057592104768 tensor_quantizer.py:179] Enable HistogramCalibrator\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee5af3b54564a6d9cf01e84a99b2e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:07:24.249449 140057592104768 histogram.py:69] Calibrator encountered negative values. It shouldn't happen after ReLU. Make sure this is the right tensor to calibrate.\n",
      "I1201 19:07:24.322341 140057592104768 max.py:60] Calibrator encountered negative values. It shouldn't happen after ReLU. Make sure this is the right tensor to calibrate.\n",
      "W1201 19:11:25.924171 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "W1201 19:11:25.925113 140057592104768 tensor_quantizer.py:238] Call .cuda() if running on GPU after loading calibrated amax.\n",
      "I1201 19:11:25.925732 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.926429 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.926960 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.927570 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.928478 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.929265 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.929759 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.930281 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.930794 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.931309 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.931917 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.932722 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.933320 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.933851 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.935375 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.936755 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.937237 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.938004 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.938564 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.939165 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.940878 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.941240 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.941617 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.942476 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.943017 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.943430 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.943952 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.944334 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.944707 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.945207 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.945593 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.945962 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.946330 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.946732 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.947101 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.948596 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.948939 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.949308 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.949787 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.950204 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.950607 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.951201 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.952383 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.952890 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.953278 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:25.953657 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.954019 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.954503 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.954883 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.955243 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.955616 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.955986 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.956342 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.956829 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.957189 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.957549 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.959388 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.959772 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.960143 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.960639 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.961327 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.961693 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.962077 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.962476 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.962854 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.963334 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.963722 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.964087 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.964465 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.964836 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.965192 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.966062 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.967302 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.967694 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.968163 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.968578 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.968949 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.969423 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.969820 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.970181 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.971506 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:25.971887 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.972258 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.973086 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.973466 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.973832 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.974312 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.974988 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.975597 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.976069 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.976484 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.977218 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.977980 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.978734 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.979220 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.979803 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.980222 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.980617 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.981182 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.983010 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.983466 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.984857 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.985381 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.985968 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.986622 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:25.987133 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.987572 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.988140 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.989069 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.989492 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.990689 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.991171 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.991599 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.992151 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.992578 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.993011 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.994227 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.994624 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.995114 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.996038 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.996421 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.996902 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:25.997333 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:25.997764 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.998190 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:25.998763 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:25.999198 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:25.999616 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.000059 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.000492 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.000916 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.001345 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.001671 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.002368 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.002961 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.003427 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.003854 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.004457 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.004840 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.005265 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.005782 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.008827 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.009523 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.010430 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.011012 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.011617 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.012572 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.013204 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.013811 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.014614 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.015094 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.015529 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.015997 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.016479 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.016898 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.018180 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.018682 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.019285 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.020093 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.020559 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.021015 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.021934 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.022428 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.022884 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.023341 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.023761 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.024163 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.025327 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:26.025758 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.026160 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.026600 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.027027 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.027422 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.028437 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.028844 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.029254 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.030018 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.030419 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.030812 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.031607 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.031997 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.032397 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.032814 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.033215 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.033596 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.034605 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.035006 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.035410 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.035823 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.036223 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.036654 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.037652 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.038041 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.038457 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.038881 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.039296 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.039678 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.040672 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.041069 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.041475 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.042263 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.042666 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.043061 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.043828 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.044213 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.044603 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.045393 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.045792 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.046184 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.046998 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.047406 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.047809 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.048225 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.048639 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.049020 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.050037 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.050438 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.051250 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.052364 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.052836 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.053314 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.054255 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.054712 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.055194 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.055634 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.056124 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.056620 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.057900 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.058500 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.058978 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.059447 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.059946 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.060393 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.062382 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.063033 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.063494 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.064432 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.064884 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.065327 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.066207 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.066751 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.067196 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.067848 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.068386 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.068830 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.069777 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.070216 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.070670 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.071287 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.071821 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.072246 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.073431 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.073822 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.074197 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.074593 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:26.074990 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.075353 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.076299 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.076682 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.077061 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.077822 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.078284 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.078689 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.079439 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.079832 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.080327 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.080979 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.081813 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.082364 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.083022 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.083666 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.084136 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.084990 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.085494 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.085910 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.086870 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.087291 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.087764 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.088692 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.089114 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.089588 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.090586 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.091059 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.091480 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.091969 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.092467 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.092957 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.094229 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.094680 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.095173 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.095866 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.096348 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.096778 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.097883 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.098311 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.098751 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.099691 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.100139 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.100568 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.101492 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.101877 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.102281 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.102719 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.103145 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.103531 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.104698 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.105144 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.105577 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.106037 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.106483 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.106919 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.107499 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.108504 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.108941 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.109427 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.109966 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.110471 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.111276 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.112012 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.112491 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.113208 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.113619 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.114001 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.114477 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.114864 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.115222 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.115676 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.117420 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.117794 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.118367 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.118748 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.119553 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.119925 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.120306 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.120646 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.121500 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.121852 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.122227 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.123033 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.123393 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.123810 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 19:11:26.124270 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.124982 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.125337 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.125682 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.126024 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.126360 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.126821 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.127168 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.127512 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.127857 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.128201 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.128534 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.130064 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.130435 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.130790 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.131241 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.132012 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.132446 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.133346 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.133914 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.134443 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.134913 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.135293 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.135643 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.136127 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.136558 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.136909 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.137258 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.137603 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.137944 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.138386 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.138761 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.139133 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.139504 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.139860 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.140220 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.142096 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.142473 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.142842 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.143534 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.144126 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.144506 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.144970 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.145310 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.145627 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.146053 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.146389 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.146715 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.147159 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.147586 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.147955 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.148300 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.148642 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.149002 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.150712 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.151114 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.151485 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.152328 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.152645 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.152975 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.153701 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.154030 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.154361 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.154783 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.155122 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.155440 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.155856 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.156772 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.157073 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.157397 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.157718 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.158034 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.158441 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.159212 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.159476 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.159883 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.160531 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.160816 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.161237 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.161567 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.161873 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.162191 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.162541 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.162867 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.163278 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.163607 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.163912 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 19:11:26.164236 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.164555 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.164862 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.165282 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.166933 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.167483 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.167847 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.168207 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.168518 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.169337 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.169656 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.169963 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.170650 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.170993 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.171299 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.171710 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.172366 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.172665 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.173085 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.173456 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.173775 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.174204 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.174545 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.174849 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.175172 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.175492 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.175794 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.177029 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.177344 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.177689 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.178099 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.178759 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.179049 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.179460 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.179778 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.180079 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.180411 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.180738 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.181051 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.181462 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.181764 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.182077 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.182408 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.182742 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.183166 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.185135 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.185495 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.185858 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.186303 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.186998 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.187295 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.187743 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.188093 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.188408 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.188751 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.189090 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.189402 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.189831 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.190161 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.190501 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.190845 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.191191 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.191504 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.191941 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.192252 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.192575 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.192912 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.193243 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.193555 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.195458 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.195774 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.196097 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.196519 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.197222 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.197522 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.198172 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.198496 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.198834 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.199260 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.199592 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.199915 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.200341 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.200659 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.201111 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.201585 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.201978 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.202338 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 19:11:26.203794 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.204129 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.204463 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.205213 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.205532 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.205856 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.206314 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.206920 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.207242 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.207595 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.207930 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.208251 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.208701 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.209014 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.209342 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.209681 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.210017 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.210337 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.210809 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.211127 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.211452 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.212960 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.213278 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.213597 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.214320 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.214647 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.214969 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.215307 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.215657 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.215990 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.216437 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.216790 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.217120 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.217470 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.218750 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.219104 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.219779 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.220130 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.220471 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.220828 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.221171 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.221508 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.222415 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.222771 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.223115 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.223901 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.224342 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.224699 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.225436 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.225807 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.226158 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.226867 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.227223 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.227569 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.228272 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.228614 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.228950 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.229305 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.229648 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.229976 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.230901 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.231261 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.231602 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.232302 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.232648 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.232999 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.233716 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.234098 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.234535 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.234896 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.235247 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.235577 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.236498 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.236858 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.237208 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.237540 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.237884 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.238211 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.239141 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.239490 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.239825 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.240611 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.241239 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.241616 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.242406 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.242787 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.243120 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 19:11:26.243480 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.243837 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.244161 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.245077 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.245416 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.245760 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.246103 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.246461 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.246803 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.247718 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.248057 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.248391 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.248733 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.249096 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.249467 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.250488 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.250917 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.251284 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.251981 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.252321 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.252659 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.253424 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.253845 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.254222 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.255028 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.255452 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.255849 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.256643 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.257052 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.257397 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.257740 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.258353 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.258865 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.259701 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.260312 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.260719 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.261451 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.261847 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.262240 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.263064 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.263426 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.263779 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.264135 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.264503 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.264840 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.265755 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.266181 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.266593 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.267354 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.267729 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.268307 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.268963 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.269316 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.269653 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.270340 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.270828 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.271171 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.271849 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.272221 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.272570 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.272931 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.273329 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.273719 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.274829 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.275363 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.275777 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.276152 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.276521 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.276897 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.277840 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.278287 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.278729 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.279121 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.279488 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.279873 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.280927 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.281318 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.281684 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.282429 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.282856 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.283243 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.284163 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.284554 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.284918 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.285676 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.286061 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.286433 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 19:11:26.287186 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.287554 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.287908 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.288288 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.288911 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.289278 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.289983 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.290372 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.290744 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.291477 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.291844 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.292200 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.292936 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.293301 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.293728 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.294102 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([3072, 1]).\n",
      "I1201 19:11:26.294512 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.294884 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.295844 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.296213 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.296561 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.296933 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([768, 1]).\n",
      "I1201 19:11:26.297616 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.297944 140057592104768 tensor_quantizer.py:173] Disable MaxCalibrator\n",
      "W1201 19:11:26.298643 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.299016 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.299408 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "W1201 19:11:26.300342 140057592104768 tensor_quantizer.py:237] Load calibrated amax, shape=torch.Size([]).\n",
      "I1201 19:11:26.300858 140057592104768 tensor_quantizer.py:187] Enable `quant` stage.\n",
      "W1201 19:11:26.301294 140057592104768 tensor_quantizer.py:173] Disable HistogramCalibrator\n",
      "Configuration saved in roberta-in-bert-trained-quantized/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.encoder.layer.0.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=4.3667 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.2286, 0.7135](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=4.3667 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.2130, 0.8616](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=4.3667 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0553, 0.3021](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=20.9483 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=19.1295 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.9434 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.9990 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.7202 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0895, 0.8283](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=2.3058 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=4.3667 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=16.3581 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0793, 0.9986](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=4.7207 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1532, 1.0122](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=8.5718 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.0.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=16.3581 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=15.4349 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1721, 0.5387](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=15.4349 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1740, 0.7034](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=15.4349 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0696, 0.3664](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.7817 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.8797 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=4.3953 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.9966 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.9805 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0581, 0.8137](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=2.8595 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=15.4349 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=24.1278 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0945, 0.9753](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=2.5225 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1333, 1.0380](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=5.5873 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.1.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=24.1278 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.4631 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1529, 0.5227](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.4631 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1589, 0.6583](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.4631 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0926, 0.5707](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.8016 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=8.9038 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.5037 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.8067 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.3959 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0550, 0.5825](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=1.5141 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=22.4631 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=25.4978 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0963, 0.6521](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=5.4928 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1195, 0.9864](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=8.5919 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.2.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=25.4978 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=19.6996 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1667, 0.6095](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=19.6996 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1594, 0.6760](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=19.6996 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1110, 0.4958](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=11.2620 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.0273 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.9673 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.6460 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.3340 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0491, 0.6411](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=1.4833 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=19.6996 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=23.9052 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0916, 0.6947](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=5.3703 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1261, 1.0282](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=4.7693 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.3.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=23.9052 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1819, 0.6175](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1740, 0.6784](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0960, 0.3665](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.7623 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.3514 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.9243 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.5537 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.2916 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0238, 0.5530](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=2.6874 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=18.7359 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.2396 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0951, 0.6780](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=6.2925 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.0993, 1.0144](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=5.8652 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.4.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.2396 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=20.8635 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1435, 0.5507](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=20.8635 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1387, 0.6393](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=20.8635 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1045, 0.3715](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.5283 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.4352 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.9734 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.5589 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.6919 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0134, 0.5827](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=1.9668 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=20.8635 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.9879 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0820, 0.6016](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=2.8023 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1060, 1.0209](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=3.0022 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.5.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.9879 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=21.0328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1642, 0.6050](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=21.0328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1604, 0.6274](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=21.0328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1190, 0.4425](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.7586 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=10.1608 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.0164 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.5824 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.1733 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0716, 0.5062](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=2.7819 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=21.0328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.0411 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1003, 0.6700](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=2.4391 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1192, 1.0108](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=5.5336 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.6.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.0411 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.1518 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1625, 0.6395](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.1518 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1711, 0.6029](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.1518 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0622, 0.3252](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=8.8614 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.6074 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.7348 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.6704 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.4070 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0548, 0.6221](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=4.4532 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=22.1518 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.6406 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0800, 0.7407](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=2.6607 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1217, 1.2869](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=7.7671 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.7.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.6406 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.6328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1686, 0.5366](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=22.6328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1653, 0.6490](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=22.6328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0951, 0.3531](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.3184 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.2597 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.5218 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.5794 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=1.8379 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0489, 0.5719](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=6.6549 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=22.6328 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=21.8385 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0863, 0.5819](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=3.4752 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1235, 1.3031](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=7.2133 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.8.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=21.8385 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.query._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=21.9573 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.query._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.1544, 0.5243](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.key._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=21.9573 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.key._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.1549, 0.5846](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.value._input_quantizer                       TensorQuantizer(8bit fake per-tensor amax=21.9573 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.value._weight_quantizer                      TensorQuantizer(8bit fake axis=(0,) amax=[0.0839, 0.3537](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.matmul_q_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=9.2343 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.matmul_k_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=8.9173 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.matmul_v_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=3.0295 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.self.matmul_a_input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=0.5832 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.output.dense._input_quantizer                     TensorQuantizer(8bit fake per-tensor amax=2.2727 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.output.dense._weight_quantizer                    TensorQuantizer(8bit fake axis=(0,) amax=[0.0700, 0.5093](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.output.add_local_input_quantizer                  TensorQuantizer(8bit fake per-tensor amax=6.9703 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.attention.output.add_residual_input_quantizer               TensorQuantizer(8bit fake per-tensor amax=21.9573 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.intermediate.dense._input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=20.2072 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.intermediate.dense._weight_quantizer                        TensorQuantizer(8bit fake axis=(0,) amax=[0.0692, 0.5374](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.output.dense._input_quantizer                               TensorQuantizer(8bit fake per-tensor amax=3.5973 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.output.dense._weight_quantizer                              TensorQuantizer(8bit fake axis=(0,) amax=[0.1114, 1.0897](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.output.add_local_input_quantizer                            TensorQuantizer(8bit fake per-tensor amax=6.9660 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.9.output.add_residual_input_quantizer                         TensorQuantizer(8bit fake per-tensor amax=20.2072 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.query._input_quantizer                      TensorQuantizer(8bit fake per-tensor amax=21.2327 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.query._weight_quantizer                     TensorQuantizer(8bit fake axis=(0,) amax=[0.1554, 0.5088](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.key._input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=21.2327 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.key._weight_quantizer                       TensorQuantizer(8bit fake axis=(0,) amax=[0.1604, 0.5558](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.value._input_quantizer                      TensorQuantizer(8bit fake per-tensor amax=21.2327 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.value._weight_quantizer                     TensorQuantizer(8bit fake axis=(0,) amax=[0.0912, 0.2958](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.matmul_q_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=7.7721 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.matmul_k_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=8.4632 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.matmul_v_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=3.1891 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.self.matmul_a_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=0.6085 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.output.dense._input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=2.7075 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.output.dense._weight_quantizer                   TensorQuantizer(8bit fake axis=(0,) amax=[0.0828, 0.5451](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.output.add_local_input_quantizer                 TensorQuantizer(8bit fake per-tensor amax=8.5943 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.attention.output.add_residual_input_quantizer              TensorQuantizer(8bit fake per-tensor amax=21.2327 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.intermediate.dense._input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=16.0322 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.intermediate.dense._weight_quantizer                       TensorQuantizer(8bit fake axis=(0,) amax=[0.0847, 0.5931](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.output.dense._input_quantizer                              TensorQuantizer(8bit fake per-tensor amax=2.9404 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.output.dense._weight_quantizer                             TensorQuantizer(8bit fake axis=(0,) amax=[0.1075, 1.0234](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.output.add_local_input_quantizer                           TensorQuantizer(8bit fake per-tensor amax=4.3903 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.10.output.add_residual_input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=16.0322 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.query._input_quantizer                      TensorQuantizer(8bit fake per-tensor amax=14.1766 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.query._weight_quantizer                     TensorQuantizer(8bit fake axis=(0,) amax=[0.1734, 0.5519](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.key._input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=14.1766 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.key._weight_quantizer                       TensorQuantizer(8bit fake axis=(0,) amax=[0.1755, 0.5546](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.value._input_quantizer                      TensorQuantizer(8bit fake per-tensor amax=14.1766 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.value._weight_quantizer                     TensorQuantizer(8bit fake axis=(0,) amax=[0.1063, 0.3849](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.matmul_q_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=8.6127 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.matmul_k_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=7.4551 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.matmul_v_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=3.4774 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.self.matmul_a_input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=0.6208 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.output.dense._input_quantizer                    TensorQuantizer(8bit fake per-tensor amax=3.7673 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.output.dense._weight_quantizer                   TensorQuantizer(8bit fake axis=(0,) amax=[0.0971, 0.5766](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.output.add_local_input_quantizer                 TensorQuantizer(8bit fake per-tensor amax=9.5412 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.attention.output.add_residual_input_quantizer              TensorQuantizer(8bit fake per-tensor amax=14.1766 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.intermediate.dense._input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=8.4928 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.intermediate.dense._weight_quantizer                       TensorQuantizer(8bit fake axis=(0,) amax=[0.0982, 0.3916](3072) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.output.dense._input_quantizer                              TensorQuantizer(8bit fake per-tensor amax=4.6082 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.output.dense._weight_quantizer                             TensorQuantizer(8bit fake axis=(0,) amax=[0.0956, 1.0014](768) calibrator=MaxCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.output.add_local_input_quantizer                           TensorQuantizer(8bit fake per-tensor amax=14.0034 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "bert.encoder.layer.11.output.add_residual_input_quantizer                        TensorQuantizer(8bit fake per-tensor amax=8.4928 calibrator=HistogramCalibrator scale=1.0 quant)\n",
      "240 TensorQuantizers found in model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in roberta-in-bert-trained-quantized/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_q = QDQBertForSequenceClassification.from_pretrained(\"roberta-in-bert-trained\", num_labels=num_labels)\n",
    "model_q = model_q.cuda()\n",
    "\n",
    "# Find the TensorQuantizer and enable calibration\n",
    "for name, module in tqdm(model_q.named_modules()):\n",
    "    if isinstance(module, quant_nn.TensorQuantizer):\n",
    "        if module._calibrator is not None:\n",
    "            module.disable_quant()\n",
    "            module.enable_calib()\n",
    "        else:\n",
    "            module.disable()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start_index in tqdm(range(0, 4*batch_size, batch_size)):\n",
    "        end_index = start_index + batch_size\n",
    "        data = encoded_dataset[\"train\"][start_index:end_index]\n",
    "        input_torch = {k: torch.tensor(list(v), dtype=torch.long, device=\"cuda\")\n",
    "                       for k, v in data.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "        model_q(**input_torch)\n",
    "\n",
    "\n",
    "print(\"calibration\")\n",
    "# Finalize calibration\n",
    "for name, module in model_q.named_modules():\n",
    "    if isinstance(module, quant_nn.TensorQuantizer):\n",
    "        if module._calibrator is not None:\n",
    "            if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                module.load_calib_amax()\n",
    "            else:\n",
    "                module.load_calib_amax(\"percentile\", percentile=99.99)\n",
    "            module.enable_quant()\n",
    "            module.disable_calib()\n",
    "        else:\n",
    "            module.enable()\n",
    "\n",
    "model_q.cuda()\n",
    "\n",
    "count = 0\n",
    "for name, mod in model_q.named_modules():\n",
    "    if isinstance(mod, pytorch_quantization.nn.TensorQuantizer):\n",
    "        print(f\"{name:80} {mod}\")\n",
    "        count += 1\n",
    "print(f\"{count} TensorQuantizers found in model\")\n",
    "model_q.save_pretrained(\"roberta-in-bert-trained-quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization aware training\n",
    "\n",
    "The query aware training is not a mandatory step, but highly recommended to get the best accuracy. Basically we will redo the training with the quantization enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file roberta-in-bert-trained-quantized/config.json\n",
      "Model config QDQBertConfig {\n",
      "  \"_name_or_path\": \"roberta-in-bert-trained\",\n",
      "  \"architectures\": [\n",
      "    \"QDQBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"qdqbert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.13.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file roberta-in-bert-trained-quantized/pytorch_model.bin\n",
      "I1201 19:11:27.208793 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.209632 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.210307 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.211149 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.223644 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.224489 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.225447 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.226618 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.241246 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.242141 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.243034 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.243859 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.244722 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.245520 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.246686 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.247766 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.264419 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.265261 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.265923 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.266575 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.267451 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.268172 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.287400 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.288151 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.288789 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.289382 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.309565 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.310387 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.311098 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.311751 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.312860 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.313377 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.324157 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.324974 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.325625 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.326216 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.336715 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.337255 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.337689 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.338090 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.347661 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.348753 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.349757 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.350883 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.351833 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.352656 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.353345 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.353943 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.364943 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.365485 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.365902 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.366509 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.367229 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.368165 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.388085 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.388865 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.389516 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.390258 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.410784 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.411631 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.412297 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.413084 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.414086 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.414799 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.425738 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.426646 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.427272 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.427896 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.438486 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.439234 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:27.439879 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.440475 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.450418 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.451315 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.451903 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.452303 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.452749 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.453151 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.453549 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.453957 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.465221 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.465954 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.466644 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.467072 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.468285 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.468727 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.489801 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.490633 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.491233 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.491766 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.514694 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.515455 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.516121 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.516760 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.518003 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.518579 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.528168 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.528827 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.529607 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.530612 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.540609 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.541207 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.541832 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.542459 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.552270 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.552871 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.553499 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.554122 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.554818 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.555415 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.556003 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.556731 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.569805 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.570413 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.570830 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.571321 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.572377 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.572752 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.592881 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.593365 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.593739 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.594096 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.614654 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.615172 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.615548 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.615923 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.616773 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.617163 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.627428 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.628115 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.628840 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.629502 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.638837 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.639514 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.640233 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.640931 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.650447 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.651207 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.651915 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.652822 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.653721 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.654456 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.655095 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.655695 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.666741 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.667589 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.668479 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.669285 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.670184 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.670979 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.691772 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.692621 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:27.693244 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.693841 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.714617 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.715979 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.716634 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.717667 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.719031 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.719666 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.731079 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.731843 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.732547 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.733175 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.746136 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.746871 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.747591 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.748189 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.760656 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.761277 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.762013 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.762758 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.763410 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.763994 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.764589 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.765226 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.779591 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.780524 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.781319 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.781931 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.782832 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.783449 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.805895 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.806693 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.807322 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.807956 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.834656 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.835458 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.836114 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.836716 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.838033 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.838794 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.849754 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.850384 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.851008 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.851848 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.866665 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.867449 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.868165 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.868940 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.878483 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.879086 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.879875 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.880830 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.881797 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.882521 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.883174 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.883836 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.895553 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.895962 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.896406 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.896936 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.898256 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.898759 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.919639 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.920164 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.920756 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.921738 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.942070 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.942806 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.943764 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.944607 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.945640 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.946361 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.958386 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.958937 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.959405 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.959839 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.969992 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.970374 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.970793 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.971190 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.980629 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:27.980991 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.981393 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.981833 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.982313 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.982766 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.983162 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.983512 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.993079 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:27.993498 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:27.993848 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.994362 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:27.995240 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:27.995689 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.016281 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.016845 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.017403 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.017889 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.037999 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.038670 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.039304 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.039771 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.040842 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.041286 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.051465 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.052168 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.053006 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.053862 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.069291 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.069987 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.070711 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.071384 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.080609 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.081198 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.081861 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.082520 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.083215 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.084009 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.084729 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.085382 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.097800 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.099022 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.100077 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.100697 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.101861 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.102420 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.124801 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.125305 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.125720 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.126162 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.148860 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.149511 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.149907 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.150279 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.151454 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.151886 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.162329 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.162862 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.163242 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.163593 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.173374 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.173928 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.174362 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.174846 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.186318 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.186879 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.187222 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.187582 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.188061 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.188420 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.188758 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.189087 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.199641 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.200148 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.200556 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.200956 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.201748 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.202094 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.225237 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.225745 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.226214 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.226853 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.249388 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1201 19:11:28.249994 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.250553 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.251643 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.253252 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.253765 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.265286 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.266306 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.267012 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.267527 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.278982 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.279747 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.280377 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.280992 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.292645 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.293354 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.294006 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.294641 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.295266 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.295871 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.296438 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.297021 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.308347 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.309117 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.309940 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.310598 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.311483 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.312092 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.334968 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.335727 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.336348 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.336934 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.360406 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.361209 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.362031 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.362898 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.363797 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.364415 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.375894 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.376536 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.377102 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.377916 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.390349 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.390866 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.391533 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.392543 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.403455 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.403934 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.404580 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.405518 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.406105 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.406684 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.407144 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.407640 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.417722 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.418179 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.418803 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.419415 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.420697 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.421152 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.444371 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.445104 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.445688 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.446305 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.469518 140057592104768 _utils.py:72] Input is fake quantized to 8 bits in QuantLinear with axis None!\n",
      "I1201 19:11:28.470333 140057592104768 _utils.py:75] Weight is fake quantized to 8 bits in QuantLinear with axis (0,)!\n",
      "I1201 19:11:28.470999 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.471604 140057592104768 tensor_quantizer.py:105] Creating Max calibrator\n",
      "I1201 19:11:28.472492 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "I1201 19:11:28.473104 140057592104768 tensor_quantizer.py:101] Creating histogram calibrator\n",
      "Some weights of the model checkpoint at roberta-in-bert-trained-quantized were not used when initializing QDQBertForSequenceClassification: ['bert.encoder.layer.2.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.4.output.dense._input_quantizer._amax', 'bert.encoder.layer.5.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.3.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.5.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.1.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.8.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.5.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.1.output.dense._weight_quantizer._amax', 'bert.encoder.layer.10.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.5.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.10.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.2.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.5.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.6.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.7.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.3.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.5.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.5.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.4.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.3.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.2.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.6.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.2.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.9.output.dense._input_quantizer._amax', 'bert.encoder.layer.11.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.0.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.10.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.0.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.1.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.9.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.2.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.0.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.5.output.dense._input_quantizer._amax', 'bert.encoder.layer.8.output.add_local_input_quantizer._amax', 'bert.encoder.layer.7.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.6.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.1.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.3.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.3.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.6.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.8.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.8.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.6.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.3.output.dense._input_quantizer._amax', 'bert.encoder.layer.11.output.add_local_input_quantizer._amax', 'bert.encoder.layer.0.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.4.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.9.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.11.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.10.output.dense._weight_quantizer._amax', 'bert.encoder.layer.8.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.3.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.4.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.3.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.1.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.1.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.11.output.dense._input_quantizer._amax', 'bert.encoder.layer.7.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.1.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.0.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.1.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.4.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.0.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.8.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.11.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.9.output.add_local_input_quantizer._amax', 'bert.encoder.layer.7.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.0.output.dense._input_quantizer._amax', 'bert.encoder.layer.0.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.10.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.10.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.4.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.7.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.8.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.5.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.6.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.10.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.6.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.1.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.4.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.1.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.9.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.7.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.5.output.dense._weight_quantizer._amax', 'bert.encoder.layer.2.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.0.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.11.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.5.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.9.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.0.output.dense._weight_quantizer._amax', 'bert.encoder.layer.1.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.8.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.8.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.11.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.11.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.4.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.9.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.6.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.1.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.7.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.11.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.4.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.8.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.2.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.8.output.dense._input_quantizer._amax', 'bert.encoder.layer.9.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.6.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.10.output.dense._input_quantizer._amax', 'bert.encoder.layer.6.output.add_local_input_quantizer._amax', 'bert.encoder.layer.4.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.9.output.dense._weight_quantizer._amax', 'bert.encoder.layer.5.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.3.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.6.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.2.output.add_local_input_quantizer._amax', 'bert.encoder.layer.3.output.add_local_input_quantizer._amax', 'bert.encoder.layer.3.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.6.output.dense._weight_quantizer._amax', 'bert.encoder.layer.7.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.11.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.1.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.0.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.3.output.dense._weight_quantizer._amax', 'bert.encoder.layer.9.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.9.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.1.output.add_local_input_quantizer._amax', 'bert.encoder.layer.4.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.3.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.7.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.0.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.8.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.11.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.8.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.2.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.8.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.8.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.7.output.dense._input_quantizer._amax', 'bert.encoder.layer.0.output.add_local_input_quantizer._amax', 'bert.encoder.layer.6.output.dense._input_quantizer._amax', 'bert.encoder.layer.11.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.1.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.5.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.6.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.3.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.1.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.4.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.5.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.7.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.5.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.0.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.5.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.10.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.1.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.5.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.3.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.11.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.4.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.6.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.0.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.8.output.dense._weight_quantizer._amax', 'bert.encoder.layer.5.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.9.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.11.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.0.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.4.output.dense._weight_quantizer._amax', 'bert.encoder.layer.10.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.6.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.4.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.9.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.11.attention.self.key._input_quantizer._amax', 'bert.encoder.layer.8.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.1.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.6.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.0.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.matmul_v_input_quantizer._amax', 'bert.encoder.layer.3.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.8.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.10.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.2.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.1.attention.self.key._weight_quantizer._amax', 'bert.encoder.layer.1.output.dense._input_quantizer._amax', 'bert.encoder.layer.7.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.5.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.3.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.7.output.dense._weight_quantizer._amax', 'bert.encoder.layer.3.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.2.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.5.output.add_local_input_quantizer._amax', 'bert.encoder.layer.7.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.8.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.4.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.7.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.2.output.dense._input_quantizer._amax', 'bert.encoder.layer.7.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.11.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.3.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.4.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.9.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.8.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.6.attention.output.add_local_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.query._input_quantizer._amax', 'bert.encoder.layer.11.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.10.output.add_local_input_quantizer._amax', 'bert.encoder.layer.0.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.11.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.7.attention.self.query._weight_quantizer._amax', 'bert.encoder.layer.6.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.2.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.matmul_k_input_quantizer._amax', 'bert.encoder.layer.9.attention.self.value._input_quantizer._amax', 'bert.encoder.layer.2.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.0.attention.self.matmul_a_input_quantizer._amax', 'bert.encoder.layer.6.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.4.attention.output.dense._input_quantizer._amax', 'bert.encoder.layer.4.attention.output.dense._weight_quantizer._amax', 'bert.encoder.layer.4.output.add_local_input_quantizer._amax', 'bert.encoder.layer.7.attention.self.matmul_q_input_quantizer._amax', 'bert.encoder.layer.2.output.dense._weight_quantizer._amax', 'bert.encoder.layer.7.output.add_local_input_quantizer._amax', 'bert.encoder.layer.11.output.dense._weight_quantizer._amax', 'bert.encoder.layer.7.attention.output.add_residual_input_quantizer._amax', 'bert.encoder.layer.3.intermediate.dense._input_quantizer._amax', 'bert.encoder.layer.10.attention.self.value._weight_quantizer._amax', 'bert.encoder.layer.0.intermediate.dense._weight_quantizer._amax', 'bert.encoder.layer.11.attention.self.value._input_quantizer._amax']\n",
      "- This IS expected if you are initializing QDQBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QDQBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All the weights of QDQBertForSequenceClassification were initialized from the model checkpoint at roberta-in-bert-trained-quantized.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use QDQBertForSequenceClassification for predictions without further training.\n",
      "Using amp half precision backend\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 07:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running training *****\n",
      "  Num examples = 392702\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5553710460662842, 'eval_accuracy': 0.7799286805909322, 'eval_runtime': 46.6334, 'eval_samples_per_second': 210.472, 'eval_steps_per_second': 3.302}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12272' max='12272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12272/12272 1:22:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.581400</td>\n",
       "      <td>0.505601</td>\n",
       "      <td>0.805807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.542400</td>\n",
       "      <td>0.481971</td>\n",
       "      <td>0.811105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.469823</td>\n",
       "      <td>0.823637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.459618</td>\n",
       "      <td>0.821905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.482700</td>\n",
       "      <td>0.418851</td>\n",
       "      <td>0.837596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.417829</td>\n",
       "      <td>0.836373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.460700</td>\n",
       "      <td>0.431540</td>\n",
       "      <td>0.834947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.402023</td>\n",
       "      <td>0.847376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.457700</td>\n",
       "      <td>0.396712</td>\n",
       "      <td>0.846052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.398412</td>\n",
       "      <td>0.846460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.436800</td>\n",
       "      <td>0.396119</td>\n",
       "      <td>0.848701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.398557</td>\n",
       "      <td>0.850229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-1000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-1000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-1000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-2000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-2000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-2000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-3000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-3000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-3000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-4000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-4000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-4000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-5000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-5000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-5000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-6000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-6000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-6000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-7000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-7000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-7000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-8000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-8000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-8000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-9000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-9000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-9000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-10000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-10000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-10000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-11000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-11000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-11000/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to roberta-base-finetuned-mnli/checkpoint-12000\n",
      "Configuration saved in roberta-base-finetuned-mnli/checkpoint-12000/config.json\n",
      "Model weights saved in roberta-base-finetuned-mnli/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in roberta-base-finetuned-mnli/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in roberta-base-finetuned-mnli/checkpoint-12000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading best model from roberta-base-finetuned-mnli/checkpoint-12000 (score: 0.8502292409577178).\n",
      "W1201 20:35:01.955523 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.956298 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.959111 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.960055 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.961468 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.962328 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.963392 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.964326 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.965610 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.966427 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.969145 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.970091 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.971021 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.971580 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.973517 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.974354 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.977516 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.978431 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.979310 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.980026 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.0.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.981464 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.982098 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.983325 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.984021 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.985316 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.986423 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.987096 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.987734 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.988366 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.988989 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.990516 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.991376 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.992448 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.993382 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.995410 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.996055 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.997875 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.998544 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:01.999530 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.000184 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.1.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.001658 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.002353 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.004154 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.004740 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.005739 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.006352 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.007008 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.007576 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.008145 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.009447 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.010714 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.011306 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.012108 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.012622 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.014365 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.015107 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.017004 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.017748 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.output.dense._weight_quantizer: Overwriting amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 20:35:02.018628 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.019512 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.2.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.021052 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.021683 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.023229 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.024051 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.025203 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.025818 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.026520 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.027173 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.027965 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.028816 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.030057 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.030714 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.031604 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.032257 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.034212 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.035150 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.037409 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.038117 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.039025 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.039702 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.3.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.041222 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.041909 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.043179 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.043793 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.044975 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.045604 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.046368 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.047130 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.047795 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.048471 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.050607 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.051483 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.052273 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.053120 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.055149 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.055891 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.057919 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.058685 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.059737 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.060414 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.4.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.062059 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.062887 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.064355 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.065141 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.066732 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.067530 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.068627 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.069675 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.070315 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.071181 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.072673 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.073366 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.074441 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.075135 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.077220 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.078051 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.intermediate.dense._weight_quantizer: Overwriting amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 20:35:02.080285 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.081100 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.082259 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.082955 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.5.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.084679 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.085377 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.086830 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.087531 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.088868 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.089610 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.090454 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.091369 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.092094 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.093008 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.094101 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.094680 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.095411 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.096111 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.098433 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.099215 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.101495 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.102191 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.103269 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.104236 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.6.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.106216 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.107016 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.108289 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.109015 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.110199 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.110914 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.111627 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.112400 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.113051 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.113741 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.115241 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.116059 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.117117 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.117922 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.120249 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.121058 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.123253 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.124119 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.125084 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.125741 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.7.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.127210 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.127854 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.128997 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.129669 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.131155 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.131786 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.132460 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.132992 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.133857 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.134664 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.136404 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.137067 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.137895 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.138522 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.140558 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.intermediate.dense._input_quantizer: Overwriting amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 20:35:02.141115 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.143184 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.143835 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.144592 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.145140 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.8.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.146503 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.147159 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.148222 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.148817 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.149873 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.150564 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.151132 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.151804 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.152360 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.153022 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.154481 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.155050 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.156115 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.156681 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.158971 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.159622 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.161546 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.162128 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.163292 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.163882 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.9.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.165225 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.165837 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.166918 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.167514 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.168765 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.169459 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.170163 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.170852 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.171326 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.172083 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.173614 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.174386 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.175272 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.175909 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.177917 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.178691 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.181160 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.181823 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.182794 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.183488 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.10.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.184913 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.query._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.185632 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.query._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.186927 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.key._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.187760 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.key._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.189118 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.value._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.189894 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.value._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.190741 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.matmul_q_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.191475 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.matmul_k_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.192274 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.matmul_v_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.193039 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.self.matmul_a_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.194473 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.195123 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.196365 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.output.add_local_input_quantizer: Overwriting amax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 20:35:02.197219 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.attention.output.add_residual_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.199527 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.intermediate.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.200487 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.intermediate.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.202985 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.output.dense._input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.203737 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.output.dense._weight_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.204701 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.output.add_local_input_quantizer: Overwriting amax.\n",
      "W1201 20:35:02.205333 140057592104768 tensor_quantizer.py:402] bert.encoder.layer.11.output.add_residual_input_quantizer: Overwriting amax.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `QDQBertForSequenceClassification.forward` and have been ignored: idx, hypothesis, premise.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 9815\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [154/154 56:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.39855679869651794, 'eval_accuracy': 0.8502292409577178, 'eval_runtime': 47.3757, 'eval_samples_per_second': 207.174, 'eval_steps_per_second': 3.251, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "model_q = QDQBertForSequenceClassification.from_pretrained(\"roberta-in-bert-trained-quantized\", num_labels=num_labels)\n",
    "model_q = model_q.cuda()\n",
    "\n",
    "args.max_steps = -1\n",
    "trainer = Trainer(\n",
    "    model_q,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "print(trainer.evaluate())\n",
    "trainer.train()\n",
    "print(trainer.evaluate())\n",
    "model_q.save_pretrained(\"roberta-in-bert-trained-quantized-bis\")\n",
    "del model_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latency measures\n",
    "\n",
    "Let's see if what we have done is useful...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1201 21:32:19.778870 140057592104768 tensor_quantizer.py:280] Use Pytorch's native experimental fake quantization.\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization-2.1.2-py3.9-linux-x86_64.egg/pytorch_quantization/nn/modules/tensor_quantizer.py:285: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  inputs, amax.item() / bound, 0,\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/pytorch_quantization-2.1.2-py3.9-linux-x86_64.egg/pytorch_quantization/nn/modules/tensor_quantizer.py:291: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  quant_dim = list(amax.shape).index(list(amax_sequeeze.shape)[0])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_quantization.nn import TensorQuantizer\n",
    "TensorQuantizer.use_fb_fake_quant = True\n",
    "model_q = QDQBertForSequenceClassification.from_pretrained(\"roberta-in-bert-trained-quantized-bis\", num_labels=num_labels)\n",
    "model_q = model_q.cuda()\n",
    "print(trainer.evaluate())\n",
    "convert_to_onnx(model_q, output_path=\"model_q.onnx\", inputs_pytorch=input_torch)\n",
    "TensorQuantizer.use_fb_fake_quant = False\n",
    "del model_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=model_q.onnx --shapes=input_ids:1x384,attention_mask:1x384 --best --workspace=6000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorRT baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\n",
    "baseline_model = baseline_model.cuda()\n",
    "convert_to_onnx(baseline_model, output_path=\"baseline.onnx\", inputs_pytorch=input_torch)\n",
    "del baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/src/tensorrt/bin/trtexec --onnx=baseline.onnx --shapes=input_ids:1x384,attention_mask:1x384 --fp16 --workspace=6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del baseline_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "whPRbBNbIrIl",
    "n9qywopnIrJH",
    "7k8ge1L1IrJk"
   ],
   "name": "Copie de Text Classification on GLUE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0022faf286b44e858e638ccd5ded38b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "023900ca566446eab5905b25b16a3de7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08286a6371584b4186014ecb5d5f164d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d3a08166846438db79b0f89314fe76a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d1ecc3d380fc4758b03190b23686a2f1",
      "value": " 481/481 [00:00&lt;00:00, 10.9kB/s]"
     }
    },
    "092db03992f24951b494fbb81da5b9d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_994cf2338c7c4899952e25723445693c",
       "IPY_MODEL_6aa2f5d46f1f454198d8e69517549ff1",
       "IPY_MODEL_72b8f11065254e5ca488cd346b5add54"
      ],
      "layout": "IPY_MODEL_023900ca566446eab5905b25b16a3de7"
     }
    },
    "0dab554959dc44b3b313ee8ae91ca88d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e1f08cf954ae4aea818c90d893486c77",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f01fdef82047471e8c1b780cae5379cc",
      "value": " 420M/420M [00:13&lt;00:00, 33.6MB/s]"
     }
    },
    "10678736bd534c63aebda414da01b4db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14648b8262944f5faac134a7c0184e47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "154200a8bc0b44fe8d0419fd56c6539d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15aae23369674f82888ed9fbd99739f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "163146c2f23440bcbf782116a35b5684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf9597523c024514b9b3e66bc77e3fa8",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cced5f1cccc2400a8fbfd7a6eaedc666",
      "value": 440473133
     }
    },
    "167874df55014291be95cd390b1e60d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "17b83e0d0fb947d7bf20319ff930e8fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_854cfd13416543fba8221093b903658b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cbbb20b5d01a4450bfb8dfbf8048d64f",
      "value": "Downloading: 100%"
     }
    },
    "17bd5357081d41c6b0161d63bd00820a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "196ffc99ad5a40109d9b1cfe12032b62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1bea379404df429b9852b62a938661ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4c444f06c0847c09a44917084d3908d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_300f01e3547648f3983a83d3d3118c54",
      "value": 1
     }
    },
    "1da1d80871f545bbb21bf5a84d2120a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8585eab4b3fe4992bd7e7c4596e2483b",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ec6da801d0d45c4bb80eeab5518e124",
      "value": 570
     }
    },
    "21ef195fa88f49c4a2c057f8028177a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26bc2038bed74279813ab5af09a2724c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0022faf286b44e858e638ccd5ded38b0",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7ff32d18c9f0473893a6a6b2941c54b0",
      "value": 456318
     }
    },
    "28b7346a9b8c4b198dd9dbea1be013b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d3a08166846438db79b0f89314fe76a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eac6b4817e14d7fae396e6458b940fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_927ad6ade85a402594074fa90ab558c2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cae29b9c6d45412fab70977fcd0f3234",
      "value": "Downloading: 100%"
     }
    },
    "300f01e3547648f3983a83d3d3118c54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "30646fa2c0dc494e9dbcbd4dc598410e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "360d6eb0e41543dba6d457912e32a77d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37cda4cae81a4d94aa831fb40b5c3b26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56fd7584b0844590936519ec3851922e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_5b1ad9f5d02c4b298a02ce6041692057",
      "value": " 4/4 [00:00&lt;00:00,  5.97ba/s]"
     }
    },
    "3bfff454943b4b04a12ec29bbe28e0aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cedca6e55b84443e82f3d01471d61048",
       "IPY_MODEL_a7d355f456eb4d3995dd91c5917a72c1",
       "IPY_MODEL_b264b220d9c444bd9da46a7e6c8fd5ed"
      ],
      "layout": "IPY_MODEL_154200a8bc0b44fe8d0419fd56c6539d"
     }
    },
    "3e7fbd1c0e534cb8abca18d1edfc9277": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4320b12de9d14c459cc88319e2d7622a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4552ee8ca6bd4a0b956651cc23f4ff3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b13c3b3435f4689b29d48e0a35bebd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e91efae49b64f038fd3fbfcfd2be510": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fae966b76844c869cdea1e53891e26f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "54c0ad5ab737433190c4a824be128a48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "561b1ede331a40c1a2bff9422e8eea0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56fd7584b0844590936519ec3851922e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59418bbeb20547e5b5e1a5728262c757": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b1ad9f5d02c4b298a02ce6041692057": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e2185bd6e4f4a10b89ac606868a43bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2eac6b4817e14d7fae396e6458b940fa",
       "IPY_MODEL_af16284f77594397a69ad0e322b5e736",
       "IPY_MODEL_a20579a9e7364fb485d79bdc4feb54dc"
      ],
      "layout": "IPY_MODEL_f44d2beebfe44186b0ac8016e89e4b49"
     }
    },
    "5f032f56105f463a8680aa2482d0b162": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65017db07d7f4e798ede741cc92488f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_86cc326e574a4fada7224e6f0c209e9a",
       "IPY_MODEL_af5b646f89024c139c695a1f058fb772",
       "IPY_MODEL_37cda4cae81a4d94aa831fb40b5c3b26"
      ],
      "layout": "IPY_MODEL_6fa74604c68543a38392fa0e1587f707"
     }
    },
    "68c4c867096d41a78740fdee30edcadb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6aa2f5d46f1f454198d8e69517549ff1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b483d17d1d14fdd922600f0c906fc2f",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4320b12de9d14c459cc88319e2d7622a",
      "value": 1355863
     }
    },
    "6d48e5ce9a854a3bb0506d774665f428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbdb7c7250d846b2880005a9012c484b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_17bd5357081d41c6b0161d63bd00820a",
      "value": " 478M/478M [00:15&lt;00:00, 34.7MB/s]"
     }
    },
    "6e54ce781ca54ad283911fa4774e3361": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e604307427a466cab51d50d363ee86d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fa74604c68543a38392fa0e1587f707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "728a9dcc79824e1eb2bfa49d915a8f08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d314c0bb87e04893b96de0e18766d3ab",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_fa35b3acd9ce4cb098fcd69bb405db00",
      "value": "Downloading: 100%"
     }
    },
    "72b8f11065254e5ca488cd346b5add54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10678736bd534c63aebda414da01b4db",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_14648b8262944f5faac134a7c0184e47",
      "value": " 1.29M/1.29M [00:00&lt;00:00, 2.22MB/s]"
     }
    },
    "7701ec898fd443f1b35b187aea3651e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78601982b0e04b80adaa502db2ef685a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6426fea2eda41dd9a31cb3f35b0877e",
       "IPY_MODEL_163146c2f23440bcbf782116a35b5684",
       "IPY_MODEL_0dab554959dc44b3b313ee8ae91ca88d"
      ],
      "layout": "IPY_MODEL_167874df55014291be95cd390b1e60d3"
     }
    },
    "788badadfd834f61926a39a43ef1d517": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a75099f99054645bf3fc1b778dac7e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b483d17d1d14fdd922600f0c906fc2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bb3b69a2f814e60b0cec253c759a16b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d731cfb34124448bbd8baab3d27b75db",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_cbb3e9bf5d07406d9768a98a6f0b5b64",
      "value": "100%"
     }
    },
    "7c875ecd9cb54405a6c45969bcb4b4c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d520bdde27742abb42803843721d101": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7ec6da801d0d45c4bb80eeab5518e124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7ff32d18c9f0473893a6a6b2941c54b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8399339998564d21ba5db6f0514c02c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "854cfd13416543fba8221093b903658b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8585eab4b3fe4992bd7e7c4596e2483b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86cc326e574a4fada7224e6f0c209e9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_561b1ede331a40c1a2bff9422e8eea0e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_28b7346a9b8c4b198dd9dbea1be013b6",
      "value": "100%"
     }
    },
    "87d85ac2d3104f68b99db880b1089638": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_728a9dcc79824e1eb2bfa49d915a8f08",
       "IPY_MODEL_c815bfd265f4480298c39c76b9eaf770",
       "IPY_MODEL_6d48e5ce9a854a3bb0506d774665f428"
      ],
      "layout": "IPY_MODEL_6e604307427a466cab51d50d363ee86d"
     }
    },
    "8a11c8fed672470b8335dc575a4a220e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_93dbcc6d23a743bab0da8af6ee5e2825",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f8a0053903c64e75ac25eab5b24d5871",
      "value": 481
     }
    },
    "8defdddee0e64a20b101e6c50bd7c60b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7bb3b69a2f814e60b0cec253c759a16b",
       "IPY_MODEL_d25cca081db3469b80163d6707f5a37d",
       "IPY_MODEL_f8abc3e44ae3428885aafbea2b37384c"
      ],
      "layout": "IPY_MODEL_f485d2b19ffa4585a1da20986f28af29"
     }
    },
    "927ad6ade85a402594074fa90ab558c2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "93dbcc6d23a743bab0da8af6ee5e2825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "969b6fdac1d6418d89a683db1e6ec6b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "990482eebca2424bb5ecbd114007e02c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "994cf2338c7c4899952e25723445693c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9a0852554284d36b6b121f579b06b41",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c7bd52ef524c4d279dfcaa3aebe4a2c5",
      "value": "Downloading: 100%"
     }
    },
    "99e94791043b4499b06601f7524f9b14": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5c8ff9e3bd849059fa7b30eab5fc940",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_196ffc99ad5a40109d9b1cfe12032b62",
      "value": "Downloading: 100%"
     }
    },
    "9bc6e14b912249e3b7d02f31bcc74667": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_969b6fdac1d6418d89a683db1e6ec6b2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6e54ce781ca54ad283911fa4774e3361",
      "value": " 446k/446k [00:00&lt;00:00, 650kB/s]"
     }
    },
    "a02624219ee84f50b1a3032eaa030a39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a0a2918e9772475cac51124b3b83fcaf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a20579a9e7364fb485d79bdc4feb54dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b13c3b3435f4689b29d48e0a35bebd6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d5d015711ae04d2f801577fc50af6c15",
      "value": " 878k/878k [00:00&lt;00:00, 1.33MB/s]"
     }
    },
    "a3e2c73d393d4e58a371f3da3dd80e6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4c444f06c0847c09a44917084d3908d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a51b461c062f4636bfa4b48823d0709b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a61d366d91c34697a55f62b754e1f3a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a9b98fd93fcd4fc4a2b2aa88c82835d0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b8722dc10d4447fe9630cbf169260cc8",
      "value": "100%"
     }
    },
    "a7d355f456eb4d3995dd91c5917a72c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f032f56105f463a8680aa2482d0b162",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a02624219ee84f50b1a3032eaa030a39",
      "value": 2
     }
    },
    "a9b98fd93fcd4fc4a2b2aa88c82835d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac14ba24dcf3404db9fd303dbb24d7a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_17b83e0d0fb947d7bf20319ff930e8fc",
       "IPY_MODEL_1da1d80871f545bbb21bf5a84d2120a0",
       "IPY_MODEL_c593f2e45e244637821cc5721788bf2c"
      ],
      "layout": "IPY_MODEL_4e91efae49b64f038fd3fbfcfd2be510"
     }
    },
    "aecf7f063234416abf3f24766481cb89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "af16284f77594397a69ad0e322b5e736": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a75099f99054645bf3fc1b778dac7e6",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_30646fa2c0dc494e9dbcbd4dc598410e",
      "value": 898823
     }
    },
    "af5b646f89024c139c695a1f058fb772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_21ef195fa88f49c4a2c057f8028177a2",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aecf7f063234416abf3f24766481cb89",
      "value": 4
     }
    },
    "b264b220d9c444bd9da46a7e6c8fd5ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8399339998564d21ba5db6f0514c02c6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7701ec898fd443f1b35b187aea3651e9",
      "value": " 2/2 [00:00&lt;00:00,  6.46ba/s]"
     }
    },
    "b4d3f284fc4c4061b58d43a738f9bc78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7d520bdde27742abb42803843721d101",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_68c4c867096d41a78740fdee30edcadb",
      "value": "Downloading: 100%"
     }
    },
    "b6be028de2ae4ff691538eedb33793af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b4d3f284fc4c4061b58d43a738f9bc78",
       "IPY_MODEL_8a11c8fed672470b8335dc575a4a220e",
       "IPY_MODEL_08286a6371584b4186014ecb5d5f164d"
      ],
      "layout": "IPY_MODEL_a3e2c73d393d4e58a371f3da3dd80e6d"
     }
    },
    "b8722dc10d4447fe9630cbf169260cc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbe3a471efb04ea8b5aabc4be819d585": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a61d366d91c34697a55f62b754e1f3a5",
       "IPY_MODEL_1bea379404df429b9852b62a938661ae",
       "IPY_MODEL_c801e1727de44b67aa7cb1c3d970e1fe"
      ],
      "layout": "IPY_MODEL_59418bbeb20547e5b5e1a5728262c757"
     }
    },
    "be4affe852b348de8fe1362582b08da9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99e94791043b4499b06601f7524f9b14",
       "IPY_MODEL_26bc2038bed74279813ab5af09a2724c",
       "IPY_MODEL_9bc6e14b912249e3b7d02f31bcc74667"
      ],
      "layout": "IPY_MODEL_c6c100b71f26405fb960598feb5eee03"
     }
    },
    "c593f2e45e244637821cc5721788bf2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c92a19dfa84142af91522bc22f21fca6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_990482eebca2424bb5ecbd114007e02c",
      "value": " 570/570 [00:00&lt;00:00, 13.1kB/s]"
     }
    },
    "c6c100b71f26405fb960598feb5eee03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7bd52ef524c4d279dfcaa3aebe4a2c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c801e1727de44b67aa7cb1c3d970e1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4552ee8ca6bd4a0b956651cc23f4ff3c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7c875ecd9cb54405a6c45969bcb4b4c6",
      "value": " 1/1 [00:00&lt;00:00,  7.22ba/s]"
     }
    },
    "c815bfd265f4480298c39c76b9eaf770": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15aae23369674f82888ed9fbd99739f2",
      "max": 501200538,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3e7fbd1c0e534cb8abca18d1edfc9277",
      "value": 501200538
     }
    },
    "c92a19dfa84142af91522bc22f21fca6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cae29b9c6d45412fab70977fcd0f3234": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbb3e9bf5d07406d9768a98a6f0b5b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cbbb20b5d01a4450bfb8dfbf8048d64f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cced5f1cccc2400a8fbfd7a6eaedc666": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cedca6e55b84443e82f3d01471d61048": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0a2918e9772475cac51124b3b83fcaf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_4fae966b76844c869cdea1e53891e26f",
      "value": "100%"
     }
    },
    "cf9597523c024514b9b3e66bc77e3fa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1ecc3d380fc4758b03190b23686a2f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d25cca081db3469b80163d6707f5a37d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_360d6eb0e41543dba6d457912e32a77d",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_788badadfd834f61926a39a43ef1d517",
      "value": 3
     }
    },
    "d314c0bb87e04893b96de0e18766d3ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5c8ff9e3bd849059fa7b30eab5fc940": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5d015711ae04d2f801577fc50af6c15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d6426fea2eda41dd9a31cb3f35b0877e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a51b461c062f4636bfa4b48823d0709b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f651eecbb6d44c24820cf6fe5ab92e7b",
      "value": "Downloading: 100%"
     }
    },
    "d731cfb34124448bbd8baab3d27b75db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9a0852554284d36b6b121f579b06b41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1f08cf954ae4aea818c90d893486c77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f01fdef82047471e8c1b780cae5379cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f237ed04039945e9aa224d1b9d04e1b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f44d2beebfe44186b0ac8016e89e4b49": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f485d2b19ffa4585a1da20986f28af29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f651eecbb6d44c24820cf6fe5ab92e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f8a0053903c64e75ac25eab5b24d5871": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8abc3e44ae3428885aafbea2b37384c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54c0ad5ab737433190c4a824be128a48",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f237ed04039945e9aa224d1b9d04e1b5",
      "value": " 3/3 [00:00&lt;00:00, 52.79it/s]"
     }
    },
    "fa35b3acd9ce4cb098fcd69bb405db00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbdb7c7250d846b2880005a9012c484b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}