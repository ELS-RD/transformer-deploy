{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from time import time\n",
    "from typing import Callable, Dict, Set\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from onnx import ModelProto\n",
    "from tensorrt import ICudaEngine\n",
    "from tensorrt.tensorrt import Logger, Runtime\n",
    "from torch.nn import Linear\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, PretrainedConfig, T5ForConditionalGeneration, TensorType\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Stack\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx\n",
    "from transformer_deploy.backends.trt_utils import (\n",
    "    TensorRTShape,\n",
    "    add_output_nodes,\n",
    "    build_engine,\n",
    "    get_adjency_dict,\n",
    "    get_fix_fp16_network_func,\n",
    "    get_list_fp32_nodes,\n",
    "    load_engine,\n",
    "    save_engine,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "input_ids: torch.Tensor = tokenizer(\"Studies show that\", return_tensors=TensorType.PYTORCH).input_ids\n",
    "input_ids = input_ids.to(\"cuda\")\n",
    "model: T5ForConditionalGeneration = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model = model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "out_full: Seq2SeqLMOutput = model(input_ids=input_ids, decoder_input_ids=input_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:529: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "class ExportT5(torch.nn.Module):\n",
    "    def __init__(self, decoder: T5Stack, lm_head: Linear):\n",
    "        super(ExportT5, self).__init__()\n",
    "        self.decoder = decoder\n",
    "        self.lm_head = lm_head\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor):\n",
    "        out_dec = self.decoder.forward(input_ids=input_ids, encoder_hidden_states=encoder_hidden_states)\n",
    "        # Rescale output before projecting on vocab\n",
    "        out_dec = out_dec[\"last_hidden_state\"] * (model.model_dim**-0.5)\n",
    "        out_lm = self.lm_head(out_dec)\n",
    "        return out_lm\n",
    "\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model.encoder,\n",
    "    output_path=\"test-enc.onnx\",\n",
    "    inputs_pytorch={\"input_ids\": input_ids},\n",
    "    var_output_seq=True,\n",
    "    quantization=False,\n",
    ")\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-enc.onnx\", onnx_optim_model_path=\"test-enc-opt.onnx\", architecture=\"bert\", use_cuda=True, fp16=True\n",
    ")\n",
    "\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "enc_onnx_out = inference_onnx_binding(\n",
    "    model_onnx=enc_onnx,\n",
    "    inputs={\"input_ids\": input_ids},\n",
    "    device=input_ids.device.type,\n",
    "    output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    ")[\"output\"]\n",
    "assert np.allclose(enc_onnx_out.detach().cpu().numpy(), out_enc.last_hidden_state.detach().cpu().numpy(), atol=1e-2)\n",
    "\n",
    "model_to_export = ExportT5(decoder=model.decoder, lm_head=model.lm_head).eval()\n",
    "out_model_export: torch.Tensor = model_to_export(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state)\n",
    "assert np.allclose(out_model_export.detach().cpu().numpy(), out_full.logits.detach().cpu().numpy(), atol=1e-5)\n",
    "\n",
    "inputs_onnx = {\"input_ids\": input_ids, \"encoder_hidden_states\": out_enc.last_hidden_state}\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model_to_export,\n",
    "    output_path=\"test-dec.onnx\",\n",
    "    inputs_pytorch=inputs_onnx,\n",
    "    var_output_seq=False,\n",
    "    quantization=False,\n",
    "    fix_output_dim_size=False,  # specific to decoder part\n",
    ")\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-dec.onnx\",\n",
    "    onnx_optim_model_path=\"test-dec-opt.onnx\",\n",
    "    architecture=\"bert\",\n",
    "    use_cuda=True,\n",
    "    fp16=True,\n",
    "    num_attention_heads=model.config.num_heads,\n",
    "    hidden_size=model.config.d_model,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Studien studies show that</s>\n",
      "<pad> Studien studies show that</s>\n",
      "11.376280784606934\n",
      "15.102983236312866\n"
     ]
    }
   ],
   "source": [
    "model = model.to(\"cuda\")\n",
    "enc_onnx = create_model_for_provider(\"test-enc-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "dec_onnx = create_model_for_provider(\"test-dec-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def decoder_pytorch_inference(input_ids: torch.Tensor, last_hidden_state: torch.Tensor):\n",
    "    out_dec = model.decoder(input_ids=input_ids, encoder_hidden_states=last_hidden_state)[\"last_hidden_state\"]\n",
    "    # Rescale output before projecting on vocab\n",
    "    out_dec = out_dec * (model.model_dim**-0.5)\n",
    "    out_lm = model.lm_head(out_dec)\n",
    "    return out_lm\n",
    "\n",
    "\n",
    "def decoder_onnx_inference(input_ids: torch.Tensor, last_hidden_state: torch.Tensor):\n",
    "    result_dict = inference_onnx_binding(\n",
    "        model_onnx=dec_onnx,\n",
    "        inputs={\"input_ids\": input_ids, \"encoder_hidden_states\": last_hidden_state},\n",
    "        device=input_ids.device.type,\n",
    "        output_shape=tuple(input_ids.shape) + (int(model.config.vocab_size),),\n",
    "    )\n",
    "    return result_dict[\"output\"]\n",
    "\n",
    "\n",
    "def decoder_onnx_standard_inference(input_ids: torch.Tensor, last_hidden_state: torch.Tensor):\n",
    "    result_list = dec_onnx.run(\n",
    "        None, {\"input_ids\": input_ids.type(torch.int32).numpy(), \"encoder_hidden_states\": last_hidden_state.numpy()}\n",
    "    )\n",
    "    return torch.from_numpy(result_list[0])\n",
    "\n",
    "\n",
    "dec_onnx_out = decoder_onnx_inference(input_ids=input_ids, last_hidden_state=out_enc.last_hidden_state)\n",
    "assert np.allclose(dec_onnx_out.detach().cpu().numpy(), out_full.logits.detach().cpu().numpy(), atol=1e-1)\n",
    "\n",
    "\n",
    "def encoder_onnx_inference(input_ids: torch.Tensor, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    result = inference_onnx_binding(\n",
    "        model_onnx=enc_onnx,  # noqa: F821\n",
    "        inputs={\"input_ids\": input_ids},\n",
    "        output_shape=tuple(input_ids.shape) + (int(model.encoder.config.d_model),),\n",
    "        device=input_ids.device.type,\n",
    "    )\n",
    "    return BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=result[\"output\"])\n",
    "\n",
    "\n",
    "def encoder_pytorch_inference(input_ids, **_) -> BaseModelOutputWithPastAndCrossAttentions:\n",
    "    return model.encoder(input_ids=input_ids)\n",
    "\n",
    "\n",
    "# https://github.com/NVIDIA/TensorRT/blob/main/demo/HuggingFace/T5/export.py\n",
    "class ExtT5(torch.nn.Module, GenerationMixin):\n",
    "    def __init__(self, config: PretrainedConfig, device: torch.device, encoder_func: Callable, decoder_func: Callable):\n",
    "        super(ExtT5, self).__init__()\n",
    "        self.main_input_name = \"input_ids\"  # https://github.com/huggingface/transformers/pull/14803\n",
    "        self.config: PretrainedConfig = config\n",
    "        self.device: torch.device = device\n",
    "\n",
    "        self.encoder_func = encoder_func\n",
    "        self.decoder_func = decoder_func\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.encoder_func\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.decoder_func\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {\n",
    "            self.main_input_name: input_ids,\n",
    "            \"encoder_hidden_states\": kwargs[\"encoder_outputs\"][\"last_hidden_state\"],\n",
    "        }\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, encoder_hidden_states: torch.Tensor, **_):\n",
    "        dec_output = self.get_decoder()(input_ids=input_ids, last_hidden_state=encoder_hidden_states)\n",
    "        return Seq2SeqLMOutput(logits=dec_output)\n",
    "\n",
    "\n",
    "model_gen = (\n",
    "    ExtT5(\n",
    "        config=model.config,\n",
    "        device=model.device,\n",
    "        encoder_func=encoder_onnx_inference,  # encoder_pytorch_inference\n",
    "        decoder_func=decoder_onnx_inference,  # decoder_pytorch_inference\n",
    "    )\n",
    "    .cuda()\n",
    "    .eval()\n",
    ")\n",
    "\n",
    "# model = model.eval()\n",
    "with torch.inference_mode():\n",
    "    out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "    a = model_gen(input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state).logits\n",
    "    b = model(input_ids=input_ids, decoder_input_ids=input_ids).logits\n",
    "    assert np.allclose(a.detach().cpu().numpy(), b.detach().cpu().numpy(), atol=1e-1)\n",
    "\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model_gen.generate(inputs=input_ids, max_length=20, num_beams=7, no_repeat_ngram_size=2)[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            model.generate(inputs=input_ids, max_length=20, num_beams=7, no_repeat_ngram_size=2)[0],\n",
    "            skip_special_tokens=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "start = time()\n",
    "for _ in range(3):\n",
    "    model_gen.generate(inputs=input_ids, max_length=500, num_beams=5, no_repeat_ngram_size=2, min_length=500)\n",
    "print(time() - start)\n",
    "\n",
    "model.config.use_cache = True\n",
    "with torch.inference_mode():\n",
    "    start = time()\n",
    "    for _ in range(3):\n",
    "        model.generate(inputs=input_ids, max_length=500, num_beams=5, no_repeat_ngram_size=2, min_length=500)\n",
    "    print(time() - start)\n",
    "\n",
    "model = model.cpu()\n",
    "del enc_onnx\n",
    "del dec_onnx\n",
    "\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "trt_model_name = \"trt-t5-dec.plan\"\n",
    "\n",
    "# create only of does not exist because it's slow to run...\n",
    "\n",
    "# 768 for base model, 512 for small, make it dependent from the Pytorch model configuration\n",
    "input_id_shape = TensorRTShape(min_shape=[5, 1], optimal_shape=[5, 500], max_shape=[5, 500], input_name=\"input_ids\")\n",
    "encoder_hidden_states_shape = TensorRTShape(\n",
    "    min_shape=[5, 1, 512], optimal_shape=[5, 500 // 2, 512], max_shape=[5, 500, 512], input_name=\"encoder_hidden_states\"\n",
    ")\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "model_onnx: ModelProto = onnx.load(\"test-dec.onnx\")\n",
    "model_onnx_all_nodes = add_output_nodes(model=model_onnx)\n",
    "onnx_graph: Dict[str, Set[str]] = get_adjency_dict(model=model_onnx)\n",
    "ort_model_all_nodes = create_model_for_provider(model_onnx_all_nodes.SerializeToString(), \"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "# use info from tokenizer size and max shape provided through the command line\n",
    "def get_random_input():\n",
    "    input = torch.randint(high=tokenizer.vocab_size, size=(5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "    hidden_state = model.encoder(input_ids=input).last_hidden_state.detach().cpu().numpy()\n",
    "    return {\"input_ids\": input.detach().cpu().numpy(), \"encoder_hidden_states\": hidden_state}\n",
    "\n",
    "\n",
    "keep_fp32 = get_list_fp32_nodes(\n",
    "    onnx_graph=onnx_graph, model=ort_model_all_nodes, get_input=get_random_input, nb_try=200\n",
    ")\n",
    "model = model.cpu()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-48.0837, -14.2924, -20.4697,  ..., -67.6401, -67.8845, -67.8307],\n",
      "        [-54.2150, -14.2975, -21.5249,  ..., -66.5756, -66.9168, -66.7166],\n",
      "        [-35.6373,  -5.6440, -15.6157,  ..., -49.5720, -49.8802, -49.7263],\n",
      "        [-31.3241,  -3.7928, -11.2605,  ..., -43.0666, -43.3176, -43.1935]],\n",
      "       device='cuda:0')\n",
      "1.600128173828125\n",
      "0.7323088645935059\n",
      "1.230060338973999\n"
     ]
    }
   ],
   "source": [
    "engine: ICudaEngine = build_engine(\n",
    "    runtime=runtime,\n",
    "    onnx_file_path=\"test-dec.onnx\",\n",
    "    logger=trt_logger,\n",
    "    workspace_size=20000 * 1024**2,\n",
    "    fp16=True,\n",
    "    int8=False,\n",
    "    input_shapes=[input_id_shape, encoder_hidden_states_shape],\n",
    "    fp16_fix=get_fix_fp16_network_func(keep_fp32=keep_fp32),\n",
    ")\n",
    "save_engine(engine, trt_model_name)\n",
    "\n",
    "tensorrt_model = load_engine(runtime=runtime, engine_file_path=trt_model_name)\n",
    "a = tensorrt_model(\n",
    "    {\n",
    "        \"input_ids\": input_ids.type(torch.int32).repeat((5, 1)),\n",
    "        \"encoder_hidden_states\": out_enc.last_hidden_state.repeat((5, 1, 1)),\n",
    "    }\n",
    ")\n",
    "print(a[0])\n",
    "\n",
    "benchmark_input = torch.ones((5, 500), dtype=torch.int32, device=\"cuda\")\n",
    "benchmark_enc_output = out_enc.last_hidden_state.repeat((5, 1, 1))\n",
    "for _ in range(10):\n",
    "    tensorrt_model(\n",
    "        {\n",
    "            \"input_ids\": benchmark_input,\n",
    "            \"encoder_hidden_states\": benchmark_enc_output,\n",
    "        }\n",
    "    )\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    tensorrt_model(\n",
    "        {\n",
    "            \"input_ids\": benchmark_input,\n",
    "            \"encoder_hidden_states\": benchmark_enc_output,\n",
    "        }\n",
    "    )\n",
    "print(time() - start)\n",
    "\n",
    "dec_onnx = create_model_for_provider(\"test-dec-opt.onnx\", \"CUDAExecutionProvider\")\n",
    "dec_onnx_out = decoder_onnx_inference(input_ids=input_ids, last_hidden_state=out_enc.last_hidden_state)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    decoder_onnx_inference(input_ids=benchmark_input, last_hidden_state=benchmark_enc_output)\n",
    "print(time() - start)\n",
    "\n",
    "model.cuda()\n",
    "for _ in range(10):\n",
    "    model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "start = time()\n",
    "for _ in range(100):\n",
    "    model.decoder(input_ids=benchmark_input, encoder_hidden_states=benchmark_enc_output)\n",
    "print(time() - start)\n",
    "\n",
    "# TensorRT, ONNX Runtime, Pytorch\n",
    "\n",
    "# sequence 500\n",
    "# 0.8640644550323486\n",
    "# 0.6695075035095215\n",
    "# 1.1308434009552002\n",
    "\n",
    "# sequence 250\n",
    "# 0.9177014827728271\n",
    "# 0.6861860752105713\n",
    "# 1.1923034191131592"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.7558e-02,  1.5035e-01, -1.0642e-01,  1.3436e-01, -2.7034e-03,\n          1.0121e-01, -3.7072e-02, -1.1961e-01,  4.2827e-01, -6.8828e-02,\n          4.7860e-03,  1.0388e-01, -1.9117e-01, -9.1395e-02,  8.0236e-01,\n         -2.9004e-02, -8.3236e-03, -4.1821e-02,  5.1209e-02, -8.1611e-02,\n          5.2725e-02,  7.5980e-02, -8.5656e-02,  2.2240e+00,  4.7778e-02,\n         -7.0944e-02, -3.6483e-02,  2.8398e-01, -3.3781e-02, -1.3103e-01,\n          4.6392e-02, -4.0270e-02, -5.6167e-02, -1.5688e-02,  4.1200e-01,\n         -3.9407e-02, -1.1113e-01, -7.7969e-02, -1.3417e-01,  1.9564e-01,\n         -4.9984e-02, -5.9329e-02,  2.2479e-01,  7.3668e-02, -5.7873e-02,\n          1.4900e-02, -6.9493e-01,  1.7280e-02,  1.0609e-01,  9.2358e-02,\n         -1.3377e-01, -5.1072e-02, -8.8388e-02,  1.4478e-01, -1.1261e-01,\n          4.1928e-03,  5.3930e-02, -7.4811e-02,  5.8460e-02,  2.1837e-02,\n          8.2831e-02,  9.2348e-02, -2.5649e-02, -8.6028e-02, -1.2986e-02,\n         -4.3584e-02, -4.8884e-02, -3.3386e-02,  5.5349e-01,  1.8710e-01,\n         -3.0446e-02,  5.1245e-02,  5.5034e-02, -5.5080e-02, -1.7116e-03,\n          9.5299e-02, -4.5908e-02, -1.2584e-02,  8.4189e-02,  2.0390e-01,\n         -2.0322e-01,  8.3806e-02,  5.3210e-02, -8.4752e-02, -4.7789e-02,\n         -1.1201e-01, -2.8879e-02, -3.3881e-02, -1.7329e-01,  6.1102e-01,\n         -1.2889e-02, -9.4040e-02,  8.2499e-02, -7.6344e-02, -8.5806e-02,\n          2.5688e-02, -1.5681e-02, -1.1451e-02,  6.4907e-03, -2.3519e-02,\n          3.4917e-02, -5.8727e-02,  1.3254e-02,  1.5825e-01,  4.7590e-02,\n          1.0409e-01,  6.9138e-02,  3.9837e-02, -3.2700e-02, -9.1413e-02,\n         -1.0145e-01,  1.4211e-03, -4.9003e-03,  7.6215e-02, -7.3899e-02,\n         -8.5377e-02,  2.2540e-02, -4.9561e-02,  3.2881e-02, -2.2287e-02,\n         -6.8049e-02, -2.0162e-02,  1.6643e-02,  2.7118e-04, -2.5598e-02,\n          4.4508e-01, -9.3383e-02,  3.1402e-02, -1.0722e-01, -7.8249e-02,\n          2.4561e-02,  1.1590e-02, -6.1977e-02, -8.2639e-03, -4.8054e-02,\n          2.7819e-02, -5.3643e-02,  6.1974e-01, -8.0894e-02,  1.1800e-02,\n         -3.8681e-02, -9.5972e-02,  8.1281e-02, -6.7938e-02,  5.6997e-02,\n          6.1085e-02,  9.6156e-02,  2.4264e-02,  1.5556e-02, -3.0101e+00,\n          1.2210e-01, -1.4733e-02,  8.8266e-02, -1.0615e-02,  9.5372e-02,\n          3.4785e-02, -4.5652e-02,  3.7928e-02,  7.2694e-02, -6.9918e+00,\n         -9.0426e-02,  5.7168e-02,  9.9028e-02,  4.6014e-02, -1.1359e-01,\n         -1.3036e-01,  1.4493e-01,  3.3658e-01, -5.2206e-02,  4.0246e-02,\n          2.0170e-02, -5.1977e-02,  4.4187e-02, -7.7361e-02,  1.9363e-02,\n          6.2262e-02, -6.1391e-02,  1.8870e-03, -3.3080e-02,  2.0343e-03,\n          3.4712e-02,  4.9128e-02,  2.6168e-03,  6.7538e-02, -3.0646e-01,\n         -1.0470e-01,  8.9288e-02,  1.2655e-01, -9.8930e-02, -2.5814e-02,\n         -1.1663e+00, -2.1569e-01,  3.9756e-03, -5.4395e-02,  4.2298e-03,\n         -1.4125e-02, -7.4977e-03, -6.1164e-02,  1.7089e-02, -6.3246e-02,\n         -3.2219e-02, -4.2473e-02, -3.9572e-02, -3.2147e-02, -7.0602e-02,\n          1.7271e-03,  3.1442e-02, -5.3144e-02, -4.6806e-02, -7.4991e-02,\n          1.7826e-02,  3.7886e-02, -5.9538e-02, -1.5893e-01, -6.2725e-02,\n         -2.4954e-02, -1.1495e-02, -5.6767e-02,  1.7637e-02, -5.2201e-02,\n         -3.3890e-02,  1.0832e-01, -3.6066e-02, -1.4835e-01,  7.2391e-03,\n          1.9350e-02, -2.5393e-01,  4.7064e-02, -1.2885e-01, -5.2605e-02,\n          3.4596e-02, -3.2223e-02,  2.3422e-02,  7.2860e-02, -2.5571e-01,\n         -6.5474e-02,  1.2896e-01,  8.3955e-02, -9.2654e-02,  7.1887e-02,\n          2.8062e-01,  4.2431e-02,  2.2218e-02,  4.0215e-02, -1.6136e-01,\n          9.2229e-01,  8.5288e-02,  4.6689e-04, -6.1003e-01,  4.4943e-02,\n          5.1845e-02,  1.8363e-02,  5.8040e-02, -6.6558e-02, -1.2740e-02,\n         -4.8374e-02, -7.0576e-03, -7.4274e-02, -2.5041e-01,  9.4789e-02,\n         -2.0065e-02,  5.5635e-03,  1.0549e-01,  6.3857e-01, -1.2988e-02,\n          6.1025e-02, -9.9692e-02,  8.6344e-02,  5.2507e-03, -8.4472e-02,\n          6.4332e-02, -3.9105e-02,  2.0621e-01,  5.0989e-01,  6.3143e-02,\n         -2.0267e-02, -4.0145e-02,  6.0277e-03, -5.9656e-02, -1.2237e-01,\n         -4.2363e-01,  3.2023e-02, -3.3462e-02, -5.3224e-03,  6.4035e-02,\n         -2.4841e-02,  7.4523e-02,  4.0785e-02,  5.0653e-02,  1.5206e-02,\n         -2.3787e-01,  4.2905e-04,  5.5600e-02,  1.7442e-01, -2.0597e+00,\n          2.2187e-01, -8.1337e-02, -3.3713e-02,  1.3336e-02,  3.6996e-02,\n          7.0871e-02,  1.7253e-02,  6.7578e-02,  1.5223e-02, -4.0918e-02,\n          4.6613e-02, -1.5509e-01, -3.2361e-01,  4.6950e-01, -9.7088e-02,\n         -2.0382e-02,  1.8947e-01, -7.4724e-01,  1.4031e-01, -9.4198e-02,\n         -5.9677e-02,  3.4392e-02,  4.5680e-02, -2.0871e-01, -6.9111e-02,\n         -1.0838e-01, -1.4020e-01, -9.6648e-02, -2.4828e-01, -2.9278e+00,\n          4.8934e-02,  7.4296e-02,  6.2194e-02,  1.5646e-02,  8.0211e-02,\n          8.4065e-02, -8.2835e-02, -1.4783e-01,  6.2116e-02,  6.1863e-03,\n         -8.3127e-02, -1.6525e-01, -4.2172e-02, -8.7703e-02, -1.0587e-01,\n          6.6061e-02, -1.2229e-01, -6.8209e-03,  1.3168e-01, -1.0773e-02,\n         -1.7031e-01,  2.6722e-02, -3.8696e-02,  7.6689e-02, -5.8725e-02,\n          1.9595e-01, -1.2188e-02,  5.9784e-02,  5.0740e-02,  2.5466e-02,\n         -2.7911e-01, -1.2255e-01,  2.8395e-02,  3.9120e-02,  1.3815e-01,\n         -9.5946e-02,  1.9945e-02,  6.0094e-02, -1.4042e-01, -5.9581e-02,\n         -1.0952e-01, -7.3304e-02, -3.3756e-01, -1.6652e-03,  8.1181e-04,\n          6.8361e-02,  1.6927e-01, -6.6210e-02,  1.1466e-02, -3.0490e-01,\n         -1.2575e-01,  8.7528e-03, -8.2843e-02,  3.2838e-03, -2.1481e-01,\n          4.4152e-02, -1.1502e+00, -2.0167e-01, -2.2349e-02,  1.5185e-01,\n          5.1614e-02,  1.1127e-01,  2.4280e-02, -2.7464e-01, -8.6794e-03,\n          1.2796e-02,  2.3091e-02, -1.4170e-01,  1.2471e-02, -6.3778e-02,\n         -1.0977e-01,  4.8351e-02,  7.4095e-02,  5.8914e-02,  7.7498e-02,\n          2.7251e-01,  1.8195e-01,  7.3700e-02, -1.1701e-01, -5.5508e-02,\n          2.0029e-02, -5.6784e-02, -3.5544e-02, -1.6008e-02,  4.2423e-02,\n          2.4862e-01, -1.2843e-01, -9.1526e-02, -9.2777e-02, -1.5552e-02,\n          2.1962e-02,  5.6796e-02,  3.9258e-01,  1.6204e-02,  2.8135e-02,\n          5.0101e-02, -1.0456e-01, -8.2916e-02,  2.7435e-02, -1.5171e-01,\n         -7.9315e-02,  3.0147e-02, -1.7285e-02, -1.2829e-01,  2.7650e-02,\n          5.3211e-02, -5.4338e-02,  1.0162e-01, -1.9675e-02, -9.0279e-02,\n         -4.6922e-02,  1.6096e-03,  1.9443e-02,  3.8715e-02, -4.2711e-02,\n         -7.7991e-02,  4.1426e-02,  8.2201e-03,  1.5081e-01,  3.3575e-03,\n         -2.2131e-02,  3.8794e-03, -1.0341e-01, -1.3226e-01,  9.7978e-04,\n          8.5674e-02, -6.0801e-02, -5.3318e-02, -1.8344e-01,  4.9798e-02,\n          2.9513e-02,  5.8625e-02, -1.1644e-02,  5.6686e-02, -2.3213e-02,\n          5.9430e-02,  3.0600e-02, -7.0081e-02, -9.4119e-02,  1.2767e-01,\n         -9.1701e-02,  5.3907e-02,  1.4844e-01,  5.5191e-02, -4.8491e-02,\n         -7.4864e-02, -4.8856e-03, -1.1108e-01,  2.9740e-02, -1.3206e-01,\n         -1.0341e-01, -5.5583e-02, -3.4414e-01, -9.1679e-03, -3.5313e-02,\n         -1.3231e-01,  4.7432e-02,  8.9266e-02, -1.2552e-01,  5.9633e-02,\n         -5.4128e-02, -9.0885e-03, -1.9906e-01, -4.2343e-03, -8.0676e-03,\n          1.4961e-03, -7.9238e-02,  6.1884e-02,  4.9478e-02, -7.5517e-02,\n          1.7999e-02,  8.1206e-02, -1.8421e-01, -6.5564e-02, -6.8393e-02,\n         -2.1154e-01, -7.4016e-01, -3.8952e-02,  1.1902e-01,  3.7813e-02,\n         -1.3577e-01, -1.5776e-01, -1.4139e-02,  4.6991e-02,  6.5095e-02,\n          3.4050e-04,  5.8639e-02]], device='cuda:0', grad_fn=<SliceBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_enc: BaseModelOutputWithPastAndCrossAttentions = model.encoder(input_ids=input_ids)\n",
    "model.decoder(\n",
    "    input_ids=input_ids, encoder_hidden_states=out_enc.last_hidden_state, past_key_values=None\n",
    ").last_hidden_state[:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 2.7559e-02,  1.5035e-01, -1.0642e-01,  1.3436e-01, -2.7034e-03,\n          1.0121e-01, -3.7072e-02, -1.1961e-01,  4.2827e-01, -6.8828e-02,\n          4.7861e-03,  1.0388e-01, -1.9117e-01, -9.1395e-02,  8.0236e-01,\n         -2.9004e-02, -8.3238e-03, -4.1821e-02,  5.1210e-02, -8.1611e-02,\n          5.2724e-02,  7.5980e-02, -8.5656e-02,  2.2240e+00,  4.7778e-02,\n         -7.0944e-02, -3.6484e-02,  2.8398e-01, -3.3781e-02, -1.3103e-01,\n          4.6392e-02, -4.0270e-02, -5.6167e-02, -1.5688e-02,  4.1200e-01,\n         -3.9406e-02, -1.1113e-01, -7.7969e-02, -1.3417e-01,  1.9564e-01,\n         -4.9984e-02, -5.9328e-02,  2.2479e-01,  7.3668e-02, -5.7874e-02,\n          1.4900e-02, -6.9493e-01,  1.7280e-02,  1.0609e-01,  9.2357e-02,\n         -1.3377e-01, -5.1072e-02, -8.8387e-02,  1.4478e-01, -1.1260e-01,\n          4.1932e-03,  5.3930e-02, -7.4811e-02,  5.8460e-02,  2.1838e-02,\n          8.2830e-02,  9.2348e-02, -2.5650e-02, -8.6029e-02, -1.2986e-02,\n         -4.3583e-02, -4.8884e-02, -3.3387e-02,  5.5351e-01,  1.8710e-01,\n         -3.0447e-02,  5.1245e-02,  5.5034e-02, -5.5080e-02, -1.7118e-03,\n          9.5299e-02, -4.5908e-02, -1.2584e-02,  8.4189e-02,  2.0390e-01,\n         -2.0322e-01,  8.3806e-02,  5.3210e-02, -8.4752e-02, -4.7789e-02,\n         -1.1201e-01, -2.8879e-02, -3.3881e-02, -1.7330e-01,  6.1102e-01,\n         -1.2889e-02, -9.4040e-02,  8.2499e-02, -7.6344e-02, -8.5807e-02,\n          2.5688e-02, -1.5682e-02, -1.1451e-02,  6.4908e-03, -2.3518e-02,\n          3.4917e-02, -5.8727e-02,  1.3254e-02,  1.5825e-01,  4.7590e-02,\n          1.0409e-01,  6.9139e-02,  3.9837e-02, -3.2700e-02, -9.1413e-02,\n         -1.0145e-01,  1.4212e-03, -4.8999e-03,  7.6215e-02, -7.3899e-02,\n         -8.5377e-02,  2.2540e-02, -4.9561e-02,  3.2881e-02, -2.2287e-02,\n         -6.8049e-02, -2.0161e-02,  1.6643e-02,  2.7154e-04, -2.5598e-02,\n          4.4508e-01, -9.3383e-02,  3.1402e-02, -1.0722e-01, -7.8250e-02,\n          2.4561e-02,  1.1590e-02, -6.1978e-02, -8.2641e-03, -4.8054e-02,\n          2.7818e-02, -5.3643e-02,  6.1974e-01, -8.0894e-02,  1.1800e-02,\n         -3.8681e-02, -9.5972e-02,  8.1281e-02, -6.7938e-02,  5.6997e-02,\n          6.1085e-02,  9.6156e-02,  2.4264e-02,  1.5555e-02, -3.0102e+00,\n          1.2210e-01, -1.4733e-02,  8.8266e-02, -1.0616e-02,  9.5372e-02,\n          3.4785e-02, -4.5652e-02,  3.7928e-02,  7.2694e-02, -6.9918e+00,\n         -9.0425e-02,  5.7168e-02,  9.9027e-02,  4.6014e-02, -1.1359e-01,\n         -1.3036e-01,  1.4493e-01,  3.3658e-01, -5.2206e-02,  4.0246e-02,\n          2.0170e-02, -5.1978e-02,  4.4187e-02, -7.7361e-02,  1.9363e-02,\n          6.2263e-02, -6.1392e-02,  1.8873e-03, -3.3080e-02,  2.0337e-03,\n          3.4712e-02,  4.9128e-02,  2.6168e-03,  6.7538e-02, -3.0646e-01,\n         -1.0470e-01,  8.9288e-02,  1.2655e-01, -9.8930e-02, -2.5814e-02,\n         -1.1664e+00, -2.1569e-01,  3.9758e-03, -5.4394e-02,  4.2295e-03,\n         -1.4125e-02, -7.4978e-03, -6.1164e-02,  1.7089e-02, -6.3246e-02,\n         -3.2219e-02, -4.2474e-02, -3.9572e-02, -3.2148e-02, -7.0602e-02,\n          1.7266e-03,  3.1441e-02, -5.3143e-02, -4.6806e-02, -7.4992e-02,\n          1.7826e-02,  3.7886e-02, -5.9538e-02, -1.5893e-01, -6.2725e-02,\n         -2.4954e-02, -1.1496e-02, -5.6767e-02,  1.7637e-02, -5.2200e-02,\n         -3.3890e-02,  1.0832e-01, -3.6066e-02, -1.4835e-01,  7.2392e-03,\n          1.9350e-02, -2.5393e-01,  4.7064e-02, -1.2885e-01, -5.2606e-02,\n          3.4595e-02, -3.2223e-02,  2.3422e-02,  7.2860e-02, -2.5571e-01,\n         -6.5474e-02,  1.2896e-01,  8.3955e-02, -9.2654e-02,  7.1888e-02,\n          2.8062e-01,  4.2430e-02,  2.2217e-02,  4.0215e-02, -1.6136e-01,\n          9.2231e-01,  8.5289e-02,  4.6700e-04, -6.0995e-01,  4.4942e-02,\n          5.1844e-02,  1.8363e-02,  5.8039e-02, -6.6558e-02, -1.2741e-02,\n         -4.8374e-02, -7.0576e-03, -7.4275e-02, -2.5042e-01,  9.4788e-02,\n         -2.0065e-02,  5.5633e-03,  1.0549e-01,  6.3857e-01, -1.2988e-02,\n          6.1024e-02, -9.9692e-02,  8.6343e-02,  5.2502e-03, -8.4473e-02,\n          6.4332e-02, -3.9104e-02,  2.0621e-01,  5.0988e-01,  6.3144e-02,\n         -2.0267e-02, -4.0145e-02,  6.0277e-03, -5.9656e-02, -1.2237e-01,\n         -4.2363e-01,  3.2023e-02, -3.3462e-02, -5.3217e-03,  6.4035e-02,\n         -2.4842e-02,  7.4523e-02,  4.0784e-02,  5.0653e-02,  1.5206e-02,\n         -2.3787e-01,  4.2893e-04,  5.5599e-02,  1.7442e-01, -2.0597e+00,\n          2.2187e-01, -8.1338e-02, -3.3712e-02,  1.3336e-02,  3.6996e-02,\n          7.0870e-02,  1.7253e-02,  6.7578e-02,  1.5224e-02, -4.0918e-02,\n          4.6613e-02, -1.5509e-01, -3.2361e-01,  4.6951e-01, -9.7088e-02,\n         -2.0382e-02,  1.8947e-01, -7.4723e-01,  1.4031e-01, -9.4199e-02,\n         -5.9677e-02,  3.4392e-02,  4.5680e-02, -2.0871e-01, -6.9111e-02,\n         -1.0838e-01, -1.4020e-01, -9.6648e-02, -2.4828e-01, -2.9278e+00,\n          4.8934e-02,  7.4296e-02,  6.2194e-02,  1.5646e-02,  8.0212e-02,\n          8.4066e-02, -8.2836e-02, -1.4783e-01,  6.2116e-02,  6.1861e-03,\n         -8.3127e-02, -1.6525e-01, -4.2172e-02, -8.7703e-02, -1.0587e-01,\n          6.6060e-02, -1.2229e-01, -6.8211e-03,  1.3168e-01, -1.0773e-02,\n         -1.7031e-01,  2.6723e-02, -3.8696e-02,  7.6689e-02, -5.8725e-02,\n          1.9595e-01, -1.2188e-02,  5.9785e-02,  5.0739e-02,  2.5466e-02,\n         -2.7911e-01, -1.2255e-01,  2.8395e-02,  3.9120e-02,  1.3815e-01,\n         -9.5946e-02,  1.9945e-02,  6.0094e-02, -1.4042e-01, -5.9581e-02,\n         -1.0952e-01, -7.3303e-02, -3.3756e-01, -1.6650e-03,  8.1169e-04,\n          6.8362e-02,  1.6927e-01, -6.6210e-02,  1.1466e-02, -3.0490e-01,\n         -1.2575e-01,  8.7527e-03, -8.2844e-02,  3.2840e-03, -2.1481e-01,\n          4.4152e-02, -1.1502e+00, -2.0167e-01, -2.2349e-02,  1.5185e-01,\n          5.1614e-02,  1.1127e-01,  2.4280e-02, -2.7464e-01, -8.6785e-03,\n          1.2796e-02,  2.3091e-02, -1.4170e-01,  1.2471e-02, -6.3778e-02,\n         -1.0977e-01,  4.8350e-02,  7.4094e-02,  5.8915e-02,  7.7498e-02,\n          2.7250e-01,  1.8195e-01,  7.3699e-02, -1.1701e-01, -5.5508e-02,\n          2.0029e-02, -5.6784e-02, -3.5544e-02, -1.6007e-02,  4.2423e-02,\n          2.4862e-01, -1.2843e-01, -9.1527e-02, -9.2774e-02, -1.5552e-02,\n          2.1962e-02,  5.6796e-02,  3.9258e-01,  1.6204e-02,  2.8135e-02,\n          5.0101e-02, -1.0456e-01, -8.2916e-02,  2.7435e-02, -1.5170e-01,\n         -7.9315e-02,  3.0147e-02, -1.7284e-02, -1.2829e-01,  2.7650e-02,\n          5.3210e-02, -5.4339e-02,  1.0162e-01, -1.9675e-02, -9.0280e-02,\n         -4.6922e-02,  1.6096e-03,  1.9443e-02,  3.8715e-02, -4.2711e-02,\n         -7.7992e-02,  4.1426e-02,  8.2198e-03,  1.5082e-01,  3.3577e-03,\n         -2.2130e-02,  3.8794e-03, -1.0341e-01, -1.3226e-01,  9.8023e-04,\n          8.5674e-02, -6.0801e-02, -5.3319e-02, -1.8344e-01,  4.9798e-02,\n          2.9513e-02,  5.8624e-02, -1.1644e-02,  5.6686e-02, -2.3213e-02,\n          5.9430e-02,  3.0600e-02, -7.0081e-02, -9.4119e-02,  1.2767e-01,\n         -9.1701e-02,  5.3907e-02,  1.4844e-01,  5.5190e-02, -4.8491e-02,\n         -7.4864e-02, -4.8855e-03, -1.1108e-01,  2.9740e-02, -1.3206e-01,\n         -1.0341e-01, -5.5583e-02, -3.4414e-01, -9.1684e-03, -3.5313e-02,\n         -1.3231e-01,  4.7432e-02,  8.9266e-02, -1.2552e-01,  5.9633e-02,\n         -5.4128e-02, -9.0889e-03, -1.9906e-01, -4.2343e-03, -8.0673e-03,\n          1.4961e-03, -7.9238e-02,  6.1884e-02,  4.9478e-02, -7.5516e-02,\n          1.7998e-02,  8.1206e-02, -1.8421e-01, -6.5565e-02, -6.8392e-02,\n         -2.1154e-01, -7.4017e-01, -3.8952e-02,  1.1902e-01,  3.7814e-02,\n         -1.3577e-01, -1.5776e-01, -1.4138e-02,  4.6991e-02,  6.5095e-02,\n          3.4050e-04,  5.8638e-02]], device='cuda:0', grad_fn=<SliceBackward0>)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "model.decoder(\n",
    "    input_ids=input_ids[:, -1:],\n",
    "    encoder_hidden_states=out_enc.last_hidden_state,\n",
    "    past_key_values=out_dec_pytorch.past_key_values,\n",
    ").last_hidden_state[:, -1, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# from itertools import chain\n",
    "# from transformers.onnx.features import FeaturesManager\n",
    "#\n",
    "# feature = \"seq2seq-lm-with-past\"\n",
    "# model = FeaturesManager.get_model_from_feature(feature, model_name)\n",
    "# model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "# onnx_config = model_onnx_config(model.config)\n",
    "#\n",
    "# with torch.no_grad():\n",
    "#     model.config.return_dict = True\n",
    "#     model.eval()\n",
    "#\n",
    "#     # Check if we need to override certain configuration item\n",
    "#     if onnx_config.values_override is not None:\n",
    "#         for override_config_key, override_config_value in onnx_config.values_override.items():\n",
    "#             setattr(model.config, override_config_key, override_config_value)\n",
    "#\n",
    "#     # Ensure inputs match\n",
    "#     model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH)\n",
    "#     for k, v in model_inputs.items():\n",
    "#         if isinstance(v, torch.Tensor):\n",
    "#             model_inputs[k] = model_inputs[k].type(torch.int32)\n",
    "#     onnx_outputs = list(onnx_config.outputs.keys())\n",
    "#\n",
    "#     onnx_config.patch_ops()\n",
    "#\n",
    "#     # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "#     torch.onnx.export(\n",
    "#         model,\n",
    "#         (model_inputs,),\n",
    "#         f=\"test-dec-cache.onnx\",\n",
    "#         input_names=list(onnx_config.inputs.keys()),\n",
    "#         output_names=onnx_outputs,\n",
    "#         dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())},\n",
    "#         do_constant_folding=True,\n",
    "#         use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()),\n",
    "#         enable_onnx_checker=True,\n",
    "#         opset_version=13,\n",
    "#     )\n",
    "#\n",
    "#     onnx_config.restore_ops()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/modeling_utils.py:529: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if causal_mask.shape[1] < attention_mask.shape[1]:\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "out_dec_pytorch = model.decoder(input_ids=input_ids[:, :-1], encoder_hidden_states=out_enc.last_hidden_state)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "model_inputs = {\n",
    "    \"input_ids\": input_ids[:, -1:].type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "    \"past_key_values\": out_dec_pytorch.past_key_values,\n",
    "}\n",
    "\n",
    "input_names = [\n",
    "    \"input_ids\",\n",
    "    \"encoder_hidden_states\",\n",
    "    \"past_key_values.0.decoder.key\",\n",
    "    \"past_key_values.0.decoder.value\",\n",
    "    \"past_key_values.0.encoder.key\",\n",
    "    \"past_key_values.0.encoder.value\",\n",
    "    \"past_key_values.1.decoder.key\",\n",
    "    \"past_key_values.1.decoder.value\",\n",
    "    \"past_key_values.1.encoder.key\",\n",
    "    \"past_key_values.1.encoder.value\",\n",
    "    \"past_key_values.2.decoder.key\",\n",
    "    \"past_key_values.2.decoder.value\",\n",
    "    \"past_key_values.2.encoder.key\",\n",
    "    \"past_key_values.2.encoder.value\",\n",
    "    \"past_key_values.3.decoder.key\",\n",
    "    \"past_key_values.3.decoder.value\",\n",
    "    \"past_key_values.3.encoder.key\",\n",
    "    \"past_key_values.3.encoder.value\",\n",
    "    \"past_key_values.4.decoder.key\",\n",
    "    \"past_key_values.4.decoder.value\",\n",
    "    \"past_key_values.4.encoder.key\",\n",
    "    \"past_key_values.4.encoder.value\",\n",
    "    \"past_key_values.5.decoder.key\",\n",
    "    \"past_key_values.5.decoder.value\",\n",
    "    \"past_key_values.5.encoder.key\",\n",
    "    \"past_key_values.5.encoder.value\",\n",
    "]\n",
    "\n",
    "output_names = [\n",
    "    \"logits\",\n",
    "    \"present.0.decoder.key\",\n",
    "    \"present.0.decoder.value\",\n",
    "    \"present.0.encoder.key\",\n",
    "    \"present.0.encoder.value\",\n",
    "    \"present.1.decoder.key\",\n",
    "    \"present.1.decoder.value\",\n",
    "    \"present.1.encoder.key\",\n",
    "    \"present.1.encoder.value\",\n",
    "    \"present.2.decoder.key\",\n",
    "    \"present.2.decoder.value\",\n",
    "    \"present.2.encoder.key\",\n",
    "    \"present.2.encoder.value\",\n",
    "    \"present.3.decoder.key\",\n",
    "    \"present.3.decoder.value\",\n",
    "    \"present.3.encoder.key\",\n",
    "    \"present.3.encoder.value\",\n",
    "    \"present.4.decoder.key\",\n",
    "    \"present.4.decoder.value\",\n",
    "    \"present.4.encoder.key\",\n",
    "    \"present.4.encoder.value\",\n",
    "    \"present.5.decoder.key\",\n",
    "    \"present.5.decoder.value\",\n",
    "    \"present.5.encoder.key\",\n",
    "    \"present.5.encoder.value\",\n",
    "]\n",
    "\n",
    "dynamic_axis = {\n",
    "    \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n",
    "    \"encoder_hidden_states\": {0: \"batch\", 2: \"encoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"past_key_values.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"logits\": {0: \"batch\", 1: \"decoder_sequence\"},\n",
    "    \"present.0.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.0.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.0.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.1.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.1.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.2.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.2.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.3.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.3.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.4.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.4.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.decoder.key\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.decoder.value\": {0: \"batch\", 2: \"past_decoder_sequence + sequence\"},\n",
    "    \"present.5.encoder.key\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "    \"present.5.encoder.value\": {0: \"batch\", 2: \"past_encoder_sequence\"},\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model.decoder,\n",
    "        (model_inputs,),\n",
    "        f=\"test-dec-cache.onnx\",\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axis,\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=13,\n",
    "    )\n",
    "\n",
    "model_inputs_no_cache = {\n",
    "    \"input_ids\": input_ids.type(torch.int32),\n",
    "    \"encoder_hidden_states\": out_enc.last_hidden_state,\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    torch.onnx.export(\n",
    "        model.decoder,\n",
    "        (model_inputs_no_cache,),\n",
    "        f=\"test-dec-no-cache.onnx\",\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes={k: v for k, v in dynamic_axis.items() if \"past_key_values\" not in k},\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=False,\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=13,\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[ 2.75505669e-02,  1.50334492e-01, -1.06381610e-01,\n          1.34366244e-01, -2.70337425e-03,  1.01188630e-01,\n         -3.70661765e-02, -1.19580179e-01,  4.28272367e-01,\n         -6.88373074e-02,  4.79299482e-03,  1.03876561e-01,\n         -1.91168815e-01, -9.14082751e-02,  8.02245319e-01,\n         -2.90097948e-02, -8.30157474e-03, -4.18420471e-02,\n          5.12353703e-02, -8.15958455e-02,  5.27289249e-02,\n          7.59666264e-02, -8.56558830e-02,  2.22404099e+00,\n          4.77618948e-02, -7.09001422e-02, -3.64811942e-02,\n          2.84006387e-01, -3.37912999e-02, -1.31012782e-01,\n          4.64338362e-02, -4.02517952e-02, -5.61397932e-02,\n         -1.57095678e-02,  4.12030399e-01, -3.93750966e-02,\n         -1.11142471e-01, -7.79706985e-02, -1.34205982e-01,\n          1.95660204e-01, -4.99962792e-02, -5.93461283e-02,\n          2.24776655e-01,  7.36797228e-02, -5.78797944e-02,\n          1.48978885e-02, -6.95030272e-01,  1.72866043e-02,\n          1.06056750e-01,  9.23829898e-02, -1.33778632e-01,\n         -5.10500260e-02, -8.84042680e-02,  1.44867465e-01,\n         -1.12630934e-01,  4.18289891e-03,  5.39499335e-02,\n         -7.48298243e-02,  5.84630221e-02,  2.18830574e-02,\n          8.28510746e-02,  9.23681408e-02, -2.56623570e-02,\n         -8.60406235e-02, -1.29731502e-02, -4.35564369e-02,\n         -4.88872454e-02, -3.33911590e-02,  5.53378284e-01,\n          1.87132001e-01, -3.04206461e-02,  5.12396544e-02,\n          5.50649501e-02, -5.50896712e-02, -1.71369372e-03,\n          9.52994972e-02, -4.58988585e-02, -1.25644580e-02,\n          8.41903165e-02,  2.03844219e-01, -2.03252688e-01,\n          8.38244483e-02,  5.32395020e-02, -8.47254694e-02,\n         -4.77962643e-02, -1.11990482e-01, -2.88542770e-02,\n         -3.38673629e-02, -1.73326656e-01,  6.11119390e-01,\n         -1.28954677e-02, -9.40872580e-02,  8.24993700e-02,\n         -7.63467252e-02, -8.58427063e-02,  2.57149804e-02,\n         -1.56978313e-02, -1.14580095e-02,  6.49371743e-03,\n         -2.35139504e-02,  3.48964855e-02, -5.87521829e-02,\n          1.32941362e-02,  1.58199355e-01,  4.75927293e-02,\n          1.04084663e-01,  6.91408888e-02,  3.98425832e-02,\n         -3.27266790e-02, -9.14220586e-02, -1.01452753e-01,\n          1.41455501e-03, -4.89438977e-03,  7.62228817e-02,\n         -7.39351287e-02, -8.53853077e-02,  2.25576740e-02,\n         -4.95822467e-02,  3.28806974e-02, -2.22904570e-02,\n         -6.80288151e-02, -2.01618802e-02,  1.66544877e-02,\n          2.85096787e-04, -2.56005712e-02,  4.45045084e-01,\n         -9.33915749e-02,  3.14040445e-02, -1.07239619e-01,\n         -7.82322660e-02,  2.45811976e-02,  1.16077932e-02,\n         -6.19486682e-02, -8.27611983e-03, -4.80316579e-02,\n          2.78309658e-02, -5.36613613e-02,  6.19741976e-01,\n         -8.08980092e-02,  1.18099879e-02, -3.86862233e-02,\n         -9.59619284e-02,  8.12875405e-02, -6.79112226e-02,\n          5.69731109e-02,  6.10916950e-02,  9.61923599e-02,\n          2.42913458e-02,  1.55501496e-02, -3.01053286e+00,\n          1.22078702e-01, -1.47193167e-02,  8.82110596e-02,\n         -1.06468052e-02,  9.53794271e-02,  3.47557738e-02,\n         -4.56553586e-02,  3.79417352e-02,  7.26638436e-02,\n         -6.99234724e+00, -9.04354453e-02,  5.71738593e-02,\n          9.90397707e-02,  4.60141525e-02, -1.13601066e-01,\n         -1.30356655e-01,  1.44960523e-01,  3.36552262e-01,\n         -5.22373058e-02,  4.02644686e-02,  2.01296583e-02,\n         -5.19665964e-02,  4.42011543e-02, -7.73665011e-02,\n          1.93538573e-02,  6.22397102e-02, -6.13869578e-02,\n          1.87150878e-03, -3.30779366e-02,  2.03479663e-03,\n          3.47195528e-02,  4.91127633e-02,  2.61692214e-03,\n          6.75373375e-02, -3.06474328e-01, -1.04688741e-01,\n          8.92900229e-02,  1.26531973e-01, -9.89445001e-02,\n         -2.58013718e-02, -1.16666865e+00, -2.15690717e-01,\n          3.95623362e-03, -5.43998480e-02,  4.23719734e-03,\n         -1.41153419e-02, -7.50378892e-03, -6.11306913e-02,\n          1.71125904e-02, -6.32590801e-02, -3.22151892e-02,\n         -4.24649641e-02, -3.95676456e-02, -3.21831144e-02,\n         -7.06271678e-02,  1.74716464e-03,  3.14379409e-02,\n         -5.31577580e-02, -4.67845239e-02, -7.49840140e-02,\n          1.78107992e-02,  3.78590487e-02, -5.94927482e-02,\n         -1.58945650e-01, -6.27402291e-02, -2.49362085e-02,\n         -1.15291663e-02, -5.67526594e-02,  1.75984614e-02,\n         -5.21787554e-02, -3.39149311e-02,  1.08341299e-01,\n         -3.60668488e-02, -1.48353875e-01,  7.23027950e-03,\n          1.93199962e-02, -2.53880173e-01,  4.70523648e-02,\n         -1.28829360e-01, -5.26054986e-02,  3.45744193e-02,\n         -3.21897976e-02,  2.34192405e-02,  7.28721842e-02,\n         -2.55722225e-01, -6.54517785e-02,  1.28955051e-01,\n          8.39653164e-02, -9.26485583e-02,  7.19099715e-02,\n          2.80648798e-01,  4.24059890e-02,  2.21914910e-02,\n          4.01960760e-02, -1.61395356e-01,  9.22247469e-01,\n          8.53097290e-02,  4.65283287e-04, -6.10001683e-01,\n          4.49052304e-02,  5.18388674e-02,  1.83407068e-02,\n          5.79948574e-02, -6.65492788e-02, -1.27804214e-02,\n         -4.83701676e-02, -7.04476750e-03, -7.42707253e-02,\n         -2.50386387e-01,  9.48228315e-02, -2.00548153e-02,\n          5.56071615e-03,  1.05463199e-01,  6.38803720e-01,\n         -1.29957581e-02,  6.10180087e-02, -9.96741578e-02,\n          8.63569379e-02,  5.24923159e-03, -8.44566450e-02,\n          6.43482208e-02, -3.91174629e-02,  2.06197068e-01,\n          5.09726822e-01,  6.31341562e-02, -2.02851221e-02,\n         -4.01217975e-02,  6.02266658e-03, -5.96888103e-02,\n         -1.22351632e-01, -4.23622698e-01,  3.19916047e-02,\n         -3.34796868e-02, -5.37158363e-03,  6.40288219e-02,\n         -2.48400960e-02,  7.45139942e-02,  4.08036597e-02,\n          5.06433621e-02,  1.52358506e-02, -2.37904429e-01,\n          4.17122123e-04,  5.56000136e-02,  1.74413249e-01,\n         -2.05990338e+00,  2.21864626e-01, -8.13438967e-02,\n         -3.37398164e-02,  1.33406371e-02,  3.70413736e-02,\n          7.08871037e-02,  1.72459632e-02,  6.75759614e-02,\n          1.52408499e-02, -4.09680754e-02,  4.66254056e-02,\n         -1.55101314e-01, -3.23641777e-01,  4.69440758e-01,\n         -9.70338881e-02, -2.03978717e-02,  1.89484656e-01,\n         -7.47351706e-01,  1.40296593e-01, -9.42021087e-02,\n         -5.96793965e-02,  3.43805067e-02,  4.56733406e-02,\n         -2.08695829e-01, -6.91013783e-02, -1.08380772e-01,\n         -1.40194729e-01, -9.66359973e-02, -2.48352885e-01,\n         -2.92813444e+00,  4.89294715e-02,  7.43332431e-02,\n          6.22023121e-02,  1.56397410e-02,  8.02663937e-02,\n          8.40593129e-02, -8.28332379e-02, -1.47811621e-01,\n          6.21178336e-02,  6.15502847e-03, -8.31501782e-02,\n         -1.65246010e-01, -4.22016755e-02, -8.77268687e-02,\n         -1.05901077e-01,  6.60945773e-02, -1.22300431e-01,\n         -6.82237092e-03,  1.31674394e-01, -1.07653327e-02,\n         -1.70249686e-01,  2.67399382e-02, -3.87064703e-02,\n          7.67047405e-02, -5.87392971e-02,  1.95931524e-01,\n         -1.21931992e-02,  5.97998910e-02,  5.07163927e-02,\n          2.54814923e-02, -2.79090285e-01, -1.22527845e-01,\n          2.83779725e-02,  3.91266085e-02,  1.38179570e-01,\n         -9.59347710e-02,  1.99452955e-02,  6.01068325e-02,\n         -1.40435591e-01, -5.96017279e-02, -1.09512679e-01,\n         -7.32924491e-02, -3.37557703e-01, -1.68209092e-03,\n          8.22439149e-04,  6.83650523e-02,  1.69284344e-01,\n         -6.62202984e-02,  1.14394808e-02, -3.04875851e-01,\n         -1.25754178e-01,  8.74009356e-03, -8.28580037e-02,\n          3.29438620e-03, -2.14821771e-01,  4.41380069e-02,\n         -1.15028191e+00, -2.01638788e-01, -2.23545264e-02,\n          1.51876837e-01,  5.15977852e-02,  1.11289926e-01,\n          2.42833514e-02, -2.74671018e-01, -8.68278276e-03,\n          1.27823809e-02,  2.31077652e-02, -1.41696304e-01,\n          1.24628963e-02, -6.37648478e-02, -1.09762974e-01,\n          4.83270399e-02,  7.41117969e-02,  5.89701198e-02,\n          7.75312185e-02,  2.72527397e-01,  1.81993008e-01,\n          7.37223700e-02, -1.17000736e-01, -5.55063337e-02,\n          2.00451426e-02, -5.67837805e-02, -3.55605371e-02,\n         -1.60436109e-02,  4.24101725e-02,  2.48647630e-01,\n         -1.28441647e-01, -9.15363953e-02, -9.27907526e-02,\n         -1.55872768e-02,  2.19560638e-02,  5.68239242e-02,\n          3.92632544e-01,  1.62125397e-02,  2.81373095e-02,\n          5.01115210e-02, -1.04565479e-01, -8.28984380e-02,\n          2.74525490e-02, -1.51698872e-01, -7.93095529e-02,\n          3.01573034e-02, -1.72931645e-02, -1.28296211e-01,\n          2.76031364e-02,  5.31899221e-02, -5.43122441e-02,\n          1.01620972e-01, -1.97175443e-02, -9.02794152e-02,\n         -4.69088852e-02,  1.62584218e-03,  1.94558427e-02,\n          3.87347713e-02, -4.27117571e-02, -7.79618323e-02,\n          4.14239839e-02,  8.21991451e-03,  1.50817811e-01,\n          3.37398378e-03, -2.21454725e-02,  3.88479885e-03,\n         -1.03438422e-01, -1.32273361e-01,  9.76363837e-04,\n          8.56895894e-02, -6.08130097e-02, -5.33081926e-02,\n         -1.83460295e-01,  4.97843362e-02,  2.95072906e-02,\n          5.86353578e-02, -1.16664600e-02,  5.66617697e-02,\n         -2.32321136e-02,  5.94319068e-02,  3.06207426e-02,\n         -7.00690895e-02, -9.40842032e-02,  1.27658740e-01,\n         -9.17317346e-02,  5.38925119e-02,  1.48466170e-01,\n          5.51729016e-02, -4.85163704e-02, -7.48542622e-02,\n         -4.85150795e-03, -1.11119539e-01,  2.97123399e-02,\n         -1.32060274e-01, -1.03436068e-01, -5.55926971e-02,\n         -3.44107360e-01, -9.16953012e-03, -3.53453010e-02,\n         -1.32327318e-01,  4.74584512e-02,  8.92623141e-02,\n         -1.25505969e-01,  5.96084595e-02, -5.41221239e-02,\n         -9.07916389e-03, -1.99068204e-01, -4.22884803e-03,\n         -8.08746554e-03,  1.49609242e-03, -7.92344362e-02,\n          6.18762821e-02,  4.94816601e-02, -7.54972547e-02,\n          1.79912765e-02,  8.11960176e-02, -1.84198350e-01,\n         -6.55509308e-02, -6.83902353e-02, -2.11571261e-01,\n         -7.40170181e-01, -3.89348604e-02,  1.19020164e-01,\n          3.78060639e-02, -1.35776162e-01, -1.57760769e-01,\n         -1.41327279e-02,  4.69983630e-02,  6.51002228e-02,\n          3.40436964e-04,  5.86275533e-02]]], dtype=float32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CPUExecutionProvider\")\n",
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = input_ids[:, -1:].type(torch.int32).detach().cpu().numpy()\n",
    "input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "\n",
    "ort_cache.run([\"logits\"], input_cache)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 2.7551e-02,  1.5033e-01, -1.0638e-01,  1.3437e-01, -2.7034e-03,\n           1.0119e-01, -3.7066e-02, -1.1958e-01,  4.2827e-01, -6.8837e-02,\n           4.7930e-03,  1.0388e-01, -1.9117e-01, -9.1408e-02,  8.0224e-01,\n          -2.9010e-02, -8.3016e-03, -4.1842e-02,  5.1235e-02, -8.1596e-02,\n           5.2729e-02,  7.5967e-02, -8.5656e-02,  2.2240e+00,  4.7762e-02,\n          -7.0900e-02, -3.6481e-02,  2.8401e-01, -3.3791e-02, -1.3101e-01,\n           4.6434e-02, -4.0252e-02, -5.6140e-02, -1.5710e-02,  4.1203e-01,\n          -3.9375e-02, -1.1114e-01, -7.7971e-02, -1.3421e-01,  1.9566e-01,\n          -4.9996e-02, -5.9346e-02,  2.2478e-01,  7.3680e-02, -5.7880e-02,\n           1.4898e-02, -6.9503e-01,  1.7287e-02,  1.0606e-01,  9.2383e-02,\n          -1.3378e-01, -5.1050e-02, -8.8404e-02,  1.4487e-01, -1.1263e-01,\n           4.1829e-03,  5.3950e-02, -7.4830e-02,  5.8463e-02,  2.1883e-02,\n           8.2851e-02,  9.2368e-02, -2.5662e-02, -8.6041e-02, -1.2973e-02,\n          -4.3556e-02, -4.8887e-02, -3.3391e-02,  5.5338e-01,  1.8713e-01,\n          -3.0421e-02,  5.1240e-02,  5.5065e-02, -5.5090e-02, -1.7137e-03,\n           9.5300e-02, -4.5899e-02, -1.2564e-02,  8.4190e-02,  2.0384e-01,\n          -2.0325e-01,  8.3824e-02,  5.3239e-02, -8.4725e-02, -4.7796e-02,\n          -1.1199e-01, -2.8854e-02, -3.3867e-02, -1.7333e-01,  6.1112e-01,\n          -1.2895e-02, -9.4087e-02,  8.2499e-02, -7.6347e-02, -8.5843e-02,\n           2.5715e-02, -1.5698e-02, -1.1458e-02,  6.4937e-03, -2.3514e-02,\n           3.4896e-02, -5.8752e-02,  1.3294e-02,  1.5820e-01,  4.7593e-02,\n           1.0408e-01,  6.9141e-02,  3.9843e-02, -3.2727e-02, -9.1422e-02,\n          -1.0145e-01,  1.4145e-03, -4.8944e-03,  7.6223e-02, -7.3935e-02,\n          -8.5385e-02,  2.2558e-02, -4.9582e-02,  3.2881e-02, -2.2290e-02,\n          -6.8029e-02, -2.0162e-02,  1.6654e-02,  2.8510e-04, -2.5601e-02,\n           4.4504e-01, -9.3392e-02,  3.1404e-02, -1.0724e-01, -7.8232e-02,\n           2.4581e-02,  1.1608e-02, -6.1949e-02, -8.2761e-03, -4.8032e-02,\n           2.7831e-02, -5.3661e-02,  6.1974e-01, -8.0898e-02,  1.1810e-02,\n          -3.8686e-02, -9.5962e-02,  8.1288e-02, -6.7911e-02,  5.6973e-02,\n           6.1092e-02,  9.6192e-02,  2.4291e-02,  1.5550e-02, -3.0105e+00,\n           1.2208e-01, -1.4719e-02,  8.8211e-02, -1.0647e-02,  9.5379e-02,\n           3.4756e-02, -4.5655e-02,  3.7942e-02,  7.2664e-02, -6.9923e+00,\n          -9.0435e-02,  5.7174e-02,  9.9040e-02,  4.6014e-02, -1.1360e-01,\n          -1.3036e-01,  1.4496e-01,  3.3655e-01, -5.2237e-02,  4.0264e-02,\n           2.0130e-02, -5.1967e-02,  4.4201e-02, -7.7366e-02,  1.9354e-02,\n           6.2240e-02, -6.1387e-02,  1.8715e-03, -3.3078e-02,  2.0348e-03,\n           3.4720e-02,  4.9113e-02,  2.6169e-03,  6.7537e-02, -3.0647e-01,\n          -1.0469e-01,  8.9290e-02,  1.2653e-01, -9.8944e-02, -2.5801e-02,\n          -1.1667e+00, -2.1569e-01,  3.9563e-03, -5.4400e-02,  4.2372e-03,\n          -1.4115e-02, -7.5038e-03, -6.1131e-02,  1.7113e-02, -6.3259e-02,\n          -3.2215e-02, -4.2465e-02, -3.9568e-02, -3.2183e-02, -7.0627e-02,\n           1.7471e-03,  3.1438e-02, -5.3158e-02, -4.6784e-02, -7.4984e-02,\n           1.7811e-02,  3.7859e-02, -5.9493e-02, -1.5895e-01, -6.2740e-02,\n          -2.4936e-02, -1.1529e-02, -5.6753e-02,  1.7598e-02, -5.2179e-02,\n          -3.3915e-02,  1.0834e-01, -3.6067e-02, -1.4835e-01,  7.2303e-03,\n           1.9320e-02, -2.5388e-01,  4.7052e-02, -1.2883e-01, -5.2605e-02,\n           3.4574e-02, -3.2190e-02,  2.3419e-02,  7.2872e-02, -2.5572e-01,\n          -6.5452e-02,  1.2896e-01,  8.3965e-02, -9.2649e-02,  7.1910e-02,\n           2.8065e-01,  4.2406e-02,  2.2191e-02,  4.0196e-02, -1.6140e-01,\n           9.2225e-01,  8.5310e-02,  4.6526e-04, -6.1000e-01,  4.4905e-02,\n           5.1839e-02,  1.8341e-02,  5.7995e-02, -6.6549e-02, -1.2780e-02,\n          -4.8370e-02, -7.0448e-03, -7.4271e-02, -2.5039e-01,  9.4823e-02,\n          -2.0055e-02,  5.5607e-03,  1.0546e-01,  6.3880e-01, -1.2996e-02,\n           6.1018e-02, -9.9674e-02,  8.6357e-02,  5.2492e-03, -8.4457e-02,\n           6.4348e-02, -3.9117e-02,  2.0620e-01,  5.0973e-01,  6.3134e-02,\n          -2.0285e-02, -4.0122e-02,  6.0226e-03, -5.9689e-02, -1.2235e-01,\n          -4.2362e-01,  3.1992e-02, -3.3480e-02, -5.3716e-03,  6.4029e-02,\n          -2.4840e-02,  7.4514e-02,  4.0804e-02,  5.0643e-02,  1.5236e-02,\n          -2.3790e-01,  4.1713e-04,  5.5600e-02,  1.7441e-01, -2.0599e+00,\n           2.2186e-01, -8.1344e-02, -3.3740e-02,  1.3341e-02,  3.7041e-02,\n           7.0887e-02,  1.7246e-02,  6.7576e-02,  1.5241e-02, -4.0968e-02,\n           4.6625e-02, -1.5510e-01, -3.2364e-01,  4.6944e-01, -9.7034e-02,\n          -2.0398e-02,  1.8948e-01, -7.4735e-01,  1.4030e-01, -9.4202e-02,\n          -5.9679e-02,  3.4381e-02,  4.5673e-02, -2.0870e-01, -6.9101e-02,\n          -1.0838e-01, -1.4019e-01, -9.6636e-02, -2.4835e-01, -2.9281e+00,\n           4.8929e-02,  7.4333e-02,  6.2202e-02,  1.5640e-02,  8.0266e-02,\n           8.4059e-02, -8.2833e-02, -1.4781e-01,  6.2118e-02,  6.1550e-03,\n          -8.3150e-02, -1.6525e-01, -4.2202e-02, -8.7727e-02, -1.0590e-01,\n           6.6095e-02, -1.2230e-01, -6.8224e-03,  1.3167e-01, -1.0765e-02,\n          -1.7025e-01,  2.6740e-02, -3.8706e-02,  7.6705e-02, -5.8739e-02,\n           1.9593e-01, -1.2193e-02,  5.9800e-02,  5.0716e-02,  2.5481e-02,\n          -2.7909e-01, -1.2253e-01,  2.8378e-02,  3.9127e-02,  1.3818e-01,\n          -9.5935e-02,  1.9945e-02,  6.0107e-02, -1.4044e-01, -5.9602e-02,\n          -1.0951e-01, -7.3292e-02, -3.3756e-01, -1.6821e-03,  8.2241e-04,\n           6.8365e-02,  1.6928e-01, -6.6220e-02,  1.1439e-02, -3.0488e-01,\n          -1.2575e-01,  8.7401e-03, -8.2858e-02,  3.2944e-03, -2.1482e-01,\n           4.4138e-02, -1.1503e+00, -2.0164e-01, -2.2355e-02,  1.5188e-01,\n           5.1598e-02,  1.1129e-01,  2.4283e-02, -2.7467e-01, -8.6828e-03,\n           1.2782e-02,  2.3108e-02, -1.4170e-01,  1.2463e-02, -6.3765e-02,\n          -1.0976e-01,  4.8327e-02,  7.4112e-02,  5.8970e-02,  7.7531e-02,\n           2.7253e-01,  1.8199e-01,  7.3722e-02, -1.1700e-01, -5.5506e-02,\n           2.0045e-02, -5.6784e-02, -3.5561e-02, -1.6044e-02,  4.2410e-02,\n           2.4865e-01, -1.2844e-01, -9.1536e-02, -9.2791e-02, -1.5587e-02,\n           2.1956e-02,  5.6824e-02,  3.9263e-01,  1.6213e-02,  2.8137e-02,\n           5.0112e-02, -1.0457e-01, -8.2898e-02,  2.7453e-02, -1.5170e-01,\n          -7.9310e-02,  3.0157e-02, -1.7293e-02, -1.2830e-01,  2.7603e-02,\n           5.3190e-02, -5.4312e-02,  1.0162e-01, -1.9718e-02, -9.0279e-02,\n          -4.6909e-02,  1.6258e-03,  1.9456e-02,  3.8735e-02, -4.2712e-02,\n          -7.7962e-02,  4.1424e-02,  8.2199e-03,  1.5082e-01,  3.3740e-03,\n          -2.2145e-02,  3.8848e-03, -1.0344e-01, -1.3227e-01,  9.7634e-04,\n           8.5690e-02, -6.0813e-02, -5.3308e-02, -1.8346e-01,  4.9784e-02,\n           2.9507e-02,  5.8635e-02, -1.1666e-02,  5.6662e-02, -2.3232e-02,\n           5.9432e-02,  3.0621e-02, -7.0069e-02, -9.4084e-02,  1.2766e-01,\n          -9.1732e-02,  5.3892e-02,  1.4847e-01,  5.5173e-02, -4.8516e-02,\n          -7.4854e-02, -4.8515e-03, -1.1112e-01,  2.9712e-02, -1.3206e-01,\n          -1.0344e-01, -5.5593e-02, -3.4411e-01, -9.1696e-03, -3.5345e-02,\n          -1.3233e-01,  4.7458e-02,  8.9262e-02, -1.2551e-01,  5.9608e-02,\n          -5.4122e-02, -9.0791e-03, -1.9907e-01, -4.2288e-03, -8.0875e-03,\n           1.4961e-03, -7.9234e-02,  6.1876e-02,  4.9482e-02, -7.5497e-02,\n           1.7991e-02,  8.1196e-02, -1.8420e-01, -6.5551e-02, -6.8390e-02,\n          -2.1157e-01, -7.4017e-01, -3.8935e-02,  1.1902e-01,  3.7806e-02,\n          -1.3578e-01, -1.5776e-01, -1.4133e-02,  4.6998e-02,  6.5100e-02,\n           3.4044e-04,  5.8627e-02]]], device='cuda:0')"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ort_cache = create_model_for_provider(\"test-dec-cache.onnx\", \"CUDAExecutionProvider\")\n",
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = input_ids[:, -1:]\n",
    "input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach()\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor, torch.Tensor)\n",
    "    input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.cuda()\n",
    "    input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.cuda()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.cuda()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.cuda()\n",
    "\n",
    "\n",
    "inference_onnx_binding(model_onnx=ort_cache, inputs=input_cache, device=\"cuda\", output_shape={\"logits\": (1, 1, 512)})[\n",
    "    \"logits\"\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import GraphProto, ModelProto, NodeProto, TensorProto, ValueInfoProto, helper\n",
    "\n",
    "onnx_model_cache = onnx.load(\"test-dec-cache.onnx\")\n",
    "onnx_model_no_cache = onnx.load(\"test-dec-no-cache.onnx\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: cache_node_1256 - size: 0.01\n",
      "name: cache_node_1257 - size: 0.01\n",
      "name: cache_node_1267 - size: 0.01\n",
      "name: cache_node_1268 - size: 0.01\n"
     ]
    }
   ],
   "source": [
    "prefix = \"cache_node_\"\n",
    "mapping_initializer_cache_to_no_cache = dict()\n",
    "to_add = list()\n",
    "for node_cache in onnx_model_cache.graph.initializer:\n",
    "    found = False\n",
    "    for node_no_cache in onnx_model_no_cache.graph.initializer:\n",
    "        if node_cache.raw_data == node_no_cache.raw_data:\n",
    "            found = True\n",
    "            mapping_initializer_cache_to_no_cache[node_cache.name] = node_no_cache.name\n",
    "            break\n",
    "    if not found:\n",
    "        node_cache.name = prefix + node_cache.name\n",
    "        to_add.append(node_cache)\n",
    "        mapping_initializer_cache_to_no_cache[node_cache.name] = node_cache.name\n",
    "        print(f\"name: {node_cache.name} - size: {len(node_cache.raw_data)/1024:.2f}\")\n",
    "\n",
    "onnx_model_no_cache.graph.initializer.extend(to_add)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# I/O model names should not be prefixed\n",
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "for node in onnx_model_cache.graph.node:\n",
    "    for index, input_name in enumerate(node.input):\n",
    "        if input_name in model_io_names:\n",
    "            continue\n",
    "        node.input[index] = mapping_initializer_cache_to_no_cache.get(input_name, prefix + input_name)\n",
    "    for index, output_name in enumerate(node.output):\n",
    "        if output_name in model_io_names:\n",
    "            continue\n",
    "        node.output[index] = prefix + output_name\n",
    "    node.name = prefix + node.name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_io_names = [n.name for n in list(onnx_model_cache.graph.input) + list(onnx_model_cache.graph.output)]\n",
    "\n",
    "prefix = \"init_\"\n",
    "cache = dict()\n",
    "for node in onnx_model_no_cache.graph.initializer:\n",
    "    if node.name in model_io_names:\n",
    "        new_name = prefix + node.name\n",
    "        cache[node.name] = new_name\n",
    "        node.name = new_name\n",
    "        # print(node.name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for node in onnx_model_no_cache.graph.node:\n",
    "    for index, n in enumerate(node.input):\n",
    "        node.input[index] = cache.get(n, n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# mandatory for subgraph in if/else node\n",
    "assert len(onnx_model_cache.graph.output) == len(onnx_model_no_cache.graph.output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_cache.graph.node),\n",
    "    name=\"graph-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "graph_no_cache: onnx.GraphProto = onnx.helper.make_graph(\n",
    "    nodes=list(onnx_model_no_cache.graph.node),\n",
    "    name=\"graph-no-cache\",\n",
    "    inputs=[],\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=[],\n",
    ")\n",
    "\n",
    "retrieve_shape: NodeProto = helper.make_node(\n",
    "    op_type=\"Shape\",\n",
    "    inputs=[\"past_key_values.0.decoder.key\"],\n",
    "    outputs=[\"shape_2\"],\n",
    ")\n",
    "\n",
    "gather_def: NodeProto = helper.make_node(\n",
    "    op_type=\"Gather\",\n",
    "    inputs=[\"shape_2\", \"gather_dim_index\"],\n",
    "    outputs=[\"gather1\"],\n",
    ")\n",
    "\n",
    "equal_def: NodeProto = helper.make_node(\n",
    "    op_type=\"Equal\",\n",
    "    inputs=[\"gather1\", \"expected_val\"],\n",
    "    outputs=[\"eq_vec\"],\n",
    ")\n",
    "\n",
    "squeeze_def: NodeProto = helper.make_node(\n",
    "    op_type=\"Squeeze\",\n",
    "    inputs=[\"eq_vec\", \"squeeze_dim\"],\n",
    "    outputs=[\"if_cond\"],\n",
    ")\n",
    "\n",
    "if_node = onnx.helper.make_node(\n",
    "    op_type=\"If\",\n",
    "    inputs=[\"if_cond\"],\n",
    "    outputs=[o.name for o in list(onnx_model_no_cache.graph.output)],\n",
    "    then_branch=graph_cache,\n",
    "    else_branch=graph_no_cache,\n",
    ")\n",
    "\n",
    "# TODO rework values of these nodes\n",
    "gather_dim_index = helper.make_tensor(\"gather_dim_index\", dims=(1,), vals=[1], data_type=TensorProto.INT32)\n",
    "expected_val = helper.make_tensor(\"expected_val\", dims=(1,), vals=[4], data_type=TensorProto.INT64)\n",
    "squeeze_dim = helper.make_tensor(\"squeeze_dim\", dims=(1,), vals=[0], data_type=TensorProto.INT64)\n",
    "\n",
    "if_graph_def: GraphProto = helper.make_graph(\n",
    "    nodes=[retrieve_shape, gather_def, equal_def, squeeze_def, if_node],\n",
    "    name=\"if-model\",\n",
    "    inputs=list(onnx_model_cache.graph.input),\n",
    "    outputs=list(onnx_model_no_cache.graph.output),\n",
    "    initializer=list(onnx_model_no_cache.graph.initializer) + [gather_dim_index, expected_val, squeeze_dim],\n",
    ")\n",
    "\n",
    "model_def: ModelProto = helper.make_model(if_graph_def, producer_name=\"onnx-example\")\n",
    "\n",
    "onnx.checker.check_model(model_def)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 18:10:14.632908560 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1207'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632927674 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1157'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632947613 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1073'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632951493 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1056'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632955047 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1006'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632959282 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_922'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632963238 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_319'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632967365 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_218'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632979282 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_1224'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632986673 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_553'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632990438 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_855'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632995166 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_754'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.632999911 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_368'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633005403 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_469'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633009000 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_905'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633014073 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_771'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633017611 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_603'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633021492 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_620'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633025013 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_704'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633028359 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_452'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633199599 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1225'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633204619 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1307'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633208254 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1143'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633213182 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1044'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633219067 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '174'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633222220 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '402'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633225274 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '945'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633230575 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '600'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633234324 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '269'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633240534 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '419'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633244475 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '301'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633248118 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '764'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633251225 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '583'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633254774 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1324'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633257988 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '682'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633261259 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '1126'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633265556 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '863'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633269219 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '501'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633272687 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '781'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.633276099 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '962'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659512008 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_164'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659532109 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_152'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659537396 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_150'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659711542 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '136'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659727289 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '122'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.659731851 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '124'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.688209951 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer 'cache_node_165'. It is not used by any node and should be removed from the model.\n",
      "2022-04-21 18:10:14.688407895 [W:onnxruntime:, graph.cc:3559 CleanUnusedInitializersAndNodeArgs] Removing initializer '137'. It is not used by any node and should be removed from the model.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[[ 7.00127780e-02,  1.68538749e-01, -6.78148493e-02,\n          1.49561435e-01, -2.49588443e-03,  7.55704939e-02,\n         -1.12354629e-01, -2.67878696e-02,  4.43890899e-01,\n          3.80889960e-02,  1.20000476e-02,  1.30867541e-01,\n         -2.23976970e-01,  1.61577761e-02,  1.60872257e+00,\n         -1.72832683e-02, -5.11663444e-02, -7.68614933e-02,\n          2.32326239e-01, -1.78768188e-01, -2.27524191e-02,\n          2.83610355e-02, -1.99572966e-02,  2.24701786e+00,\n         -9.11868811e-02, -7.10381642e-02, -1.81767717e-01,\n          8.80976796e-01, -1.22718178e-02, -1.71892688e-01,\n          1.21773660e-01, -6.66140486e-03,  6.58169249e-03,\n         -1.07853204e-01,  3.97635251e-01, -9.23367888e-02,\n         -2.66088307e-01, -3.84792797e-02, -2.45335743e-01,\n          1.97110698e-01,  1.11844666e-01, -9.55803692e-02,\n          1.64988384e-01,  1.60169620e-02, -1.83289230e-01,\n         -1.39587790e-01, -3.13569129e-01, -3.07556111e-02,\n          2.37558380e-01,  6.19243085e-02, -1.38004288e-01,\n          1.85756758e-02, -3.00179541e-01, -4.47815768e-02,\n         -2.52019584e-01,  5.87493852e-02,  2.42263779e-01,\n         -1.33155016e-02,  6.00716285e-03, -1.32885026e-02,\n          4.72612754e-02,  2.05073029e-01, -6.68275519e-04,\n         -1.20915972e-01, -2.66511273e-02, -3.95622067e-02,\n          2.16748435e-02,  3.74277979e-02,  4.93818730e-01,\n          1.81325823e-01,  6.35712743e-02,  2.49804676e-01,\n          1.39790803e-01, -1.82767004e-01,  3.13326567e-02,\n          1.57800257e-01, -5.49784042e-02, -2.39192113e-01,\n         -2.30638939e-03,  7.82200173e-02, -1.15589157e-01,\n         -5.39161190e-02,  7.07954392e-02, -6.37629181e-02,\n         -5.45343980e-02,  4.67599593e-02, -1.57194838e-01,\n          1.79365680e-01, -1.62422776e-01,  4.77899611e-01,\n         -9.91956517e-02, -3.88440862e-02,  5.94012365e-02,\n         -1.16467118e-01, -2.90060163e-01,  4.60838228e-02,\n         -1.11732341e-01, -3.20125595e-02, -1.84530988e-02,\n         -9.66136381e-02,  9.84076560e-02, -1.49557769e-01,\n          1.74938813e-01,  4.49659258e-01,  1.91954762e-01,\n          1.93003610e-01, -2.75830226e-03, -6.90452754e-02,\n          6.97517768e-02, -2.03113094e-01, -5.39336652e-02,\n          1.04918085e-01, -8.14357847e-02,  2.18917996e-01,\n         -2.01121449e-01, -1.69526070e-01,  3.48094702e-02,\n         -6.67460188e-02,  8.81031379e-02,  7.02129453e-02,\n         -1.02647185e-01,  9.03097838e-02, -3.31789665e-02,\n          9.68361124e-02, -1.11277044e-01,  4.99193281e-01,\n         -1.25003129e-01,  1.43054217e-01, -1.92123011e-01,\n         -2.36136720e-01,  2.58775968e-02,  2.24422798e-01,\n         -1.93266869e-01, -1.16870704e-03, -2.41958588e-01,\n          1.73112541e-01, -3.74324769e-01,  5.72013378e-01,\n         -1.32789373e-01,  1.08853048e-02,  6.48786500e-02,\n         -2.29829371e-01, -8.69643688e-03, -6.65177628e-02,\n         -5.59155755e-02,  2.05243856e-01,  3.49361330e-01,\n          4.73274328e-02,  8.67541730e-02,  4.77195024e-01,\n          2.29899004e-01, -1.06976718e-01,  1.40253589e-01,\n         -6.88241199e-02,  2.02643741e-02,  1.95283070e-02,\n         -9.54582635e-03,  4.19438817e-02, -2.19040990e-01,\n         -2.04256058e+01, -1.03360549e-01,  2.00724795e-01,\n          1.02517128e-01,  2.21558556e-01, -1.32530350e-02,\n         -7.80836865e-02,  2.27823369e-02,  3.16992313e-01,\n          7.28334263e-02,  8.07188377e-02,  4.43090964e-03,\n         -1.27919674e-01,  3.48024368e-02, -1.35906622e-01,\n         -6.09030649e-02,  1.11954421e-01,  1.06732517e-01,\n         -2.44528756e-01, -1.03147425e-01,  4.97499220e-02,\n         -4.31105718e-02, -2.99197901e-02, -3.25886859e-03,\n          8.27963799e-02, -6.40519857e-01, -5.90961091e-02,\n         -2.55283937e-02,  1.05181351e-01, -2.95380384e-01,\n         -2.76293725e-01, -8.86060810e+00, -1.07264176e-01,\n         -6.31250488e-03,  1.74239904e-01, -1.06132917e-01,\n         -5.29939774e-03,  1.98878702e-02,  1.14385542e-02,\n         -2.29926826e-03, -3.32067870e-02, -1.85570940e-01,\n         -1.20900519e-01, -2.04237029e-01,  8.03954005e-02,\n         -8.31297040e-02,  1.09082222e-01,  1.13523088e-03,\n         -9.17106122e-02, -1.40312880e-01, -6.79304376e-02,\n         -7.73559650e-03, -5.14314957e-02,  4.21541817e-02,\n          4.63828668e-02, -5.84732667e-02,  1.70891527e-02,\n          1.23352092e-02, -1.57803193e-01,  1.40547574e-01,\n         -9.31281671e-02, -1.89901054e-01,  2.43612111e-01,\n          1.44732548e-02, -1.32129848e-01, -1.32590160e-01,\n          1.11386836e-01, -4.15091842e-01, -6.95472257e-03,\n         -1.68533385e-01, -1.39357254e-01,  3.12964231e-01,\n          3.76928113e-02, -2.10932475e-02, -4.05231304e-02,\n         -6.61987245e-01, -1.08806878e-01,  1.93079636e-01,\n          2.09082395e-01, -3.69326472e-02,  3.16812247e-01,\n          6.18796110e-01,  4.48958755e-01, -1.35017663e-01,\n          6.47870637e-03, -7.40142643e-01,  7.94956326e-01,\n         -1.83599591e-02,  4.02027816e-02, -2.37514943e-01,\n          1.84226498e-01, -3.46719316e-04,  4.74154279e-02,\n          2.83712726e-02, -6.25850037e-02, -1.00293778e-01,\n         -1.32221892e-01,  8.82665515e-02, -3.43632735e-02,\n         -2.83578306e-01,  2.65470356e-01,  5.53518422e-02,\n         -9.25031453e-02,  1.95560142e-01,  1.57853401e+00,\n          1.54266343e-01, -5.64714894e-02, -2.01045930e-01,\n          2.56011039e-02, -2.24926937e-02, -1.06096029e-01,\n          1.33169070e-01,  9.31012556e-02,  1.27240419e-01,\n          8.53982449e-01,  3.89761999e-02, -4.40572768e-01,\n         -1.76508456e-01, -9.17297676e-02, -1.61253735e-01,\n         -1.80518664e-02, -6.83658183e-01, -5.09499498e-02,\n          3.72695141e-02, -1.85609475e-01,  3.82131003e-02,\n          4.88966927e-02, -1.10325646e-02, -9.52726416e-03,\n          1.16536401e-01,  6.92600980e-02, -2.24863514e-01,\n         -5.38921058e-02,  6.06430843e-02,  1.35073751e-01,\n         -6.27001464e-01,  3.16427082e-01, -1.96762934e-01,\n         -3.74829359e-02, -5.58846630e-02,  9.16758358e-01,\n          1.76361039e-01,  5.86912818e-02,  2.47685462e-02,\n         -1.38846124e-02,  4.22738232e-02,  1.28914386e-01,\n         -4.82610203e-02, -2.71694567e-02,  1.10000432e+00,\n         -9.88721028e-02, -2.54727737e-03,  4.59739178e-01,\n         -8.58646870e-01,  3.27783108e-01, -5.88435382e-02,\n         -1.55505568e-01,  5.81655465e-02,  1.07198045e-01,\n         -3.98677677e-01,  5.67509606e-02, -2.12598309e-01,\n         -5.98554723e-02, -1.38397843e-01, -7.04999208e-01,\n         -4.01993656e+00,  8.58209357e-02,  4.50207014e-03,\n          7.63604715e-02, -3.25510912e-02, -2.47663900e-01,\n          2.37412483e-01, -3.88256311e-02, -1.17593020e-01,\n          1.22606168e-02, -1.55378312e-01, -3.04269791e-01,\n         -4.54277605e-01,  4.94333170e-02, -2.08603069e-02,\n         -2.46288329e-01,  1.61688775e-01,  8.85363016e-03,\n          1.49003074e-01,  1.42407075e-01, -1.17990807e-01,\n         -2.62452632e-01,  3.82006839e-02, -6.01293743e-02,\n          1.67689651e-01,  3.67207825e-02,  2.36629844e-02,\n          5.56530803e-02, -1.81133999e-03,  6.71054870e-02,\n          1.07726663e-01, -2.44704098e-01, -1.23557329e-01,\n          5.49549749e-03,  2.53158838e-01,  2.84964651e-01,\n         -6.84751794e-02,  4.01700810e-02,  9.16891322e-02,\n         -1.07201099e-01, -1.89630613e-01,  2.03880705e-02,\n          1.29642949e-01, -2.10797086e-01, -2.28829514e-02,\n          5.48862368e-02,  6.55449629e-02,  7.50453174e-02,\n         -6.85577989e-02, -1.47397518e-01, -2.81722337e-01,\n         -2.27899000e-01,  1.33981049e-01, -1.19902559e-01,\n          1.09841399e-01, -6.48154855e-01,  1.49930507e-01,\n         -9.57918704e-01, -1.43815950e-01, -2.36089639e-02,\n          2.90586144e-01, -2.44676080e-02,  2.65422612e-01,\n          1.57389268e-01, -1.51952937e-01, -1.16571918e-01,\n         -3.86023037e-02,  8.71506184e-02,  4.07203436e-02,\n          1.63971372e-02,  3.18016075e-02, -6.88964203e-02,\n          6.90685138e-02,  2.54662216e-01,  5.48537970e-02,\n          1.86177343e-01, -5.36094010e-02,  9.93977021e-03,\n         -1.50505111e-01,  4.07481194e-02, -1.85948536e-02,\n         -2.53585749e-04, -3.42341587e-02, -4.06899303e-02,\n         -1.02025226e-01,  1.20022215e-01,  1.48426682e-01,\n         -1.24613836e-01, -7.27273449e-02,  1.46636710e-01,\n         -5.12129292e-02,  3.02789398e-02,  1.73055992e-01,\n          6.61251307e-01, -8.23393017e-02,  9.92280245e-02,\n          1.54259764e-02, -1.71383590e-01,  4.32077656e-03,\n         -2.59583909e-02, -1.56892508e-01, -6.40883520e-02,\n         -2.58656759e-02, -8.10219795e-02, -1.23424679e-01,\n         -1.17149912e-01,  1.03431813e-01, -6.72288910e-02,\n         -6.83683390e-03, -2.42928877e-01, -9.16570947e-02,\n         -5.22193573e-02, -1.09814899e-02,  1.02760857e-02,\n          1.50885493e-01, -9.57131535e-02, -3.71505879e-02,\n          6.27032816e-02,  1.31218806e-01,  1.80049494e-01,\n          3.09896935e-02, -9.36059281e-02, -8.87410566e-02,\n         -2.69554764e-01, -1.16269313e-01, -2.95375716e-02,\n          1.06401250e-01, -2.52766511e-03, -5.30732758e-02,\n         -1.96390972e-01,  1.64778069e-01,  8.42469037e-02,\n          1.42025530e-01,  1.09797582e-01,  3.02517600e-02,\n         -4.02440764e-02,  1.31522596e-01,  1.98681820e-02,\n         -3.25541534e-02, -1.67487592e-01,  1.93134323e-01,\n         -1.27862766e-01,  5.15547348e-04,  6.73537403e-02,\n          1.99790999e-01, -9.83303636e-02, -1.09647073e-01,\n          1.10905822e-02, -1.67899936e-01, -9.27964970e-02,\n         -3.37191969e-01, -5.04076183e-01, -3.31075154e-02,\n         -6.60938323e-01,  1.23304963e-01, -1.36388674e-01,\n         -4.20862168e-01,  9.17616114e-02,  1.16795965e-01,\n         -2.84991443e-01,  2.32037857e-01, -2.56459236e-01,\n         -2.02070251e-01, -2.19646290e-01, -1.08312540e-01,\n         -6.09674044e-02,  7.55654532e-04, -5.43660969e-02,\n         -8.38679820e-02,  5.35485381e-03, -1.45013565e-02,\n          1.77639157e-01,  1.25858650e-01, -1.77778766e-01,\n         -2.82290488e-01, -1.38688684e-02, -1.75514296e-01,\n         -1.38034141e+00,  7.08263889e-02,  9.23888460e-02,\n          6.58023208e-02, -3.19939107e-02, -2.80833900e-01,\n         -1.24659613e-02,  1.29334927e-01,  5.47961406e-02,\n         -6.71747548e-05, -1.21578529e-01]]], dtype=float32)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_def.SerializeToString()\n",
    "# \"test-dec-cache.onnx\"\n",
    "ort_cache = create_model_for_provider(model_def.SerializeToString(), \"CPUExecutionProvider\")\n",
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = input_ids[:, -1:].type(torch.int32).detach().cpu().numpy()\n",
    "input_cache[\"encoder_hidden_states\"] = out_enc.last_hidden_state.detach().cpu().numpy()\n",
    "\n",
    "for index, (k_dec, v_dec, k_enc, v_enc) in enumerate(\n",
    "    out_dec_pytorch.past_key_values\n",
    "):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_cache[f\"past_key_values.{index}.decoder.key\"] = k_dec.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.decoder.value\"] = v_dec.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.key\"] = k_enc.detach().cpu().numpy()\n",
    "    input_cache[f\"past_key_values.{index}.encoder.value\"] = v_enc.detach().cpu().numpy()\n",
    "\n",
    "ort_cache.run([\"logits\"], input_cache)[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}