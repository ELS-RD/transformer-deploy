{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Accelerating GPT-2 model\n",
    "\n",
    "### *(and any decoder based transformer models)*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from tensorrt import ICudaEngine\n",
    "from tensorrt.tensorrt import Logger, Runtime\n",
    "from torch.nn import Module\n",
    "from transformers import AutoTokenizer, BatchEncoding, GPT2LMHeadModel, PretrainedConfig\n",
    "from transformers.generation_utils import GenerationMixin\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size\n",
    "from transformer_deploy.backends.trt_utils import build_engine, load_engine, save_engine"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "af4254e2-11fd-4bc7-ac0b-60b1a9e07c4e",
   "metadata": {},
   "source": [
    "## Load model\n",
    "\n",
    "As a reminder:\n",
    "\n",
    "* gpt2 (117M)\n",
    "* gpt2-large (774M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # choices: gpt2 | gpt2-large\n",
    "\n",
    "# use GPT2LMHeadModel to oupinstead of AutoModel\n",
    "model: GPT2LMHeadModel = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# to avoid error message or passing some args to each generate call\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "Output predictions for the next token. Those values will be used during the decoding part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5c5fe7-7733-49b5-89c5-c8278ff54fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   616,  3290,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "----\n",
      "tensor([[[ -35.2362,  -35.3266,  -38.9753,  ...,  -44.4645,  -43.9974,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5831, -116.5724,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [ -88.7435,  -89.8643,  -93.1977,  ...,  -92.3839,  -96.1782,\n",
      "           -92.1273],\n",
      "         [ -85.1646,  -88.3379,  -92.8703,  ...,  -99.8017,  -94.7657,\n",
      "           -90.9330],\n",
      "         [-116.7280, -119.3950, -121.7259,  ..., -129.1003, -124.6102,\n",
      "          -121.6092],\n",
      "         [ -61.9847,  -63.7082,  -65.6898,  ...,  -76.0924,  -71.7898,\n",
      "           -66.1154]]])\n"
     ]
    }
   ],
   "source": [
    "# carry out inference with a single sample\n",
    "inputs = tokenizer(\"Hello, my dog is \", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(\"----\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Shape: [batch size, nb tokens, vocabulary size]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 6, 50257])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "## Build ONNX graph\n",
    "\n",
    "Prior to converting the model to a TensorRT engine, we will first convert the PyTorch model to an intermediate universal format: ONNX.\n",
    "\n",
    "ONNX is an open format for machine learning and deep learning models. It allows you to convert deep learning and machine learning models from different frameworks such as TensorFlow, PyTorch, MATLAB, Caffe, and Keras to a single format.\n",
    "\n",
    "At a high level, the steps to convert a PyTorch model to TensorRT are as follows:\n",
    "- Convert the pretrained image segmentation PyTorch model into ONNX.\n",
    "- Import the ONNX model into TensorRT.\n",
    "- Apply optimizations and generate an engine.\n",
    "- Perform inference on the GPU with the TensorRT engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a80b6a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    }
   ],
   "source": [
    "input_ids: BatchEncoding = tokenizer(\n",
    "    \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\"\n",
    ")\n",
    "# some inference engines don't support int64 tensor as inputs, we convert all input tensors\n",
    "for k, v in input_ids.items():  # type: str, torch.Tensor\n",
    "    input_ids[k] = v.type(dtype=torch.int32)\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model,\n",
    "    output_path=\"test-gpt2.onnx\",\n",
    "    inputs_pytorch=dict(input_ids),\n",
    "    quantization=False,\n",
    "    var_output_seq=True,  # we inform ONNX export tool that the output shape will vary with the input shape\n",
    ")\n",
    "\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a148c2",
   "metadata": {},
   "source": [
    "### Optimize ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15f49fd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fusion_base:Fused LayerNormalization count: 25\n",
      "INFO:fusion_base:Fused FastGelu count: 12\n",
      "INFO:fusion_utils:Remove reshape node Reshape_9 since its input shape is same as output: ['batch_size', 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_19 since its input shape is same as output: [1, 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_2700 since its input shape is same as output: ['batch_size', 'sequence', 768]\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 23 nodes are removed\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 864 nodes are removed\n",
      "INFO:onnx_model_gpt2:postprocess: remove Reshape count:72\n",
      "INFO:fusion_base:Fused FastGelu(add bias) count: 12\n",
      "INFO:onnx_model_bert:opset verion: 13\n",
      "INFO:onnx_model_bert:Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:root:optimizations applied: {'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:onnx_model:Sort graphs in topological order\n",
      "INFO:onnx_model:Output model to test-gpt2-opt.onnx\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "num_attention_heads, hidden_size = get_model_size(path=model_name)\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-gpt2.onnx\",\n",
    "    onnx_optim_model_path=\"test-gpt2-opt.onnx\",\n",
    "    fp16=True,\n",
    "    use_cuda=True,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    architecture=\"gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578751c",
   "metadata": {},
   "source": [
    "## Build TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "747ebae8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/27/2022-20:53:13] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[01/27/2022-20:53:13] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2034, GPU 861 (MiB)\n",
      "[01/27/2022-20:53:13] [TRT] [I] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[01/27/2022-20:53:13] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2034, GPU 861 (MiB)\n",
      "[01/27/2022-20:53:14] [TRT] [I] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 2034 MiB, GPU 861 MiB\n",
      "[01/27/2022-20:53:14] [TRT] [I] [MemUsageSnapshot] End constructing builder kernel library: CPU 2188 MiB, GPU 905 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
      "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 664882988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:366: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.0.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.1.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:15] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:15] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:16] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:16] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:16] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:16] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:16] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:16] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.2.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:16] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:16] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:17] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:17] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:17] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:17] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:17] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:17] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.3.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:17] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:17] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:18] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:18] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:18] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:19] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:19] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:19] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.4.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:19] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:19] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:20] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:20] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:21] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:21] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:21] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:21] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.5.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:22] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:22] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:23] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:23] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:24] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:24] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:24] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:24] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.6.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:25] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:25] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:27] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:27] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:27] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:27] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:28] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:28] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.7.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:29] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:29] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:31] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:31] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:32] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:32] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:33] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:33] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.8.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:34] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:34] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:37] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:37] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:38] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:38] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:39] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:39] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.9.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:40] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:40] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:44] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:44] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:45] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:45] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:47] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:47] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.10.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:48] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:48] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.attn.c_attn.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:52] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:53] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.attn.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:54] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:54] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.mlp.c_fc.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:55] [TRT] [W] onnx2trt_utils.cpp:392: One or more weights outside the range of INT32 was clamped\n",
      "[01/27/2022-20:53:56] [TRT] [W] ShapedWeights.cpp:173: Weights transformer.h.11.mlp.c_proj.weight has been transposed with permutation of (1, 0)! If you plan on overwriting the weights with the Refitter API, the new weights must be pre-transposed.\n",
      "[01/27/2022-20:53:59] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +815, GPU +352, now: CPU 4021, GPU 1257 (MiB)\n",
      "[01/27/2022-20:53:59] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[01/27/2022-21:05:27] [TRT] [I] Detected 1 inputs and 1 output network tensors.\n",
      "[01/27/2022-21:05:38] [TRT] [I] Total Host Persistent Memory: 288\n",
      "[01/27/2022-21:05:38] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[01/27/2022-21:05:38] [TRT] [I] Total Scratch Memory: 52166400\n",
      "[01/27/2022-21:05:38] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 384 MiB, GPU 1004 MiB\n",
      "[01/27/2022-21:05:38] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 0.025841ms to assign 4 blocks to 4 nodes requiring 90766336 bytes.\n",
      "[01/27/2022-21:05:38] [TRT] [I] Total Activation Memory: 90766336\n",
      "[01/27/2022-21:05:38] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 5124, GPU 4235 (MiB)\n",
      "[01/27/2022-21:05:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in building engine: CPU +0, GPU +512, now: CPU 0, GPU 512 (MiB)\n",
      "[01/27/2022-21:05:38] [TRT] [I] Loaded engine size: 978 MiB\n",
      "[01/27/2022-21:05:38] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 6101, GPU 4199 (MiB)\n",
      "[01/27/2022-21:05:38] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +486, now: CPU 0, GPU 486 (MiB)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.INFO)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "trt_model_name = \"test-gpt2.plan\"\n",
    "\n",
    "# create only of does not exist because it's slow to run...\n",
    "if not Path(trt_model_name).exists():\n",
    "    engine: ICudaEngine = build_engine(\n",
    "        runtime=runtime,\n",
    "        onnx_file_path=\"test-gpt2.onnx\",\n",
    "        logger=trt_logger,\n",
    "        min_shape=(1, 1),\n",
    "        optimal_shape=(1, 128),  # num beam -> batch size\n",
    "        max_shape=(1, 384),  # num beam -> batch size\n",
    "        workspace_size=12000 * 1024 * 1024,\n",
    "        fp16=True,\n",
    "        int8=False,\n",
    "    )\n",
    "    save_engine(engine, trt_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe349f",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465f6165",
   "metadata": {},
   "source": [
    "## Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa67b468",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GPTModelWrapper(Module, GenerationMixin):\n",
    "    def __init__(\n",
    "        self, config: PretrainedConfig, device: torch.device, inference: Callable[[torch.Tensor], torch.Tensor]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.config: PretrainedConfig = config\n",
    "        self.device: torch.device = device\n",
    "        self.inference: Callable[[torch.Tensor], torch.Tensor] = inference\n",
    "        self.infer_time = list()\n",
    "        self.to(device=device)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "        }\n",
    "\n",
    "    def forward(self, input_ids, **_):\n",
    "        start = time.time()\n",
    "        logits = self.inference(input_ids)\n",
    "        self.infer_time.append(time.time() - start)\n",
    "        return CausalLMOutputWithCrossAttentions(logits=logits)\n",
    "\n",
    "    def timing(self) -> float:\n",
    "        return np.sum(self.infer_time)\n",
    "\n",
    "\n",
    "inputs = tokenizer(\n",
    "    \"Here is some text to encode Hello World\",\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac120c3",
   "metadata": {},
   "source": [
    "## Pytorch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f34f5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "torch: 0.36\n",
      "infer timing: 0.34\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "def inference_torch(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids)\n",
    "    return model.lm_head(transformer_outputs.last_hidden_state)\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "inputs.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    gpt2_model = GPTModelWrapper(config=model.config, device=model.device, inference=inference_torch)\n",
    "    sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "    for _ in range(2):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    gpt2_model.infer_time.clear()\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=True)\n",
    "    print(f\"torch: {(time.time() - start)/60:.2f}\")\n",
    "    print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "    print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "_ = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420d8d8",
   "metadata": {},
   "source": [
    "## Naive ONNX Runtime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bd67d6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "onnx: 0.66\n",
      "infer timing: 0.64\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_naive(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids.detach().cpu().numpy().astype(np.int32)}\n",
    "    logit = model_onnx.run(None, data)\n",
    "    np_logit = np.array(logit)\n",
    "    return torch.squeeze(torch.from_numpy(np_logit), dim=0)\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cpu\"), inference=inference_onnx_naive)\n",
    "inputs.to(\"cpu\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"onnx: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de843e68",
   "metadata": {},
   "source": [
    "## Optimized ONNX Runtime inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fd5c3ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "onnx binding: 0.15\n",
      "infer timing: 0.13\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_onnx_optimized)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"onnx binding: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de4c17",
   "metadata": {},
   "source": [
    "## TensorRT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4941aab6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/24/2022-14:13:52] [TRT] [I] Loaded engine size: 978 MiB\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 15811, GPU 10528 (MiB)\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +487, now: CPU 0, GPU 1560 (MiB)\n",
      "[01/24/2022-14:13:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 14832, GPU 10528 (MiB)\n",
      "[01/24/2022-14:13:53] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +87, now: CPU 0, GPU 1647 (MiB)\n",
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "tensorrt: 0.09\n",
      "infer timing: 0.08\n",
      "# inf: 2480\n"
     ]
    }
   ],
   "source": [
    "tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine(\n",
    "    engine_file_path=\"test-gpt2.plan\", runtime=runtime\n",
    ")\n",
    "\n",
    "\n",
    "def inference_tensorrt(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return tensorrt_model(data)[0]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_tensorrt)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "gpt2_model.infer_time.clear()\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"tensorrt: {(time.time() - start)/60:.2f}\")\n",
    "print(f\"infer timing: {gpt2_model.timing()/60:.2f}\")\n",
    "print(f\"# inf: {len(gpt2_model.infer_time)}\")\n",
    "\n",
    "del tensorrt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "110e38c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}