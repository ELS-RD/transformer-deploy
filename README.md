# Hugging Face Transformer submillisecond inferenceï¸ and deployment to production: ğŸ¤— â†’ ğŸ¤¯

[![Documentation](https://img.shields.io/website?label=documentation&style=for-the-badge&up_message=online&url=https%3A%2F%2Fels-rd.github.io%2Ftransformer-deploy%2F)](https://els-rd.github.io/transformer-deploy/) [![tests](https://img.shields.io/github/workflow/status/ELS-RD/transformer-deploy/tests/main?label=tests&style=for-the-badge)](https://github.com/ELS-RD/transformer-deploy/actions/workflows/python-app.yml) [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg?style=for-the-badge)](./LICENCE) [![Python 3.6](https://img.shields.io/badge/python-3.6-blue.svg?style=for-the-badge)](https://www.python.org/downloads/release/python-360/)


### Optimize and deploy in **production** Hugging Face Transformer models in a single command line.  

=> Up to 10X faster inference! <=

#### Why this tool?

<!--why-start-->

At [Lefebvre Dalloz](https://www.lefebvre-dalloz.fr/) we run in production several *semantic search engine* in the legal domain, 
in non-marketing language it's a reranker, and we based ours on `Transformer`.  
In those setup, latency is very important for the user experience we provide, as relevancy inference is done online for hundreds of snippets.  
We have tested many solutions, and below is what we found:

[`Pytorch`](https://pytorch.org/) + [`FastAPI`](https://fastapi.tiangolo.com/) = ğŸ¢  
Most tutorials on `Transformer` deployment in production are built over Pytorch and FastAPI.
Both are great tools but not very performant in inference.  

[`Microsoft ONNX Runtime`](https://github.com/microsoft/onnxruntime/) + [`Nvidia Triton inference server`](https://github.com/triton-inference-server/server) = ï¸ğŸƒğŸ’¨  
Then, if you spend some time, you can build something over ONNX Runtime and Triton inference server.
You will usually get from 2X to 4X faster inference compared to vanilla Pytorch. It's cool!  

[`Nvidia TensorRT`](https://github.com/NVIDIA/TensorRT/)  + [`Nvidia Triton inference server`](https://github.com/triton-inference-server/server) = âš¡ï¸ğŸƒğŸ’¨ğŸ’¨  
However, if you want the best in class performances on GPU, there is only a single possible combination: Nvidia TensorRT and Triton.
You will usually get 5X faster inference compared to vanilla Pytorch.  
Sometimes it can raises up to **10X faster inference**.  
Buuuuttt... TensorRT is not an easy to master, it requires specific tricks not easy to come with.  

<!--why-end-->

> Want to understand how it works under the hood?  
> read [ğŸ“• Hugging Face Transformer inference UNDER 1 millisecond latency ğŸ“–](https://towardsdatascience.com/hugging-face-transformer-inference-under-1-millisecond-latency-e1be0057a51c?source=friends_link&sk=cd880e05c501c7880f2b9454830b8915)  
> <img src="resources/rabbit.jpg" width="120">

# Check our [documentation](https://els-rd.github.io/transformer-deploy/) for detailed instructions on how to use the package, including setup, GPU quantization support and Nvidia Triton inference server deployment.
