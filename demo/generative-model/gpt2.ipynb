{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc2bef0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Accelerating GPT-2 model\n",
    "\n",
    "### *(and any decoder based transformer models)*\n",
    "\n",
    "In this notebook we will see how to accelerate generative models (decoder only) like GPT-2.\n",
    "The main thing we will learn is that in generative models executed on GPU, you need to take care of memory transfer.\n",
    "\n",
    "## GPT-2 loading\n",
    "\n",
    "As a reminder:\n",
    "\n",
    "* `gpt2`: 117M parameters\n",
    "* `gpt2-large` 774M parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bd1aac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Callable, Dict\n",
    "\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import torch\n",
    "from tensorrt import ICudaEngine\n",
    "from tensorrt.tensorrt import Logger, Runtime\n",
    "from transformers import AutoTokenizer, BatchEncoding, GPT2LMHeadModel, AutoModelForCausalLM\n",
    "from transformers.modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n",
    "from transformer_deploy.utils.generative_model import GPTModelWrapper\n",
    "import inspect\n",
    "from transformers import TensorType\n",
    "\n",
    "from transformer_deploy.backends.ort_utils import create_model_for_provider, inference_onnx_binding, optimize_onnx\n",
    "from transformer_deploy.backends.pytorch_utils import convert_to_onnx, get_model_size\n",
    "from transformer_deploy.backends.trt_utils import build_engine, load_engine, save_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fae66d58-f994-4987-8f1d-1fa8ac2ec8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt2\"  # choices: gpt2 | gpt2-large\n",
    "\n",
    "# use GPT2LMHeadModel and not AutoModel to export raw outputs to predict next token\n",
    "model: GPT2LMHeadModel = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# to avoid error message or passing some args to each generate call\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84c5766-97ed-4d04-bab5-7fa18e89dee8",
   "metadata": {},
   "source": [
    "### Model output\n",
    "\n",
    "Below we output predictions for the next token.\n",
    "Those values will be used by the decoding algorithm.\n",
    "Output shape looks like: [batch size, nb tokens, vocabulary size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5c5fe7-7733-49b5-89c5-c8278ff54fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   616,  3290,   318,   220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 6])\n",
      "----\n",
      "tensor([[[ -35.2362,  -35.3266,  -38.9753,  ...,  -44.4645,  -43.9974,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5831, -116.5724,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [ -88.7435,  -89.8643,  -93.1977,  ...,  -92.3839,  -96.1782,\n",
      "           -92.1273],\n",
      "         [ -85.1646,  -88.3379,  -92.8703,  ...,  -99.8017,  -94.7657,\n",
      "           -90.9330],\n",
      "         [-116.7280, -119.3950, -121.7259,  ..., -129.1003, -124.6102,\n",
      "          -121.6092],\n",
      "         [ -61.9847,  -63.7082,  -65.6898,  ...,  -76.0924,  -71.7898,\n",
      "           -66.1154]]])\n",
      "shape: torch.Size([1, 6, 50257])\n",
      "tensor size: 1.15 Mb\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is \", return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "print(inputs[\"input_ids\"].size())\n",
    "print(\"----\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "print(logits)\n",
    "print(f\"shape: {logits.shape}\")\n",
    "print(f\"tensor size: {np.prod(logits.shape)*32/8/1024**2:.2f} Mb\")  # same as sys.getsizeof(logits.storage())/1024**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d0977",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Total tensor size\n",
    "\n",
    "GPT-2 will generate a sequence 1 token at a time.\n",
    "So to generates 256 tokens from a 6 tokens prompt, it will perform 249 inference.\n",
    "\n",
    "To simplify things, we assume that we are using a greedy decoding algorithm.\n",
    "Let's compute the total size of the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e92bc363",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size (input+output): 6.11 Gb\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "for i in range(6, 256, 1):\n",
    "    # input sequence (input_ids) made of int-32 (4 bytes)\n",
    "    size += np.prod([1, i]) * 4\n",
    "    # output tensor made of float-32 (4 bytes)\n",
    "    size += np.prod([1, i, 50257]) * 4\n",
    "print(f\"total size (input+output): {size / 1024**3:.2f} Gb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120fabd0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "It's important to keep the order of magnitude in mind when we will try to optimize GPT-2 inference.\n",
    "In particular, we will try to limit tensor movement from GPU memory to host."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d662701-e430-4fdc-ad46-1f296defcf8f",
   "metadata": {},
   "source": [
    "## Build ONNX graph\n",
    "\n",
    "Performant inference engines tend to consume graph instead of imperative Pytorch code. We use ONNX for that purpose.\n",
    "\n",
    "> ONNX is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.\n",
    "> https://onnx.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a80b6a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:196: TracerWarning: Converting a tensor to a Python float might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  attn_weights = attn_weights / (float(value.size(-1)) ** 0.5)\n"
     ]
    }
   ],
   "source": [
    "input_ids: BatchEncoding = tokenizer(\n",
    "    \"Here is some text to encode Hello World\", add_special_tokens=True, return_attention_mask=False, return_tensors=\"pt\"\n",
    ")\n",
    "# some inference engines don't support int64 tensor as inputs, we convert all input tensors to int32 type\n",
    "for k, v in input_ids.items():  # type: str, torch.Tensor\n",
    "    input_ids[k] = v.type(dtype=torch.int32)\n",
    "\n",
    "convert_to_onnx(\n",
    "    model_pytorch=model,\n",
    "    output_path=\"test-gpt2.onnx\",\n",
    "    inputs_pytorch=dict(input_ids),\n",
    "    quantization=False,\n",
    "    var_output_seq=True,  # we inform ONNX export tool that the output shape will vary with the input shape\n",
    ")\n",
    "# model may switch to train mode for some unknown reasons, we force the eval mode.\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a148c2",
   "metadata": {},
   "source": [
    "### Optimize ONNX graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f49fd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fusion_base:Fused LayerNormalization count: 25\n",
      "INFO:fusion_base:Fused FastGelu count: 12\n",
      "INFO:fusion_utils:Remove reshape node Reshape_9 since its input shape is same as output: ['batch_size', 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_19 since its input shape is same as output: [1, 'sequence']\n",
      "INFO:fusion_utils:Remove reshape node Reshape_2700 since its input shape is same as output: ['batch_size', 'sequence', 768]\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 23 nodes are removed\n",
      "INFO:onnx_model:Graph pruned: 0 inputs, 0 outputs and 864 nodes are removed\n",
      "INFO:onnx_model_gpt2:postprocess: remove Reshape count:72\n",
      "INFO:fusion_base:Fused FastGelu(add bias) count: 12\n",
      "INFO:onnx_model_bert:opset verion: 13\n",
      "INFO:onnx_model_bert:Optimized operators:{'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:root:optimizations applied: {'EmbedLayerNormalization': 0, 'Attention': 0, 'Gelu': 0, 'FastGelu': 12, 'BiasGelu': 0, 'LayerNormalization': 25, 'SkipLayerNormalization': 0}\n",
      "INFO:onnx_model:Sort graphs in topological order\n",
      "INFO:onnx_model:Output model to test-gpt2-opt.onnx\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "num_attention_heads, hidden_size = get_model_size(path=model_name)\n",
    "optimize_onnx(\n",
    "    onnx_path=\"test-gpt2.onnx\",\n",
    "    onnx_optim_model_path=\"test-gpt2-opt.onnx\",\n",
    "    fp16=True,\n",
    "    use_cuda=True,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    hidden_size=hidden_size,\n",
    "    architecture=\"gpt2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4578751c",
   "metadata": {},
   "source": [
    "## Build TensorRT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "747ebae8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "trt_model_name = \"test-gpt2.plan\"\n",
    "\n",
    "# create only of does not exist because it's slow to run...\n",
    "if not Path(trt_model_name).exists():\n",
    "    engine: ICudaEngine = build_engine(\n",
    "        runtime=runtime,\n",
    "        onnx_file_path=\"test-gpt2.onnx\",\n",
    "        logger=trt_logger,\n",
    "        min_shape=(1, 1),\n",
    "        optimal_shape=(1, 128),  # num beam -> batch size\n",
    "        max_shape=(1, 384),  # num beam -> batch size\n",
    "        workspace_size=12000 * 1024 * 1024,\n",
    "        fp16=True,\n",
    "        int8=False,\n",
    "    )\n",
    "    save_engine(engine, trt_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe349f",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "We will benchmark 3 inference engines:\n",
    "- *Pytorch* (Hugging Face implementation)\n",
    "- *ONNX Runtime* with optimized ONNX graph and standard API (tensors stored as numpy objects, on host RAM)\n",
    "- *ONNX Runtime* with optimized ONNX graph and binding IO API (tensors stored on CUDA, to limit IO)\n",
    "- *Nvidia TensorRT* (tensors stored on CUDA, to limit IO)\n",
    "\n",
    "For each of them we print the output of the result to check that it generates the same string.\n",
    "Then we run 2 run for the warmup and 10 to measure the latency.\n",
    "\n",
    "### Generative model wrapper\n",
    "\n",
    "The most interesting thing in the class below is that we herit from `GenerationMixin` (from Hugging Face transformers library), it will give our class some super powers like a method to generate sequences (aka running some decoding algorithm on top of the model output).\n",
    "Note that the actual model inference is done by a function provided through the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa67b468",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GPTModelWrapper(Module, GenerationMixin):\n",
      "    def __init__(\n",
      "        self, config: PretrainedConfig, device: torch.device, inference: Callable[[torch.Tensor], torch.Tensor]\n",
      "    ):\n",
      "        super().__init__()\n",
      "        self.config: PretrainedConfig = config\n",
      "        self.device: torch.device = device\n",
      "        self.inference: Callable[[torch.Tensor], torch.Tensor] = inference\n",
      "        self.main_input_name = \"input_ids\"  # https://github.com/huggingface/transformers/pull/14803\n",
      "\n",
      "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
      "        return {\n",
      "            self.main_input_name: input_ids,\n",
      "        }\n",
      "\n",
      "    def forward(self, input_ids, **_):\n",
      "        logits = self.inference(input_ids)\n",
      "        return CausalLMOutputWithCrossAttentions(logits=logits)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(GPTModelWrapper))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5517f7c1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    \"Here is some text to encode Hello World\",  # Nvidia example prompt\n",
    "    add_special_tokens=True,\n",
    "    return_attention_mask=False,  # Not used\n",
    "    return_tensors=TensorType.PYTORCH,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac120c3",
   "metadata": {},
   "source": [
    "### Pytorch inference\n",
    "\n",
    "We use vanilla Hugging face implementation with Pytorch backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f34f5f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "----\n",
      "Pytorch: 2.17s/sequence\n"
     ]
    }
   ],
   "source": [
    "def inference_torch(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    transformer_outputs: BaseModelOutputWithPastAndCrossAttentions = model.transformer(input_ids=input_ids)\n",
    "    return model.lm_head(transformer_outputs.last_hidden_state)\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.eval()\n",
    "inputs.to(\"cuda\")\n",
    "with torch.inference_mode():\n",
    "    gpt2_model = GPTModelWrapper(config=model.config, device=model.device, inference=inference_torch)\n",
    "    sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "    for _ in range(2):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "    start = time.time()\n",
    "    for _ in range(10):\n",
    "        _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=True)\n",
    "    print(f\"----\\nPytorch: {(time.time() - start)/10:.2f}s/sequence\")\n",
    "_ = model.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1420d8d8",
   "metadata": {},
   "source": [
    "### Naive ONNX Runtime inference\n",
    "\n",
    "Below we use `ONNX Runtime` and its standard API (CUDA provider).\n",
    "It takes as input and output `numpy` tensors, meaning they are stored on host memory (RAM).\n",
    "Then we convert the `numpy` tensor to `Pytorch` one.\n",
    "It means that for each token the whole output tensor is moved from GPU memory to host.\n",
    "It reprensents more than 6Gb of memory transfer.\n",
    "\n",
    "This move is not really useful as the decoding algorithm is coded in Pytorch and would also work with GPU stored Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bd67d6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "----\n",
      "ONNX Runtime (standard API): 4.05s/sequence\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_naive(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids.detach().cpu().numpy().astype(np.int32)}\n",
    "    logit = model_onnx.run(None, data)\n",
    "    np_logit = np.array(logit)  # convert list of numpy arrays to a numpy array\n",
    "    # we convert numpy tensor to Pytorch tensor as it's the type expected by HF decoding algorithm\n",
    "    return torch.squeeze(torch.from_numpy(np_logit), dim=0)\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cpu\"), inference=inference_onnx_naive)\n",
    "inputs.to(\"cpu\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"----\\nONNX Runtime (standard API): {(time.time() - start)/10:.2f}s/sequence\")\n",
    "\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de843e68",
   "metadata": {},
   "source": [
    "### Optimized ONNX Runtime inference\n",
    "\n",
    "Here we use ONNX Runtime and its binding IO API.\n",
    "The main difference compared to the previous benchmark is that we keep everything on GPU.\n",
    "By just removing memory movement, we reduce the inference time by a large margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fd5c3ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "----\n",
      "ONNX Runtime (binding io API): 0.88/sequence\n"
     ]
    }
   ],
   "source": [
    "model_onnx = create_model_for_provider(path=\"test-gpt2-opt.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "\n",
    "\n",
    "def inference_onnx_optimized(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return inference_onnx_binding(model_onnx=model_onnx, inputs=data, device=\"cuda\")[\"output\"]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_onnx_optimized)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"----\\nONNX Runtime (binding io API): {(time.time() - start)/10:.2f}/sequence\")\n",
    "del model_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1de4c17",
   "metadata": {},
   "source": [
    "### TensorRT Inference\n",
    "\n",
    "To conclude we use the Nvidia engine, all tensors are stored on CUDA.\n",
    "Unlike ONNX Runtime, each optimization applied has been checked on the target GPU with the expected tensor shape.\n",
    "It explains most of its performance compared to ONNX Runtime optimized performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4941aab6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some text to encode Hello World.\n",
      "\n",
      "Hello World\n",
      "\n",
      "Hello World is a simple program that takes a string and returns a string.\n",
      "\n",
      "The program is written in C.\n",
      "\n",
      "The program is written in C. The program is written in C. The program is written in C. The program\n",
      "----\n",
      "TensorRT + CUDA tensors: 0.52/sequence\n"
     ]
    }
   ],
   "source": [
    "tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine(\n",
    "    engine_file_path=\"test-gpt2.plan\", runtime=runtime\n",
    ")\n",
    "\n",
    "\n",
    "def inference_tensorrt(input_ids: torch.Tensor) -> torch.Tensor:\n",
    "    data = {\"input_ids\": input_ids}\n",
    "    return tensorrt_model(data)[0]\n",
    "\n",
    "\n",
    "gpt2_model = GPTModelWrapper(config=model.config, device=torch.device(\"cuda\"), inference=inference_tensorrt)\n",
    "inputs.to(\"cuda\")\n",
    "sample_output = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))\n",
    "for _ in range(2):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=64)\n",
    "start = time.time()\n",
    "for _ in range(10):\n",
    "    _ = gpt2_model.generate(inputs.input_ids, max_length=256, use_cache=False)\n",
    "print(f\"----\\nTensorRT + CUDA tensors: {(time.time() - start)/10:.2f}/sequence\")\n",
    "\n",
    "del tensorrt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b907a1c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## IS caching of Key / Values (self attention) a good strategy on GPU?\n",
    "\n",
    "In the self-attention block, the first step is to compute key, query and value (known as K, Q and V) representation for each input token.\n",
    "This computation is done for each self-attention block (as each of them have their own memory).\n",
    "In a generative model, we need to recompute those values for each generated token.\n",
    "Because, for a specific input token, the result won't change from one inference to the next one, it may be interesting to cache and reuse the results instead of recomputing it.\n",
    "\n",
    "However, this won't come for free as we would need 2 ONNX / TensorRT models, one generating the values for the first time (to boot the sequence generation), and one able to reuse the cached values.\n",
    "\n",
    "But first, let's measure the gain in performance, if any.\n",
    "\n",
    "### Export a model able to reuse cache\n",
    "\n",
    "To simplify code, we just use the ONNX exporter tool from Hugging Face library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1967dce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:90: UserWarning: 'enable_onnx_checker' is deprecated and ignored. It will be removed in the next PyTorch release. To proceed despite ONNX checker failures, catch torch.onnx.ONNXCheckerError.\n",
      "  warnings.warn(\"'enable_onnx_checker' is deprecated and ignored. It will be removed in \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/torch/onnx/utils.py:103: UserWarning: `use_external_data_format' is deprecated and ignored. Will be removed in next PyTorch release. The code will work as it is False if models are not larger than 2GB, Otherwise set to False because of size limits imposed by Protocol Buffers.\n",
      "  warnings.warn(\"`use_external_data_format' is deprecated and ignored. Will be removed in next \"\n",
      "/home/geantvert/.local/share/virtualenvs/fast_transformer/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:797: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if batch_size <= 0:\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from torch.onnx import export\n",
    "from transformers.models.gpt2 import GPT2OnnxConfig\n",
    "from transformers.onnx.features import FeaturesManager\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "feature = \"causal-lm-with-past\"\n",
    "seq_len = 256\n",
    "atol = 0.2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model: GPT2LMHeadModel = FeaturesManager.get_model_from_feature(feature, model_name)\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config: GPT2OnnxConfig = model_onnx_config(model.config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.config.return_dict = True\n",
    "    model.eval()\n",
    "\n",
    "    # Check if we need to override certain configuration item\n",
    "    if onnx_config.values_override is not None:\n",
    "        for override_config_key, override_config_value in onnx_config.values_override.items():\n",
    "            setattr(model.config, override_config_key, override_config_value)\n",
    "\n",
    "    # Ensure inputs match\n",
    "    model_inputs = onnx_config.generate_dummy_inputs(tokenizer, framework=TensorType.PYTORCH)\n",
    "    for k, v in model_inputs.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            model_inputs[k] = model_inputs[k].type(torch.int32)\n",
    "    onnx_outputs = list(onnx_config.outputs.keys())\n",
    "\n",
    "    onnx_config.patch_ops()\n",
    "\n",
    "    # export can works with named args but the dict containing named args as to be last element of the args tuple\n",
    "    export(\n",
    "        model,\n",
    "        (model_inputs,),\n",
    "        f=\"model-support-cache.onnx\",\n",
    "        input_names=list(onnx_config.inputs.keys()),\n",
    "        output_names=onnx_outputs,\n",
    "        dynamic_axes={name: axes for name, axes in chain(onnx_config.inputs.items(), onnx_config.outputs.items())},\n",
    "        do_constant_folding=True,\n",
    "        use_external_data_format=onnx_config.use_external_data_format(model.num_parameters()),\n",
    "        enable_onnx_checker=True,\n",
    "        opset_version=13,\n",
    "    )\n",
    "\n",
    "    onnx_config.restore_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3999d61",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The ONNX graph with cache support expects as input:\n",
    "* 1 tensor name `input_ids` of shape [batch, sequence]\n",
    "* 12 tensors named `past_key_values.X.key` (X replaced by layer ID) of shape [batch, 12, \"past_sequence + sequence\", 64]\n",
    "* 12 tensors named `past_key_values.X.value` (X replaced by layer ID) of shape [batch, 12, \"past_sequence + sequence\", 64]\n",
    "* 1 tensor named `attention_mask` of shape [batch, \"past_sequence + sequence\"]\n",
    "\n",
    "To list model inputs (according to their ONNX graph):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ba08db1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"input_ids\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 6\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"sequence\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.0.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 12\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_sequence + sequence\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 64\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.0.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 12\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_sequence + sequence\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 64\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.1.key\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n",
      "        dim_param: \"batch\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 12\n",
      "      }\n",
      "      dim {\n",
      "        dim_param: \"past_sequence + sequence\"\n",
      "      }\n",
      "      dim {\n",
      "        dim_value: 64\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ", name: \"past_key_values.1.value\"\n",
      "type {\n",
      "  tensor_type {\n",
      "    elem_type: 1\n",
      "    shape {\n",
      "      dim {\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "model_cache = onnx.load(\"model-support-cache.onnx\")\n",
    "# first 100 lines\n",
    "text = \"\\n\".join(str(model_cache.graph.input).split(\"\\n\")[:80])\n",
    "print(text)\n",
    "del model_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daca662",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Benchmark ONNX Runtime, CUDA tensors, with cache\n",
    "\n",
    "When using cache, we only test non optimized ONNX model (no kernel fusion) as kernel fusion fails in this setup.\n",
    "It's probably because ONNX Runtime doesn't find some patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699b53ef",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 256 tokens WITH cache:\n",
      "[Pytorch / CPU] 23.15ms\n",
      "[ONNX Runtime / CPU - with copy (naive)] 27.90ms\n",
      "[ONNX Runtime / CPU - no copy] 24.58ms\n",
      "[Pytorch / CUDA] 10.56ms\n",
      "[ONNX Runtime / CUDA - with copy (naive)] 14.15ms\n",
      "[ONNX Runtime / CUDA - no copy] 4.29ms\n"
     ]
    }
   ],
   "source": [
    "batch = 1\n",
    "sequence = 256\n",
    "\n",
    "# input with random values\n",
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32)\n",
    "for i in range(12):  # 12 layers\n",
    "    input_cache[f\"past_key_values.{i}.key\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32)\n",
    "    input_cache[f\"past_key_values.{i}.value\"] = torch.empty((batch, 12, sequence, 64), dtype=torch.float32)\n",
    "input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32)\n",
    "\n",
    "print(f\"for {sequence} tokens WITH cache:\")\n",
    "for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]:\n",
    "    model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=provider)\n",
    "\n",
    "    # Pytorch\n",
    "    model = model.to(device=device, non_blocking=False)\n",
    "    output_pytorch = model(\n",
    "        input_ids=torch.ones((batch, sequence), dtype=torch.int32, device=device), past_key_values=None\n",
    "    )\n",
    "    pytorch_input = torch.ones((batch, 1), dtype=torch.int32, device=device)\n",
    "    for i in range(nb_inference):\n",
    "        _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values)\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for i in range(nb_inference):\n",
    "        _ = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values)\n",
    "        torch.cuda.synchronize()\n",
    "    print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\")\n",
    "\n",
    "    # naive implementation (tensor copies)\n",
    "    inputs_ort_np = {k: v.cpu().numpy() for k, v in input_cache.items()}\n",
    "    # warmup\n",
    "    for _ in range(nb_inference):\n",
    "        _ = model_onnx.run(None, inputs_ort_np)\n",
    "    start = time.time()\n",
    "    for _ in range(nb_inference):\n",
    "        _ = model_onnx.run(None, inputs_ort_np)\n",
    "    print(f\"[ONNX Runtime / {device.upper()} - with copy (naive)] {1e3*(time.time() - start)/nb_inference:.2f}ms\")\n",
    "\n",
    "    # ONNX Runtime optimized (no tensor copy)\n",
    "    inputs = {k: v.to(device) for k, v in input_cache.items()}\n",
    "    for _ in range(nb_inference):\n",
    "        inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device)\n",
    "    start = time.time()\n",
    "    for _ in range(nb_inference):\n",
    "        inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device)\n",
    "    print(f\"[ONNX Runtime / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38433c83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we note that ONNX Runtime with tensor copy is equivalent to Pytorch which keep all its tensors on GPU.\n",
    "When we remove the overhead of tensor copy, ONNX Runtime is 1/3 faster compared to Pytorch.\n",
    "On CPU no copy latency is similar to tensor copy implementation, probably because tensors are all on host RAM (no GPU memory transfer).\n",
    "On GPU, no copy latency is 3 times smaller than tensor copy implementation which shows that IO is crucial.\n",
    "\n",
    "We compare outputs of Pytorch and ONNX Runtime, with and without cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40f1c20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch - do not use cache output\n",
      "tensor([[-252.7095, -233.3065, -248.4932,  ..., -274.1753, -281.5232,\n",
      "         -249.9053]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "pytorch - use cache output\n",
      "tensor([[-252.6975, -233.2973, -248.4842,  ..., -274.1697, -281.5168,\n",
      "         -249.8984]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "ONNX Runtime - use cache output\n",
      "tensor([[[-252.6790, -233.2568, -248.4554,  ..., -274.1343, -281.4962,\n",
      "          -249.8820]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "a = model(input_ids=torch.ones((batch, sequence + 1), dtype=torch.int32, device=device), past_key_values=None)\n",
    "print(\"pytorch - do not use cache output\")\n",
    "print(a.logits[:, -1, :])\n",
    "b = model(input_ids=pytorch_input, past_key_values=output_pytorch.past_key_values)\n",
    "print(\"pytorch - use cache output\")\n",
    "print(b.logits[:, -1, :])\n",
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = torch.ones((batch, 1), dtype=torch.int32, device=device)\n",
    "input_cache[\"attention_mask\"] = torch.ones((batch, sequence + 1), dtype=torch.int32, device=device)\n",
    "for index, (k, v) in enumerate(output_pytorch.past_key_values):  # type: int, (torch.Tensor, torch.Tensor)\n",
    "    input_cache[f\"past_key_values.{index}.key\"] = k\n",
    "    input_cache[f\"past_key_values.{index}.value\"] = v\n",
    "model_onnx = create_model_for_provider(path=\"model-support-cache.onnx\", provider_to_use=\"CUDAExecutionProvider\")\n",
    "print(\"ONNX Runtime - use cache output\")\n",
    "print(inference_onnx_binding(model_onnx=model_onnx, inputs=input_cache, device=device)[\"logits\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef18434",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Benchmark ONNX Runtime, CUDA tensors, without cache\n",
    "\n",
    "We compare the following setup, on both CPU and GPU:\n",
    "\n",
    "- Pytorch\n",
    "- ONNX Runtime (FP32): `test-gpt2.onnx`\n",
    "- ONNX Runtime (FP16): `test-gpt2-opt.onnx`\n",
    "\n",
    "For ONNX Runtime, we test 2 models, one without kernel fusion and one with it and FP16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d59f2321",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 256 tokens WITHOUT cache:\n",
      "[Pytorch / CPU] 12.17ms\n",
      "[ONNX Runtime / CPU - with copy (naive) - test-gpt2.onnx] 203.05ms\n",
      "[ONNX Runtime / CPU - no copy - test-gpt2.onnx] 173.47ms\n",
      "[ONNX Runtime / CPU - with copy (naive) - test-gpt2-opt.onnx] 387.59ms\n",
      "[ONNX Runtime / CPU - no copy - test-gpt2-opt.onnx] 385.84ms\n",
      "[Pytorch / CUDA] 11.27ms\n",
      "[ONNX Runtime / CUDA - with copy (naive) - test-gpt2.onnx] 13.74ms\n",
      "[ONNX Runtime / CUDA - no copy - test-gpt2.onnx] 4.93ms\n",
      "[ONNX Runtime / CUDA - with copy (naive) - test-gpt2-opt.onnx] 13.67ms\n",
      "[ONNX Runtime / CUDA - no copy - test-gpt2-opt.onnx] 3.51ms\n"
     ]
    }
   ],
   "source": [
    "input_cache = dict()\n",
    "input_cache[\"input_ids\"] = torch.ones((batch, sequence), dtype=torch.int32, device=\"cuda\")\n",
    "\n",
    "print(f\"for {sequence} tokens WITHOUT cache:\")\n",
    "for nb_inference, provider, device in [(10, \"CPUExecutionProvider\", \"cpu\"), (100, \"CUDAExecutionProvider\", \"cuda\")]:\n",
    "    inputs = {k: v.to(device) for k, v in input_cache.items()}\n",
    "\n",
    "    for _ in range(nb_inference):\n",
    "        _ = model(**input_cache)\n",
    "        torch.cuda.synchronize()\n",
    "    start = time.time()\n",
    "    for _ in range(nb_inference):\n",
    "        _ = model(**input_cache)\n",
    "        torch.cuda.synchronize()\n",
    "    print(f\"[Pytorch / {device.upper()}] {1e3*(time.time() - start)/nb_inference:.2f}ms\")\n",
    "\n",
    "    for model_path in [\"test-gpt2.onnx\", \"test-gpt2-opt.onnx\"]:\n",
    "        model_onnx = create_model_for_provider(path=model_path, provider_to_use=provider)\n",
    "\n",
    "        # naive implementation\n",
    "        inputs_ort_numpy = {k: v.cpu().numpy() for k, v in input_cache.items()}\n",
    "        # warmup\n",
    "        for _ in range(nb_inference):\n",
    "            _ = model_onnx.run(None, inputs_ort_numpy)\n",
    "        start = time.time()\n",
    "        for _ in range(nb_inference):\n",
    "            _ = model_onnx.run(None, inputs_ort_numpy)\n",
    "        print(\n",
    "            f\"[ONNX Runtime / {device.upper()} - with copy (naive) - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\"\n",
    "        )\n",
    "\n",
    "        # no copy\n",
    "        for _ in range(nb_inference):\n",
    "            inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device)\n",
    "        start = time.time()\n",
    "        for _ in range(nb_inference):\n",
    "            inference_onnx_binding(model_onnx=model_onnx, inputs=inputs, device=device)\n",
    "        print(\n",
    "            f\"[ONNX Runtime / {device.upper()} - no copy - {model_path}] {1e3*(time.time() - start)/nb_inference:.2f}ms\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfcba54b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT / CUDA - no copy] 2.20ms\n"
     ]
    }
   ],
   "source": [
    "trt_logger: Logger = trt.Logger(trt.Logger.ERROR)\n",
    "runtime: Runtime = trt.Runtime(trt_logger)\n",
    "trt_model_name = \"test-gpt2.plan\"\n",
    "\n",
    "tensorrt_model: Callable[[Dict[str, torch.Tensor]], torch.Tensor] = load_engine(\n",
    "    engine_file_path=\"test-gpt2.plan\", runtime=runtime\n",
    ")\n",
    "nb_inference = 100\n",
    "for _ in range(nb_inference):\n",
    "    tensorrt_model(input_cache)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(nb_inference):\n",
    "    tensorrt_model(input_cache)\n",
    "print(f\"[TensorRT / {device.upper()} - no copy] {1e3*(time.time() - start)/nb_inference:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855560a",
   "metadata": {},
   "source": [
    "The most interesting thing to note is that on ONNX Runtime, without cache and kernel fusion we get similar latency than with cache.\n",
    "It means that cache overhead is bigger than recomputing the tensors! It's probably the most unexpected result of this experience.\n",
    "\n",
    "As usual, FP16 models on CPU are slower than FP32.\n",
    "\n",
    "And to conclude, surprise, TensorRT is the fastest option by a large margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dd5c93d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}