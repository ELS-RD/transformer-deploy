
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Efficient, scalable and enterprise-grade CPU/GPU inference server for Hugging Face transformer models">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.15">
    
    
      
        <title>Ort utils - transformer-deploy by Lefebvre Dalloz</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.c382b1dc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.cc9b2e1e.min.css">
        
          
          
          <meta name="theme-color" content="#000000">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../assets/_mkdocstrings.css">
    
    <script>__md_scope=new URL("../../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="deep-orange">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#src.transformer_deploy.backends.ort_utils" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
        <aside class="md-banner">
          <div class="md-banner__inner md-grid md-typeset">
            
<a href="https://twitter.com/pommedeterre33" style="color: orangered">
For updates follow <strong>@pommedeterre33</strong> on <span class="twemoji twitter"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg> </span> <strong>Twitter</strong>
</a>

          </div>
        </aside>
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="transformer-deploy by Lefebvre Dalloz" class="md-header__button md-logo" aria-label="transformer-deploy by Lefebvre Dalloz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 16a3 3 0 0 1-3-3c0-1.12.61-2.1 1.5-2.61l9.71-5.62-5.53 9.58c-.5.98-1.51 1.65-2.68 1.65m0-13c1.81 0 3.5.5 4.97 1.32l-2.1 1.21C14 5.19 13 5 12 5a8 8 0 0 0-8 8c0 2.21.89 4.21 2.34 5.65h.01c.39.39.39 1.02 0 1.41-.39.39-1.03.39-1.42.01A9.969 9.969 0 0 1 2 13 10 10 0 0 1 12 3m10 10c0 2.76-1.12 5.26-2.93 7.07-.39.38-1.02.38-1.41-.01a.996.996 0 0 1 0-1.41A7.95 7.95 0 0 0 20 13c0-1-.19-2-.54-2.9L20.67 8C21.5 9.5 22 11.18 22 13Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            transformer-deploy by Lefebvre Dalloz
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Ort utils
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ELS-RD/transformer-deploy/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ELS-RD/transformer-deploy/
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="transformer-deploy by Lefebvre Dalloz" class="md-nav__button md-logo" aria-label="transformer-deploy by Lefebvre Dalloz" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 16a3 3 0 0 1-3-3c0-1.12.61-2.1 1.5-2.61l9.71-5.62-5.53 9.58c-.5.98-1.51 1.65-2.68 1.65m0-13c1.81 0 3.5.5 4.97 1.32l-2.1 1.21C14 5.19 13 5 12 5a8 8 0 0 0-8 8c0 2.21.89 4.21 2.34 5.65h.01c.39.39.39 1.02 0 1.41-.39.39-1.03.39-1.42.01A9.969 9.969 0 0 1 2 13 10 10 0 0 1 12 3m10 10c0 2.76-1.12 5.26-2.93 7.07-.39.38-1.02.38-1.41-.01a.996.996 0 0 1 0-1.41A7.95 7.95 0 0 0 20 13c0-1-.19-2-.54-2.9L20.67 8C21.5 9.5 22 11.18 22 13Z"/></svg>

    </a>
    transformer-deploy by Lefebvre Dalloz
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ELS-RD/transformer-deploy/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
  </div>
  <div class="md-source__repository">
    ELS-RD/transformer-deploy/
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../setup_local/" class="md-nav__link">
        Installation (local or Docker only)
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../run/" class="md-nav__link">
        Run (1 command)
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../compare/" class="md-nav__link">
        Which tool to choose for your inference?
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../onnx_convert/" class="md-nav__link">
        How ONNX conversion works?
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../optimizations/" class="md-nav__link">
        Understanding model optimization
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../python/" class="md-nav__link">
        Direct use TensorRT in Python script (no server)
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_8" type="checkbox" id="__nav_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_8">
          GPU quantization for X2 speed-up
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="GPU quantization for X2 speed-up" data-md-level="1">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          GPU quantization for X2 speed-up
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../quantization/quantization_intro/" class="md-nav__link">
        Why using quantization?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../quantization/quantization_theory/" class="md-nav__link">
        Quantization theory
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../quantization/quantization_ast/" class="md-nav__link">
        How is it implemented in this library?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../quantization/quantization_ptq/" class="md-nav__link">
        PTQ and QAT, what are they?
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../quantization/quantization/" class="md-nav__link">
        End to end demo
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../demo/" class="md-nav__link">
        From optimization to deployment: end to end demo
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../gpt2/" class="md-nav__link">
        Accelerate text generation with GPT-2
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../t5/" class="md-nav__link">
        Accelerate text generation with T5
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../benchmarks/" class="md-nav__link">
        Benchmarks run on AWS GPU instances
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../faq/" class="md-nav__link">
        FAQ
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14" type="checkbox" id="__nav_14" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_14">
          API
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="API" data-md-level="1">
        <label class="md-nav__title" for="__nav_14">
          <span class="md-nav__icon md-icon"></span>
          API
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../convert/" class="md-nav__link">
        Convert
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14_2" type="checkbox" id="__nav_14_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_14_2">
          QDQModels
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="QDQModels" data-md-level="2">
        <label class="md-nav__title" for="__nav_14_2">
          <span class="md-nav__icon md-icon"></span>
          QDQModels
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQAlbert/" class="md-nav__link">
        QDQAlbert
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQBert/" class="md-nav__link">
        QDQBert
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQDeberta/" class="md-nav__link">
        QDQDeberta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQDistilbert/" class="md-nav__link">
        QDQDistilbert
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQElectra/" class="md-nav__link">
        QDQElectra
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/QDQRoberta/" class="md-nav__link">
        QDQRoberta
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/ast_operator_patch/" class="md-nav__link">
        Ast operator patch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/ast_utils/" class="md-nav__link">
        Ast utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/calibration_utils/" class="md-nav__link">
        Calibration utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../QDQModels/patch/" class="md-nav__link">
        Patch
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14_3" type="checkbox" id="__nav_14_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_14_3">
          Backends
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Backends" data-md-level="2">
        <label class="md-nav__title" for="__nav_14_3">
          <span class="md-nav__icon md-icon"></span>
          Backends
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Ort utils
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Ort utils
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils" class="md-nav__link">
    src.transformer_deploy.backends.ort_utils
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.add_output_nodes" class="md-nav__link">
    add_output_nodes()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.convert_fp16" class="md-nav__link">
    convert_fp16()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.cpu_quantization" class="md-nav__link">
    cpu_quantization()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.create_model_for_provider" class="md-nav__link">
    create_model_for_provider()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.find_node_fp32" class="md-nav__link">
    find_node_fp32()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.get_io_to_node_mapping" class="md-nav__link">
    get_io_to_node_mapping()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.get_keep_fp32_nodes" class="md-nav__link">
    get_keep_fp32_nodes()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.inference_onnx_binding" class="md-nav__link">
    inference_onnx_binding()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.optimize_onnx" class="md-nav__link">
    optimize_onnx()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.to_pytorch" class="md-nav__link">
    to_pytorch()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.use_external_data" class="md-nav__link">
    use_external_data()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../pytorch_utils/" class="md-nav__link">
        Pytorch utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../st_utils/" class="md-nav__link">
        St utils
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../trt_utils/" class="md-nav__link">
        Trt utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14_4" type="checkbox" id="__nav_14_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_14_4">
          Benchmarks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Benchmarks" data-md-level="2">
        <label class="md-nav__title" for="__nav_14_4">
          <span class="md-nav__icon md-icon"></span>
          Benchmarks
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../benchmarks/utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14_5" type="checkbox" id="__nav_14_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_14_5">
          Triton
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Triton" data-md-level="2">
        <label class="md-nav__title" for="__nav_14_5">
          <span class="md-nav__icon md-icon"></span>
          Triton
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../triton/configuration/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../triton/configuration_decoder/" class="md-nav__link">
        Configuration decoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../triton/configuration_encoder/" class="md-nav__link">
        Configuration encoder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../triton/configuration_token_classifier/" class="md-nav__link">
        Configuration token classifier
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_14_6" type="checkbox" id="__nav_14_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_14_6">
          Utils
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Utils" data-md-level="2">
        <label class="md-nav__title" for="__nav_14_6">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/args/" class="md-nav__link">
        Args
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/generative_model/" class="md-nav__link">
        Generative model
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/token_classifier/" class="md-nav__link">
        Token classifier
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils" class="md-nav__link">
    src.transformer_deploy.backends.ort_utils
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.add_output_nodes" class="md-nav__link">
    add_output_nodes()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.convert_fp16" class="md-nav__link">
    convert_fp16()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.cpu_quantization" class="md-nav__link">
    cpu_quantization()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.create_model_for_provider" class="md-nav__link">
    create_model_for_provider()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.find_node_fp32" class="md-nav__link">
    find_node_fp32()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.get_io_to_node_mapping" class="md-nav__link">
    get_io_to_node_mapping()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.get_keep_fp32_nodes" class="md-nav__link">
    get_keep_fp32_nodes()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.inference_onnx_binding" class="md-nav__link">
    inference_onnx_binding()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.optimize_onnx" class="md-nav__link">
    optimize_onnx()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.to_pytorch" class="md-nav__link">
    to_pytorch()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#src.transformer_deploy.backends.ort_utils.use_external_data" class="md-nav__link">
    use_external_data()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                


  <h1>Ort utils</h1>

<div class="doc doc-object doc-module">

<a id="src.transformer_deploy.backends.ort_utils"></a>
    <div class="doc doc-contents first">

      <p>All the tooling to ease ONNX Runtime usage.</p>



  <div class="doc doc-children">











  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.add_output_nodes" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_output_nodes</span><span class="p">(</span><span class="n">model</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.add_output_nodes" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Set each node as output node for debugging purpose.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td><code>ModelProto</code></td>
        <td><p>ONNX model in protobuf format</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ModelProto</code></td>
      <td><p>modified ONNX model</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">add_output_nodes</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">ModelProto</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">ModelProto</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Set each node as output node for debugging purpose.</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    :param model: ONNX model in protobuf format</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :return: modified ONNX model</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">model</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">output_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span class="k">for</span> <span class="n">output_name</span> <span class="ow">in</span> <span class="n">n</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>            <span class="n">output_nodes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">onnx</span><span class="o">.</span><span class="n">ValueInfoProto</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">output_name</span><span class="p">))</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    <span class="c1"># clear output array (protobuff way...)</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="k">while</span> <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">pop</span><span class="p">()</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">output_nodes</span><span class="p">)</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="k">return</span> <span class="n">model</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.convert_fp16" class="doc doc-heading">
<code class="highlight language-python"><span class="n">convert_fp16</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">nodes_to_exclude</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.convert_fp16" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Convert ONNX model in FP16, and still being able to exclude a list of nodes.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>onnx_model</code></td>
        <td><code>str</code></td>
        <td><p>original FP32 model</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>nodes_to_exclude</code></td>
        <td><code>List[str]</code></td>
        <td><p>nodes that should stay in FP32</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ModelProto</code></td>
      <td><p>mostly FP16 model</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">convert_fp16</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">nodes_to_exclude</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">ModelProto</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Convert ONNX model in FP16, and still being able to exclude a list of nodes.</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    :param onnx_model: original FP32 model</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :param nodes_to_exclude: nodes that should stay in FP32</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    :return: mostly FP16 model</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="c1"># add value info related to each node, required for the conversion</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">output_path</span> <span class="o">=</span> <span class="n">onnx_model</span> <span class="o">+</span> <span class="s2">&quot;_shape_inference.onnx&quot;</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">infer_shapes_path</span><span class="p">(</span><span class="n">model_path</span><span class="o">=</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">output_path</span><span class="o">=</span><span class="n">output_path</span><span class="p">)</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">model_fp16</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">output_path</span><span class="p">)</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    <span class="n">model_fp16</span> <span class="o">=</span> <span class="n">convert_float_to_float16</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model_fp16</span><span class="p">,</span> <span class="n">keep_io_types</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">node_block_list</span><span class="o">=</span><span class="n">nodes_to_exclude</span><span class="p">)</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    <span class="c1"># clean casting nodes before returning the model</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    <span class="n">wrapped_fp16_model</span> <span class="o">=</span> <span class="n">OnnxModel</span><span class="p">(</span><span class="n">model_fp16</span><span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    <span class="n">fusion_utils</span> <span class="o">=</span> <span class="n">FusionUtils</span><span class="p">(</span><span class="n">wrapped_fp16_model</span><span class="p">)</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="n">fusion_utils</span><span class="o">.</span><span class="n">remove_cascaded_cast_nodes</span><span class="p">()</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span class="n">fusion_utils</span><span class="o">.</span><span class="n">remove_useless_cast_nodes</span><span class="p">()</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    <span class="n">wrapped_fp16_model</span><span class="o">.</span><span class="n">topological_sort</span><span class="p">()</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    <span class="k">return</span> <span class="n">wrapped_fp16_model</span><span class="o">.</span><span class="n">model</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.cpu_quantization" class="doc doc-heading">
<code class="highlight language-python"><span class="n">cpu_quantization</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">output_model_path</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.cpu_quantization" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>ONNX CPU only dynamic quantization.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_model_path</code></td>
        <td><code>str</code></td>
        <td><p>ONNX graph (float) to quantize</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_model_path</code></td>
        <td><code>str</code></td>
        <td><p>where to save quantized model</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">cpu_quantization</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">output_model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    ONNX CPU only dynamic quantization.</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :param input_model_path: ONNX graph (float) to quantize</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    :param output_model_path: where to save quantized model</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">quantize_dynamic</span><span class="p">(</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>        <span class="n">model_input</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">),</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span class="n">model_output</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="n">output_model_path</span><span class="p">),</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span class="n">op_types_to_quantize</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;MatMul&quot;</span><span class="p">,</span> <span class="s2">&quot;Attention&quot;</span><span class="p">],</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span class="n">per_channel</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;WeightSymmetric&quot;</span><span class="p">:</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;MatMulConstBOnly&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">},</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.create_model_for_provider" class="doc doc-heading">
<code class="highlight language-python"><span class="n">create_model_for_provider</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">provider_to_use</span><span class="p">,</span> <span class="n">nb_threads</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">nb_instances</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">optimization_level</span><span class="o">=&lt;</span><span class="n">GraphOptimizationLevel</span><span class="o">.</span><span class="n">ORT_ENABLE_EXTENDED</span><span class="p">:</span> <span class="mi">2</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">enable_profiling</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">log_severity</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.create_model_for_provider" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Create an ONNX Runtime instance.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>path to ONNX file or serialized to string model</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>provider_to_use</code></td>
        <td><code>Union[str, List]</code></td>
        <td><p>provider to use for inference</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>nb_threads</code></td>
        <td><code>int</code></td>
        <td><p>intra_op_num_threads to use. You may want to try different parameters, more core does not always provide best performances.</p></td>
        <td><code>12</code></td>
      </tr>
      <tr>
        <td><code>nb_instances</code></td>
        <td><code>int</code></td>
        <td><p>inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>optimization_level</code></td>
        <td><code>GraphOptimizationLevel</code></td>
        <td><p>expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one providing kernel fusion of element wise operations. Enable all level is for CPU inference. see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations</p></td>
        <td><code>&lt;GraphOptimizationLevel.ORT_ENABLE_EXTENDED: 2&gt;</code></td>
      </tr>
      <tr>
        <td><code>enable_profiling</code></td>
        <td><code>bool</code></td>
        <td><p>let Onnx Runtime log each kernel time.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>log_severity</code></td>
        <td><code>int</code></td>
        <td><p>Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal.</p></td>
        <td><code>2</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>InferenceSession</code></td>
      <td><p>ONNX Runtime inference session</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">create_model_for_provider</span><span class="p">(</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">provider_to_use</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">],</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">nb_threads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">multiprocessing</span><span class="o">.</span><span class="n">cpu_count</span><span class="p">(),</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">nb_instances</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">optimization_level</span><span class="p">:</span> <span class="n">GraphOptimizationLevel</span> <span class="o">=</span> <span class="n">GraphOptimizationLevel</span><span class="o">.</span><span class="n">ORT_ENABLE_EXTENDED</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">enable_profiling</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">log_severity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">InferenceSession</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="sd">    Create an ONNX Runtime instance.</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="sd">    :param path: path to ONNX file or serialized to string model</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="sd">    :param provider_to_use: provider to use for inference</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="sd">    :param nb_threads: intra_op_num_threads to use. You may want to try different parameters,</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="sd">        more core does not always provide best performances.</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="sd">    :param nb_instances: inter_op_num_threads to use, to execute multiple subgraphs in parallel when possible.</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="sd">    :param optimization_level: expected level of ONNX Runtime optimization. For GPU and NLP, extended is the one</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="sd">        providing kernel fusion of element wise operations. Enable all level is for CPU inference.</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="sd">        see https://onnxruntime.ai/docs/performance/graph-optimizations.html#layout-optimizations</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="sd">    :param enable_profiling: let Onnx Runtime log each kernel time.</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="sd">    :param log_severity: Log severity level. 0:Verbose, 1:Info, 2:Warning. 3:Error, 4:Fatal.</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a><span class="sd">    :return: ONNX Runtime inference session</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    <span class="n">options</span> <span class="o">=</span> <span class="n">SessionOptions</span><span class="p">()</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    <span class="n">options</span><span class="o">.</span><span class="n">graph_optimization_level</span> <span class="o">=</span> <span class="n">optimization_level</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span class="n">options</span><span class="o">.</span><span class="n">enable_profiling</span> <span class="o">=</span> <span class="n">enable_profiling</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    <span class="n">options</span><span class="o">.</span><span class="n">log_severity_level</span> <span class="o">=</span> <span class="n">log_severity</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">provider_to_use</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        <span class="n">provider_to_use</span> <span class="o">=</span> <span class="p">[</span><span class="n">provider_to_use</span><span class="p">]</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>    <span class="k">if</span> <span class="n">provider_to_use</span> <span class="o">==</span> <span class="p">[</span><span class="s2">&quot;CPUExecutionProvider&quot;</span><span class="p">]:</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="n">options</span><span class="o">.</span><span class="n">execution_mode</span> <span class="o">=</span> <span class="n">ExecutionMode</span><span class="o">.</span><span class="n">ORT_SEQUENTIAL</span> <span class="k">if</span> <span class="n">nb_instances</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">ExecutionMode</span><span class="o">.</span><span class="n">ORT_PARALLEL</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        <span class="n">options</span><span class="o">.</span><span class="n">intra_op_num_threads</span> <span class="o">=</span> <span class="n">nb_threads</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>        <span class="k">if</span> <span class="n">nb_instances</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>            <span class="n">options</span><span class="o">.</span><span class="n">inter_op_num_threads</span> <span class="o">=</span> <span class="n">nb_instances</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>    <span class="k">return</span> <span class="n">InferenceSession</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">options</span><span class="p">,</span> <span class="n">providers</span><span class="o">=</span><span class="n">provider_to_use</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.find_node_fp32" class="doc doc-heading">
<code class="highlight language-python"><span class="n">find_node_fp32</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">output_nodes</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.find_node_fp32" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Identify out of range values in node outputs.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>graph</code></td>
        <td><code>Dict[str, str]</code></td>
        <td><p>graph as adjency nodes dict</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_nodes</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>output of each node</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>list of nodes producing outputs outside fp16 tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">find_node_fp32</span><span class="p">(</span><span class="n">graph</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">output_nodes</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Identify out of range values in node outputs.</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    :param graph: graph as adjency nodes dict</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :param output_nodes: output of each node</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    :return: list of nodes producing outputs outside fp16 tensor</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">keep_fp32</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="n">min_float16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">min</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="n">max_float16</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="n">resolution</span> <span class="o">=</span> <span class="mf">5.96e-08</span>  <span class="c1"># torch.finfo(torch.float16).eps  # minimum value that can be represented by FP16</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="n">output_nodes</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>            <span class="k">continue</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        <span class="c1"># out of FP16 range</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>        <span class="k">if</span> <span class="p">(</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>            <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tensor</span> <span class="o">&gt;</span> <span class="n">max_float16</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>            <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tensor</span> <span class="o">&lt;</span> <span class="n">min_float16</span><span class="p">)</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>            <span class="ow">or</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">tensor</span> <span class="o">&lt;</span> <span class="n">resolution</span> <span class="o">&amp;</span> <span class="n">tensor</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">resolution</span> <span class="o">&amp;</span> <span class="n">tensor</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">))</span>  <span class="c1"># limited memory footprint</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="p">):</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>            <span class="n">keep_fp32</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">graph</span><span class="p">[</span><span class="n">k</span><span class="p">])</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    <span class="k">return</span> <span class="n">keep_fp32</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.get_io_to_node_mapping" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_io_to_node_mapping</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.get_io_to_node_mapping" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Extract output-&gt;node and input-&gt;node mappings</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>onnx_model</code></td>
        <td><code>ModelProto</code></td>
        <td><p>ONNX model</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, str], Dict[str, str]]</code></td>
      <td><p>2 mappings, (i-&gt;node, o-&gt;node)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">get_io_to_node_mapping</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">:</span> <span class="n">ModelProto</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Extract output-&gt;node and input-&gt;node mappings</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    :param onnx_model: ONNX model</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :return: 2 mappings, (i-&gt;node, o-&gt;node)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">output_mapping</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">input_mapping</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">:</span>  <span class="c1"># type: NodeProto</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">node</span><span class="o">.</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span class="n">output_node</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        <span class="n">output_mapping</span><span class="p">[</span><span class="n">output_node</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">input</span><span class="p">:</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>            <span class="n">input_mapping</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">node</span><span class="o">.</span><span class="n">name</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="k">return</span> <span class="n">input_mapping</span><span class="p">,</span> <span class="n">output_mapping</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.get_keep_fp32_nodes" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_keep_fp32_nodes</span><span class="p">(</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">get_input</span><span class="p">,</span> <span class="n">early_stop</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.get_keep_fp32_nodes" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Find the list of nodes to keep in FP32 to avoid out of range values</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>onnx_model_path</code></td>
        <td><code>str</code></td>
        <td><p>ONNX model path</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>get_input</code></td>
        <td><code>Callable[[], Dict[str, torch.Tensor]]</code></td>
        <td><p>generate input to test the model. Output should change from call to call</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>early_stop</code></td>
        <td><code>int</code></td>
        <td><p>will test until <code>early_stop</code> tests are done without any new node to keep in FP32</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>where to run the inference</p></td>
        <td><code>&#39;cuda&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>list of names of nodes to keep in FP32</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">get_keep_fp32_nodes</span><span class="p">(</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">onnx_model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">get_input</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">early_stop</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cuda&quot;</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="sd">    Find the list of nodes to keep in FP32 to avoid out of range values</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="sd">    :param onnx_model_path: ONNX model path</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="sd">    :param get_input: generate input to test the model. Output should change from call to call</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="sd">    :param early_stop: will test until `early_stop` tests are done without any new node to keep in FP32</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="sd">    :param device: where to run the inference</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="sd">    :return: list of names of nodes to keep in FP32</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    <span class="c1"># do not load weights on LLM (&gt;2Gb), we only need to modify the computation graph</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    <span class="n">onnx_model</span><span class="p">:</span> <span class="n">ModelProto</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">onnx_model_path</span><span class="p">,</span> <span class="n">load_external_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span class="n">onnx_model_fp32_all_nodes</span> <span class="o">=</span> <span class="n">add_output_nodes</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">onnx_model</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    <span class="n">path_onnx_model_fp32_all_nodes</span> <span class="o">=</span> <span class="n">onnx_model_path</span> <span class="o">+</span> <span class="s2">&quot;_all_nodes.onnx&quot;</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    <span class="n">onnx</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">proto</span><span class="o">=</span><span class="n">onnx_model_fp32_all_nodes</span><span class="p">,</span> <span class="n">f</span><span class="o">=</span><span class="n">path_onnx_model_fp32_all_nodes</span><span class="p">,</span> <span class="n">save_as_external_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    <span class="n">provider</span> <span class="o">=</span> <span class="s2">&quot;CUDAExecutionProvider&quot;</span> <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span> <span class="k">else</span> <span class="s2">&quot;CPUExecutionProvider&quot;</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    <span class="n">ort_model_fp32_all_nodes</span> <span class="o">=</span> <span class="n">create_model_for_provider</span><span class="p">(</span><span class="n">path_onnx_model_fp32_all_nodes</span><span class="p">,</span> <span class="n">provider</span><span class="p">)</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    <span class="n">ort_binding</span> <span class="o">=</span> <span class="n">ort_model_fp32_all_nodes</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    <span class="n">input_mapping</span><span class="p">,</span> <span class="n">output_mapping</span> <span class="o">=</span> <span class="n">get_io_to_node_mapping</span><span class="p">(</span><span class="n">onnx_model</span><span class="o">=</span><span class="n">onnx_model</span><span class="p">)</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    <span class="c1"># list all nodes which have an output out of the FP16 range</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    <span class="n">keep_fp32_nodes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span class="n">no_new_node_counter</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    <span class="k">while</span> <span class="n">no_new_node_counter</span> <span class="o">&lt;</span> <span class="n">early_stop</span><span class="p">:</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>        <span class="n">inputs</span> <span class="o">=</span> <span class="n">get_input</span><span class="p">()</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        <span class="n">outputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="n">inference_onnx_binding</span><span class="p">(</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>            <span class="n">model_onnx</span><span class="o">=</span><span class="n">ort_model_fp32_all_nodes</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">binding</span><span class="o">=</span><span class="n">ort_binding</span><span class="p">,</span> <span class="n">clone_tensor</span><span class="o">=</span><span class="kc">False</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="p">)</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        <span class="n">keep_node_io</span> <span class="o">=</span> <span class="n">find_node_fp32</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">output_mapping</span><span class="p">,</span> <span class="n">output_nodes</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>        <span class="n">nodes_to_add</span> <span class="o">=</span> <span class="p">[</span><span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">keep_node_io</span> <span class="k">if</span> <span class="n">n</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">keep_fp32_nodes</span><span class="p">]</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>        <span class="n">keep_fp32_nodes</span> <span class="o">+=</span> <span class="n">nodes_to_add</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">nodes_to_add</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>            <span class="n">no_new_node_counter</span> <span class="o">+=</span> <span class="mi">1</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>        <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>            <span class="n">no_new_node_counter</span> <span class="o">=</span> <span class="mi">0</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>    <span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>        <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>    <span class="c1"># I/O names that can&#39;t be found in the graph</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a>    <span class="n">nodes_to_skip</span> <span class="o">=</span> <span class="p">(</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>        <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">input</span><span class="p">]</span>
<a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>        <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">output</span><span class="p">]</span>
<a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>        <span class="o">+</span> <span class="p">[</span><span class="n">n</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">initializer</span><span class="p">]</span>
<a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>    <span class="p">)</span>
<a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a>
<a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a>    <span class="c1"># for each node to keep in FP32, we keep its children in FP32 too as they will receive FP32 values as input</span>
<a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>    <span class="n">map_children</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
<a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a>    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">onnx_model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">node</span><span class="p">:</span>
<a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a>        <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">node</span><span class="o">.</span><span class="n">output</span><span class="p">:</span>
<a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>            <span class="k">if</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">nodes_to_skip</span><span class="p">:</span>
<a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a>                <span class="k">continue</span>
<a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a>            <span class="n">child</span> <span class="o">=</span> <span class="n">input_mapping</span><span class="p">[</span><span class="n">o</span><span class="p">]</span>
<a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a>            <span class="n">map_children</span><span class="p">[</span><span class="n">node</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
<a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a>    <span class="n">keep_fp32_nodes</span> <span class="o">+=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keep_fp32_nodes</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">map_children</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">map_children</span><span class="p">[</span><span class="n">k</span><span class="p">]]</span>
<a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>    <span class="k">return</span> <span class="n">keep_fp32_nodes</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.inference_onnx_binding" class="doc doc-heading">
<code class="highlight language-python"><span class="n">inference_onnx_binding</span><span class="p">(</span><span class="n">model_onnx</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">device_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">binding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">clone_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.inference_onnx_binding" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Performs inference on ONNX Runtime in an optimized way.
In particular, it avoids any Onnx Runtime output tensor copy.
It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another
inference. To avoid any issue, just set clone_tensor to True (default).
For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model_onnx</code></td>
        <td><code>InferenceSession</code></td>
        <td><p>ONNX model</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>inputs</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>input torch tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>where to run the inference. One of [cpu, cuda]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>device_id</code></td>
        <td><code>int</code></td>
        <td><p>ID of the device where to run the inference, to be used when there are multiple GPUs, etc.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>binding</code></td>
        <td><code>Optional[onnxruntime.capi.onnxruntime_inference_collection.IOBinding]</code></td>
        <td><p>previously generated binding IO, will be reset.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>clone_tensor</code></td>
        <td><code>bool</code></td>
        <td><p>clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime at the next inference call.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, torch.Tensor]</code></td>
      <td><p>a dict {axis name: output tensor}</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">inference_onnx_binding</span><span class="p">(</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">model_onnx</span><span class="p">:</span> <span class="n">InferenceSession</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">inputs</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">device</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">device_id</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">binding</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">IOBinding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">clone_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="sd">    Performs inference on ONNX Runtime in an optimized way.</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="sd">    In particular, it avoids any Onnx Runtime output tensor copy.</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="sd">    It means that Onnx Runtime is still owner of the array, and it will overwrite its content if you do another</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="sd">    inference. To avoid any issue, just set clone_tensor to True (default).</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="sd">    For best performance and lowest memory footprint, if you know what you are doing, set clone_tensor to True.</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="sd">    :param model_onnx: ONNX model</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="sd">    :param inputs: input torch tensor</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="sd">    :param device: where to run the inference. One of [cpu, cuda]</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="sd">    :param device_id: ID of the device where to run the inference, to be used when there are multiple GPUs, etc.</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="sd">    :param binding: previously generated binding IO, will be reset.</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="sd">    :param clone_tensor: clone Pytorch tensor to avoid its content being overwritten by Onnx Runtime</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a><span class="sd">        at the next inference call.</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="sd">    :return: a dict {axis name: output tensor}</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span class="k">assert</span> <span class="n">device</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda&quot;</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;unexpected inference device: &#39;</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&#39;&quot;</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    <span class="k">if</span> <span class="n">binding</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>        <span class="n">binding</span><span class="p">:</span> <span class="n">IOBinding</span> <span class="o">=</span> <span class="n">model_onnx</span><span class="o">.</span><span class="n">io_binding</span><span class="p">()</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>        <span class="n">binding</span><span class="o">.</span><span class="n">clear_binding_inputs</span><span class="p">()</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="n">binding</span><span class="o">.</span><span class="n">clear_binding_outputs</span><span class="p">()</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>    <span class="k">for</span> <span class="n">input_onnx</span> <span class="ow">in</span> <span class="n">model_onnx</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">():</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>        <span class="k">if</span> <span class="n">input_onnx</span><span class="o">.</span><span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>  <span class="c1"># some inputs may be optional</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>            <span class="k">continue</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>        <span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">input_onnx</span><span class="o">.</span><span class="n">name</span><span class="p">]</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        <span class="k">if</span> <span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">in</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">]:</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>            <span class="c1"># int32 mandatory as input of bindings, int64 not supported</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>            <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>        <span class="n">tensor</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a>        <span class="n">binding</span><span class="o">.</span><span class="n">bind_input</span><span class="p">(</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a>            <span class="n">name</span><span class="o">=</span><span class="n">input_onnx</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a>            <span class="n">device_type</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a>            <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>            <span class="n">element_type</span><span class="o">=</span><span class="n">torch_to_numpy_dtype_dict</span><span class="p">[</span><span class="n">tensor</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span>
<a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>            <span class="n">shape</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="n">tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
<a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>            <span class="n">buffer_ptr</span><span class="o">=</span><span class="n">tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span>
<a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a>        <span class="p">)</span>
<a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a>        <span class="n">inputs</span><span class="p">[</span><span class="n">input_onnx</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span>
<a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a>
<a id="__codelineno-0-51" name="__codelineno-0-51" href="#__codelineno-0-51"></a>    <span class="k">for</span> <span class="n">out</span> <span class="ow">in</span> <span class="n">model_onnx</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">():</span>
<a id="__codelineno-0-52" name="__codelineno-0-52" href="#__codelineno-0-52"></a>        <span class="n">binding</span><span class="o">.</span><span class="n">bind_output</span><span class="p">(</span>
<a id="__codelineno-0-53" name="__codelineno-0-53" href="#__codelineno-0-53"></a>            <span class="n">name</span><span class="o">=</span><span class="n">out</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
<a id="__codelineno-0-54" name="__codelineno-0-54" href="#__codelineno-0-54"></a>            <span class="n">device_type</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
<a id="__codelineno-0-55" name="__codelineno-0-55" href="#__codelineno-0-55"></a>            <span class="n">device_id</span><span class="o">=</span><span class="n">device_id</span><span class="p">,</span>
<a id="__codelineno-0-56" name="__codelineno-0-56" href="#__codelineno-0-56"></a>        <span class="p">)</span>
<a id="__codelineno-0-57" name="__codelineno-0-57" href="#__codelineno-0-57"></a>    <span class="n">binding</span><span class="o">.</span><span class="n">synchronize_inputs</span><span class="p">()</span>
<a id="__codelineno-0-58" name="__codelineno-0-58" href="#__codelineno-0-58"></a>    <span class="n">model_onnx</span><span class="o">.</span><span class="n">run_with_iobinding</span><span class="p">(</span><span class="n">binding</span><span class="p">)</span>
<a id="__codelineno-0-59" name="__codelineno-0-59" href="#__codelineno-0-59"></a>    <span class="n">binding</span><span class="o">.</span><span class="n">synchronize_outputs</span><span class="p">()</span>
<a id="__codelineno-0-60" name="__codelineno-0-60" href="#__codelineno-0-60"></a>    <span class="n">outputs</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
<a id="__codelineno-0-61" name="__codelineno-0-61" href="#__codelineno-0-61"></a>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">model_onnx</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">())</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span>
<a id="__codelineno-0-62" name="__codelineno-0-62" href="#__codelineno-0-62"></a>        <span class="n">binding</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()</span>
<a id="__codelineno-0-63" name="__codelineno-0-63" href="#__codelineno-0-63"></a>    <span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">model_onnx</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">())</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">binding</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-64" name="__codelineno-0-64" href="#__codelineno-0-64"></a>    <span class="k">for</span> <span class="n">out</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">model_onnx</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">(),</span> <span class="n">binding</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()):</span>
<a id="__codelineno-0-65" name="__codelineno-0-65" href="#__codelineno-0-65"></a>        <span class="n">outputs</span><span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">to_pytorch</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">clone_tensor</span><span class="o">=</span><span class="n">clone_tensor</span><span class="p">)</span>
<a id="__codelineno-0-66" name="__codelineno-0-66" href="#__codelineno-0-66"></a>    <span class="k">return</span> <span class="n">outputs</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.optimize_onnx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">optimize_onnx</span><span class="p">(</span><span class="n">onnx_path</span><span class="p">,</span> <span class="n">onnx_optim_model_path</span><span class="p">,</span> <span class="n">fp16</span><span class="p">,</span> <span class="n">use_cuda</span><span class="p">,</span> <span class="n">num_attention_heads</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">architecture</span><span class="o">=</span><span class="s1">&#39;bert&#39;</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.optimize_onnx" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>ONNX Runtime transformer graph optimization.
Performs some operator fusion (merge several nodes of the graph in a single one)
and may convert some nodes to reduced precision.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>onnx_path</code></td>
        <td><code>str</code></td>
        <td><p>ONNX input path</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>onnx_optim_model_path</code></td>
        <td><code>str</code></td>
        <td><p>where to save optimized model</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fp16</code></td>
        <td><code>bool</code></td>
        <td><p>use mixed precision (faster inference)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>use_cuda</code></td>
        <td><code>bool</code></td>
        <td><p>perform optimization on GPU (should )</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_attention_heads</code></td>
        <td><code>int</code></td>
        <td><p>number of attention heads of a model (0 -&gt; try to detect)</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>hidden layer size of a model (0 -&gt; try to detect)</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>architecture</code></td>
        <td><code>str</code></td>
        <td><p>model architecture to optimize. One of [bert, bart, gpt2]</p></td>
        <td><code>&#39;bert&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">optimize_onnx</span><span class="p">(</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="n">onnx_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    <span class="n">onnx_optim_model_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    <span class="n">fp16</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    <span class="n">use_cuda</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="n">architecture</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bert&quot;</span><span class="p">,</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="sd">    ONNX Runtime transformer graph optimization.</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="sd">    Performs some operator fusion (merge several nodes of the graph in a single one)</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="sd">    and may convert some nodes to reduced precision.</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="sd">    :param onnx_path: ONNX input path</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="sd">    :param onnx_optim_model_path: where to save optimized model</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="sd">    :param fp16: use mixed precision (faster inference)</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="sd">    :param use_cuda: perform optimization on GPU (should )</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="sd">    :param num_attention_heads: number of attention heads of a model (0 -&gt; try to detect)</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a><span class="sd">    :param hidden_size: hidden layer size of a model (0 -&gt; try to detect)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="sd">    :param architecture: model architecture to optimize. One of [bert, bart, gpt2]</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    <span class="k">assert</span> <span class="n">architecture</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;bert&quot;</span><span class="p">,</span> <span class="s2">&quot;bart&quot;</span><span class="p">,</span> <span class="s2">&quot;gpt2&quot;</span><span class="p">],</span> <span class="sa">f</span><span class="s2">&quot;unsupported architecture: </span><span class="si">{</span><span class="n">architecture</span><span class="si">}</span><span class="s2">&quot;</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    <span class="n">opt_level</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">architecture</span> <span class="o">==</span> <span class="s2">&quot;bert&quot;</span> <span class="k">else</span> <span class="mi">0</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    <span class="n">optimization_options</span> <span class="o">=</span> <span class="n">FusionOptions</span><span class="p">(</span><span class="n">model_type</span><span class="o">=</span><span class="n">architecture</span><span class="p">)</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    <span class="n">optimization_options</span><span class="o">.</span><span class="n">enable_gelu_approximation</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># additional optimization</span>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span class="n">optimized_model</span><span class="p">:</span> <span class="n">BertOnnxModel</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize_model</span><span class="p">(</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>        <span class="nb">input</span><span class="o">=</span><span class="n">onnx_path</span><span class="p">,</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>        <span class="n">model_type</span><span class="o">=</span><span class="n">architecture</span><span class="p">,</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>        <span class="n">use_gpu</span><span class="o">=</span><span class="n">use_cuda</span><span class="p">,</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>        <span class="n">opt_level</span><span class="o">=</span><span class="n">opt_level</span><span class="p">,</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_attention_heads</span><span class="p">,</span>  <span class="c1"># automatic detection with 0 may not work with opset 13 or distilbert models</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>  <span class="c1"># automatic detection with 0</span>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a>        <span class="n">optimization_options</span><span class="o">=</span><span class="n">optimization_options</span><span class="p">,</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a>    <span class="p">)</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a>    <span class="k">if</span> <span class="n">fp16</span><span class="p">:</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a>        <span class="c1"># use_symbolic_shape_infer set to false because doesn&#39;t work after ONNX package v1.10.2</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a>        <span class="n">optimized_model</span><span class="o">.</span><span class="n">convert_float_to_float16</span><span class="p">(</span><span class="n">use_symbolic_shape_infer</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># FP32 -&gt; FP16</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;optimizations applied: </span><span class="si">{</span><span class="n">optimized_model</span><span class="o">.</span><span class="n">get_fused_operator_statistics</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>    <span class="n">optimized_model</span><span class="o">.</span><span class="n">save_model_to_file</span><span class="p">(</span><span class="n">onnx_optim_model_path</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.to_pytorch" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_pytorch</span><span class="p">(</span><span class="n">ort_tensor</span><span class="p">,</span> <span class="n">clone_tensor</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.to_pytorch" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Convert OrtValue output by Onnx Runtime to Pytorch tensor.
The process can be done in a zero copy way (depending of clone parameter).</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ort_tensor</code></td>
        <td><code>OrtValue</code></td>
        <td><p>output from Onnx Runtime</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>clone_tensor</code></td>
        <td><code>bool</code></td>
        <td><p>Onnx Runtime owns the storage array and will write on the next inference. By cloning you guarantee that the data won't change.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>Pytorch tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">to_pytorch</span><span class="p">(</span><span class="n">ort_tensor</span><span class="p">:</span> <span class="n">OrtValue</span><span class="p">,</span> <span class="n">clone_tensor</span><span class="p">:</span> <span class="nb">bool</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Convert OrtValue output by Onnx Runtime to Pytorch tensor.</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    The process can be done in a zero copy way (depending of clone parameter).</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :param ort_tensor: output from Onnx Runtime</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    :param clone_tensor: Onnx Runtime owns the storage array and will write on the next inference.</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="sd">        By cloning you guarantee that the data won&#39;t change.</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="sd">    :return: Pytorch tensor</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="k">if</span> <span class="n">ort_tensor</span><span class="o">.</span><span class="n">device_name</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>        <span class="n">np_type</span> <span class="o">=</span> <span class="n">ort_to_numpy_dtype_dict</span><span class="p">[</span><span class="n">ort_tensor</span><span class="o">.</span><span class="n">data_type</span><span class="p">()]</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        <span class="n">fake_owner</span> <span class="o">=</span> <span class="mi">1</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        <span class="c1"># size not used anywhere, so just put 0</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        <span class="n">memory</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">UnownedMemory</span><span class="p">(</span><span class="n">ort_tensor</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">fake_owner</span><span class="p">)</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>        <span class="n">memory_ptr</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">MemoryPointer</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>        <span class="c1"># make sure you interpret the array shape/dtype/strides correctly</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>        <span class="n">cp_array</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">ndarray</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">ort_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">(),</span> <span class="n">memptr</span><span class="o">=</span><span class="n">memory_ptr</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np_type</span><span class="p">)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>        <span class="c1"># cloning required otherwise ORT will recycle the storage array and put new values into it if new inf is done.</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>        <span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">cp_array</span><span class="o">.</span><span class="n">toDlpack</span><span class="p">())</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        <span class="k">if</span> <span class="n">clone_tensor</span><span class="p">:</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>            <span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch_tensor</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>        <span class="k">return</span> <span class="n">torch_tensor</span>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    <span class="k">else</span><span class="p">:</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>        <span class="n">np_tensor</span> <span class="o">=</span> <span class="n">ort_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">np_tensor</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="src.transformer_deploy.backends.ort_utils.use_external_data" class="doc doc-heading">
<code class="highlight language-python"><span class="n">use_external_data</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></code>


<a href="#src.transformer_deploy.backends.ort_utils.use_external_data" class="headerlink" title="Permanent link">#</a></h2>

    <div class="doc doc-contents ">

      <p>Check if a model uses external data</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>Onnx model path</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bool</code></td>
      <td><p>True if any initalizer (model weight) is stored in an external file</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>src/transformer_deploy/backends/ort_utils.py</code></summary>
          <div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="k">def</span> <span class="nf">use_external_data</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    <span class="sd">&quot;&quot;&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="sd">    Check if a model uses external data</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="sd">    :param path: Onnx model path</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="sd">    :return: True if any initalizer (model weight) is stored in an external file</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="sd">    &quot;&quot;&quot;</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    <span class="n">model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">f</span><span class="o">=</span><span class="n">path</span><span class="p">,</span> <span class="n">load_external_data</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">initializer</span><span class="p">:</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>        <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">HasField</span><span class="p">(</span><span class="s2">&quot;data_location&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">i</span><span class="o">.</span><span class="n">data_location</span> <span class="o">==</span> <span class="n">onnx</span><span class="o">.</span><span class="n">TensorProto</span><span class="o">.</span><span class="n">EXTERNAL</span><span class="p">:</span>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>            <span class="k">return</span> <span class="kc">True</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="k">return</span> <span class="kc">False</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>

              
            </article>
          </div>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../../QDQModels/patch/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Patch" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Patch
            </div>
          </div>
        </a>
      
      
        
        <a href="../pytorch_utils/" class="md-footer__link md-footer__link--next" aria-label="Next: Pytorch utils" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Pytorch utils
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2020 - 2021 Lefebvre Dalloz
    </div>
  
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://twitter.com/pommedeterre33" target="_blank" rel="noopener" title="twitter.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://medium.com/@pommedeterre33" target="_blank" rel="noopener" title="medium.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M180.5 74.262C80.813 74.262 0 155.633 0 256s80.819 181.738 180.5 181.738S361 356.373 361 256 280.191 74.262 180.5 74.262Zm288.25 10.646c-49.845 0-90.245 76.619-90.245 171.095s40.406 171.1 90.251 171.1 90.251-76.619 90.251-171.1H559c0-94.503-40.4-171.095-90.248-171.095Zm139.506 17.821c-17.526 0-31.735 68.628-31.735 153.274s14.2 153.274 31.735 153.274S640 340.631 640 256c0-84.649-14.215-153.271-31.742-153.271Z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.a6c66575.min.js"></script>
      
    
  </body>
</html>